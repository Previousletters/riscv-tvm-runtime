; ModuleID = 'TVMMod'
source_filename = "TVMMod"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-unknown-linux-gnu"

%0 = type { i32*, i32 }
%1 = type { i8*, %2, i32, %3, i64*, i64*, i64 }
%2 = type { i32, i32 }
%3 = type { i8, i8, i16 }
%4 = type { i8*, i8* }
%5 = type { i8*, i8*, i8*, i8* }
%6 = type { i8*, i8* }
%7 = type { i8*, i8*, i8*, i8* }
%8 = type { i8*, i8* }
%9 = type { i8*, i8*, i8*, i8* }
%10 = type { i8*, i8* }
%11 = type { i8*, i8*, i8*, i8* }
%12 = type { i8*, i8* }
%13 = type { i8*, i8*, i8*, i8* }
%14 = type { i8*, i8* }
%15 = type { i8*, i8*, i8*, i8* }
%16 = type { i8*, i8* }
%17 = type { i8*, i8*, i8*, i8* }
%18 = type { i8*, i8* }
%19 = type { i8*, i8*, i8*, i8* }
%20 = type { i8*, i8* }
%21 = type { i8*, i8*, i8*, i8* }
%22 = type { i8*, i8* }
%23 = type { i8*, i8*, i8*, i8* }

@__TVMAPISetLastError = linkonce dllexport local_unnamed_addr global void (i8*)* null, align 8
@__TVMBackendParallelLaunch = linkonce dllexport local_unnamed_addr global i32 (i32 (i32, %0*, i8*)*, i8*, i32)* null, align 8
@.str = private constant [88 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_16: num_args should be 4\00", align 1
@.str.1 = private constant [163 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_16: Expect arg[0] to be pointer\00", align 1
@.str.2 = private constant [163 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_16: Expect arg[1] to be pointer\00", align 1
@.str.3 = private constant [163 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_16: Expect arg[2] to be pointer\00", align 1
@.str.4 = private constant [163 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_16: Expect arg[3] to be pointer\00", align 1
@.str.5 = private constant [85 x i8] c"Assert fail: (4 == tir.tvm_struct_get(arg0, 0, 4)), arg0.ndim is expected to equal 4\00", align 1
@.str.6 = private constant [198 x i8] c"Assert fail: (((tir.tvm_struct_get(arg0, 0, 5) == (uint8)2) && (tir.tvm_struct_get(arg0, 0, 6) == (uint8)32)) && (tir.tvm_struct_get(arg0, 0, 7) == (uint16)1)), arg0.dtype is expected to be float32\00", align 1
@.str.7 = private constant [124 x i8] c"Assert fail: (1 == int32(arg0.shape[0])), Argument arg0.shape[0] has an unsatisfied constraint: (1 == int32(arg0.shape[0]))\00", align 1
@.str.8 = private constant [128 x i8] c"Assert fail: (112 == int32(arg0.shape[1])), Argument arg0.shape[1] has an unsatisfied constraint: (112 == int32(arg0.shape[1]))\00", align 1
@.str.9 = private constant [128 x i8] c"Assert fail: (112 == int32(arg0.shape[2])), Argument arg0.shape[2] has an unsatisfied constraint: (112 == int32(arg0.shape[2]))\00", align 1
@.str.10 = private constant [126 x i8] c"Assert fail: (32 == int32(arg0.shape[3])), Argument arg0.shape[3] has an unsatisfied constraint: (32 == int32(arg0.shape[3]))\00", align 1
@.str.11 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg0.strides[3])) && (32 == int32(arg0.strides[2]))) && (3584 == int32(arg0.strides[1]))) && (401408 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.12 = private constant [163 x i8] c"Assert fail: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8)), Argument arg0.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg0, 0, 8))\00", align 1
@.str.13 = private constant [149 x i8] c"Assert fail: (1 == tir.tvm_struct_get(arg0, 0, 10)), Argument arg0.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg0, 0, 10))\00", align 1
@.str.14 = private constant [85 x i8] c"Assert fail: (4 == tir.tvm_struct_get(arg1, 0, 4)), arg1.ndim is expected to equal 4\00", align 1
@.str.15 = private constant [198 x i8] c"Assert fail: (((tir.tvm_struct_get(arg1, 0, 5) == (uint8)2) && (tir.tvm_struct_get(arg1, 0, 6) == (uint8)32)) && (tir.tvm_struct_get(arg1, 0, 7) == (uint16)1)), arg1.dtype is expected to be float32\00", align 1
@.str.16 = private constant [124 x i8] c"Assert fail: (1 == int32(arg1.shape[0])), Argument arg1.shape[0] has an unsatisfied constraint: (1 == int32(arg1.shape[0]))\00", align 1
@.str.17 = private constant [124 x i8] c"Assert fail: (1 == int32(arg1.shape[1])), Argument arg1.shape[1] has an unsatisfied constraint: (1 == int32(arg1.shape[1]))\00", align 1
@.str.18 = private constant [126 x i8] c"Assert fail: (32 == int32(arg1.shape[2])), Argument arg1.shape[2] has an unsatisfied constraint: (32 == int32(arg1.shape[2]))\00", align 1
@.str.19 = private constant [126 x i8] c"Assert fail: (64 == int32(arg1.shape[3])), Argument arg1.shape[3] has an unsatisfied constraint: (64 == int32(arg1.shape[3]))\00", align 1
@.str.20 = private constant [199 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (64 == int32(arg1.strides[2]))) && (2048 == int32(arg1.strides[1]))) && (2048 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.21 = private constant [163 x i8] c"Assert fail: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8)), Argument arg1.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg1, 0, 8))\00", align 1
@.str.22 = private constant [149 x i8] c"Assert fail: (1 == tir.tvm_struct_get(arg1, 0, 10)), Argument arg1.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg1, 0, 10))\00", align 1
@.str.23 = private constant [155 x i8] c"Assert fail: (dev_id == tir.tvm_struct_get(arg1, 0, 9)), Argument arg1.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg1, 0, 9))\00", align 1
@.str.24 = private constant [85 x i8] c"Assert fail: (1 == tir.tvm_struct_get(arg2, 0, 4)), arg2.ndim is expected to equal 1\00", align 1
@.str.25 = private constant [198 x i8] c"Assert fail: (((tir.tvm_struct_get(arg2, 0, 5) == (uint8)2) && (tir.tvm_struct_get(arg2, 0, 6) == (uint8)32)) && (tir.tvm_struct_get(arg2, 0, 7) == (uint16)1)), arg2.dtype is expected to be float32\00", align 1
@.str.26 = private constant [126 x i8] c"Assert fail: (64 == int32(arg2.shape[0])), Argument arg2.shape[0] has an unsatisfied constraint: (64 == int32(arg2.shape[0]))\00", align 1
@.str.27 = private constant [87 x i8] c"Assert fail: (1 == int32(arg2.strides[0])), arg2.strides: expected to be compact array\00", align 1
@.str.28 = private constant [163 x i8] c"Assert fail: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8)), Argument arg2.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg2, 0, 8))\00", align 1
@.str.29 = private constant [149 x i8] c"Assert fail: (1 == tir.tvm_struct_get(arg2, 0, 10)), Argument arg2.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg2, 0, 10))\00", align 1
@.str.30 = private constant [155 x i8] c"Assert fail: (dev_id == tir.tvm_struct_get(arg2, 0, 9)), Argument arg2.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg2, 0, 9))\00", align 1
@.str.31 = private constant [85 x i8] c"Assert fail: (4 == tir.tvm_struct_get(arg3, 0, 4)), arg3.ndim is expected to equal 4\00", align 1
@.str.32 = private constant [198 x i8] c"Assert fail: (((tir.tvm_struct_get(arg3, 0, 5) == (uint8)2) && (tir.tvm_struct_get(arg3, 0, 6) == (uint8)32)) && (tir.tvm_struct_get(arg3, 0, 7) == (uint16)1)), arg3.dtype is expected to be float32\00", align 1
@.str.33 = private constant [124 x i8] c"Assert fail: (1 == int32(arg3.shape[0])), Argument arg3.shape[0] has an unsatisfied constraint: (1 == int32(arg3.shape[0]))\00", align 1
@.str.34 = private constant [128 x i8] c"Assert fail: (112 == int32(arg3.shape[1])), Argument arg3.shape[1] has an unsatisfied constraint: (112 == int32(arg3.shape[1]))\00", align 1
@.str.35 = private constant [128 x i8] c"Assert fail: (112 == int32(arg3.shape[2])), Argument arg3.shape[2] has an unsatisfied constraint: (112 == int32(arg3.shape[2]))\00", align 1
@.str.36 = private constant [126 x i8] c"Assert fail: (64 == int32(arg3.shape[3])), Argument arg3.shape[3] has an unsatisfied constraint: (64 == int32(arg3.shape[3]))\00", align 1
@.str.37 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg3.strides[3])) && (64 == int32(arg3.strides[2]))) && (7168 == int32(arg3.strides[1]))) && (802816 == int32(arg3.strides[0]))), arg3.strides: expected to be compact array\00", align 1
@.str.38 = private constant [163 x i8] c"Assert fail: ((uint64)0 == tir.tvm_struct_get(arg3, 0, 8)), Argument arg3.byte_offset has an unsatisfied constraint: ((uint64)0 == tir.tvm_struct_get(arg3, 0, 8))\00", align 1
@.str.39 = private constant [149 x i8] c"Assert fail: (1 == tir.tvm_struct_get(arg3, 0, 10)), Argument arg3.device_type has an unsatisfied constraint: (1 == tir.tvm_struct_get(arg3, 0, 10))\00", align 1
@.str.40 = private constant [155 x i8] c"Assert fail: (dev_id == tir.tvm_struct_get(arg3, 0, 9)), Argument arg3.device_id has an unsatisfied constraint: (dev_id == tir.tvm_struct_get(arg3, 0, 9))\00", align 1
@__TVMBackendAllocWorkspace = linkonce dllexport local_unnamed_addr global i8* (i32, i32, i64, i32, i32)* null, align 8
@__TVMBackendFreeWorkspace = linkonce dllexport local_unnamed_addr global i32 (i32, i32, i8*)* null, align 8
@.str.42 = private constant [87 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_5: num_args should be 4\00", align 1
@.str.43 = private constant [162 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_5: Expect arg[0] to be pointer\00", align 1
@.str.44 = private constant [162 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_5: Expect arg[1] to be pointer\00", align 1
@.str.45 = private constant [162 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_5: Expect arg[2] to be pointer\00", align 1
@.str.46 = private constant [162 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_5: Expect arg[3] to be pointer\00", align 1
@.str.47 = private constant [126 x i8] c"Assert fail: (14 == int32(arg0.shape[1])), Argument arg0.shape[1] has an unsatisfied constraint: (14 == int32(arg0.shape[1]))\00", align 1
@.str.48 = private constant [126 x i8] c"Assert fail: (14 == int32(arg0.shape[2])), Argument arg0.shape[2] has an unsatisfied constraint: (14 == int32(arg0.shape[2]))\00", align 1
@.str.49 = private constant [128 x i8] c"Assert fail: (512 == int32(arg0.shape[3])), Argument arg0.shape[3] has an unsatisfied constraint: (512 == int32(arg0.shape[3]))\00", align 1
@.str.50 = private constant [202 x i8] c"Assert fail: ((((1 == int32(arg0.strides[3])) && (512 == int32(arg0.strides[2]))) && (7168 == int32(arg0.strides[1]))) && (100352 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.51 = private constant [124 x i8] c"Assert fail: (3 == int32(arg1.shape[0])), Argument arg1.shape[0] has an unsatisfied constraint: (3 == int32(arg1.shape[0]))\00", align 1
@.str.52 = private constant [124 x i8] c"Assert fail: (3 == int32(arg1.shape[1])), Argument arg1.shape[1] has an unsatisfied constraint: (3 == int32(arg1.shape[1]))\00", align 1
@.str.53 = private constant [128 x i8] c"Assert fail: (512 == int32(arg1.shape[2])), Argument arg1.shape[2] has an unsatisfied constraint: (512 == int32(arg1.shape[2]))\00", align 1
@.str.54 = private constant [124 x i8] c"Assert fail: (1 == int32(arg1.shape[3])), Argument arg1.shape[3] has an unsatisfied constraint: (1 == int32(arg1.shape[3]))\00", align 1
@.str.55 = private constant [197 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (1 == int32(arg1.strides[2]))) && (512 == int32(arg1.strides[1]))) && (1536 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.56 = private constant [128 x i8] c"Assert fail: (512 == int32(arg2.shape[0])), Argument arg2.shape[0] has an unsatisfied constraint: (512 == int32(arg2.shape[0]))\00", align 1
@.str.57 = private constant [126 x i8] c"Assert fail: (14 == int32(arg3.shape[1])), Argument arg3.shape[1] has an unsatisfied constraint: (14 == int32(arg3.shape[1]))\00", align 1
@.str.58 = private constant [126 x i8] c"Assert fail: (14 == int32(arg3.shape[2])), Argument arg3.shape[2] has an unsatisfied constraint: (14 == int32(arg3.shape[2]))\00", align 1
@.str.59 = private constant [128 x i8] c"Assert fail: (512 == int32(arg3.shape[3])), Argument arg3.shape[3] has an unsatisfied constraint: (512 == int32(arg3.shape[3]))\00", align 1
@.str.60 = private constant [202 x i8] c"Assert fail: ((((1 == int32(arg3.strides[3])) && (512 == int32(arg3.strides[2]))) && (7168 == int32(arg3.strides[1]))) && (100352 == int32(arg3.strides[0]))), arg3.strides: expected to be compact array\00", align 1
@.str.61 = private constant [88 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_11: num_args should be 4\00", align 1
@.str.62 = private constant [163 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_11: Expect arg[0] to be pointer\00", align 1
@.str.63 = private constant [163 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_11: Expect arg[1] to be pointer\00", align 1
@.str.64 = private constant [163 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_11: Expect arg[2] to be pointer\00", align 1
@.str.65 = private constant [163 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_11: Expect arg[3] to be pointer\00", align 1
@.str.66 = private constant [126 x i8] c"Assert fail: (56 == int32(arg0.shape[1])), Argument arg0.shape[1] has an unsatisfied constraint: (56 == int32(arg0.shape[1]))\00", align 1
@.str.67 = private constant [126 x i8] c"Assert fail: (56 == int32(arg0.shape[2])), Argument arg0.shape[2] has an unsatisfied constraint: (56 == int32(arg0.shape[2]))\00", align 1
@.str.68 = private constant [128 x i8] c"Assert fail: (128 == int32(arg0.shape[3])), Argument arg0.shape[3] has an unsatisfied constraint: (128 == int32(arg0.shape[3]))\00", align 1
@.str.69 = private constant [202 x i8] c"Assert fail: ((((1 == int32(arg0.strides[3])) && (128 == int32(arg0.strides[2]))) && (7168 == int32(arg0.strides[1]))) && (401408 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.70 = private constant [128 x i8] c"Assert fail: (128 == int32(arg1.shape[2])), Argument arg1.shape[2] has an unsatisfied constraint: (128 == int32(arg1.shape[2]))\00", align 1
@.str.71 = private constant [196 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (1 == int32(arg1.strides[2]))) && (128 == int32(arg1.strides[1]))) && (384 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.72 = private constant [128 x i8] c"Assert fail: (128 == int32(arg2.shape[0])), Argument arg2.shape[0] has an unsatisfied constraint: (128 == int32(arg2.shape[0]))\00", align 1
@.str.73 = private constant [126 x i8] c"Assert fail: (28 == int32(arg3.shape[1])), Argument arg3.shape[1] has an unsatisfied constraint: (28 == int32(arg3.shape[1]))\00", align 1
@.str.74 = private constant [126 x i8] c"Assert fail: (28 == int32(arg3.shape[2])), Argument arg3.shape[2] has an unsatisfied constraint: (28 == int32(arg3.shape[2]))\00", align 1
@.str.75 = private constant [128 x i8] c"Assert fail: (128 == int32(arg3.shape[3])), Argument arg3.shape[3] has an unsatisfied constraint: (128 == int32(arg3.shape[3]))\00", align 1
@.str.76 = private constant [202 x i8] c"Assert fail: ((((1 == int32(arg3.strides[3])) && (128 == int32(arg3.strides[2]))) && (3584 == int32(arg3.strides[1]))) && (100352 == int32(arg3.strides[0]))), arg3.strides: expected to be compact array\00", align 1
@.str.77 = private constant [88 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_13: num_args should be 4\00", align 1
@.str.78 = private constant [163 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_13: Expect arg[0] to be pointer\00", align 1
@.str.79 = private constant [163 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_13: Expect arg[1] to be pointer\00", align 1
@.str.80 = private constant [163 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_13: Expect arg[2] to be pointer\00", align 1
@.str.81 = private constant [163 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_13: Expect arg[3] to be pointer\00", align 1
@.str.82 = private constant [126 x i8] c"Assert fail: (56 == int32(arg3.shape[1])), Argument arg3.shape[1] has an unsatisfied constraint: (56 == int32(arg3.shape[1]))\00", align 1
@.str.83 = private constant [126 x i8] c"Assert fail: (56 == int32(arg3.shape[2])), Argument arg3.shape[2] has an unsatisfied constraint: (56 == int32(arg3.shape[2]))\00", align 1
@.str.84 = private constant [202 x i8] c"Assert fail: ((((1 == int32(arg3.strides[3])) && (128 == int32(arg3.strides[2]))) && (7168 == int32(arg3.strides[1]))) && (401408 == int32(arg3.strides[0]))), arg3.strides: expected to be compact array\00", align 1
@.str.85 = private constant [87 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_8: num_args should be 4\00", align 1
@.str.86 = private constant [162 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_8: Expect arg[0] to be pointer\00", align 1
@.str.87 = private constant [162 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_8: Expect arg[1] to be pointer\00", align 1
@.str.88 = private constant [162 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_8: Expect arg[2] to be pointer\00", align 1
@.str.89 = private constant [162 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_8: Expect arg[3] to be pointer\00", align 1
@.str.90 = private constant [126 x i8] c"Assert fail: (28 == int32(arg0.shape[1])), Argument arg0.shape[1] has an unsatisfied constraint: (28 == int32(arg0.shape[1]))\00", align 1
@.str.91 = private constant [126 x i8] c"Assert fail: (28 == int32(arg0.shape[2])), Argument arg0.shape[2] has an unsatisfied constraint: (28 == int32(arg0.shape[2]))\00", align 1
@.str.92 = private constant [128 x i8] c"Assert fail: (256 == int32(arg0.shape[3])), Argument arg0.shape[3] has an unsatisfied constraint: (256 == int32(arg0.shape[3]))\00", align 1
@.str.93 = private constant [202 x i8] c"Assert fail: ((((1 == int32(arg0.strides[3])) && (256 == int32(arg0.strides[2]))) && (7168 == int32(arg0.strides[1]))) && (200704 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.94 = private constant [128 x i8] c"Assert fail: (256 == int32(arg1.shape[2])), Argument arg1.shape[2] has an unsatisfied constraint: (256 == int32(arg1.shape[2]))\00", align 1
@.str.95 = private constant [128 x i8] c"Assert fail: (256 == int32(arg1.shape[3])), Argument arg1.shape[3] has an unsatisfied constraint: (256 == int32(arg1.shape[3]))\00", align 1
@.str.96 = private constant [202 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (256 == int32(arg1.strides[2]))) && (65536 == int32(arg1.strides[1]))) && (65536 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.97 = private constant [128 x i8] c"Assert fail: (256 == int32(arg2.shape[0])), Argument arg2.shape[0] has an unsatisfied constraint: (256 == int32(arg2.shape[0]))\00", align 1
@.str.98 = private constant [128 x i8] c"Assert fail: (256 == int32(arg3.shape[3])), Argument arg3.shape[3] has an unsatisfied constraint: (256 == int32(arg3.shape[3]))\00", align 1
@.str.99 = private constant [202 x i8] c"Assert fail: ((((1 == int32(arg3.strides[3])) && (256 == int32(arg3.strides[2]))) && (7168 == int32(arg3.strides[1]))) && (200704 == int32(arg3.strides[0]))), arg3.strides: expected to be compact array\00", align 1
@.str.102 = private constant [69 x i8] c"Assert fail: (num_args == 2), fused_nn_softmax: num_args should be 2\00", align 1
@.str.103 = private constant [144 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_softmax: Expect arg[0] to be pointer\00", align 1
@.str.104 = private constant [144 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_softmax: Expect arg[1] to be pointer\00", align 1
@.str.105 = private constant [85 x i8] c"Assert fail: (2 == tir.tvm_struct_get(arg0, 0, 4)), arg0.ndim is expected to equal 2\00", align 1
@.str.106 = private constant [130 x i8] c"Assert fail: (1001 == int32(arg0.shape[1])), Argument arg0.shape[1] has an unsatisfied constraint: (1001 == int32(arg0.shape[1]))\00", align 1
@.str.107 = private constant [125 x i8] c"Assert fail: ((1 == int32(arg0.strides[1])) && (1001 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.108 = private constant [85 x i8] c"Assert fail: (2 == tir.tvm_struct_get(arg1, 0, 4)), arg1.ndim is expected to equal 2\00", align 1
@.str.109 = private constant [130 x i8] c"Assert fail: (1001 == int32(arg1.shape[1])), Argument arg1.shape[1] has an unsatisfied constraint: (1001 == int32(arg1.shape[1]))\00", align 1
@.str.110 = private constant [125 x i8] c"Assert fail: ((1 == int32(arg1.strides[1])) && (1001 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.111 = private constant [87 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_7: num_args should be 4\00", align 1
@.str.112 = private constant [162 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_7: Expect arg[0] to be pointer\00", align 1
@.str.113 = private constant [162 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_7: Expect arg[1] to be pointer\00", align 1
@.str.114 = private constant [162 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_7: Expect arg[2] to be pointer\00", align 1
@.str.115 = private constant [162 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_7: Expect arg[3] to be pointer\00", align 1
@.str.116 = private constant [196 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (1 == int32(arg1.strides[2]))) && (256 == int32(arg1.strides[1]))) && (768 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.117 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg3.strides[3])) && (256 == int32(arg3.strides[2]))) && (3584 == int32(arg3.strides[1]))) && (50176 == int32(arg3.strides[0]))), arg3.strides: expected to be compact array\00", align 1
@.str.118 = private constant [85 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip: num_args should be 4\00", align 1
@.str.119 = private constant [160 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip: Expect arg[0] to be pointer\00", align 1
@.str.120 = private constant [160 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip: Expect arg[1] to be pointer\00", align 1
@.str.121 = private constant [160 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip: Expect arg[2] to be pointer\00", align 1
@.str.122 = private constant [160 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip: Expect arg[3] to be pointer\00", align 1
@.str.123 = private constant [124 x i8] c"Assert fail: (7 == int32(arg0.shape[1])), Argument arg0.shape[1] has an unsatisfied constraint: (7 == int32(arg0.shape[1]))\00", align 1
@.str.124 = private constant [124 x i8] c"Assert fail: (7 == int32(arg0.shape[2])), Argument arg0.shape[2] has an unsatisfied constraint: (7 == int32(arg0.shape[2]))\00", align 1
@.str.125 = private constant [130 x i8] c"Assert fail: (1024 == int32(arg0.shape[3])), Argument arg0.shape[3] has an unsatisfied constraint: (1024 == int32(arg0.shape[3]))\00", align 1
@.str.126 = private constant [202 x i8] c"Assert fail: ((((1 == int32(arg0.strides[3])) && (1024 == int32(arg0.strides[2]))) && (7168 == int32(arg0.strides[1]))) && (50176 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.127 = private constant [130 x i8] c"Assert fail: (1024 == int32(arg1.shape[2])), Argument arg1.shape[2] has an unsatisfied constraint: (1024 == int32(arg1.shape[2]))\00", align 1
@.str.128 = private constant [130 x i8] c"Assert fail: (1024 == int32(arg1.shape[3])), Argument arg1.shape[3] has an unsatisfied constraint: (1024 == int32(arg1.shape[3]))\00", align 1
@.str.129 = private constant [207 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (1024 == int32(arg1.strides[2]))) && (1048576 == int32(arg1.strides[1]))) && (1048576 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.130 = private constant [130 x i8] c"Assert fail: (1024 == int32(arg2.shape[0])), Argument arg2.shape[0] has an unsatisfied constraint: (1024 == int32(arg2.shape[0]))\00", align 1
@.str.131 = private constant [124 x i8] c"Assert fail: (7 == int32(arg3.shape[1])), Argument arg3.shape[1] has an unsatisfied constraint: (7 == int32(arg3.shape[1]))\00", align 1
@.str.132 = private constant [124 x i8] c"Assert fail: (7 == int32(arg3.shape[2])), Argument arg3.shape[2] has an unsatisfied constraint: (7 == int32(arg3.shape[2]))\00", align 1
@.str.133 = private constant [130 x i8] c"Assert fail: (1024 == int32(arg3.shape[3])), Argument arg3.shape[3] has an unsatisfied constraint: (1024 == int32(arg3.shape[3]))\00", align 1
@.str.134 = private constant [202 x i8] c"Assert fail: ((((1 == int32(arg3.strides[3])) && (1024 == int32(arg3.strides[2]))) && (7168 == int32(arg3.strides[1]))) && (50176 == int32(arg3.strides[0]))), arg3.strides: expected to be compact array\00", align 1
@.str.137 = private constant [88 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_18: num_args should be 4\00", align 1
@.str.138 = private constant [163 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_18: Expect arg[0] to be pointer\00", align 1
@.str.139 = private constant [163 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_18: Expect arg[1] to be pointer\00", align 1
@.str.140 = private constant [163 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_18: Expect arg[2] to be pointer\00", align 1
@.str.141 = private constant [163 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_18: Expect arg[3] to be pointer\00", align 1
@.str.142 = private constant [128 x i8] c"Assert fail: (224 == int32(arg0.shape[1])), Argument arg0.shape[1] has an unsatisfied constraint: (224 == int32(arg0.shape[1]))\00", align 1
@.str.143 = private constant [128 x i8] c"Assert fail: (224 == int32(arg0.shape[2])), Argument arg0.shape[2] has an unsatisfied constraint: (224 == int32(arg0.shape[2]))\00", align 1
@.str.144 = private constant [124 x i8] c"Assert fail: (3 == int32(arg0.shape[3])), Argument arg0.shape[3] has an unsatisfied constraint: (3 == int32(arg0.shape[3]))\00", align 1
@.str.145 = private constant [199 x i8] c"Assert fail: ((((1 == int32(arg0.strides[3])) && (3 == int32(arg0.strides[2]))) && (672 == int32(arg0.strides[1]))) && (150528 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.146 = private constant [124 x i8] c"Assert fail: (3 == int32(arg1.shape[2])), Argument arg1.shape[2] has an unsatisfied constraint: (3 == int32(arg1.shape[2]))\00", align 1
@.str.147 = private constant [126 x i8] c"Assert fail: (32 == int32(arg1.shape[3])), Argument arg1.shape[3] has an unsatisfied constraint: (32 == int32(arg1.shape[3]))\00", align 1
@.str.148 = private constant [196 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (32 == int32(arg1.strides[2]))) && (96 == int32(arg1.strides[1]))) && (288 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.149 = private constant [126 x i8] c"Assert fail: (32 == int32(arg2.shape[0])), Argument arg2.shape[0] has an unsatisfied constraint: (32 == int32(arg2.shape[0]))\00", align 1
@.str.150 = private constant [126 x i8] c"Assert fail: (32 == int32(arg3.shape[3])), Argument arg3.shape[3] has an unsatisfied constraint: (32 == int32(arg3.shape[3]))\00", align 1
@.str.151 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg3.strides[3])) && (32 == int32(arg3.strides[2]))) && (3584 == int32(arg3.strides[1]))) && (401408 == int32(arg3.strides[0]))), arg3.strides: expected to be compact array\00", align 1
@.str.154 = private constant [88 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_12: num_args should be 4\00", align 1
@.str.155 = private constant [163 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_12: Expect arg[0] to be pointer\00", align 1
@.str.156 = private constant [163 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_12: Expect arg[1] to be pointer\00", align 1
@.str.157 = private constant [163 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_12: Expect arg[2] to be pointer\00", align 1
@.str.158 = private constant [163 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_12: Expect arg[3] to be pointer\00", align 1
@.str.159 = private constant [128 x i8] c"Assert fail: (128 == int32(arg1.shape[3])), Argument arg1.shape[3] has an unsatisfied constraint: (128 == int32(arg1.shape[3]))\00", align 1
@.str.160 = private constant [202 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (128 == int32(arg1.strides[2]))) && (16384 == int32(arg1.strides[1]))) && (16384 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.163 = private constant [66 x i8] c"Assert fail: (num_args == 2), fused_reshape: num_args should be 2\00", align 1
@.str.164 = private constant [141 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_reshape: Expect arg[0] to be pointer\00", align 1
@.str.165 = private constant [141 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_reshape: Expect arg[1] to be pointer\00", align 1
@.str.166 = private constant [124 x i8] c"Assert fail: (1 == int32(arg0.shape[1])), Argument arg0.shape[1] has an unsatisfied constraint: (1 == int32(arg0.shape[1]))\00", align 1
@.str.167 = private constant [124 x i8] c"Assert fail: (1 == int32(arg0.shape[2])), Argument arg0.shape[2] has an unsatisfied constraint: (1 == int32(arg0.shape[2]))\00", align 1
@.str.168 = private constant [130 x i8] c"Assert fail: (1001 == int32(arg0.shape[3])), Argument arg0.shape[3] has an unsatisfied constraint: (1001 == int32(arg0.shape[3]))\00", align 1
@.str.169 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg0.strides[3])) && (1001 == int32(arg0.strides[2]))) && (1001 == int32(arg0.strides[1]))) && (1001 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.170 = private constant [88 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_17: num_args should be 4\00", align 1
@.str.171 = private constant [163 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_17: Expect arg[0] to be pointer\00", align 1
@.str.172 = private constant [163 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_17: Expect arg[1] to be pointer\00", align 1
@.str.173 = private constant [163 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_17: Expect arg[2] to be pointer\00", align 1
@.str.174 = private constant [163 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_17: Expect arg[3] to be pointer\00", align 1
@.str.175 = private constant [194 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (1 == int32(arg1.strides[2]))) && (32 == int32(arg1.strides[1]))) && (96 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.176 = private constant [87 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_4: num_args should be 4\00", align 1
@.str.177 = private constant [162 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_4: Expect arg[0] to be pointer\00", align 1
@.str.178 = private constant [162 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_4: Expect arg[1] to be pointer\00", align 1
@.str.179 = private constant [162 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_4: Expect arg[2] to be pointer\00", align 1
@.str.180 = private constant [162 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_4: Expect arg[3] to be pointer\00", align 1
@.str.181 = private constant [128 x i8] c"Assert fail: (512 == int32(arg1.shape[3])), Argument arg1.shape[3] has an unsatisfied constraint: (512 == int32(arg1.shape[3]))\00", align 1
@.str.182 = private constant [204 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (512 == int32(arg1.strides[2]))) && (262144 == int32(arg1.strides[1]))) && (262144 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.185 = private constant [87 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_1: num_args should be 4\00", align 1
@.str.186 = private constant [162 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_1: Expect arg[0] to be pointer\00", align 1
@.str.187 = private constant [162 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_1: Expect arg[1] to be pointer\00", align 1
@.str.188 = private constant [162 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_1: Expect arg[2] to be pointer\00", align 1
@.str.189 = private constant [162 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_1: Expect arg[3] to be pointer\00", align 1
@.str.190 = private constant [198 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (1 == int32(arg1.strides[2]))) && (1024 == int32(arg1.strides[1]))) && (3072 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.191 = private constant [72 x i8] c"Assert fail: (num_args == 2), fused_nn_avg_pool2d: num_args should be 2\00", align 1
@.str.192 = private constant [147 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_avg_pool2d: Expect arg[0] to be pointer\00", align 1
@.str.193 = private constant [147 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_avg_pool2d: Expect arg[1] to be pointer\00", align 1
@.str.194 = private constant [124 x i8] c"Assert fail: (1 == int32(arg1.shape[2])), Argument arg1.shape[2] has an unsatisfied constraint: (1 == int32(arg1.shape[2]))\00", align 1
@.str.195 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (1024 == int32(arg1.strides[2]))) && (1024 == int32(arg1.strides[1]))) && (1024 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.196 = private constant [88 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_14: num_args should be 4\00", align 1
@.str.197 = private constant [163 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_14: Expect arg[0] to be pointer\00", align 1
@.str.198 = private constant [163 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_14: Expect arg[1] to be pointer\00", align 1
@.str.199 = private constant [163 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_14: Expect arg[2] to be pointer\00", align 1
@.str.200 = private constant [163 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_14: Expect arg[3] to be pointer\00", align 1
@.str.201 = private constant [126 x i8] c"Assert fail: (64 == int32(arg0.shape[3])), Argument arg0.shape[3] has an unsatisfied constraint: (64 == int32(arg0.shape[3]))\00", align 1
@.str.202 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg0.strides[3])) && (64 == int32(arg0.strides[2]))) && (3584 == int32(arg0.strides[1]))) && (200704 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.203 = private constant [126 x i8] c"Assert fail: (64 == int32(arg1.shape[2])), Argument arg1.shape[2] has an unsatisfied constraint: (64 == int32(arg1.shape[2]))\00", align 1
@.str.204 = private constant [200 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (128 == int32(arg1.strides[2]))) && (8192 == int32(arg1.strides[1]))) && (8192 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.207 = private constant [87 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_3: num_args should be 4\00", align 1
@.str.208 = private constant [162 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_3: Expect arg[0] to be pointer\00", align 1
@.str.209 = private constant [162 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_3: Expect arg[1] to be pointer\00", align 1
@.str.210 = private constant [162 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_3: Expect arg[2] to be pointer\00", align 1
@.str.211 = private constant [162 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_3: Expect arg[3] to be pointer\00", align 1
@.str.212 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg3.strides[3])) && (512 == int32(arg3.strides[2]))) && (3584 == int32(arg3.strides[1]))) && (25088 == int32(arg3.strides[0]))), arg3.strides: expected to be compact array\00", align 1
@.str.213 = private constant [88 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_10: num_args should be 4\00", align 1
@.str.214 = private constant [163 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_10: Expect arg[0] to be pointer\00", align 1
@.str.215 = private constant [163 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_10: Expect arg[1] to be pointer\00", align 1
@.str.216 = private constant [163 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_10: Expect arg[2] to be pointer\00", align 1
@.str.217 = private constant [163 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_10: Expect arg[3] to be pointer\00", align 1
@.str.218 = private constant [202 x i8] c"Assert fail: ((((1 == int32(arg0.strides[3])) && (128 == int32(arg0.strides[2]))) && (3584 == int32(arg0.strides[1]))) && (100352 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.219 = private constant [202 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (256 == int32(arg1.strides[2]))) && (32768 == int32(arg1.strides[1]))) && (32768 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.222 = private constant [87 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_2: num_args should be 4\00", align 1
@.str.223 = private constant [162 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_2: Expect arg[0] to be pointer\00", align 1
@.str.224 = private constant [162 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_2: Expect arg[1] to be pointer\00", align 1
@.str.225 = private constant [162 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_2: Expect arg[2] to be pointer\00", align 1
@.str.226 = private constant [162 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_2: Expect arg[3] to be pointer\00", align 1
@.str.227 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg0.strides[3])) && (512 == int32(arg0.strides[2]))) && (3584 == int32(arg0.strides[1]))) && (25088 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.228 = private constant [205 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (1024 == int32(arg1.strides[2]))) && (524288 == int32(arg1.strides[1]))) && (524288 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.231 = private constant [87 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_6: num_args should be 4\00", align 1
@.str.232 = private constant [162 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_6: Expect arg[0] to be pointer\00", align 1
@.str.233 = private constant [162 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_6: Expect arg[1] to be pointer\00", align 1
@.str.234 = private constant [162 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_6: Expect arg[2] to be pointer\00", align 1
@.str.235 = private constant [162 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_6: Expect arg[3] to be pointer\00", align 1
@.str.236 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg0.strides[3])) && (256 == int32(arg0.strides[2]))) && (3584 == int32(arg0.strides[1]))) && (50176 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.237 = private constant [204 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (512 == int32(arg1.strides[2]))) && (131072 == int32(arg1.strides[1]))) && (131072 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.240 = private constant [87 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_9: num_args should be 4\00", align 1
@.str.241 = private constant [162 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_9: Expect arg[0] to be pointer\00", align 1
@.str.242 = private constant [162 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_9: Expect arg[1] to be pointer\00", align 1
@.str.243 = private constant [162 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_9: Expect arg[2] to be pointer\00", align 1
@.str.244 = private constant [162 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_9: Expect arg[3] to be pointer\00", align 1
@.str.245 = private constant [88 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add_clip_15: num_args should be 4\00", align 1
@.str.246 = private constant [163 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add_clip_15: Expect arg[0] to be pointer\00", align 1
@.str.247 = private constant [163 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add_clip_15: Expect arg[1] to be pointer\00", align 1
@.str.248 = private constant [163 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add_clip_15: Expect arg[2] to be pointer\00", align 1
@.str.249 = private constant [163 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add_clip_15: Expect arg[3] to be pointer\00", align 1
@.str.250 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg0.strides[3])) && (64 == int32(arg0.strides[2]))) && (7168 == int32(arg0.strides[1]))) && (802816 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.251 = private constant [195 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (1 == int32(arg1.strides[2]))) && (64 == int32(arg1.strides[1]))) && (192 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.252 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg3.strides[3])) && (64 == int32(arg3.strides[2]))) && (3584 == int32(arg3.strides[1]))) && (200704 == int32(arg3.strides[0]))), arg3.strides: expected to be compact array\00", align 1
@.str.253 = private constant [80 x i8] c"Assert fail: (num_args == 4), fused_nn_conv2d_nn_bias_add: num_args should be 4\00", align 1
@.str.254 = private constant [155 x i8] c"Assert fail: ((((arg0.code == 3) || (arg0.code == 13)) || (arg0.code == 7)) || (arg0.code == 4)), fused_nn_conv2d_nn_bias_add: Expect arg[0] to be pointer\00", align 1
@.str.255 = private constant [155 x i8] c"Assert fail: ((((arg1.code == 3) || (arg1.code == 13)) || (arg1.code == 7)) || (arg1.code == 4)), fused_nn_conv2d_nn_bias_add: Expect arg[1] to be pointer\00", align 1
@.str.256 = private constant [155 x i8] c"Assert fail: ((((arg2.code == 3) || (arg2.code == 13)) || (arg2.code == 7)) || (arg2.code == 4)), fused_nn_conv2d_nn_bias_add: Expect arg[2] to be pointer\00", align 1
@.str.257 = private constant [155 x i8] c"Assert fail: ((((arg3.code == 3) || (arg3.code == 13)) || (arg3.code == 7)) || (arg3.code == 4)), fused_nn_conv2d_nn_bias_add: Expect arg[3] to be pointer\00", align 1
@.str.258 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg0.strides[3])) && (1024 == int32(arg0.strides[2]))) && (1024 == int32(arg0.strides[1]))) && (1024 == int32(arg0.strides[0]))), arg0.strides: expected to be compact array\00", align 1
@.str.259 = private constant [130 x i8] c"Assert fail: (1001 == int32(arg1.shape[3])), Argument arg1.shape[3] has an unsatisfied constraint: (1001 == int32(arg1.shape[3]))\00", align 1
@.str.260 = private constant [207 x i8] c"Assert fail: ((((1 == int32(arg1.strides[3])) && (1001 == int32(arg1.strides[2]))) && (1025024 == int32(arg1.strides[1]))) && (1025024 == int32(arg1.strides[0]))), arg1.strides: expected to be compact array\00", align 1
@.str.261 = private constant [130 x i8] c"Assert fail: (1001 == int32(arg2.shape[0])), Argument arg2.shape[0] has an unsatisfied constraint: (1001 == int32(arg2.shape[0]))\00", align 1
@.str.262 = private constant [124 x i8] c"Assert fail: (1 == int32(arg3.shape[1])), Argument arg3.shape[1] has an unsatisfied constraint: (1 == int32(arg3.shape[1]))\00", align 1
@.str.263 = private constant [124 x i8] c"Assert fail: (1 == int32(arg3.shape[2])), Argument arg3.shape[2] has an unsatisfied constraint: (1 == int32(arg3.shape[2]))\00", align 1
@.str.264 = private constant [130 x i8] c"Assert fail: (1001 == int32(arg3.shape[3])), Argument arg3.shape[3] has an unsatisfied constraint: (1001 == int32(arg3.shape[3]))\00", align 1
@.str.265 = private constant [201 x i8] c"Assert fail: ((((1 == int32(arg3.strides[3])) && (1001 == int32(arg3.strides[2]))) && (1001 == int32(arg3.strides[1]))) && (1001 == int32(arg3.strides[0]))), arg3.strides: expected to be compact array\00", align 1
@llvm.global_ctors = appending global [0 x { i32, void ()*, i8* }] zeroinitializer

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_16(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([88 x i8], [88 x i8]* @.str, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !9
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !23
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !25
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !28
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.1, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.2, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.3, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.4, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !30
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !44
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 112
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.8, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !46
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 112
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.9, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !49
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 32
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.10, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !52
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 401408, i32 3584, i32 32, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.11, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !64
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 1
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.16, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !78
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 1
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.17, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !80
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 32
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.18, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !83
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 64
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.19, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !85
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 2048, i32 2048, i32 64, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([199 x i8], [199 x i8]* @.str.20, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !97
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 64
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.26, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !111
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !125
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !139
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 112
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.34, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !141
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 112
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.35, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !144
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 64
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.36, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !146
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 802816, i32 7168, i32 64, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.37, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_16_compute_(i8* %31, i8* %39, i8* %51, i8* %45, i32 %37)
  ret i32 %251
}

; Function Attrs: nofree nosync nounwind willreturn
declare void @llvm.assume(i1 noundef) #0

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_16_compute_(i8* noalias align 128 %0, i8* noalias align 128 %1, i8* noalias align 128 %2, i8* noalias align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 1605632, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %call_end, %if_end, %entry
  %merge = phi i32 [ -1, %entry ], [ %13, %if_end ], [ %22, %call_end ]
  ret i32 %merge

if_end:                                           ; preds = %entry
  %8 = alloca %4, align 8
  %9 = getelementptr inbounds %4, %4* %8, i64 0, i32 0
  store i8* %6, i8** %9, align 8
  %10 = getelementptr inbounds %4, %4* %8, i64 0, i32 1
  store i8* %0, i8** %10, align 8
  %11 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %12 = bitcast %4* %8 to i8*
  %13 = call i32 %11(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda, i8* nonnull %12, i32 0)
  %14 = icmp eq i32 %13, 0
  br i1 %14, label %call_end, label %if_then, !prof !5

call_end:                                         ; preds = %if_end
  %15 = alloca %5, align 8
  %16 = getelementptr inbounds %5, %5* %15, i64 0, i32 0
  store i8* %6, i8** %16, align 8
  %17 = getelementptr inbounds %5, %5* %15, i64 0, i32 1
  store i8* %1, i8** %17, align 8
  %18 = getelementptr inbounds %5, %5* %15, i64 0, i32 2
  store i8* %2, i8** %18, align 8
  %19 = getelementptr inbounds %5, %5* %15, i64 0, i32 3
  store i8* %3, i8** %19, align 8
  %20 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %21 = bitcast %5* %15 to i8*
  %22 = call i32 %20(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.41, i8* nonnull %21, i32 0)
  %23 = icmp eq i32 %22, 0
  br i1 %23, label %call_end2, label %if_then, !prof !5

call_end2:                                        ; preds = %call_end
  %24 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %25 = call i32 %24(i32 1, i32 %4, i8* nonnull %6)
  %.not = icmp ne i32 %25, 0
  %spec.select = sext i1 %.not to i32
  ret i32 %spec.select
}

; Function Attrs: nofree norecurse nounwind
define private i32 @__tvm_parallel_lambda(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #2 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %9 = load i32, i32* %8, align 4
  %10 = add nsw i32 %9, 111
  %11 = sdiv i32 %10, %9
  %12 = add nsw i32 %0, 1
  %13 = mul nsw i32 %11, %12
  %14 = icmp slt i32 %13, 112
  %15 = select i1 %14, i32 %13, i32 112
  %16 = mul nsw i32 %11, %0
  %17 = icmp slt i32 %16, 112
  %18 = select i1 %17, i32 %16, i32 112
  %19 = icmp slt i32 %18, %15
  br i1 %19, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %20 = sext i32 %18 to i64
  %wide.trip.count = sext i32 %15 to i64
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end3
  %indvars.iv10 = phi i64 [ %20, %for_begin1.preheader.preheader ], [ %indvars.iv.next11, %for_end3 ]
  %21 = mul nsw i64 %indvars.iv10, 3584
  br label %for_begin4.preheader

for_end:                                          ; preds = %for_end3, %entry
  ret i32 0

for_begin4.preheader:                             ; preds = %for_begin1.preheader, %for_begin4.preheader
  %indvars.iv = phi i64 [ 0, %for_begin1.preheader ], [ %indvars.iv.next, %for_begin4.preheader ]
  %22 = shl nsw i64 %indvars.iv, 5
  %23 = add nsw i64 %22, %21
  %24 = getelementptr inbounds float, float* %7, i64 %23
  %25 = getelementptr inbounds float, float* %4, i64 %23
  %26 = bitcast float* %24 to <4 x float>*
  %27 = load <4 x float>, <4 x float>* %26, align 4, !tbaa !158
  %28 = bitcast float* %25 to <4 x float>*
  store <4 x float> %27, <4 x float>* %28, align 4, !tbaa !161
  %29 = or i64 %23, 4
  %30 = getelementptr inbounds float, float* %7, i64 %29
  %31 = getelementptr inbounds float, float* %4, i64 %29
  %32 = bitcast float* %30 to <4 x float>*
  %33 = load <4 x float>, <4 x float>* %32, align 4, !tbaa !158
  %34 = bitcast float* %31 to <4 x float>*
  store <4 x float> %33, <4 x float>* %34, align 4, !tbaa !161
  %35 = or i64 %23, 8
  %36 = getelementptr inbounds float, float* %7, i64 %35
  %37 = getelementptr inbounds float, float* %4, i64 %35
  %38 = bitcast float* %36 to <4 x float>*
  %39 = load <4 x float>, <4 x float>* %38, align 4, !tbaa !158
  %40 = bitcast float* %37 to <4 x float>*
  store <4 x float> %39, <4 x float>* %40, align 4, !tbaa !161
  %41 = or i64 %23, 12
  %42 = getelementptr inbounds float, float* %7, i64 %41
  %43 = getelementptr inbounds float, float* %4, i64 %41
  %44 = bitcast float* %42 to <4 x float>*
  %45 = load <4 x float>, <4 x float>* %44, align 4, !tbaa !158
  %46 = bitcast float* %43 to <4 x float>*
  store <4 x float> %45, <4 x float>* %46, align 4, !tbaa !161
  %47 = or i64 %23, 16
  %48 = getelementptr inbounds float, float* %7, i64 %47
  %49 = getelementptr inbounds float, float* %4, i64 %47
  %50 = bitcast float* %48 to <4 x float>*
  %51 = load <4 x float>, <4 x float>* %50, align 4, !tbaa !158
  %52 = bitcast float* %49 to <4 x float>*
  store <4 x float> %51, <4 x float>* %52, align 4, !tbaa !161
  %53 = or i64 %23, 20
  %54 = getelementptr inbounds float, float* %7, i64 %53
  %55 = getelementptr inbounds float, float* %4, i64 %53
  %56 = bitcast float* %54 to <4 x float>*
  %57 = load <4 x float>, <4 x float>* %56, align 4, !tbaa !158
  %58 = bitcast float* %55 to <4 x float>*
  store <4 x float> %57, <4 x float>* %58, align 4, !tbaa !161
  %59 = or i64 %23, 24
  %60 = getelementptr inbounds float, float* %7, i64 %59
  %61 = getelementptr inbounds float, float* %4, i64 %59
  %62 = bitcast float* %60 to <4 x float>*
  %63 = load <4 x float>, <4 x float>* %62, align 4, !tbaa !158
  %64 = bitcast float* %61 to <4 x float>*
  store <4 x float> %63, <4 x float>* %64, align 4, !tbaa !161
  %65 = or i64 %23, 28
  %66 = getelementptr inbounds float, float* %7, i64 %65
  %67 = getelementptr inbounds float, float* %4, i64 %65
  %68 = bitcast float* %66 to <4 x float>*
  %69 = load <4 x float>, <4 x float>* %68, align 4, !tbaa !158
  %70 = bitcast float* %67 to <4 x float>*
  store <4 x float> %69, <4 x float>* %70, align 4, !tbaa !161
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, 112
  br i1 %exitcond.not, label %for_end3, label %for_begin4.preheader, !prof !51

for_end3:                                         ; preds = %for_begin4.preheader
  %indvars.iv.next11 = add nsw i64 %indvars.iv10, 1
  %exitcond12.not = icmp eq i64 %indvars.iv.next11, %wide.trip.count
  br i1 %exitcond12.not, label %for_end, label %for_begin1.preheader, !prof !51
}

; Function Attrs: nofree nounwind
define private i32 @__tvm_parallel_lambda.41(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #3 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds i8, i8* %2, i64 16
  %9 = bitcast i8* %8 to float**
  %10 = load float*, float** %9, align 8
  %11 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %12 = load i32, i32* %11, align 4
  %13 = add nsw i32 %12, 12543
  %14 = sdiv i32 %13, %12
  %15 = add nsw i32 %0, 1
  %16 = mul nsw i32 %14, %15
  %17 = icmp slt i32 %16, 12544
  %18 = select i1 %17, i32 %16, i32 12544
  %19 = mul nsw i32 %14, %0
  %20 = icmp slt i32 %19, 12544
  %21 = select i1 %20, i32 %19, i32 12544
  %22 = icmp slt i32 %21, %18
  br i1 %22, label %for_begin1.preheader.lr.ph, label %for_end, !prof !5

for_begin1.preheader.lr.ph:                       ; preds = %entry
  %23 = getelementptr inbounds i8, i8* %2, i64 24
  %24 = bitcast i8* %23 to <64 x float>**
  %25 = load <64 x float>*, <64 x float>** %24, align 8
  %26 = load <64 x float>, <64 x float>* %25, align 128, !tbaa !164
  %27 = sext i32 %21 to i64
  %wide.trip.count = sext i32 %18 to i64
  %28 = bitcast float* %7 to <64 x float>*
  %29 = load <64 x float>, <64 x float>* %28, align 128, !tbaa !172
  %30 = getelementptr inbounds float, float* %7, i64 64
  %31 = bitcast float* %30 to <64 x float>*
  %32 = load <64 x float>, <64 x float>* %31, align 128, !tbaa !172
  %33 = getelementptr inbounds float, float* %7, i64 128
  %34 = bitcast float* %33 to <64 x float>*
  %35 = load <64 x float>, <64 x float>* %34, align 128, !tbaa !172
  %36 = getelementptr inbounds float, float* %7, i64 192
  %37 = bitcast float* %36 to <64 x float>*
  %38 = load <64 x float>, <64 x float>* %37, align 128, !tbaa !172
  %39 = getelementptr inbounds float, float* %7, i64 256
  %40 = bitcast float* %39 to <64 x float>*
  %41 = load <64 x float>, <64 x float>* %40, align 128, !tbaa !172
  %42 = getelementptr inbounds float, float* %7, i64 320
  %43 = bitcast float* %42 to <64 x float>*
  %44 = load <64 x float>, <64 x float>* %43, align 128, !tbaa !172
  %45 = getelementptr inbounds float, float* %7, i64 384
  %46 = bitcast float* %45 to <64 x float>*
  %47 = load <64 x float>, <64 x float>* %46, align 128, !tbaa !172
  %48 = getelementptr inbounds float, float* %7, i64 448
  %49 = bitcast float* %48 to <64 x float>*
  %50 = load <64 x float>, <64 x float>* %49, align 128, !tbaa !172
  %51 = getelementptr inbounds float, float* %7, i64 512
  %52 = bitcast float* %51 to <64 x float>*
  %53 = load <64 x float>, <64 x float>* %52, align 128, !tbaa !172
  %54 = getelementptr inbounds float, float* %7, i64 576
  %55 = bitcast float* %54 to <64 x float>*
  %56 = load <64 x float>, <64 x float>* %55, align 128, !tbaa !172
  %57 = getelementptr inbounds float, float* %7, i64 640
  %58 = bitcast float* %57 to <64 x float>*
  %59 = load <64 x float>, <64 x float>* %58, align 128, !tbaa !172
  %60 = getelementptr inbounds float, float* %7, i64 704
  %61 = bitcast float* %60 to <64 x float>*
  %62 = load <64 x float>, <64 x float>* %61, align 128, !tbaa !172
  %63 = getelementptr inbounds float, float* %7, i64 768
  %64 = bitcast float* %63 to <64 x float>*
  %65 = load <64 x float>, <64 x float>* %64, align 128, !tbaa !172
  %66 = getelementptr inbounds float, float* %7, i64 832
  %67 = bitcast float* %66 to <64 x float>*
  %68 = load <64 x float>, <64 x float>* %67, align 128, !tbaa !172
  %69 = getelementptr inbounds float, float* %7, i64 896
  %70 = bitcast float* %69 to <64 x float>*
  %71 = load <64 x float>, <64 x float>* %70, align 128, !tbaa !172
  %72 = getelementptr inbounds float, float* %7, i64 960
  %73 = bitcast float* %72 to <64 x float>*
  %74 = load <64 x float>, <64 x float>* %73, align 128, !tbaa !172
  %75 = getelementptr inbounds float, float* %7, i64 1024
  %76 = bitcast float* %75 to <64 x float>*
  %77 = load <64 x float>, <64 x float>* %76, align 128, !tbaa !172
  %78 = getelementptr inbounds float, float* %7, i64 1088
  %79 = bitcast float* %78 to <64 x float>*
  %80 = load <64 x float>, <64 x float>* %79, align 128, !tbaa !172
  %81 = getelementptr inbounds float, float* %7, i64 1152
  %82 = bitcast float* %81 to <64 x float>*
  %83 = load <64 x float>, <64 x float>* %82, align 128, !tbaa !172
  %84 = getelementptr inbounds float, float* %7, i64 1216
  %85 = bitcast float* %84 to <64 x float>*
  %86 = load <64 x float>, <64 x float>* %85, align 128, !tbaa !172
  %87 = getelementptr inbounds float, float* %7, i64 1280
  %88 = bitcast float* %87 to <64 x float>*
  %89 = load <64 x float>, <64 x float>* %88, align 128, !tbaa !172
  %90 = getelementptr inbounds float, float* %7, i64 1344
  %91 = bitcast float* %90 to <64 x float>*
  %92 = load <64 x float>, <64 x float>* %91, align 128, !tbaa !172
  %93 = getelementptr inbounds float, float* %7, i64 1408
  %94 = bitcast float* %93 to <64 x float>*
  %95 = load <64 x float>, <64 x float>* %94, align 128, !tbaa !172
  %96 = getelementptr inbounds float, float* %7, i64 1472
  %97 = bitcast float* %96 to <64 x float>*
  %98 = load <64 x float>, <64 x float>* %97, align 128, !tbaa !172
  %99 = getelementptr inbounds float, float* %7, i64 1536
  %100 = bitcast float* %99 to <64 x float>*
  %101 = load <64 x float>, <64 x float>* %100, align 128, !tbaa !172
  %102 = getelementptr inbounds float, float* %7, i64 1600
  %103 = bitcast float* %102 to <64 x float>*
  %104 = load <64 x float>, <64 x float>* %103, align 128, !tbaa !172
  %105 = getelementptr inbounds float, float* %7, i64 1664
  %106 = bitcast float* %105 to <64 x float>*
  %107 = load <64 x float>, <64 x float>* %106, align 128, !tbaa !172
  %108 = getelementptr inbounds float, float* %7, i64 1728
  %109 = bitcast float* %108 to <64 x float>*
  %110 = load <64 x float>, <64 x float>* %109, align 128, !tbaa !172
  %111 = getelementptr inbounds float, float* %7, i64 1792
  %112 = bitcast float* %111 to <64 x float>*
  %113 = load <64 x float>, <64 x float>* %112, align 128, !tbaa !172
  %114 = getelementptr inbounds float, float* %7, i64 1856
  %115 = bitcast float* %114 to <64 x float>*
  %116 = load <64 x float>, <64 x float>* %115, align 128, !tbaa !172
  %117 = getelementptr inbounds float, float* %7, i64 1920
  %118 = bitcast float* %117 to <64 x float>*
  %119 = load <64 x float>, <64 x float>* %118, align 128, !tbaa !172
  %120 = getelementptr inbounds float, float* %7, i64 1984
  %121 = bitcast float* %120 to <64 x float>*
  %122 = load <64 x float>, <64 x float>* %121, align 128, !tbaa !172
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.lr.ph, %for_begin1.preheader
  %indvars.iv = phi i64 [ %27, %for_begin1.preheader.lr.ph ], [ %indvars.iv.next, %for_begin1.preheader ]
  %123 = phi i32 [ %21, %for_begin1.preheader.lr.ph ], [ %327, %for_begin1.preheader ]
  %124 = shl nsw i32 %123, 5
  %125 = sext i32 %124 to i64
  %126 = getelementptr inbounds float, float* %4, i64 %125
  %127 = load float, float* %126, align 4, !tbaa !161
  %128 = insertelement <64 x float> undef, float %127, i32 0
  %129 = shufflevector <64 x float> %128, <64 x float> undef, <64 x i32> zeroinitializer
  %130 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %129, <64 x float> %29, <64 x float> zeroinitializer)
  %131 = or i64 %125, 1
  %132 = getelementptr inbounds float, float* %4, i64 %131
  %133 = load float, float* %132, align 4, !tbaa !161
  %134 = insertelement <64 x float> undef, float %133, i32 0
  %135 = shufflevector <64 x float> %134, <64 x float> undef, <64 x i32> zeroinitializer
  %136 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %135, <64 x float> %32, <64 x float> %130)
  %137 = or i64 %125, 2
  %138 = getelementptr inbounds float, float* %4, i64 %137
  %139 = load float, float* %138, align 4, !tbaa !161
  %140 = insertelement <64 x float> undef, float %139, i32 0
  %141 = shufflevector <64 x float> %140, <64 x float> undef, <64 x i32> zeroinitializer
  %142 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %141, <64 x float> %35, <64 x float> %136)
  %143 = or i64 %125, 3
  %144 = getelementptr inbounds float, float* %4, i64 %143
  %145 = load float, float* %144, align 4, !tbaa !161
  %146 = insertelement <64 x float> undef, float %145, i32 0
  %147 = shufflevector <64 x float> %146, <64 x float> undef, <64 x i32> zeroinitializer
  %148 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %147, <64 x float> %38, <64 x float> %142)
  %149 = or i64 %125, 4
  %150 = getelementptr inbounds float, float* %4, i64 %149
  %151 = load float, float* %150, align 4, !tbaa !161
  %152 = insertelement <64 x float> undef, float %151, i32 0
  %153 = shufflevector <64 x float> %152, <64 x float> undef, <64 x i32> zeroinitializer
  %154 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %153, <64 x float> %41, <64 x float> %148)
  %155 = or i64 %125, 5
  %156 = getelementptr inbounds float, float* %4, i64 %155
  %157 = load float, float* %156, align 4, !tbaa !161
  %158 = insertelement <64 x float> undef, float %157, i32 0
  %159 = shufflevector <64 x float> %158, <64 x float> undef, <64 x i32> zeroinitializer
  %160 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %159, <64 x float> %44, <64 x float> %154)
  %161 = or i64 %125, 6
  %162 = getelementptr inbounds float, float* %4, i64 %161
  %163 = load float, float* %162, align 4, !tbaa !161
  %164 = insertelement <64 x float> undef, float %163, i32 0
  %165 = shufflevector <64 x float> %164, <64 x float> undef, <64 x i32> zeroinitializer
  %166 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %165, <64 x float> %47, <64 x float> %160)
  %167 = or i64 %125, 7
  %168 = getelementptr inbounds float, float* %4, i64 %167
  %169 = load float, float* %168, align 4, !tbaa !161
  %170 = insertelement <64 x float> undef, float %169, i32 0
  %171 = shufflevector <64 x float> %170, <64 x float> undef, <64 x i32> zeroinitializer
  %172 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %171, <64 x float> %50, <64 x float> %166)
  %173 = or i64 %125, 8
  %174 = getelementptr inbounds float, float* %4, i64 %173
  %175 = load float, float* %174, align 4, !tbaa !161
  %176 = insertelement <64 x float> undef, float %175, i32 0
  %177 = shufflevector <64 x float> %176, <64 x float> undef, <64 x i32> zeroinitializer
  %178 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %177, <64 x float> %53, <64 x float> %172)
  %179 = or i64 %125, 9
  %180 = getelementptr inbounds float, float* %4, i64 %179
  %181 = load float, float* %180, align 4, !tbaa !161
  %182 = insertelement <64 x float> undef, float %181, i32 0
  %183 = shufflevector <64 x float> %182, <64 x float> undef, <64 x i32> zeroinitializer
  %184 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %183, <64 x float> %56, <64 x float> %178)
  %185 = or i64 %125, 10
  %186 = getelementptr inbounds float, float* %4, i64 %185
  %187 = load float, float* %186, align 4, !tbaa !161
  %188 = insertelement <64 x float> undef, float %187, i32 0
  %189 = shufflevector <64 x float> %188, <64 x float> undef, <64 x i32> zeroinitializer
  %190 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %189, <64 x float> %59, <64 x float> %184)
  %191 = or i64 %125, 11
  %192 = getelementptr inbounds float, float* %4, i64 %191
  %193 = load float, float* %192, align 4, !tbaa !161
  %194 = insertelement <64 x float> undef, float %193, i32 0
  %195 = shufflevector <64 x float> %194, <64 x float> undef, <64 x i32> zeroinitializer
  %196 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %195, <64 x float> %62, <64 x float> %190)
  %197 = or i64 %125, 12
  %198 = getelementptr inbounds float, float* %4, i64 %197
  %199 = load float, float* %198, align 4, !tbaa !161
  %200 = insertelement <64 x float> undef, float %199, i32 0
  %201 = shufflevector <64 x float> %200, <64 x float> undef, <64 x i32> zeroinitializer
  %202 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %201, <64 x float> %65, <64 x float> %196)
  %203 = or i64 %125, 13
  %204 = getelementptr inbounds float, float* %4, i64 %203
  %205 = load float, float* %204, align 4, !tbaa !161
  %206 = insertelement <64 x float> undef, float %205, i32 0
  %207 = shufflevector <64 x float> %206, <64 x float> undef, <64 x i32> zeroinitializer
  %208 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %207, <64 x float> %68, <64 x float> %202)
  %209 = or i64 %125, 14
  %210 = getelementptr inbounds float, float* %4, i64 %209
  %211 = load float, float* %210, align 4, !tbaa !161
  %212 = insertelement <64 x float> undef, float %211, i32 0
  %213 = shufflevector <64 x float> %212, <64 x float> undef, <64 x i32> zeroinitializer
  %214 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %213, <64 x float> %71, <64 x float> %208)
  %215 = or i64 %125, 15
  %216 = getelementptr inbounds float, float* %4, i64 %215
  %217 = load float, float* %216, align 4, !tbaa !161
  %218 = insertelement <64 x float> undef, float %217, i32 0
  %219 = shufflevector <64 x float> %218, <64 x float> undef, <64 x i32> zeroinitializer
  %220 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %219, <64 x float> %74, <64 x float> %214)
  %221 = or i64 %125, 16
  %222 = getelementptr inbounds float, float* %4, i64 %221
  %223 = load float, float* %222, align 4, !tbaa !161
  %224 = insertelement <64 x float> undef, float %223, i32 0
  %225 = shufflevector <64 x float> %224, <64 x float> undef, <64 x i32> zeroinitializer
  %226 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %225, <64 x float> %77, <64 x float> %220)
  %227 = or i64 %125, 17
  %228 = getelementptr inbounds float, float* %4, i64 %227
  %229 = load float, float* %228, align 4, !tbaa !161
  %230 = insertelement <64 x float> undef, float %229, i32 0
  %231 = shufflevector <64 x float> %230, <64 x float> undef, <64 x i32> zeroinitializer
  %232 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %231, <64 x float> %80, <64 x float> %226)
  %233 = or i64 %125, 18
  %234 = getelementptr inbounds float, float* %4, i64 %233
  %235 = load float, float* %234, align 4, !tbaa !161
  %236 = insertelement <64 x float> undef, float %235, i32 0
  %237 = shufflevector <64 x float> %236, <64 x float> undef, <64 x i32> zeroinitializer
  %238 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %237, <64 x float> %83, <64 x float> %232)
  %239 = or i64 %125, 19
  %240 = getelementptr inbounds float, float* %4, i64 %239
  %241 = load float, float* %240, align 4, !tbaa !161
  %242 = insertelement <64 x float> undef, float %241, i32 0
  %243 = shufflevector <64 x float> %242, <64 x float> undef, <64 x i32> zeroinitializer
  %244 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %243, <64 x float> %86, <64 x float> %238)
  %245 = or i64 %125, 20
  %246 = getelementptr inbounds float, float* %4, i64 %245
  %247 = load float, float* %246, align 4, !tbaa !161
  %248 = insertelement <64 x float> undef, float %247, i32 0
  %249 = shufflevector <64 x float> %248, <64 x float> undef, <64 x i32> zeroinitializer
  %250 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %249, <64 x float> %89, <64 x float> %244)
  %251 = or i64 %125, 21
  %252 = getelementptr inbounds float, float* %4, i64 %251
  %253 = load float, float* %252, align 4, !tbaa !161
  %254 = insertelement <64 x float> undef, float %253, i32 0
  %255 = shufflevector <64 x float> %254, <64 x float> undef, <64 x i32> zeroinitializer
  %256 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %255, <64 x float> %92, <64 x float> %250)
  %257 = or i64 %125, 22
  %258 = getelementptr inbounds float, float* %4, i64 %257
  %259 = load float, float* %258, align 4, !tbaa !161
  %260 = insertelement <64 x float> undef, float %259, i32 0
  %261 = shufflevector <64 x float> %260, <64 x float> undef, <64 x i32> zeroinitializer
  %262 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %261, <64 x float> %95, <64 x float> %256)
  %263 = or i64 %125, 23
  %264 = getelementptr inbounds float, float* %4, i64 %263
  %265 = load float, float* %264, align 4, !tbaa !161
  %266 = insertelement <64 x float> undef, float %265, i32 0
  %267 = shufflevector <64 x float> %266, <64 x float> undef, <64 x i32> zeroinitializer
  %268 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %267, <64 x float> %98, <64 x float> %262)
  %269 = or i64 %125, 24
  %270 = getelementptr inbounds float, float* %4, i64 %269
  %271 = load float, float* %270, align 4, !tbaa !161
  %272 = insertelement <64 x float> undef, float %271, i32 0
  %273 = shufflevector <64 x float> %272, <64 x float> undef, <64 x i32> zeroinitializer
  %274 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %273, <64 x float> %101, <64 x float> %268)
  %275 = or i64 %125, 25
  %276 = getelementptr inbounds float, float* %4, i64 %275
  %277 = load float, float* %276, align 4, !tbaa !161
  %278 = insertelement <64 x float> undef, float %277, i32 0
  %279 = shufflevector <64 x float> %278, <64 x float> undef, <64 x i32> zeroinitializer
  %280 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %279, <64 x float> %104, <64 x float> %274)
  %281 = or i64 %125, 26
  %282 = getelementptr inbounds float, float* %4, i64 %281
  %283 = load float, float* %282, align 4, !tbaa !161
  %284 = insertelement <64 x float> undef, float %283, i32 0
  %285 = shufflevector <64 x float> %284, <64 x float> undef, <64 x i32> zeroinitializer
  %286 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %285, <64 x float> %107, <64 x float> %280)
  %287 = or i64 %125, 27
  %288 = getelementptr inbounds float, float* %4, i64 %287
  %289 = load float, float* %288, align 4, !tbaa !161
  %290 = insertelement <64 x float> undef, float %289, i32 0
  %291 = shufflevector <64 x float> %290, <64 x float> undef, <64 x i32> zeroinitializer
  %292 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %291, <64 x float> %110, <64 x float> %286)
  %293 = or i64 %125, 28
  %294 = getelementptr inbounds float, float* %4, i64 %293
  %295 = load float, float* %294, align 4, !tbaa !161
  %296 = insertelement <64 x float> undef, float %295, i32 0
  %297 = shufflevector <64 x float> %296, <64 x float> undef, <64 x i32> zeroinitializer
  %298 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %297, <64 x float> %113, <64 x float> %292)
  %299 = or i64 %125, 29
  %300 = getelementptr inbounds float, float* %4, i64 %299
  %301 = load float, float* %300, align 4, !tbaa !161
  %302 = insertelement <64 x float> undef, float %301, i32 0
  %303 = shufflevector <64 x float> %302, <64 x float> undef, <64 x i32> zeroinitializer
  %304 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %303, <64 x float> %116, <64 x float> %298)
  %305 = or i64 %125, 30
  %306 = getelementptr inbounds float, float* %4, i64 %305
  %307 = load float, float* %306, align 4, !tbaa !161
  %308 = insertelement <64 x float> undef, float %307, i32 0
  %309 = shufflevector <64 x float> %308, <64 x float> undef, <64 x i32> zeroinitializer
  %310 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %309, <64 x float> %119, <64 x float> %304)
  %311 = or i64 %125, 31
  %312 = getelementptr inbounds float, float* %4, i64 %311
  %313 = load float, float* %312, align 4, !tbaa !161
  %314 = insertelement <64 x float> undef, float %313, i32 0
  %315 = shufflevector <64 x float> %314, <64 x float> undef, <64 x i32> zeroinitializer
  %316 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %315, <64 x float> %122, <64 x float> %310)
  %317 = trunc i64 %indvars.iv to i32
  %318 = shl nsw i32 %317, 6
  %319 = fadd <64 x float> %316, %26
  %320 = fcmp olt <64 x float> %319, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %321 = select <64 x i1> %320, <64 x float> %319, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %322 = fcmp ogt <64 x float> %321, zeroinitializer
  %323 = select <64 x i1> %322, <64 x float> %321, <64 x float> zeroinitializer
  %324 = sext i32 %318 to i64
  %325 = getelementptr inbounds float, float* %10, i64 %324
  %326 = bitcast float* %325 to <64 x float>*
  store <64 x float> %323, <64 x float>* %326, align 128, !tbaa !175
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %327 = add nsw i32 %123, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %wide.trip.count
  br i1 %exitcond.not, label %for_end, label %for_begin1.preheader, !prof !51

for_end:                                          ; preds = %for_begin1.preheader, %entry
  ret i32 0
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare <64 x float> @llvm.fmuladd.v64f32(<64 x float>, <64 x float>, <64 x float>) #4

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_5(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.42, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !178
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !192
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !194
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !197
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.43, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.44, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.45, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.46, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !199
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !213
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 14
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.47, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !215
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 14
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.48, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !218
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 512
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.49, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !220
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 100352, i32 7168, i32 512, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.50, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !232
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 3
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.51, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !246
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 3
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.52, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !248
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 512
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.53, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !251
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 1
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.54, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !253
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 1536, i32 512, i32 1, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([197 x i8], [197 x i8]* @.str.55, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !265
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 512
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.56, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !279
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !293
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !307
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 14
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.57, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !309
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 14
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.58, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !312
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 512
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.59, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !314
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 100352, i32 7168, i32 512, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.60, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_5_compute_(i8* %31, i8* %39, i8* %45, i8* %51, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_5_compute_(i8* noalias nocapture readonly align 128 %0, i8* noalias nocapture readonly align 128 %1, i8* noalias nocapture readonly align 128 %2, i8* noalias nocapture align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 524288, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %for_end38, %if_end, %entry
  ret i32 -1

if_end:                                           ; preds = %entry
  %8 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %9 = tail call i8* %8(i32 1, i32 %4, i64 401408, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %9, i64 128) ]
  %10 = icmp eq i8* %9, null
  br i1 %10, label %if_then, label %for_begin4.preheader, !prof !5

for_begin4.preheader:                             ; preds = %if_end, %for_end6
  %indvar = phi i64 [ %indvar.next, %for_end6 ], [ 0, %if_end ]
  %11 = phi i32 [ %47, %for_end6 ], [ 0, %if_end ]
  %12 = shl nuw nsw i64 %indvar, 15
  %13 = mul nuw nsw i64 %indvar, 28672
  %.off = add nsw i32 %11, -1
  %14 = icmp ult i32 %.off, 14
  br i1 %14, label %for_body8.us61.preheader, label %for_begin7.preheader.preheader

for_begin7.preheader.preheader:                   ; preds = %for_begin4.preheader
  %scevgep104 = getelementptr i8, i8* %6, i64 %12
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(32768) %scevgep104, i8 0, i64 32768, i1 false)
  br label %for_end6

for_body8.us61.preheader:                         ; preds = %for_begin4.preheader
  %scevgep109 = getelementptr i8, i8* %6, i64 %12
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109, i8 0, i64 2048, i1 false)
  %15 = or i64 %12, 2048
  %scevgep109.1 = getelementptr i8, i8* %6, i64 %15
  %16 = add nsw i64 %13, -28672
  %scevgep110.1 = getelementptr i8, i8* %0, i64 %16
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.1, i8* nonnull align 128 dereferenceable(2048) %scevgep110.1, i64 2048, i1 false)
  %17 = or i64 %12, 4096
  %scevgep109.2 = getelementptr i8, i8* %6, i64 %17
  %18 = add nsw i64 %13, -26624
  %scevgep110.2 = getelementptr i8, i8* %0, i64 %18
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.2, i8* nonnull align 128 dereferenceable(2048) %scevgep110.2, i64 2048, i1 false)
  %19 = or i64 %12, 6144
  %scevgep109.3 = getelementptr i8, i8* %6, i64 %19
  %20 = add nsw i64 %13, -24576
  %scevgep110.3 = getelementptr i8, i8* %0, i64 %20
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.3, i8* nonnull align 128 dereferenceable(2048) %scevgep110.3, i64 2048, i1 false)
  %21 = or i64 %12, 8192
  %scevgep109.4 = getelementptr i8, i8* %6, i64 %21
  %22 = add nsw i64 %13, -22528
  %scevgep110.4 = getelementptr i8, i8* %0, i64 %22
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.4, i8* nonnull align 128 dereferenceable(2048) %scevgep110.4, i64 2048, i1 false)
  %23 = or i64 %12, 10240
  %scevgep109.5 = getelementptr i8, i8* %6, i64 %23
  %24 = add nsw i64 %13, -20480
  %scevgep110.5 = getelementptr i8, i8* %0, i64 %24
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.5, i8* nonnull align 128 dereferenceable(2048) %scevgep110.5, i64 2048, i1 false)
  %25 = or i64 %12, 12288
  %scevgep109.6 = getelementptr i8, i8* %6, i64 %25
  %26 = add nsw i64 %13, -18432
  %scevgep110.6 = getelementptr i8, i8* %0, i64 %26
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.6, i8* nonnull align 128 dereferenceable(2048) %scevgep110.6, i64 2048, i1 false)
  %27 = or i64 %12, 14336
  %scevgep109.7 = getelementptr i8, i8* %6, i64 %27
  %28 = add nsw i64 %13, -16384
  %scevgep110.7 = getelementptr i8, i8* %0, i64 %28
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.7, i8* nonnull align 128 dereferenceable(2048) %scevgep110.7, i64 2048, i1 false)
  %29 = or i64 %12, 16384
  %scevgep109.8 = getelementptr i8, i8* %6, i64 %29
  %30 = add nsw i64 %13, -14336
  %scevgep110.8 = getelementptr i8, i8* %0, i64 %30
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.8, i8* nonnull align 128 dereferenceable(2048) %scevgep110.8, i64 2048, i1 false)
  %31 = or i64 %12, 18432
  %scevgep109.9 = getelementptr i8, i8* %6, i64 %31
  %32 = add nsw i64 %13, -12288
  %scevgep110.9 = getelementptr i8, i8* %0, i64 %32
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.9, i8* nonnull align 128 dereferenceable(2048) %scevgep110.9, i64 2048, i1 false)
  %33 = or i64 %12, 20480
  %scevgep109.10 = getelementptr i8, i8* %6, i64 %33
  %34 = add nsw i64 %13, -10240
  %scevgep110.10 = getelementptr i8, i8* %0, i64 %34
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.10, i8* nonnull align 128 dereferenceable(2048) %scevgep110.10, i64 2048, i1 false)
  %35 = or i64 %12, 22528
  %scevgep109.11 = getelementptr i8, i8* %6, i64 %35
  %36 = add nsw i64 %13, -8192
  %scevgep110.11 = getelementptr i8, i8* %0, i64 %36
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.11, i8* nonnull align 128 dereferenceable(2048) %scevgep110.11, i64 2048, i1 false)
  %37 = or i64 %12, 24576
  %scevgep109.12 = getelementptr i8, i8* %6, i64 %37
  %38 = add nsw i64 %13, -6144
  %scevgep110.12 = getelementptr i8, i8* %0, i64 %38
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.12, i8* nonnull align 128 dereferenceable(2048) %scevgep110.12, i64 2048, i1 false)
  %39 = or i64 %12, 26624
  %scevgep109.13 = getelementptr i8, i8* %6, i64 %39
  %40 = add nsw i64 %13, -4096
  %scevgep110.13 = getelementptr i8, i8* %0, i64 %40
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.13, i8* nonnull align 128 dereferenceable(2048) %scevgep110.13, i64 2048, i1 false)
  %41 = or i64 %12, 28672
  %scevgep109.14 = getelementptr i8, i8* %6, i64 %41
  %42 = add nsw i64 %13, -2048
  %scevgep110.14 = getelementptr i8, i8* %0, i64 %42
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.14, i8* nonnull align 128 dereferenceable(2048) %scevgep110.14, i64 2048, i1 false)
  %43 = or i64 %12, 30720
  %scevgep109.15 = getelementptr i8, i8* %6, i64 %43
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep109.15, i8 0, i64 2048, i1 false)
  br label %for_end6

for_begin12.preheader:                            ; preds = %for_end6
  %44 = bitcast i8* %9 to float*
  %45 = bitcast i8* %6 to float*
  %46 = bitcast i8* %1 to float*
  br label %for_begin15.preheader

for_end6:                                         ; preds = %for_body8.us61.preheader, %for_begin7.preheader.preheader
  %47 = add nuw nsw i32 %11, 1
  %indvar.next = add nuw nsw i64 %indvar, 1
  %exitcond114.not = icmp eq i64 %indvar.next, 16
  br i1 %exitcond114.not, label %for_begin12.preheader, label %for_begin4.preheader, !prof !51

for_begin15.preheader:                            ; preds = %for_begin12.preheader, %for_end17
  %indvars.iv97 = phi i64 [ 0, %for_begin12.preheader ], [ %50, %for_end17 ]
  %48 = mul nuw nsw i64 %indvars.iv97, 7168
  %49 = shl i64 %indvars.iv97, 13
  %50 = add nuw nsw i64 %indvars.iv97, 1
  %51 = shl i64 %50, 13
  %52 = shl i64 %indvars.iv97, 13
  %53 = add i64 %52, 16384
  br label %for_begin18.preheader

for_begin27.preheader:                            ; preds = %for_end17
  %54 = bitcast i8* %2 to float*
  br label %for_begin30.preheader

for_begin18.preheader:                            ; preds = %for_begin15.preheader, %for_end20
  %indvars.iv94 = phi i64 [ 0, %for_begin15.preheader ], [ %indvars.iv.next95, %for_end20 ]
  %55 = shl nsw i64 %indvars.iv94, 9
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_begin18.preheader
  %index = phi i64 [ 0, %for_begin18.preheader ], [ %index.next, %vector.body ]
  %56 = add nuw nsw i64 %index, %55
  %57 = add nuw nsw i64 %56, %48
  %58 = getelementptr inbounds float, float* %44, i64 %57
  %59 = add nuw nsw i64 %56, %49
  %60 = getelementptr inbounds float, float* %45, i64 %59
  %61 = bitcast float* %60 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %61, align 16, !tbaa !326
  %62 = getelementptr inbounds float, float* %46, i64 %index
  %63 = bitcast float* %62 to <4 x float>*
  %wide.load115 = load <4 x float>, <4 x float>* %63, align 16, !tbaa !329
  %64 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load, <4 x float> %wide.load115, <4 x float> zeroinitializer)
  %65 = add nuw nsw i64 %59, 512
  %66 = getelementptr inbounds float, float* %45, i64 %65
  %67 = bitcast float* %66 to <4 x float>*
  %wide.load116 = load <4 x float>, <4 x float>* %67, align 16, !tbaa !326
  %68 = add nuw nsw i64 %index, 512
  %69 = getelementptr inbounds float, float* %46, i64 %68
  %70 = bitcast float* %69 to <4 x float>*
  %wide.load117 = load <4 x float>, <4 x float>* %70, align 16, !tbaa !329
  %71 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load116, <4 x float> %wide.load117, <4 x float> %64)
  %72 = add nuw nsw i64 %59, 1024
  %73 = getelementptr inbounds float, float* %45, i64 %72
  %74 = bitcast float* %73 to <4 x float>*
  %wide.load118 = load <4 x float>, <4 x float>* %74, align 16, !tbaa !326
  %75 = add nuw nsw i64 %index, 1024
  %76 = getelementptr inbounds float, float* %46, i64 %75
  %77 = bitcast float* %76 to <4 x float>*
  %wide.load119 = load <4 x float>, <4 x float>* %77, align 16, !tbaa !329
  %78 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load118, <4 x float> %wide.load119, <4 x float> %71)
  %79 = add nuw nsw i64 %56, %51
  %80 = getelementptr inbounds float, float* %45, i64 %79
  %81 = bitcast float* %80 to <4 x float>*
  %wide.load120 = load <4 x float>, <4 x float>* %81, align 16, !tbaa !326
  %82 = add nuw nsw i64 %index, 1536
  %83 = getelementptr inbounds float, float* %46, i64 %82
  %84 = bitcast float* %83 to <4 x float>*
  %wide.load121 = load <4 x float>, <4 x float>* %84, align 16, !tbaa !329
  %85 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load120, <4 x float> %wide.load121, <4 x float> %78)
  %86 = add nuw nsw i64 %79, 512
  %87 = getelementptr inbounds float, float* %45, i64 %86
  %88 = bitcast float* %87 to <4 x float>*
  %wide.load122 = load <4 x float>, <4 x float>* %88, align 16, !tbaa !326
  %89 = add nuw nsw i64 %index, 2048
  %90 = getelementptr inbounds float, float* %46, i64 %89
  %91 = bitcast float* %90 to <4 x float>*
  %wide.load123 = load <4 x float>, <4 x float>* %91, align 16, !tbaa !329
  %92 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load122, <4 x float> %wide.load123, <4 x float> %85)
  %93 = add nuw nsw i64 %79, 1024
  %94 = getelementptr inbounds float, float* %45, i64 %93
  %95 = bitcast float* %94 to <4 x float>*
  %wide.load124 = load <4 x float>, <4 x float>* %95, align 16, !tbaa !326
  %96 = add nuw nsw i64 %index, 2560
  %97 = getelementptr inbounds float, float* %46, i64 %96
  %98 = bitcast float* %97 to <4 x float>*
  %wide.load125 = load <4 x float>, <4 x float>* %98, align 16, !tbaa !329
  %99 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load124, <4 x float> %wide.load125, <4 x float> %92)
  %100 = add nuw nsw i64 %56, %53
  %101 = getelementptr inbounds float, float* %45, i64 %100
  %102 = bitcast float* %101 to <4 x float>*
  %wide.load126 = load <4 x float>, <4 x float>* %102, align 16, !tbaa !326
  %103 = add nuw nsw i64 %index, 3072
  %104 = getelementptr inbounds float, float* %46, i64 %103
  %105 = bitcast float* %104 to <4 x float>*
  %wide.load127 = load <4 x float>, <4 x float>* %105, align 16, !tbaa !329
  %106 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load126, <4 x float> %wide.load127, <4 x float> %99)
  %107 = add nuw nsw i64 %100, 512
  %108 = getelementptr inbounds float, float* %45, i64 %107
  %109 = bitcast float* %108 to <4 x float>*
  %wide.load128 = load <4 x float>, <4 x float>* %109, align 16, !tbaa !326
  %110 = add nuw nsw i64 %index, 3584
  %111 = getelementptr inbounds float, float* %46, i64 %110
  %112 = bitcast float* %111 to <4 x float>*
  %wide.load129 = load <4 x float>, <4 x float>* %112, align 16, !tbaa !329
  %113 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load128, <4 x float> %wide.load129, <4 x float> %106)
  %114 = add nuw nsw i64 %100, 1024
  %115 = getelementptr inbounds float, float* %45, i64 %114
  %116 = bitcast float* %115 to <4 x float>*
  %wide.load130 = load <4 x float>, <4 x float>* %116, align 16, !tbaa !326
  %117 = add nuw nsw i64 %index, 4096
  %118 = getelementptr inbounds float, float* %46, i64 %117
  %119 = bitcast float* %118 to <4 x float>*
  %wide.load131 = load <4 x float>, <4 x float>* %119, align 16, !tbaa !329
  %120 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load130, <4 x float> %wide.load131, <4 x float> %113)
  %121 = bitcast float* %58 to <4 x float>*
  store <4 x float> %120, <4 x float>* %121, align 16, !tbaa !332
  %index.next = add i64 %index, 4
  %122 = icmp eq i64 %index.next, 512
  br i1 %122, label %for_end20, label %vector.body, !prof !335, !llvm.loop !336

for_end17:                                        ; preds = %for_end20
  %exitcond99.not = icmp eq i64 %50, 14
  br i1 %exitcond99.not, label %for_begin27.preheader, label %for_begin15.preheader, !prof !51

for_end20:                                        ; preds = %vector.body
  %indvars.iv.next95 = add nuw nsw i64 %indvars.iv94, 1
  %exitcond96.not = icmp eq i64 %indvars.iv.next95, 14
  br i1 %exitcond96.not, label %for_end17, label %for_begin18.preheader, !prof !51

for_begin30.preheader:                            ; preds = %for_begin27.preheader, %for_end35.13
  %indvars.iv80 = phi i64 [ 0, %for_begin27.preheader ], [ %indvars.iv.next81, %for_end35.13 ]
  %123 = mul nuw nsw i64 %indvars.iv80, 7168
  br label %vector.body290

vector.body290:                                   ; preds = %vector.body290, %for_begin30.preheader
  %index292 = phi i64 [ 0, %for_begin30.preheader ], [ %index.next293.1, %vector.body290 ]
  %124 = add nuw nsw i64 %123, %index292
  %125 = getelementptr inbounds float, float* %54, i64 %index292
  %126 = bitcast float* %125 to <4 x float>*
  %wide.load296 = load <4 x float>, <4 x float>* %126, align 64, !tbaa !338
  %127 = getelementptr inbounds float, float* %125, i64 4
  %128 = bitcast float* %127 to <4 x float>*
  %wide.load297 = load <4 x float>, <4 x float>* %128, align 16, !tbaa !338
  %129 = getelementptr inbounds float, float* %44, i64 %124
  %130 = bitcast float* %129 to <4 x float>*
  %wide.load298 = load <4 x float>, <4 x float>* %130, align 64, !tbaa !332
  %131 = getelementptr inbounds float, float* %129, i64 4
  %132 = bitcast float* %131 to <4 x float>*
  %wide.load299 = load <4 x float>, <4 x float>* %132, align 16, !tbaa !332
  %133 = fadd <4 x float> %wide.load296, %wide.load298
  %134 = fadd <4 x float> %wide.load297, %wide.load299
  %135 = bitcast float* %129 to <4 x float>*
  store <4 x float> %133, <4 x float>* %135, align 64, !tbaa !332
  %136 = bitcast float* %131 to <4 x float>*
  store <4 x float> %134, <4 x float>* %136, align 16, !tbaa !332
  %index.next293 = or i64 %index292, 8
  %137 = add nuw nsw i64 %123, %index.next293
  %138 = getelementptr inbounds float, float* %54, i64 %index.next293
  %139 = bitcast float* %138 to <4 x float>*
  %wide.load296.1 = load <4 x float>, <4 x float>* %139, align 32, !tbaa !338
  %140 = getelementptr inbounds float, float* %138, i64 4
  %141 = bitcast float* %140 to <4 x float>*
  %wide.load297.1 = load <4 x float>, <4 x float>* %141, align 16, !tbaa !338
  %142 = getelementptr inbounds float, float* %44, i64 %137
  %143 = bitcast float* %142 to <4 x float>*
  %wide.load298.1 = load <4 x float>, <4 x float>* %143, align 32, !tbaa !332
  %144 = getelementptr inbounds float, float* %142, i64 4
  %145 = bitcast float* %144 to <4 x float>*
  %wide.load299.1 = load <4 x float>, <4 x float>* %145, align 16, !tbaa !332
  %146 = fadd <4 x float> %wide.load296.1, %wide.load298.1
  %147 = fadd <4 x float> %wide.load297.1, %wide.load299.1
  %148 = bitcast float* %142 to <4 x float>*
  store <4 x float> %146, <4 x float>* %148, align 32, !tbaa !332
  %149 = bitcast float* %144 to <4 x float>*
  store <4 x float> %147, <4 x float>* %149, align 16, !tbaa !332
  %index.next293.1 = add nuw nsw i64 %index292, 16
  %150 = icmp eq i64 %index.next293.1, 512
  br i1 %150, label %for_end35, label %vector.body290, !prof !341, !llvm.loop !342

for_begin36.preheader:                            ; preds = %for_end35.13
  %151 = bitcast i8* %3 to float*
  br label %for_begin39.preheader

for_end35:                                        ; preds = %vector.body290
  %152 = or i64 %123, 512
  br label %vector.body278

vector.body278:                                   ; preds = %vector.body278, %for_end35
  %index280 = phi i64 [ 0, %for_end35 ], [ %index.next281.1, %vector.body278 ]
  %153 = add nuw nsw i64 %152, %index280
  %154 = getelementptr inbounds float, float* %54, i64 %index280
  %155 = bitcast float* %154 to <4 x float>*
  %wide.load284 = load <4 x float>, <4 x float>* %155, align 64, !tbaa !338
  %156 = getelementptr inbounds float, float* %154, i64 4
  %157 = bitcast float* %156 to <4 x float>*
  %wide.load285 = load <4 x float>, <4 x float>* %157, align 16, !tbaa !338
  %158 = getelementptr inbounds float, float* %44, i64 %153
  %159 = bitcast float* %158 to <4 x float>*
  %wide.load286 = load <4 x float>, <4 x float>* %159, align 64, !tbaa !332
  %160 = getelementptr inbounds float, float* %158, i64 4
  %161 = bitcast float* %160 to <4 x float>*
  %wide.load287 = load <4 x float>, <4 x float>* %161, align 16, !tbaa !332
  %162 = fadd <4 x float> %wide.load284, %wide.load286
  %163 = fadd <4 x float> %wide.load285, %wide.load287
  %164 = bitcast float* %158 to <4 x float>*
  store <4 x float> %162, <4 x float>* %164, align 64, !tbaa !332
  %165 = bitcast float* %160 to <4 x float>*
  store <4 x float> %163, <4 x float>* %165, align 16, !tbaa !332
  %index.next281 = or i64 %index280, 8
  %166 = add nuw nsw i64 %152, %index.next281
  %167 = getelementptr inbounds float, float* %54, i64 %index.next281
  %168 = bitcast float* %167 to <4 x float>*
  %wide.load284.1 = load <4 x float>, <4 x float>* %168, align 32, !tbaa !338
  %169 = getelementptr inbounds float, float* %167, i64 4
  %170 = bitcast float* %169 to <4 x float>*
  %wide.load285.1 = load <4 x float>, <4 x float>* %170, align 16, !tbaa !338
  %171 = getelementptr inbounds float, float* %44, i64 %166
  %172 = bitcast float* %171 to <4 x float>*
  %wide.load286.1 = load <4 x float>, <4 x float>* %172, align 32, !tbaa !332
  %173 = getelementptr inbounds float, float* %171, i64 4
  %174 = bitcast float* %173 to <4 x float>*
  %wide.load287.1 = load <4 x float>, <4 x float>* %174, align 16, !tbaa !332
  %175 = fadd <4 x float> %wide.load284.1, %wide.load286.1
  %176 = fadd <4 x float> %wide.load285.1, %wide.load287.1
  %177 = bitcast float* %171 to <4 x float>*
  store <4 x float> %175, <4 x float>* %177, align 32, !tbaa !332
  %178 = bitcast float* %173 to <4 x float>*
  store <4 x float> %176, <4 x float>* %178, align 16, !tbaa !332
  %index.next281.1 = add nuw nsw i64 %index280, 16
  %179 = icmp eq i64 %index.next281.1, 512
  br i1 %179, label %for_end35.1, label %vector.body278, !prof !341, !llvm.loop !343

for_begin39.preheader:                            ; preds = %for_begin36.preheader, %for_end44.13
  %indvars.iv71 = phi i64 [ 0, %for_begin36.preheader ], [ %indvars.iv.next72, %for_end44.13 ]
  %180 = mul nuw nsw i64 %indvars.iv71, 7168
  br label %vector.body432

vector.body432:                                   ; preds = %vector.body432, %for_begin39.preheader
  %index434 = phi i64 [ 0, %for_begin39.preheader ], [ %index.next435, %vector.body432 ]
  %181 = add nuw nsw i64 %180, %index434
  %182 = getelementptr inbounds float, float* %44, i64 %181
  %183 = bitcast float* %182 to <4 x float>*
  %wide.load438 = load <4 x float>, <4 x float>* %183, align 32, !tbaa !332
  %184 = getelementptr inbounds float, float* %182, i64 4
  %185 = bitcast float* %184 to <4 x float>*
  %wide.load439 = load <4 x float>, <4 x float>* %185, align 16, !tbaa !332
  %186 = fcmp olt <4 x float> %wide.load438, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %187 = fcmp olt <4 x float> %wide.load439, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %188 = select <4 x i1> %186, <4 x float> %wide.load438, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %189 = select <4 x i1> %187, <4 x float> %wide.load439, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %190 = fcmp ogt <4 x float> %188, zeroinitializer
  %191 = fcmp ogt <4 x float> %189, zeroinitializer
  %192 = select <4 x i1> %190, <4 x float> %188, <4 x float> zeroinitializer
  %193 = select <4 x i1> %191, <4 x float> %189, <4 x float> zeroinitializer
  %194 = getelementptr inbounds float, float* %151, i64 %181
  %195 = bitcast float* %194 to <4 x float>*
  store <4 x float> %192, <4 x float>* %195, align 32, !tbaa !344
  %196 = getelementptr inbounds float, float* %194, i64 4
  %197 = bitcast float* %196 to <4 x float>*
  store <4 x float> %193, <4 x float>* %197, align 16, !tbaa !344
  %index.next435 = add i64 %index434, 8
  %198 = icmp eq i64 %index.next435, 512
  br i1 %198, label %for_end44, label %vector.body432, !prof !341, !llvm.loop !347

for_end38:                                        ; preds = %for_end44.13
  %199 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %200 = tail call i32 %199(i32 1, i32 %4, i8* nonnull %9)
  %.not = icmp eq i32 %200, 0
  br i1 %.not, label %if_end46, label %if_then, !prof !51

for_end44:                                        ; preds = %vector.body432
  %201 = or i64 %180, 512
  br label %vector.body422

vector.body422:                                   ; preds = %vector.body422, %for_end44
  %index424 = phi i64 [ 0, %for_end44 ], [ %index.next425, %vector.body422 ]
  %202 = add nuw nsw i64 %201, %index424
  %203 = getelementptr inbounds float, float* %44, i64 %202
  %204 = bitcast float* %203 to <4 x float>*
  %wide.load428 = load <4 x float>, <4 x float>* %204, align 32, !tbaa !332
  %205 = getelementptr inbounds float, float* %203, i64 4
  %206 = bitcast float* %205 to <4 x float>*
  %wide.load429 = load <4 x float>, <4 x float>* %206, align 16, !tbaa !332
  %207 = fcmp olt <4 x float> %wide.load428, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %208 = fcmp olt <4 x float> %wide.load429, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %209 = select <4 x i1> %207, <4 x float> %wide.load428, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %210 = select <4 x i1> %208, <4 x float> %wide.load429, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %211 = fcmp ogt <4 x float> %209, zeroinitializer
  %212 = fcmp ogt <4 x float> %210, zeroinitializer
  %213 = select <4 x i1> %211, <4 x float> %209, <4 x float> zeroinitializer
  %214 = select <4 x i1> %212, <4 x float> %210, <4 x float> zeroinitializer
  %215 = getelementptr inbounds float, float* %151, i64 %202
  %216 = bitcast float* %215 to <4 x float>*
  store <4 x float> %213, <4 x float>* %216, align 32, !tbaa !344
  %217 = getelementptr inbounds float, float* %215, i64 4
  %218 = bitcast float* %217 to <4 x float>*
  store <4 x float> %214, <4 x float>* %218, align 16, !tbaa !344
  %index.next425 = add i64 %index424, 8
  %219 = icmp eq i64 %index.next425, 512
  br i1 %219, label %for_end44.1, label %vector.body422, !prof !341, !llvm.loop !348

if_end46:                                         ; preds = %for_end38
  %220 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %221 = tail call i32 %220(i32 1, i32 %4, i8* nonnull %6)
  %.not57 = icmp ne i32 %221, 0
  %spec.select = sext i1 %.not57 to i32
  ret i32 %spec.select

for_end44.1:                                      ; preds = %vector.body422
  %222 = add nuw nsw i64 %180, 1024
  br label %vector.body412

vector.body412:                                   ; preds = %vector.body412, %for_end44.1
  %index414 = phi i64 [ 0, %for_end44.1 ], [ %index.next415, %vector.body412 ]
  %223 = add nuw nsw i64 %222, %index414
  %224 = getelementptr inbounds float, float* %44, i64 %223
  %225 = bitcast float* %224 to <4 x float>*
  %wide.load418 = load <4 x float>, <4 x float>* %225, align 32, !tbaa !332
  %226 = getelementptr inbounds float, float* %224, i64 4
  %227 = bitcast float* %226 to <4 x float>*
  %wide.load419 = load <4 x float>, <4 x float>* %227, align 16, !tbaa !332
  %228 = fcmp olt <4 x float> %wide.load418, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %229 = fcmp olt <4 x float> %wide.load419, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %230 = select <4 x i1> %228, <4 x float> %wide.load418, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %231 = select <4 x i1> %229, <4 x float> %wide.load419, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %232 = fcmp ogt <4 x float> %230, zeroinitializer
  %233 = fcmp ogt <4 x float> %231, zeroinitializer
  %234 = select <4 x i1> %232, <4 x float> %230, <4 x float> zeroinitializer
  %235 = select <4 x i1> %233, <4 x float> %231, <4 x float> zeroinitializer
  %236 = getelementptr inbounds float, float* %151, i64 %223
  %237 = bitcast float* %236 to <4 x float>*
  store <4 x float> %234, <4 x float>* %237, align 32, !tbaa !344
  %238 = getelementptr inbounds float, float* %236, i64 4
  %239 = bitcast float* %238 to <4 x float>*
  store <4 x float> %235, <4 x float>* %239, align 16, !tbaa !344
  %index.next415 = add i64 %index414, 8
  %240 = icmp eq i64 %index.next415, 512
  br i1 %240, label %for_end44.2, label %vector.body412, !prof !341, !llvm.loop !349

for_end44.2:                                      ; preds = %vector.body412
  %241 = add nuw nsw i64 %180, 1536
  br label %vector.body402

vector.body402:                                   ; preds = %vector.body402, %for_end44.2
  %index404 = phi i64 [ 0, %for_end44.2 ], [ %index.next405, %vector.body402 ]
  %242 = add nuw nsw i64 %241, %index404
  %243 = getelementptr inbounds float, float* %44, i64 %242
  %244 = bitcast float* %243 to <4 x float>*
  %wide.load408 = load <4 x float>, <4 x float>* %244, align 32, !tbaa !332
  %245 = getelementptr inbounds float, float* %243, i64 4
  %246 = bitcast float* %245 to <4 x float>*
  %wide.load409 = load <4 x float>, <4 x float>* %246, align 16, !tbaa !332
  %247 = fcmp olt <4 x float> %wide.load408, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %248 = fcmp olt <4 x float> %wide.load409, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %249 = select <4 x i1> %247, <4 x float> %wide.load408, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %250 = select <4 x i1> %248, <4 x float> %wide.load409, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %251 = fcmp ogt <4 x float> %249, zeroinitializer
  %252 = fcmp ogt <4 x float> %250, zeroinitializer
  %253 = select <4 x i1> %251, <4 x float> %249, <4 x float> zeroinitializer
  %254 = select <4 x i1> %252, <4 x float> %250, <4 x float> zeroinitializer
  %255 = getelementptr inbounds float, float* %151, i64 %242
  %256 = bitcast float* %255 to <4 x float>*
  store <4 x float> %253, <4 x float>* %256, align 32, !tbaa !344
  %257 = getelementptr inbounds float, float* %255, i64 4
  %258 = bitcast float* %257 to <4 x float>*
  store <4 x float> %254, <4 x float>* %258, align 16, !tbaa !344
  %index.next405 = add i64 %index404, 8
  %259 = icmp eq i64 %index.next405, 512
  br i1 %259, label %for_end44.3, label %vector.body402, !prof !341, !llvm.loop !350

for_end44.3:                                      ; preds = %vector.body402
  %260 = add nuw nsw i64 %180, 2048
  br label %vector.body392

vector.body392:                                   ; preds = %vector.body392, %for_end44.3
  %index394 = phi i64 [ 0, %for_end44.3 ], [ %index.next395, %vector.body392 ]
  %261 = add nuw nsw i64 %260, %index394
  %262 = getelementptr inbounds float, float* %44, i64 %261
  %263 = bitcast float* %262 to <4 x float>*
  %wide.load398 = load <4 x float>, <4 x float>* %263, align 32, !tbaa !332
  %264 = getelementptr inbounds float, float* %262, i64 4
  %265 = bitcast float* %264 to <4 x float>*
  %wide.load399 = load <4 x float>, <4 x float>* %265, align 16, !tbaa !332
  %266 = fcmp olt <4 x float> %wide.load398, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %267 = fcmp olt <4 x float> %wide.load399, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %268 = select <4 x i1> %266, <4 x float> %wide.load398, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %269 = select <4 x i1> %267, <4 x float> %wide.load399, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %270 = fcmp ogt <4 x float> %268, zeroinitializer
  %271 = fcmp ogt <4 x float> %269, zeroinitializer
  %272 = select <4 x i1> %270, <4 x float> %268, <4 x float> zeroinitializer
  %273 = select <4 x i1> %271, <4 x float> %269, <4 x float> zeroinitializer
  %274 = getelementptr inbounds float, float* %151, i64 %261
  %275 = bitcast float* %274 to <4 x float>*
  store <4 x float> %272, <4 x float>* %275, align 32, !tbaa !344
  %276 = getelementptr inbounds float, float* %274, i64 4
  %277 = bitcast float* %276 to <4 x float>*
  store <4 x float> %273, <4 x float>* %277, align 16, !tbaa !344
  %index.next395 = add i64 %index394, 8
  %278 = icmp eq i64 %index.next395, 512
  br i1 %278, label %for_end44.4, label %vector.body392, !prof !341, !llvm.loop !351

for_end44.4:                                      ; preds = %vector.body392
  %279 = add nuw nsw i64 %180, 2560
  br label %vector.body382

vector.body382:                                   ; preds = %vector.body382, %for_end44.4
  %index384 = phi i64 [ 0, %for_end44.4 ], [ %index.next385, %vector.body382 ]
  %280 = add nuw nsw i64 %279, %index384
  %281 = getelementptr inbounds float, float* %44, i64 %280
  %282 = bitcast float* %281 to <4 x float>*
  %wide.load388 = load <4 x float>, <4 x float>* %282, align 32, !tbaa !332
  %283 = getelementptr inbounds float, float* %281, i64 4
  %284 = bitcast float* %283 to <4 x float>*
  %wide.load389 = load <4 x float>, <4 x float>* %284, align 16, !tbaa !332
  %285 = fcmp olt <4 x float> %wide.load388, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %286 = fcmp olt <4 x float> %wide.load389, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %287 = select <4 x i1> %285, <4 x float> %wide.load388, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %288 = select <4 x i1> %286, <4 x float> %wide.load389, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %289 = fcmp ogt <4 x float> %287, zeroinitializer
  %290 = fcmp ogt <4 x float> %288, zeroinitializer
  %291 = select <4 x i1> %289, <4 x float> %287, <4 x float> zeroinitializer
  %292 = select <4 x i1> %290, <4 x float> %288, <4 x float> zeroinitializer
  %293 = getelementptr inbounds float, float* %151, i64 %280
  %294 = bitcast float* %293 to <4 x float>*
  store <4 x float> %291, <4 x float>* %294, align 32, !tbaa !344
  %295 = getelementptr inbounds float, float* %293, i64 4
  %296 = bitcast float* %295 to <4 x float>*
  store <4 x float> %292, <4 x float>* %296, align 16, !tbaa !344
  %index.next385 = add i64 %index384, 8
  %297 = icmp eq i64 %index.next385, 512
  br i1 %297, label %for_end44.5, label %vector.body382, !prof !341, !llvm.loop !352

for_end44.5:                                      ; preds = %vector.body382
  %298 = add nuw nsw i64 %180, 3072
  br label %vector.body372

vector.body372:                                   ; preds = %vector.body372, %for_end44.5
  %index374 = phi i64 [ 0, %for_end44.5 ], [ %index.next375, %vector.body372 ]
  %299 = add nuw nsw i64 %298, %index374
  %300 = getelementptr inbounds float, float* %44, i64 %299
  %301 = bitcast float* %300 to <4 x float>*
  %wide.load378 = load <4 x float>, <4 x float>* %301, align 32, !tbaa !332
  %302 = getelementptr inbounds float, float* %300, i64 4
  %303 = bitcast float* %302 to <4 x float>*
  %wide.load379 = load <4 x float>, <4 x float>* %303, align 16, !tbaa !332
  %304 = fcmp olt <4 x float> %wide.load378, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %305 = fcmp olt <4 x float> %wide.load379, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %306 = select <4 x i1> %304, <4 x float> %wide.load378, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %307 = select <4 x i1> %305, <4 x float> %wide.load379, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %308 = fcmp ogt <4 x float> %306, zeroinitializer
  %309 = fcmp ogt <4 x float> %307, zeroinitializer
  %310 = select <4 x i1> %308, <4 x float> %306, <4 x float> zeroinitializer
  %311 = select <4 x i1> %309, <4 x float> %307, <4 x float> zeroinitializer
  %312 = getelementptr inbounds float, float* %151, i64 %299
  %313 = bitcast float* %312 to <4 x float>*
  store <4 x float> %310, <4 x float>* %313, align 32, !tbaa !344
  %314 = getelementptr inbounds float, float* %312, i64 4
  %315 = bitcast float* %314 to <4 x float>*
  store <4 x float> %311, <4 x float>* %315, align 16, !tbaa !344
  %index.next375 = add i64 %index374, 8
  %316 = icmp eq i64 %index.next375, 512
  br i1 %316, label %for_end44.6, label %vector.body372, !prof !341, !llvm.loop !353

for_end44.6:                                      ; preds = %vector.body372
  %317 = add nuw nsw i64 %180, 3584
  br label %vector.body362

vector.body362:                                   ; preds = %vector.body362, %for_end44.6
  %index364 = phi i64 [ 0, %for_end44.6 ], [ %index.next365, %vector.body362 ]
  %318 = add nuw nsw i64 %317, %index364
  %319 = getelementptr inbounds float, float* %44, i64 %318
  %320 = bitcast float* %319 to <4 x float>*
  %wide.load368 = load <4 x float>, <4 x float>* %320, align 32, !tbaa !332
  %321 = getelementptr inbounds float, float* %319, i64 4
  %322 = bitcast float* %321 to <4 x float>*
  %wide.load369 = load <4 x float>, <4 x float>* %322, align 16, !tbaa !332
  %323 = fcmp olt <4 x float> %wide.load368, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %324 = fcmp olt <4 x float> %wide.load369, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %325 = select <4 x i1> %323, <4 x float> %wide.load368, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %326 = select <4 x i1> %324, <4 x float> %wide.load369, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %327 = fcmp ogt <4 x float> %325, zeroinitializer
  %328 = fcmp ogt <4 x float> %326, zeroinitializer
  %329 = select <4 x i1> %327, <4 x float> %325, <4 x float> zeroinitializer
  %330 = select <4 x i1> %328, <4 x float> %326, <4 x float> zeroinitializer
  %331 = getelementptr inbounds float, float* %151, i64 %318
  %332 = bitcast float* %331 to <4 x float>*
  store <4 x float> %329, <4 x float>* %332, align 32, !tbaa !344
  %333 = getelementptr inbounds float, float* %331, i64 4
  %334 = bitcast float* %333 to <4 x float>*
  store <4 x float> %330, <4 x float>* %334, align 16, !tbaa !344
  %index.next365 = add i64 %index364, 8
  %335 = icmp eq i64 %index.next365, 512
  br i1 %335, label %for_end44.7, label %vector.body362, !prof !341, !llvm.loop !354

for_end44.7:                                      ; preds = %vector.body362
  %336 = add nuw nsw i64 %180, 4096
  br label %vector.body352

vector.body352:                                   ; preds = %vector.body352, %for_end44.7
  %index354 = phi i64 [ 0, %for_end44.7 ], [ %index.next355, %vector.body352 ]
  %337 = add nuw nsw i64 %336, %index354
  %338 = getelementptr inbounds float, float* %44, i64 %337
  %339 = bitcast float* %338 to <4 x float>*
  %wide.load358 = load <4 x float>, <4 x float>* %339, align 32, !tbaa !332
  %340 = getelementptr inbounds float, float* %338, i64 4
  %341 = bitcast float* %340 to <4 x float>*
  %wide.load359 = load <4 x float>, <4 x float>* %341, align 16, !tbaa !332
  %342 = fcmp olt <4 x float> %wide.load358, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %343 = fcmp olt <4 x float> %wide.load359, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %344 = select <4 x i1> %342, <4 x float> %wide.load358, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %345 = select <4 x i1> %343, <4 x float> %wide.load359, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %346 = fcmp ogt <4 x float> %344, zeroinitializer
  %347 = fcmp ogt <4 x float> %345, zeroinitializer
  %348 = select <4 x i1> %346, <4 x float> %344, <4 x float> zeroinitializer
  %349 = select <4 x i1> %347, <4 x float> %345, <4 x float> zeroinitializer
  %350 = getelementptr inbounds float, float* %151, i64 %337
  %351 = bitcast float* %350 to <4 x float>*
  store <4 x float> %348, <4 x float>* %351, align 32, !tbaa !344
  %352 = getelementptr inbounds float, float* %350, i64 4
  %353 = bitcast float* %352 to <4 x float>*
  store <4 x float> %349, <4 x float>* %353, align 16, !tbaa !344
  %index.next355 = add i64 %index354, 8
  %354 = icmp eq i64 %index.next355, 512
  br i1 %354, label %for_end44.8, label %vector.body352, !prof !341, !llvm.loop !355

for_end44.8:                                      ; preds = %vector.body352
  %355 = add nuw nsw i64 %180, 4608
  br label %vector.body342

vector.body342:                                   ; preds = %vector.body342, %for_end44.8
  %index344 = phi i64 [ 0, %for_end44.8 ], [ %index.next345, %vector.body342 ]
  %356 = add nuw nsw i64 %355, %index344
  %357 = getelementptr inbounds float, float* %44, i64 %356
  %358 = bitcast float* %357 to <4 x float>*
  %wide.load348 = load <4 x float>, <4 x float>* %358, align 32, !tbaa !332
  %359 = getelementptr inbounds float, float* %357, i64 4
  %360 = bitcast float* %359 to <4 x float>*
  %wide.load349 = load <4 x float>, <4 x float>* %360, align 16, !tbaa !332
  %361 = fcmp olt <4 x float> %wide.load348, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %362 = fcmp olt <4 x float> %wide.load349, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %363 = select <4 x i1> %361, <4 x float> %wide.load348, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %364 = select <4 x i1> %362, <4 x float> %wide.load349, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %365 = fcmp ogt <4 x float> %363, zeroinitializer
  %366 = fcmp ogt <4 x float> %364, zeroinitializer
  %367 = select <4 x i1> %365, <4 x float> %363, <4 x float> zeroinitializer
  %368 = select <4 x i1> %366, <4 x float> %364, <4 x float> zeroinitializer
  %369 = getelementptr inbounds float, float* %151, i64 %356
  %370 = bitcast float* %369 to <4 x float>*
  store <4 x float> %367, <4 x float>* %370, align 32, !tbaa !344
  %371 = getelementptr inbounds float, float* %369, i64 4
  %372 = bitcast float* %371 to <4 x float>*
  store <4 x float> %368, <4 x float>* %372, align 16, !tbaa !344
  %index.next345 = add i64 %index344, 8
  %373 = icmp eq i64 %index.next345, 512
  br i1 %373, label %for_end44.9, label %vector.body342, !prof !341, !llvm.loop !356

for_end44.9:                                      ; preds = %vector.body342
  %374 = add nuw nsw i64 %180, 5120
  br label %vector.body332

vector.body332:                                   ; preds = %vector.body332, %for_end44.9
  %index334 = phi i64 [ 0, %for_end44.9 ], [ %index.next335, %vector.body332 ]
  %375 = add nuw nsw i64 %374, %index334
  %376 = getelementptr inbounds float, float* %44, i64 %375
  %377 = bitcast float* %376 to <4 x float>*
  %wide.load338 = load <4 x float>, <4 x float>* %377, align 32, !tbaa !332
  %378 = getelementptr inbounds float, float* %376, i64 4
  %379 = bitcast float* %378 to <4 x float>*
  %wide.load339 = load <4 x float>, <4 x float>* %379, align 16, !tbaa !332
  %380 = fcmp olt <4 x float> %wide.load338, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %381 = fcmp olt <4 x float> %wide.load339, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %382 = select <4 x i1> %380, <4 x float> %wide.load338, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %383 = select <4 x i1> %381, <4 x float> %wide.load339, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %384 = fcmp ogt <4 x float> %382, zeroinitializer
  %385 = fcmp ogt <4 x float> %383, zeroinitializer
  %386 = select <4 x i1> %384, <4 x float> %382, <4 x float> zeroinitializer
  %387 = select <4 x i1> %385, <4 x float> %383, <4 x float> zeroinitializer
  %388 = getelementptr inbounds float, float* %151, i64 %375
  %389 = bitcast float* %388 to <4 x float>*
  store <4 x float> %386, <4 x float>* %389, align 32, !tbaa !344
  %390 = getelementptr inbounds float, float* %388, i64 4
  %391 = bitcast float* %390 to <4 x float>*
  store <4 x float> %387, <4 x float>* %391, align 16, !tbaa !344
  %index.next335 = add i64 %index334, 8
  %392 = icmp eq i64 %index.next335, 512
  br i1 %392, label %for_end44.10, label %vector.body332, !prof !341, !llvm.loop !357

for_end44.10:                                     ; preds = %vector.body332
  %393 = add nuw nsw i64 %180, 5632
  br label %vector.body322

vector.body322:                                   ; preds = %vector.body322, %for_end44.10
  %index324 = phi i64 [ 0, %for_end44.10 ], [ %index.next325, %vector.body322 ]
  %394 = add nuw nsw i64 %393, %index324
  %395 = getelementptr inbounds float, float* %44, i64 %394
  %396 = bitcast float* %395 to <4 x float>*
  %wide.load328 = load <4 x float>, <4 x float>* %396, align 32, !tbaa !332
  %397 = getelementptr inbounds float, float* %395, i64 4
  %398 = bitcast float* %397 to <4 x float>*
  %wide.load329 = load <4 x float>, <4 x float>* %398, align 16, !tbaa !332
  %399 = fcmp olt <4 x float> %wide.load328, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %400 = fcmp olt <4 x float> %wide.load329, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %401 = select <4 x i1> %399, <4 x float> %wide.load328, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %402 = select <4 x i1> %400, <4 x float> %wide.load329, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %403 = fcmp ogt <4 x float> %401, zeroinitializer
  %404 = fcmp ogt <4 x float> %402, zeroinitializer
  %405 = select <4 x i1> %403, <4 x float> %401, <4 x float> zeroinitializer
  %406 = select <4 x i1> %404, <4 x float> %402, <4 x float> zeroinitializer
  %407 = getelementptr inbounds float, float* %151, i64 %394
  %408 = bitcast float* %407 to <4 x float>*
  store <4 x float> %405, <4 x float>* %408, align 32, !tbaa !344
  %409 = getelementptr inbounds float, float* %407, i64 4
  %410 = bitcast float* %409 to <4 x float>*
  store <4 x float> %406, <4 x float>* %410, align 16, !tbaa !344
  %index.next325 = add i64 %index324, 8
  %411 = icmp eq i64 %index.next325, 512
  br i1 %411, label %for_end44.11, label %vector.body322, !prof !341, !llvm.loop !358

for_end44.11:                                     ; preds = %vector.body322
  %412 = add nuw nsw i64 %180, 6144
  br label %vector.body312

vector.body312:                                   ; preds = %vector.body312, %for_end44.11
  %index314 = phi i64 [ 0, %for_end44.11 ], [ %index.next315, %vector.body312 ]
  %413 = add nuw nsw i64 %412, %index314
  %414 = getelementptr inbounds float, float* %44, i64 %413
  %415 = bitcast float* %414 to <4 x float>*
  %wide.load318 = load <4 x float>, <4 x float>* %415, align 32, !tbaa !332
  %416 = getelementptr inbounds float, float* %414, i64 4
  %417 = bitcast float* %416 to <4 x float>*
  %wide.load319 = load <4 x float>, <4 x float>* %417, align 16, !tbaa !332
  %418 = fcmp olt <4 x float> %wide.load318, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %419 = fcmp olt <4 x float> %wide.load319, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %420 = select <4 x i1> %418, <4 x float> %wide.load318, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %421 = select <4 x i1> %419, <4 x float> %wide.load319, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %422 = fcmp ogt <4 x float> %420, zeroinitializer
  %423 = fcmp ogt <4 x float> %421, zeroinitializer
  %424 = select <4 x i1> %422, <4 x float> %420, <4 x float> zeroinitializer
  %425 = select <4 x i1> %423, <4 x float> %421, <4 x float> zeroinitializer
  %426 = getelementptr inbounds float, float* %151, i64 %413
  %427 = bitcast float* %426 to <4 x float>*
  store <4 x float> %424, <4 x float>* %427, align 32, !tbaa !344
  %428 = getelementptr inbounds float, float* %426, i64 4
  %429 = bitcast float* %428 to <4 x float>*
  store <4 x float> %425, <4 x float>* %429, align 16, !tbaa !344
  %index.next315 = add i64 %index314, 8
  %430 = icmp eq i64 %index.next315, 512
  br i1 %430, label %for_end44.12, label %vector.body312, !prof !341, !llvm.loop !359

for_end44.12:                                     ; preds = %vector.body312
  %431 = add nuw nsw i64 %180, 6656
  br label %vector.body302

vector.body302:                                   ; preds = %vector.body302, %for_end44.12
  %index304 = phi i64 [ 0, %for_end44.12 ], [ %index.next305, %vector.body302 ]
  %432 = add nuw nsw i64 %431, %index304
  %433 = getelementptr inbounds float, float* %44, i64 %432
  %434 = bitcast float* %433 to <4 x float>*
  %wide.load308 = load <4 x float>, <4 x float>* %434, align 32, !tbaa !332
  %435 = getelementptr inbounds float, float* %433, i64 4
  %436 = bitcast float* %435 to <4 x float>*
  %wide.load309 = load <4 x float>, <4 x float>* %436, align 16, !tbaa !332
  %437 = fcmp olt <4 x float> %wide.load308, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %438 = fcmp olt <4 x float> %wide.load309, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %439 = select <4 x i1> %437, <4 x float> %wide.load308, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %440 = select <4 x i1> %438, <4 x float> %wide.load309, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %441 = fcmp ogt <4 x float> %439, zeroinitializer
  %442 = fcmp ogt <4 x float> %440, zeroinitializer
  %443 = select <4 x i1> %441, <4 x float> %439, <4 x float> zeroinitializer
  %444 = select <4 x i1> %442, <4 x float> %440, <4 x float> zeroinitializer
  %445 = getelementptr inbounds float, float* %151, i64 %432
  %446 = bitcast float* %445 to <4 x float>*
  store <4 x float> %443, <4 x float>* %446, align 32, !tbaa !344
  %447 = getelementptr inbounds float, float* %445, i64 4
  %448 = bitcast float* %447 to <4 x float>*
  store <4 x float> %444, <4 x float>* %448, align 16, !tbaa !344
  %index.next305 = add i64 %index304, 8
  %449 = icmp eq i64 %index.next305, 512
  br i1 %449, label %for_end44.13, label %vector.body302, !prof !341, !llvm.loop !360

for_end44.13:                                     ; preds = %vector.body302
  %indvars.iv.next72 = add nuw nsw i64 %indvars.iv71, 1
  %exitcond73.not = icmp eq i64 %indvars.iv.next72, 14
  br i1 %exitcond73.not, label %for_end38, label %for_begin39.preheader, !prof !51

for_end35.1:                                      ; preds = %vector.body278
  %450 = add nuw nsw i64 %123, 1024
  br label %vector.body266

vector.body266:                                   ; preds = %vector.body266, %for_end35.1
  %index268 = phi i64 [ 0, %for_end35.1 ], [ %index.next269.1, %vector.body266 ]
  %451 = add nuw nsw i64 %450, %index268
  %452 = getelementptr inbounds float, float* %54, i64 %index268
  %453 = bitcast float* %452 to <4 x float>*
  %wide.load272 = load <4 x float>, <4 x float>* %453, align 64, !tbaa !338
  %454 = getelementptr inbounds float, float* %452, i64 4
  %455 = bitcast float* %454 to <4 x float>*
  %wide.load273 = load <4 x float>, <4 x float>* %455, align 16, !tbaa !338
  %456 = getelementptr inbounds float, float* %44, i64 %451
  %457 = bitcast float* %456 to <4 x float>*
  %wide.load274 = load <4 x float>, <4 x float>* %457, align 64, !tbaa !332
  %458 = getelementptr inbounds float, float* %456, i64 4
  %459 = bitcast float* %458 to <4 x float>*
  %wide.load275 = load <4 x float>, <4 x float>* %459, align 16, !tbaa !332
  %460 = fadd <4 x float> %wide.load272, %wide.load274
  %461 = fadd <4 x float> %wide.load273, %wide.load275
  %462 = bitcast float* %456 to <4 x float>*
  store <4 x float> %460, <4 x float>* %462, align 64, !tbaa !332
  %463 = bitcast float* %458 to <4 x float>*
  store <4 x float> %461, <4 x float>* %463, align 16, !tbaa !332
  %index.next269 = or i64 %index268, 8
  %464 = add nuw nsw i64 %450, %index.next269
  %465 = getelementptr inbounds float, float* %54, i64 %index.next269
  %466 = bitcast float* %465 to <4 x float>*
  %wide.load272.1 = load <4 x float>, <4 x float>* %466, align 32, !tbaa !338
  %467 = getelementptr inbounds float, float* %465, i64 4
  %468 = bitcast float* %467 to <4 x float>*
  %wide.load273.1 = load <4 x float>, <4 x float>* %468, align 16, !tbaa !338
  %469 = getelementptr inbounds float, float* %44, i64 %464
  %470 = bitcast float* %469 to <4 x float>*
  %wide.load274.1 = load <4 x float>, <4 x float>* %470, align 32, !tbaa !332
  %471 = getelementptr inbounds float, float* %469, i64 4
  %472 = bitcast float* %471 to <4 x float>*
  %wide.load275.1 = load <4 x float>, <4 x float>* %472, align 16, !tbaa !332
  %473 = fadd <4 x float> %wide.load272.1, %wide.load274.1
  %474 = fadd <4 x float> %wide.load273.1, %wide.load275.1
  %475 = bitcast float* %469 to <4 x float>*
  store <4 x float> %473, <4 x float>* %475, align 32, !tbaa !332
  %476 = bitcast float* %471 to <4 x float>*
  store <4 x float> %474, <4 x float>* %476, align 16, !tbaa !332
  %index.next269.1 = add nuw nsw i64 %index268, 16
  %477 = icmp eq i64 %index.next269.1, 512
  br i1 %477, label %for_end35.2, label %vector.body266, !prof !341, !llvm.loop !361

for_end35.2:                                      ; preds = %vector.body266
  %478 = add nuw nsw i64 %123, 1536
  br label %vector.body254

vector.body254:                                   ; preds = %vector.body254, %for_end35.2
  %index256 = phi i64 [ 0, %for_end35.2 ], [ %index.next257.1, %vector.body254 ]
  %479 = add nuw nsw i64 %478, %index256
  %480 = getelementptr inbounds float, float* %54, i64 %index256
  %481 = bitcast float* %480 to <4 x float>*
  %wide.load260 = load <4 x float>, <4 x float>* %481, align 64, !tbaa !338
  %482 = getelementptr inbounds float, float* %480, i64 4
  %483 = bitcast float* %482 to <4 x float>*
  %wide.load261 = load <4 x float>, <4 x float>* %483, align 16, !tbaa !338
  %484 = getelementptr inbounds float, float* %44, i64 %479
  %485 = bitcast float* %484 to <4 x float>*
  %wide.load262 = load <4 x float>, <4 x float>* %485, align 64, !tbaa !332
  %486 = getelementptr inbounds float, float* %484, i64 4
  %487 = bitcast float* %486 to <4 x float>*
  %wide.load263 = load <4 x float>, <4 x float>* %487, align 16, !tbaa !332
  %488 = fadd <4 x float> %wide.load260, %wide.load262
  %489 = fadd <4 x float> %wide.load261, %wide.load263
  %490 = bitcast float* %484 to <4 x float>*
  store <4 x float> %488, <4 x float>* %490, align 64, !tbaa !332
  %491 = bitcast float* %486 to <4 x float>*
  store <4 x float> %489, <4 x float>* %491, align 16, !tbaa !332
  %index.next257 = or i64 %index256, 8
  %492 = add nuw nsw i64 %478, %index.next257
  %493 = getelementptr inbounds float, float* %54, i64 %index.next257
  %494 = bitcast float* %493 to <4 x float>*
  %wide.load260.1 = load <4 x float>, <4 x float>* %494, align 32, !tbaa !338
  %495 = getelementptr inbounds float, float* %493, i64 4
  %496 = bitcast float* %495 to <4 x float>*
  %wide.load261.1 = load <4 x float>, <4 x float>* %496, align 16, !tbaa !338
  %497 = getelementptr inbounds float, float* %44, i64 %492
  %498 = bitcast float* %497 to <4 x float>*
  %wide.load262.1 = load <4 x float>, <4 x float>* %498, align 32, !tbaa !332
  %499 = getelementptr inbounds float, float* %497, i64 4
  %500 = bitcast float* %499 to <4 x float>*
  %wide.load263.1 = load <4 x float>, <4 x float>* %500, align 16, !tbaa !332
  %501 = fadd <4 x float> %wide.load260.1, %wide.load262.1
  %502 = fadd <4 x float> %wide.load261.1, %wide.load263.1
  %503 = bitcast float* %497 to <4 x float>*
  store <4 x float> %501, <4 x float>* %503, align 32, !tbaa !332
  %504 = bitcast float* %499 to <4 x float>*
  store <4 x float> %502, <4 x float>* %504, align 16, !tbaa !332
  %index.next257.1 = add nuw nsw i64 %index256, 16
  %505 = icmp eq i64 %index.next257.1, 512
  br i1 %505, label %for_end35.3, label %vector.body254, !prof !341, !llvm.loop !362

for_end35.3:                                      ; preds = %vector.body254
  %506 = add nuw nsw i64 %123, 2048
  br label %vector.body242

vector.body242:                                   ; preds = %vector.body242, %for_end35.3
  %index244 = phi i64 [ 0, %for_end35.3 ], [ %index.next245.1, %vector.body242 ]
  %507 = add nuw nsw i64 %506, %index244
  %508 = getelementptr inbounds float, float* %54, i64 %index244
  %509 = bitcast float* %508 to <4 x float>*
  %wide.load248 = load <4 x float>, <4 x float>* %509, align 64, !tbaa !338
  %510 = getelementptr inbounds float, float* %508, i64 4
  %511 = bitcast float* %510 to <4 x float>*
  %wide.load249 = load <4 x float>, <4 x float>* %511, align 16, !tbaa !338
  %512 = getelementptr inbounds float, float* %44, i64 %507
  %513 = bitcast float* %512 to <4 x float>*
  %wide.load250 = load <4 x float>, <4 x float>* %513, align 64, !tbaa !332
  %514 = getelementptr inbounds float, float* %512, i64 4
  %515 = bitcast float* %514 to <4 x float>*
  %wide.load251 = load <4 x float>, <4 x float>* %515, align 16, !tbaa !332
  %516 = fadd <4 x float> %wide.load248, %wide.load250
  %517 = fadd <4 x float> %wide.load249, %wide.load251
  %518 = bitcast float* %512 to <4 x float>*
  store <4 x float> %516, <4 x float>* %518, align 64, !tbaa !332
  %519 = bitcast float* %514 to <4 x float>*
  store <4 x float> %517, <4 x float>* %519, align 16, !tbaa !332
  %index.next245 = or i64 %index244, 8
  %520 = add nuw nsw i64 %506, %index.next245
  %521 = getelementptr inbounds float, float* %54, i64 %index.next245
  %522 = bitcast float* %521 to <4 x float>*
  %wide.load248.1 = load <4 x float>, <4 x float>* %522, align 32, !tbaa !338
  %523 = getelementptr inbounds float, float* %521, i64 4
  %524 = bitcast float* %523 to <4 x float>*
  %wide.load249.1 = load <4 x float>, <4 x float>* %524, align 16, !tbaa !338
  %525 = getelementptr inbounds float, float* %44, i64 %520
  %526 = bitcast float* %525 to <4 x float>*
  %wide.load250.1 = load <4 x float>, <4 x float>* %526, align 32, !tbaa !332
  %527 = getelementptr inbounds float, float* %525, i64 4
  %528 = bitcast float* %527 to <4 x float>*
  %wide.load251.1 = load <4 x float>, <4 x float>* %528, align 16, !tbaa !332
  %529 = fadd <4 x float> %wide.load248.1, %wide.load250.1
  %530 = fadd <4 x float> %wide.load249.1, %wide.load251.1
  %531 = bitcast float* %525 to <4 x float>*
  store <4 x float> %529, <4 x float>* %531, align 32, !tbaa !332
  %532 = bitcast float* %527 to <4 x float>*
  store <4 x float> %530, <4 x float>* %532, align 16, !tbaa !332
  %index.next245.1 = add nuw nsw i64 %index244, 16
  %533 = icmp eq i64 %index.next245.1, 512
  br i1 %533, label %for_end35.4, label %vector.body242, !prof !341, !llvm.loop !363

for_end35.4:                                      ; preds = %vector.body242
  %534 = add nuw nsw i64 %123, 2560
  br label %vector.body230

vector.body230:                                   ; preds = %vector.body230, %for_end35.4
  %index232 = phi i64 [ 0, %for_end35.4 ], [ %index.next233.1, %vector.body230 ]
  %535 = add nuw nsw i64 %534, %index232
  %536 = getelementptr inbounds float, float* %54, i64 %index232
  %537 = bitcast float* %536 to <4 x float>*
  %wide.load236 = load <4 x float>, <4 x float>* %537, align 64, !tbaa !338
  %538 = getelementptr inbounds float, float* %536, i64 4
  %539 = bitcast float* %538 to <4 x float>*
  %wide.load237 = load <4 x float>, <4 x float>* %539, align 16, !tbaa !338
  %540 = getelementptr inbounds float, float* %44, i64 %535
  %541 = bitcast float* %540 to <4 x float>*
  %wide.load238 = load <4 x float>, <4 x float>* %541, align 64, !tbaa !332
  %542 = getelementptr inbounds float, float* %540, i64 4
  %543 = bitcast float* %542 to <4 x float>*
  %wide.load239 = load <4 x float>, <4 x float>* %543, align 16, !tbaa !332
  %544 = fadd <4 x float> %wide.load236, %wide.load238
  %545 = fadd <4 x float> %wide.load237, %wide.load239
  %546 = bitcast float* %540 to <4 x float>*
  store <4 x float> %544, <4 x float>* %546, align 64, !tbaa !332
  %547 = bitcast float* %542 to <4 x float>*
  store <4 x float> %545, <4 x float>* %547, align 16, !tbaa !332
  %index.next233 = or i64 %index232, 8
  %548 = add nuw nsw i64 %534, %index.next233
  %549 = getelementptr inbounds float, float* %54, i64 %index.next233
  %550 = bitcast float* %549 to <4 x float>*
  %wide.load236.1 = load <4 x float>, <4 x float>* %550, align 32, !tbaa !338
  %551 = getelementptr inbounds float, float* %549, i64 4
  %552 = bitcast float* %551 to <4 x float>*
  %wide.load237.1 = load <4 x float>, <4 x float>* %552, align 16, !tbaa !338
  %553 = getelementptr inbounds float, float* %44, i64 %548
  %554 = bitcast float* %553 to <4 x float>*
  %wide.load238.1 = load <4 x float>, <4 x float>* %554, align 32, !tbaa !332
  %555 = getelementptr inbounds float, float* %553, i64 4
  %556 = bitcast float* %555 to <4 x float>*
  %wide.load239.1 = load <4 x float>, <4 x float>* %556, align 16, !tbaa !332
  %557 = fadd <4 x float> %wide.load236.1, %wide.load238.1
  %558 = fadd <4 x float> %wide.load237.1, %wide.load239.1
  %559 = bitcast float* %553 to <4 x float>*
  store <4 x float> %557, <4 x float>* %559, align 32, !tbaa !332
  %560 = bitcast float* %555 to <4 x float>*
  store <4 x float> %558, <4 x float>* %560, align 16, !tbaa !332
  %index.next233.1 = add nuw nsw i64 %index232, 16
  %561 = icmp eq i64 %index.next233.1, 512
  br i1 %561, label %for_end35.5, label %vector.body230, !prof !341, !llvm.loop !364

for_end35.5:                                      ; preds = %vector.body230
  %562 = add nuw nsw i64 %123, 3072
  br label %vector.body218

vector.body218:                                   ; preds = %vector.body218, %for_end35.5
  %index220 = phi i64 [ 0, %for_end35.5 ], [ %index.next221.1, %vector.body218 ]
  %563 = add nuw nsw i64 %562, %index220
  %564 = getelementptr inbounds float, float* %54, i64 %index220
  %565 = bitcast float* %564 to <4 x float>*
  %wide.load224 = load <4 x float>, <4 x float>* %565, align 64, !tbaa !338
  %566 = getelementptr inbounds float, float* %564, i64 4
  %567 = bitcast float* %566 to <4 x float>*
  %wide.load225 = load <4 x float>, <4 x float>* %567, align 16, !tbaa !338
  %568 = getelementptr inbounds float, float* %44, i64 %563
  %569 = bitcast float* %568 to <4 x float>*
  %wide.load226 = load <4 x float>, <4 x float>* %569, align 64, !tbaa !332
  %570 = getelementptr inbounds float, float* %568, i64 4
  %571 = bitcast float* %570 to <4 x float>*
  %wide.load227 = load <4 x float>, <4 x float>* %571, align 16, !tbaa !332
  %572 = fadd <4 x float> %wide.load224, %wide.load226
  %573 = fadd <4 x float> %wide.load225, %wide.load227
  %574 = bitcast float* %568 to <4 x float>*
  store <4 x float> %572, <4 x float>* %574, align 64, !tbaa !332
  %575 = bitcast float* %570 to <4 x float>*
  store <4 x float> %573, <4 x float>* %575, align 16, !tbaa !332
  %index.next221 = or i64 %index220, 8
  %576 = add nuw nsw i64 %562, %index.next221
  %577 = getelementptr inbounds float, float* %54, i64 %index.next221
  %578 = bitcast float* %577 to <4 x float>*
  %wide.load224.1 = load <4 x float>, <4 x float>* %578, align 32, !tbaa !338
  %579 = getelementptr inbounds float, float* %577, i64 4
  %580 = bitcast float* %579 to <4 x float>*
  %wide.load225.1 = load <4 x float>, <4 x float>* %580, align 16, !tbaa !338
  %581 = getelementptr inbounds float, float* %44, i64 %576
  %582 = bitcast float* %581 to <4 x float>*
  %wide.load226.1 = load <4 x float>, <4 x float>* %582, align 32, !tbaa !332
  %583 = getelementptr inbounds float, float* %581, i64 4
  %584 = bitcast float* %583 to <4 x float>*
  %wide.load227.1 = load <4 x float>, <4 x float>* %584, align 16, !tbaa !332
  %585 = fadd <4 x float> %wide.load224.1, %wide.load226.1
  %586 = fadd <4 x float> %wide.load225.1, %wide.load227.1
  %587 = bitcast float* %581 to <4 x float>*
  store <4 x float> %585, <4 x float>* %587, align 32, !tbaa !332
  %588 = bitcast float* %583 to <4 x float>*
  store <4 x float> %586, <4 x float>* %588, align 16, !tbaa !332
  %index.next221.1 = add nuw nsw i64 %index220, 16
  %589 = icmp eq i64 %index.next221.1, 512
  br i1 %589, label %for_end35.6, label %vector.body218, !prof !341, !llvm.loop !365

for_end35.6:                                      ; preds = %vector.body218
  %590 = add nuw nsw i64 %123, 3584
  br label %vector.body206

vector.body206:                                   ; preds = %vector.body206, %for_end35.6
  %index208 = phi i64 [ 0, %for_end35.6 ], [ %index.next209.1, %vector.body206 ]
  %591 = add nuw nsw i64 %590, %index208
  %592 = getelementptr inbounds float, float* %54, i64 %index208
  %593 = bitcast float* %592 to <4 x float>*
  %wide.load212 = load <4 x float>, <4 x float>* %593, align 64, !tbaa !338
  %594 = getelementptr inbounds float, float* %592, i64 4
  %595 = bitcast float* %594 to <4 x float>*
  %wide.load213 = load <4 x float>, <4 x float>* %595, align 16, !tbaa !338
  %596 = getelementptr inbounds float, float* %44, i64 %591
  %597 = bitcast float* %596 to <4 x float>*
  %wide.load214 = load <4 x float>, <4 x float>* %597, align 64, !tbaa !332
  %598 = getelementptr inbounds float, float* %596, i64 4
  %599 = bitcast float* %598 to <4 x float>*
  %wide.load215 = load <4 x float>, <4 x float>* %599, align 16, !tbaa !332
  %600 = fadd <4 x float> %wide.load212, %wide.load214
  %601 = fadd <4 x float> %wide.load213, %wide.load215
  %602 = bitcast float* %596 to <4 x float>*
  store <4 x float> %600, <4 x float>* %602, align 64, !tbaa !332
  %603 = bitcast float* %598 to <4 x float>*
  store <4 x float> %601, <4 x float>* %603, align 16, !tbaa !332
  %index.next209 = or i64 %index208, 8
  %604 = add nuw nsw i64 %590, %index.next209
  %605 = getelementptr inbounds float, float* %54, i64 %index.next209
  %606 = bitcast float* %605 to <4 x float>*
  %wide.load212.1 = load <4 x float>, <4 x float>* %606, align 32, !tbaa !338
  %607 = getelementptr inbounds float, float* %605, i64 4
  %608 = bitcast float* %607 to <4 x float>*
  %wide.load213.1 = load <4 x float>, <4 x float>* %608, align 16, !tbaa !338
  %609 = getelementptr inbounds float, float* %44, i64 %604
  %610 = bitcast float* %609 to <4 x float>*
  %wide.load214.1 = load <4 x float>, <4 x float>* %610, align 32, !tbaa !332
  %611 = getelementptr inbounds float, float* %609, i64 4
  %612 = bitcast float* %611 to <4 x float>*
  %wide.load215.1 = load <4 x float>, <4 x float>* %612, align 16, !tbaa !332
  %613 = fadd <4 x float> %wide.load212.1, %wide.load214.1
  %614 = fadd <4 x float> %wide.load213.1, %wide.load215.1
  %615 = bitcast float* %609 to <4 x float>*
  store <4 x float> %613, <4 x float>* %615, align 32, !tbaa !332
  %616 = bitcast float* %611 to <4 x float>*
  store <4 x float> %614, <4 x float>* %616, align 16, !tbaa !332
  %index.next209.1 = add nuw nsw i64 %index208, 16
  %617 = icmp eq i64 %index.next209.1, 512
  br i1 %617, label %for_end35.7, label %vector.body206, !prof !341, !llvm.loop !366

for_end35.7:                                      ; preds = %vector.body206
  %618 = add nuw nsw i64 %123, 4096
  br label %vector.body194

vector.body194:                                   ; preds = %vector.body194, %for_end35.7
  %index196 = phi i64 [ 0, %for_end35.7 ], [ %index.next197.1, %vector.body194 ]
  %619 = add nuw nsw i64 %618, %index196
  %620 = getelementptr inbounds float, float* %54, i64 %index196
  %621 = bitcast float* %620 to <4 x float>*
  %wide.load200 = load <4 x float>, <4 x float>* %621, align 64, !tbaa !338
  %622 = getelementptr inbounds float, float* %620, i64 4
  %623 = bitcast float* %622 to <4 x float>*
  %wide.load201 = load <4 x float>, <4 x float>* %623, align 16, !tbaa !338
  %624 = getelementptr inbounds float, float* %44, i64 %619
  %625 = bitcast float* %624 to <4 x float>*
  %wide.load202 = load <4 x float>, <4 x float>* %625, align 64, !tbaa !332
  %626 = getelementptr inbounds float, float* %624, i64 4
  %627 = bitcast float* %626 to <4 x float>*
  %wide.load203 = load <4 x float>, <4 x float>* %627, align 16, !tbaa !332
  %628 = fadd <4 x float> %wide.load200, %wide.load202
  %629 = fadd <4 x float> %wide.load201, %wide.load203
  %630 = bitcast float* %624 to <4 x float>*
  store <4 x float> %628, <4 x float>* %630, align 64, !tbaa !332
  %631 = bitcast float* %626 to <4 x float>*
  store <4 x float> %629, <4 x float>* %631, align 16, !tbaa !332
  %index.next197 = or i64 %index196, 8
  %632 = add nuw nsw i64 %618, %index.next197
  %633 = getelementptr inbounds float, float* %54, i64 %index.next197
  %634 = bitcast float* %633 to <4 x float>*
  %wide.load200.1 = load <4 x float>, <4 x float>* %634, align 32, !tbaa !338
  %635 = getelementptr inbounds float, float* %633, i64 4
  %636 = bitcast float* %635 to <4 x float>*
  %wide.load201.1 = load <4 x float>, <4 x float>* %636, align 16, !tbaa !338
  %637 = getelementptr inbounds float, float* %44, i64 %632
  %638 = bitcast float* %637 to <4 x float>*
  %wide.load202.1 = load <4 x float>, <4 x float>* %638, align 32, !tbaa !332
  %639 = getelementptr inbounds float, float* %637, i64 4
  %640 = bitcast float* %639 to <4 x float>*
  %wide.load203.1 = load <4 x float>, <4 x float>* %640, align 16, !tbaa !332
  %641 = fadd <4 x float> %wide.load200.1, %wide.load202.1
  %642 = fadd <4 x float> %wide.load201.1, %wide.load203.1
  %643 = bitcast float* %637 to <4 x float>*
  store <4 x float> %641, <4 x float>* %643, align 32, !tbaa !332
  %644 = bitcast float* %639 to <4 x float>*
  store <4 x float> %642, <4 x float>* %644, align 16, !tbaa !332
  %index.next197.1 = add nuw nsw i64 %index196, 16
  %645 = icmp eq i64 %index.next197.1, 512
  br i1 %645, label %for_end35.8, label %vector.body194, !prof !341, !llvm.loop !367

for_end35.8:                                      ; preds = %vector.body194
  %646 = add nuw nsw i64 %123, 4608
  br label %vector.body182

vector.body182:                                   ; preds = %vector.body182, %for_end35.8
  %index184 = phi i64 [ 0, %for_end35.8 ], [ %index.next185.1, %vector.body182 ]
  %647 = add nuw nsw i64 %646, %index184
  %648 = getelementptr inbounds float, float* %54, i64 %index184
  %649 = bitcast float* %648 to <4 x float>*
  %wide.load188 = load <4 x float>, <4 x float>* %649, align 64, !tbaa !338
  %650 = getelementptr inbounds float, float* %648, i64 4
  %651 = bitcast float* %650 to <4 x float>*
  %wide.load189 = load <4 x float>, <4 x float>* %651, align 16, !tbaa !338
  %652 = getelementptr inbounds float, float* %44, i64 %647
  %653 = bitcast float* %652 to <4 x float>*
  %wide.load190 = load <4 x float>, <4 x float>* %653, align 64, !tbaa !332
  %654 = getelementptr inbounds float, float* %652, i64 4
  %655 = bitcast float* %654 to <4 x float>*
  %wide.load191 = load <4 x float>, <4 x float>* %655, align 16, !tbaa !332
  %656 = fadd <4 x float> %wide.load188, %wide.load190
  %657 = fadd <4 x float> %wide.load189, %wide.load191
  %658 = bitcast float* %652 to <4 x float>*
  store <4 x float> %656, <4 x float>* %658, align 64, !tbaa !332
  %659 = bitcast float* %654 to <4 x float>*
  store <4 x float> %657, <4 x float>* %659, align 16, !tbaa !332
  %index.next185 = or i64 %index184, 8
  %660 = add nuw nsw i64 %646, %index.next185
  %661 = getelementptr inbounds float, float* %54, i64 %index.next185
  %662 = bitcast float* %661 to <4 x float>*
  %wide.load188.1 = load <4 x float>, <4 x float>* %662, align 32, !tbaa !338
  %663 = getelementptr inbounds float, float* %661, i64 4
  %664 = bitcast float* %663 to <4 x float>*
  %wide.load189.1 = load <4 x float>, <4 x float>* %664, align 16, !tbaa !338
  %665 = getelementptr inbounds float, float* %44, i64 %660
  %666 = bitcast float* %665 to <4 x float>*
  %wide.load190.1 = load <4 x float>, <4 x float>* %666, align 32, !tbaa !332
  %667 = getelementptr inbounds float, float* %665, i64 4
  %668 = bitcast float* %667 to <4 x float>*
  %wide.load191.1 = load <4 x float>, <4 x float>* %668, align 16, !tbaa !332
  %669 = fadd <4 x float> %wide.load188.1, %wide.load190.1
  %670 = fadd <4 x float> %wide.load189.1, %wide.load191.1
  %671 = bitcast float* %665 to <4 x float>*
  store <4 x float> %669, <4 x float>* %671, align 32, !tbaa !332
  %672 = bitcast float* %667 to <4 x float>*
  store <4 x float> %670, <4 x float>* %672, align 16, !tbaa !332
  %index.next185.1 = add nuw nsw i64 %index184, 16
  %673 = icmp eq i64 %index.next185.1, 512
  br i1 %673, label %for_end35.9, label %vector.body182, !prof !341, !llvm.loop !368

for_end35.9:                                      ; preds = %vector.body182
  %674 = add nuw nsw i64 %123, 5120
  br label %vector.body170

vector.body170:                                   ; preds = %vector.body170, %for_end35.9
  %index172 = phi i64 [ 0, %for_end35.9 ], [ %index.next173.1, %vector.body170 ]
  %675 = add nuw nsw i64 %674, %index172
  %676 = getelementptr inbounds float, float* %54, i64 %index172
  %677 = bitcast float* %676 to <4 x float>*
  %wide.load176 = load <4 x float>, <4 x float>* %677, align 64, !tbaa !338
  %678 = getelementptr inbounds float, float* %676, i64 4
  %679 = bitcast float* %678 to <4 x float>*
  %wide.load177 = load <4 x float>, <4 x float>* %679, align 16, !tbaa !338
  %680 = getelementptr inbounds float, float* %44, i64 %675
  %681 = bitcast float* %680 to <4 x float>*
  %wide.load178 = load <4 x float>, <4 x float>* %681, align 64, !tbaa !332
  %682 = getelementptr inbounds float, float* %680, i64 4
  %683 = bitcast float* %682 to <4 x float>*
  %wide.load179 = load <4 x float>, <4 x float>* %683, align 16, !tbaa !332
  %684 = fadd <4 x float> %wide.load176, %wide.load178
  %685 = fadd <4 x float> %wide.load177, %wide.load179
  %686 = bitcast float* %680 to <4 x float>*
  store <4 x float> %684, <4 x float>* %686, align 64, !tbaa !332
  %687 = bitcast float* %682 to <4 x float>*
  store <4 x float> %685, <4 x float>* %687, align 16, !tbaa !332
  %index.next173 = or i64 %index172, 8
  %688 = add nuw nsw i64 %674, %index.next173
  %689 = getelementptr inbounds float, float* %54, i64 %index.next173
  %690 = bitcast float* %689 to <4 x float>*
  %wide.load176.1 = load <4 x float>, <4 x float>* %690, align 32, !tbaa !338
  %691 = getelementptr inbounds float, float* %689, i64 4
  %692 = bitcast float* %691 to <4 x float>*
  %wide.load177.1 = load <4 x float>, <4 x float>* %692, align 16, !tbaa !338
  %693 = getelementptr inbounds float, float* %44, i64 %688
  %694 = bitcast float* %693 to <4 x float>*
  %wide.load178.1 = load <4 x float>, <4 x float>* %694, align 32, !tbaa !332
  %695 = getelementptr inbounds float, float* %693, i64 4
  %696 = bitcast float* %695 to <4 x float>*
  %wide.load179.1 = load <4 x float>, <4 x float>* %696, align 16, !tbaa !332
  %697 = fadd <4 x float> %wide.load176.1, %wide.load178.1
  %698 = fadd <4 x float> %wide.load177.1, %wide.load179.1
  %699 = bitcast float* %693 to <4 x float>*
  store <4 x float> %697, <4 x float>* %699, align 32, !tbaa !332
  %700 = bitcast float* %695 to <4 x float>*
  store <4 x float> %698, <4 x float>* %700, align 16, !tbaa !332
  %index.next173.1 = add nuw nsw i64 %index172, 16
  %701 = icmp eq i64 %index.next173.1, 512
  br i1 %701, label %for_end35.10, label %vector.body170, !prof !341, !llvm.loop !369

for_end35.10:                                     ; preds = %vector.body170
  %702 = add nuw nsw i64 %123, 5632
  br label %vector.body158

vector.body158:                                   ; preds = %vector.body158, %for_end35.10
  %index160 = phi i64 [ 0, %for_end35.10 ], [ %index.next161.1, %vector.body158 ]
  %703 = add nuw nsw i64 %702, %index160
  %704 = getelementptr inbounds float, float* %54, i64 %index160
  %705 = bitcast float* %704 to <4 x float>*
  %wide.load164 = load <4 x float>, <4 x float>* %705, align 64, !tbaa !338
  %706 = getelementptr inbounds float, float* %704, i64 4
  %707 = bitcast float* %706 to <4 x float>*
  %wide.load165 = load <4 x float>, <4 x float>* %707, align 16, !tbaa !338
  %708 = getelementptr inbounds float, float* %44, i64 %703
  %709 = bitcast float* %708 to <4 x float>*
  %wide.load166 = load <4 x float>, <4 x float>* %709, align 64, !tbaa !332
  %710 = getelementptr inbounds float, float* %708, i64 4
  %711 = bitcast float* %710 to <4 x float>*
  %wide.load167 = load <4 x float>, <4 x float>* %711, align 16, !tbaa !332
  %712 = fadd <4 x float> %wide.load164, %wide.load166
  %713 = fadd <4 x float> %wide.load165, %wide.load167
  %714 = bitcast float* %708 to <4 x float>*
  store <4 x float> %712, <4 x float>* %714, align 64, !tbaa !332
  %715 = bitcast float* %710 to <4 x float>*
  store <4 x float> %713, <4 x float>* %715, align 16, !tbaa !332
  %index.next161 = or i64 %index160, 8
  %716 = add nuw nsw i64 %702, %index.next161
  %717 = getelementptr inbounds float, float* %54, i64 %index.next161
  %718 = bitcast float* %717 to <4 x float>*
  %wide.load164.1 = load <4 x float>, <4 x float>* %718, align 32, !tbaa !338
  %719 = getelementptr inbounds float, float* %717, i64 4
  %720 = bitcast float* %719 to <4 x float>*
  %wide.load165.1 = load <4 x float>, <4 x float>* %720, align 16, !tbaa !338
  %721 = getelementptr inbounds float, float* %44, i64 %716
  %722 = bitcast float* %721 to <4 x float>*
  %wide.load166.1 = load <4 x float>, <4 x float>* %722, align 32, !tbaa !332
  %723 = getelementptr inbounds float, float* %721, i64 4
  %724 = bitcast float* %723 to <4 x float>*
  %wide.load167.1 = load <4 x float>, <4 x float>* %724, align 16, !tbaa !332
  %725 = fadd <4 x float> %wide.load164.1, %wide.load166.1
  %726 = fadd <4 x float> %wide.load165.1, %wide.load167.1
  %727 = bitcast float* %721 to <4 x float>*
  store <4 x float> %725, <4 x float>* %727, align 32, !tbaa !332
  %728 = bitcast float* %723 to <4 x float>*
  store <4 x float> %726, <4 x float>* %728, align 16, !tbaa !332
  %index.next161.1 = add nuw nsw i64 %index160, 16
  %729 = icmp eq i64 %index.next161.1, 512
  br i1 %729, label %for_end35.11, label %vector.body158, !prof !341, !llvm.loop !370

for_end35.11:                                     ; preds = %vector.body158
  %730 = add nuw nsw i64 %123, 6144
  br label %vector.body146

vector.body146:                                   ; preds = %vector.body146, %for_end35.11
  %index148 = phi i64 [ 0, %for_end35.11 ], [ %index.next149.1, %vector.body146 ]
  %731 = add nuw nsw i64 %730, %index148
  %732 = getelementptr inbounds float, float* %54, i64 %index148
  %733 = bitcast float* %732 to <4 x float>*
  %wide.load152 = load <4 x float>, <4 x float>* %733, align 64, !tbaa !338
  %734 = getelementptr inbounds float, float* %732, i64 4
  %735 = bitcast float* %734 to <4 x float>*
  %wide.load153 = load <4 x float>, <4 x float>* %735, align 16, !tbaa !338
  %736 = getelementptr inbounds float, float* %44, i64 %731
  %737 = bitcast float* %736 to <4 x float>*
  %wide.load154 = load <4 x float>, <4 x float>* %737, align 64, !tbaa !332
  %738 = getelementptr inbounds float, float* %736, i64 4
  %739 = bitcast float* %738 to <4 x float>*
  %wide.load155 = load <4 x float>, <4 x float>* %739, align 16, !tbaa !332
  %740 = fadd <4 x float> %wide.load152, %wide.load154
  %741 = fadd <4 x float> %wide.load153, %wide.load155
  %742 = bitcast float* %736 to <4 x float>*
  store <4 x float> %740, <4 x float>* %742, align 64, !tbaa !332
  %743 = bitcast float* %738 to <4 x float>*
  store <4 x float> %741, <4 x float>* %743, align 16, !tbaa !332
  %index.next149 = or i64 %index148, 8
  %744 = add nuw nsw i64 %730, %index.next149
  %745 = getelementptr inbounds float, float* %54, i64 %index.next149
  %746 = bitcast float* %745 to <4 x float>*
  %wide.load152.1 = load <4 x float>, <4 x float>* %746, align 32, !tbaa !338
  %747 = getelementptr inbounds float, float* %745, i64 4
  %748 = bitcast float* %747 to <4 x float>*
  %wide.load153.1 = load <4 x float>, <4 x float>* %748, align 16, !tbaa !338
  %749 = getelementptr inbounds float, float* %44, i64 %744
  %750 = bitcast float* %749 to <4 x float>*
  %wide.load154.1 = load <4 x float>, <4 x float>* %750, align 32, !tbaa !332
  %751 = getelementptr inbounds float, float* %749, i64 4
  %752 = bitcast float* %751 to <4 x float>*
  %wide.load155.1 = load <4 x float>, <4 x float>* %752, align 16, !tbaa !332
  %753 = fadd <4 x float> %wide.load152.1, %wide.load154.1
  %754 = fadd <4 x float> %wide.load153.1, %wide.load155.1
  %755 = bitcast float* %749 to <4 x float>*
  store <4 x float> %753, <4 x float>* %755, align 32, !tbaa !332
  %756 = bitcast float* %751 to <4 x float>*
  store <4 x float> %754, <4 x float>* %756, align 16, !tbaa !332
  %index.next149.1 = add nuw nsw i64 %index148, 16
  %757 = icmp eq i64 %index.next149.1, 512
  br i1 %757, label %for_end35.12, label %vector.body146, !prof !341, !llvm.loop !371

for_end35.12:                                     ; preds = %vector.body146
  %758 = add nuw nsw i64 %123, 6656
  br label %vector.body134

vector.body134:                                   ; preds = %vector.body134, %for_end35.12
  %index136 = phi i64 [ 0, %for_end35.12 ], [ %index.next137.1, %vector.body134 ]
  %759 = add nuw nsw i64 %758, %index136
  %760 = getelementptr inbounds float, float* %54, i64 %index136
  %761 = bitcast float* %760 to <4 x float>*
  %wide.load140 = load <4 x float>, <4 x float>* %761, align 64, !tbaa !338
  %762 = getelementptr inbounds float, float* %760, i64 4
  %763 = bitcast float* %762 to <4 x float>*
  %wide.load141 = load <4 x float>, <4 x float>* %763, align 16, !tbaa !338
  %764 = getelementptr inbounds float, float* %44, i64 %759
  %765 = bitcast float* %764 to <4 x float>*
  %wide.load142 = load <4 x float>, <4 x float>* %765, align 64, !tbaa !332
  %766 = getelementptr inbounds float, float* %764, i64 4
  %767 = bitcast float* %766 to <4 x float>*
  %wide.load143 = load <4 x float>, <4 x float>* %767, align 16, !tbaa !332
  %768 = fadd <4 x float> %wide.load140, %wide.load142
  %769 = fadd <4 x float> %wide.load141, %wide.load143
  %770 = bitcast float* %764 to <4 x float>*
  store <4 x float> %768, <4 x float>* %770, align 64, !tbaa !332
  %771 = bitcast float* %766 to <4 x float>*
  store <4 x float> %769, <4 x float>* %771, align 16, !tbaa !332
  %index.next137 = or i64 %index136, 8
  %772 = add nuw nsw i64 %758, %index.next137
  %773 = getelementptr inbounds float, float* %54, i64 %index.next137
  %774 = bitcast float* %773 to <4 x float>*
  %wide.load140.1 = load <4 x float>, <4 x float>* %774, align 32, !tbaa !338
  %775 = getelementptr inbounds float, float* %773, i64 4
  %776 = bitcast float* %775 to <4 x float>*
  %wide.load141.1 = load <4 x float>, <4 x float>* %776, align 16, !tbaa !338
  %777 = getelementptr inbounds float, float* %44, i64 %772
  %778 = bitcast float* %777 to <4 x float>*
  %wide.load142.1 = load <4 x float>, <4 x float>* %778, align 32, !tbaa !332
  %779 = getelementptr inbounds float, float* %777, i64 4
  %780 = bitcast float* %779 to <4 x float>*
  %wide.load143.1 = load <4 x float>, <4 x float>* %780, align 16, !tbaa !332
  %781 = fadd <4 x float> %wide.load140.1, %wide.load142.1
  %782 = fadd <4 x float> %wide.load141.1, %wide.load143.1
  %783 = bitcast float* %777 to <4 x float>*
  store <4 x float> %781, <4 x float>* %783, align 32, !tbaa !332
  %784 = bitcast float* %779 to <4 x float>*
  store <4 x float> %782, <4 x float>* %784, align 16, !tbaa !332
  %index.next137.1 = add nuw nsw i64 %index136, 16
  %785 = icmp eq i64 %index.next137.1, 512
  br i1 %785, label %for_end35.13, label %vector.body134, !prof !341, !llvm.loop !372

for_end35.13:                                     ; preds = %vector.body134
  %indvars.iv.next81 = add nuw nsw i64 %indvars.iv80, 1
  %exitcond82.not = icmp eq i64 %indvars.iv.next81, 14
  br i1 %exitcond82.not, label %for_begin36.preheader, label %for_begin30.preheader, !prof !51
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.fmuladd.f32(float, float, float) #4

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_11(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([88 x i8], [88 x i8]* @.str.61, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !373
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !387
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !389
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !392
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.62, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.63, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.64, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.65, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !394
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !408
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 56
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.66, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !410
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 56
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.67, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !413
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 128
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.68, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !415
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 401408, i32 7168, i32 128, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.69, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !427
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 3
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.51, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !441
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 3
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.52, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !443
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 128
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.70, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !446
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 1
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.54, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !448
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 384, i32 128, i32 1, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.71, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !460
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 128
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.72, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !474
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !488
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !502
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 28
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.73, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !504
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 28
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.74, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !507
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 128
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.75, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !509
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 100352, i32 3584, i32 128, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.76, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_11_compute_(i8* %31, i8* %39, i8* %45, i8* %51, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_11_compute_(i8* noalias nocapture readonly align 128 %0, i8* noalias nocapture readonly align 128 %1, i8* noalias nocapture readonly align 128 %2, i8* noalias nocapture align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 1663488, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %for_end38, %if_end, %entry
  ret i32 -1

if_end:                                           ; preds = %entry
  %8 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %9 = tail call i8* %8(i32 1, i32 %4, i64 401408, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %9, i64 128) ]
  %10 = icmp eq i8* %9, null
  br i1 %10, label %if_then, label %for_begin4.preheader, !prof !5

for_begin4.preheader:                             ; preds = %if_end, %for_end6
  %indvar = phi i64 [ %indvar.next, %for_end6 ], [ 0, %if_end ]
  %11 = phi i32 [ %24, %for_end6 ], [ 0, %if_end ]
  %12 = mul nuw nsw i64 %indvar, 29184
  %13 = mul nuw nsw i64 %indvar, 28672
  %14 = icmp ult i32 %11, 56
  br i1 %14, label %for_begin7.preheader.us, label %for_begin7.preheader.preheader

for_begin7.preheader.preheader:                   ; preds = %for_begin4.preheader
  %scevgep103 = getelementptr i8, i8* %6, i64 %12
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(29184) %scevgep103, i8 0, i64 29184, i1 false)
  br label %for_end6

for_begin7.preheader.us:                          ; preds = %for_begin4.preheader, %for_end9.us
  %indvar104 = phi i64 [ %indvar.next105, %for_end9.us ], [ 0, %for_begin4.preheader ]
  %15 = phi i32 [ %20, %for_end9.us ], [ 0, %for_begin4.preheader ]
  %16 = shl nuw nsw i64 %indvar104, 9
  %17 = add nuw nsw i64 %12, %16
  %scevgep108 = getelementptr i8, i8* %6, i64 %17
  %18 = icmp ult i32 %15, 56
  br i1 %18, label %for_body8.us.us.preheader, label %for_body8.us60.preheader

for_body8.us60.preheader:                         ; preds = %for_begin7.preheader.us
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(512) %scevgep108, i8 0, i64 512, i1 false)
  br label %for_end9.us

for_body8.us.us.preheader:                        ; preds = %for_begin7.preheader.us
  %19 = add nuw nsw i64 %13, %16
  %scevgep109 = getelementptr i8, i8* %0, i64 %19
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(512) %scevgep108, i8* nonnull align 128 dereferenceable(512) %scevgep109, i64 512, i1 false)
  br label %for_end9.us

for_end9.us:                                      ; preds = %for_body8.us60.preheader, %for_body8.us.us.preheader
  %20 = add nuw nsw i32 %15, 1
  %indvar.next105 = add nuw nsw i64 %indvar104, 1
  %exitcond112.not = icmp eq i64 %indvar.next105, 57
  br i1 %exitcond112.not, label %for_end6, label %for_begin7.preheader.us, !prof !51

for_begin12.preheader:                            ; preds = %for_end6
  %21 = bitcast i8* %9 to float*
  %22 = bitcast i8* %6 to float*
  %23 = bitcast i8* %1 to float*
  br label %for_begin15.preheader

for_end6:                                         ; preds = %for_end9.us, %for_begin7.preheader.preheader
  %24 = add nuw nsw i32 %11, 1
  %indvar.next = add nuw nsw i64 %indvar, 1
  %exitcond113.not = icmp eq i64 %indvar.next, 57
  br i1 %exitcond113.not, label %for_begin12.preheader, label %for_begin4.preheader, !prof !51

for_begin15.preheader:                            ; preds = %for_begin12.preheader, %for_end17
  %indvars.iv96 = phi i64 [ 0, %for_begin12.preheader ], [ %indvars.iv.next97, %for_end17 ]
  %25 = mul nuw nsw i64 %indvars.iv96, 3584
  %26 = mul nuw nsw i64 %indvars.iv96, 14592
  br label %for_begin18.preheader

for_begin27.preheader:                            ; preds = %for_end17
  %27 = bitcast i8* %2 to <4 x float>*
  %wide.load139 = load <4 x float>, <4 x float>* %27, align 128, !tbaa !521
  %28 = getelementptr inbounds i8, i8* %2, i64 16
  %29 = bitcast i8* %28 to <4 x float>*
  %wide.load140 = load <4 x float>, <4 x float>* %29, align 16, !tbaa !521
  %30 = getelementptr inbounds i8, i8* %2, i64 32
  %31 = bitcast i8* %30 to <4 x float>*
  %wide.load139.1 = load <4 x float>, <4 x float>* %31, align 32, !tbaa !521
  %32 = getelementptr inbounds i8, i8* %2, i64 48
  %33 = bitcast i8* %32 to <4 x float>*
  %wide.load140.1 = load <4 x float>, <4 x float>* %33, align 16, !tbaa !521
  %34 = getelementptr inbounds i8, i8* %2, i64 64
  %35 = bitcast i8* %34 to <4 x float>*
  %wide.load139.2 = load <4 x float>, <4 x float>* %35, align 64, !tbaa !521
  %36 = getelementptr inbounds i8, i8* %2, i64 80
  %37 = bitcast i8* %36 to <4 x float>*
  %wide.load140.2 = load <4 x float>, <4 x float>* %37, align 16, !tbaa !521
  %38 = getelementptr inbounds i8, i8* %2, i64 96
  %39 = bitcast i8* %38 to <4 x float>*
  %wide.load139.3 = load <4 x float>, <4 x float>* %39, align 32, !tbaa !521
  %40 = getelementptr inbounds i8, i8* %2, i64 112
  %41 = bitcast i8* %40 to <4 x float>*
  %wide.load140.3 = load <4 x float>, <4 x float>* %41, align 16, !tbaa !521
  %42 = getelementptr inbounds i8, i8* %2, i64 128
  %43 = bitcast i8* %42 to <4 x float>*
  %wide.load139.4 = load <4 x float>, <4 x float>* %43, align 128, !tbaa !521
  %44 = getelementptr inbounds i8, i8* %2, i64 144
  %45 = bitcast i8* %44 to <4 x float>*
  %wide.load140.4 = load <4 x float>, <4 x float>* %45, align 16, !tbaa !521
  %46 = getelementptr inbounds i8, i8* %2, i64 160
  %47 = bitcast i8* %46 to <4 x float>*
  %wide.load139.5 = load <4 x float>, <4 x float>* %47, align 32, !tbaa !521
  %48 = getelementptr inbounds i8, i8* %2, i64 176
  %49 = bitcast i8* %48 to <4 x float>*
  %wide.load140.5 = load <4 x float>, <4 x float>* %49, align 16, !tbaa !521
  %50 = getelementptr inbounds i8, i8* %2, i64 192
  %51 = bitcast i8* %50 to <4 x float>*
  %wide.load139.6 = load <4 x float>, <4 x float>* %51, align 64, !tbaa !521
  %52 = getelementptr inbounds i8, i8* %2, i64 208
  %53 = bitcast i8* %52 to <4 x float>*
  %wide.load140.6 = load <4 x float>, <4 x float>* %53, align 16, !tbaa !521
  %54 = getelementptr inbounds i8, i8* %2, i64 224
  %55 = bitcast i8* %54 to <4 x float>*
  %wide.load139.7 = load <4 x float>, <4 x float>* %55, align 32, !tbaa !521
  %56 = getelementptr inbounds i8, i8* %2, i64 240
  %57 = bitcast i8* %56 to <4 x float>*
  %wide.load140.7 = load <4 x float>, <4 x float>* %57, align 16, !tbaa !521
  %58 = getelementptr inbounds i8, i8* %2, i64 256
  %59 = bitcast i8* %58 to <4 x float>*
  %wide.load139.8 = load <4 x float>, <4 x float>* %59, align 128, !tbaa !521
  %60 = getelementptr inbounds i8, i8* %2, i64 272
  %61 = bitcast i8* %60 to <4 x float>*
  %wide.load140.8 = load <4 x float>, <4 x float>* %61, align 16, !tbaa !521
  %62 = getelementptr inbounds i8, i8* %2, i64 288
  %63 = bitcast i8* %62 to <4 x float>*
  %wide.load139.9 = load <4 x float>, <4 x float>* %63, align 32, !tbaa !521
  %64 = getelementptr inbounds i8, i8* %2, i64 304
  %65 = bitcast i8* %64 to <4 x float>*
  %wide.load140.9 = load <4 x float>, <4 x float>* %65, align 16, !tbaa !521
  %66 = getelementptr inbounds i8, i8* %2, i64 320
  %67 = bitcast i8* %66 to <4 x float>*
  %wide.load139.10 = load <4 x float>, <4 x float>* %67, align 64, !tbaa !521
  %68 = getelementptr inbounds i8, i8* %2, i64 336
  %69 = bitcast i8* %68 to <4 x float>*
  %wide.load140.10 = load <4 x float>, <4 x float>* %69, align 16, !tbaa !521
  %70 = getelementptr inbounds i8, i8* %2, i64 352
  %71 = bitcast i8* %70 to <4 x float>*
  %wide.load139.11 = load <4 x float>, <4 x float>* %71, align 32, !tbaa !521
  %72 = getelementptr inbounds i8, i8* %2, i64 368
  %73 = bitcast i8* %72 to <4 x float>*
  %wide.load140.11 = load <4 x float>, <4 x float>* %73, align 16, !tbaa !521
  %74 = getelementptr inbounds i8, i8* %2, i64 384
  %75 = bitcast i8* %74 to <4 x float>*
  %wide.load139.12 = load <4 x float>, <4 x float>* %75, align 128, !tbaa !521
  %76 = getelementptr inbounds i8, i8* %2, i64 400
  %77 = bitcast i8* %76 to <4 x float>*
  %wide.load140.12 = load <4 x float>, <4 x float>* %77, align 16, !tbaa !521
  %78 = getelementptr inbounds i8, i8* %2, i64 416
  %79 = bitcast i8* %78 to <4 x float>*
  %wide.load139.13 = load <4 x float>, <4 x float>* %79, align 32, !tbaa !521
  %80 = getelementptr inbounds i8, i8* %2, i64 432
  %81 = bitcast i8* %80 to <4 x float>*
  %wide.load140.13 = load <4 x float>, <4 x float>* %81, align 16, !tbaa !521
  %82 = getelementptr inbounds i8, i8* %2, i64 448
  %83 = bitcast i8* %82 to <4 x float>*
  %wide.load139.14 = load <4 x float>, <4 x float>* %83, align 64, !tbaa !521
  %84 = getelementptr inbounds i8, i8* %2, i64 464
  %85 = bitcast i8* %84 to <4 x float>*
  %wide.load140.14 = load <4 x float>, <4 x float>* %85, align 16, !tbaa !521
  %86 = getelementptr inbounds i8, i8* %2, i64 480
  %87 = bitcast i8* %86 to <4 x float>*
  %wide.load139.15 = load <4 x float>, <4 x float>* %87, align 32, !tbaa !521
  %88 = getelementptr inbounds i8, i8* %2, i64 496
  %89 = bitcast i8* %88 to <4 x float>*
  %wide.load140.15 = load <4 x float>, <4 x float>* %89, align 16, !tbaa !521
  br label %for_begin30.preheader

for_begin18.preheader:                            ; preds = %for_begin15.preheader, %for_end20
  %indvars.iv93 = phi i64 [ 0, %for_begin15.preheader ], [ %indvars.iv.next94, %for_end20 ]
  %90 = shl nsw i64 %indvars.iv93, 7
  %91 = add nuw nsw i64 %90, %25
  %92 = shl nsw i64 %indvars.iv93, 8
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_begin18.preheader
  %index = phi i64 [ 0, %for_begin18.preheader ], [ %index.next, %vector.body ]
  %93 = add nuw nsw i64 %91, %index
  %94 = getelementptr inbounds float, float* %21, i64 %93
  %95 = add nuw nsw i64 %index, %26
  %96 = add nuw nsw i64 %95, %92
  %97 = getelementptr inbounds float, float* %22, i64 %96
  %98 = bitcast float* %97 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %98, align 16, !tbaa !524
  %99 = getelementptr inbounds float, float* %23, i64 %index
  %100 = bitcast float* %99 to <4 x float>*
  %wide.load114 = load <4 x float>, <4 x float>* %100, align 16, !tbaa !527
  %101 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load, <4 x float> %wide.load114, <4 x float> zeroinitializer)
  %102 = add nuw nsw i64 %index, 128
  %103 = add nuw nsw i64 %102, %26
  %104 = add nuw nsw i64 %103, %92
  %105 = getelementptr inbounds float, float* %22, i64 %104
  %106 = bitcast float* %105 to <4 x float>*
  %wide.load115 = load <4 x float>, <4 x float>* %106, align 16, !tbaa !524
  %107 = getelementptr inbounds float, float* %23, i64 %102
  %108 = bitcast float* %107 to <4 x float>*
  %wide.load116 = load <4 x float>, <4 x float>* %108, align 16, !tbaa !527
  %109 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load115, <4 x float> %wide.load116, <4 x float> %101)
  %110 = add nuw nsw i64 %index, 256
  %111 = add nuw nsw i64 %110, %26
  %112 = add nuw nsw i64 %111, %92
  %113 = getelementptr inbounds float, float* %22, i64 %112
  %114 = bitcast float* %113 to <4 x float>*
  %wide.load117 = load <4 x float>, <4 x float>* %114, align 16, !tbaa !524
  %115 = getelementptr inbounds float, float* %23, i64 %110
  %116 = bitcast float* %115 to <4 x float>*
  %wide.load118 = load <4 x float>, <4 x float>* %116, align 16, !tbaa !527
  %117 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load117, <4 x float> %wide.load118, <4 x float> %109)
  %118 = add nuw nsw i64 %96, 7296
  %119 = getelementptr inbounds float, float* %22, i64 %118
  %120 = bitcast float* %119 to <4 x float>*
  %wide.load119 = load <4 x float>, <4 x float>* %120, align 16, !tbaa !524
  %121 = add nuw nsw i64 %index, 384
  %122 = getelementptr inbounds float, float* %23, i64 %121
  %123 = bitcast float* %122 to <4 x float>*
  %wide.load120 = load <4 x float>, <4 x float>* %123, align 16, !tbaa !527
  %124 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load119, <4 x float> %wide.load120, <4 x float> %117)
  %125 = add nuw nsw i64 %104, 7296
  %126 = getelementptr inbounds float, float* %22, i64 %125
  %127 = bitcast float* %126 to <4 x float>*
  %wide.load121 = load <4 x float>, <4 x float>* %127, align 16, !tbaa !524
  %128 = add nuw nsw i64 %index, 512
  %129 = getelementptr inbounds float, float* %23, i64 %128
  %130 = bitcast float* %129 to <4 x float>*
  %wide.load122 = load <4 x float>, <4 x float>* %130, align 16, !tbaa !527
  %131 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load121, <4 x float> %wide.load122, <4 x float> %124)
  %132 = add nuw nsw i64 %112, 7296
  %133 = getelementptr inbounds float, float* %22, i64 %132
  %134 = bitcast float* %133 to <4 x float>*
  %wide.load123 = load <4 x float>, <4 x float>* %134, align 16, !tbaa !524
  %135 = add nuw nsw i64 %index, 640
  %136 = getelementptr inbounds float, float* %23, i64 %135
  %137 = bitcast float* %136 to <4 x float>*
  %wide.load124 = load <4 x float>, <4 x float>* %137, align 16, !tbaa !527
  %138 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load123, <4 x float> %wide.load124, <4 x float> %131)
  %139 = add nuw nsw i64 %96, 14592
  %140 = getelementptr inbounds float, float* %22, i64 %139
  %141 = bitcast float* %140 to <4 x float>*
  %wide.load125 = load <4 x float>, <4 x float>* %141, align 16, !tbaa !524
  %142 = add nuw nsw i64 %index, 768
  %143 = getelementptr inbounds float, float* %23, i64 %142
  %144 = bitcast float* %143 to <4 x float>*
  %wide.load126 = load <4 x float>, <4 x float>* %144, align 16, !tbaa !527
  %145 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load125, <4 x float> %wide.load126, <4 x float> %138)
  %146 = add nuw nsw i64 %104, 14592
  %147 = getelementptr inbounds float, float* %22, i64 %146
  %148 = bitcast float* %147 to <4 x float>*
  %wide.load127 = load <4 x float>, <4 x float>* %148, align 16, !tbaa !524
  %149 = add nuw nsw i64 %index, 896
  %150 = getelementptr inbounds float, float* %23, i64 %149
  %151 = bitcast float* %150 to <4 x float>*
  %wide.load128 = load <4 x float>, <4 x float>* %151, align 16, !tbaa !527
  %152 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load127, <4 x float> %wide.load128, <4 x float> %145)
  %153 = add nuw nsw i64 %112, 14592
  %154 = getelementptr inbounds float, float* %22, i64 %153
  %155 = bitcast float* %154 to <4 x float>*
  %wide.load129 = load <4 x float>, <4 x float>* %155, align 16, !tbaa !524
  %156 = add nuw nsw i64 %index, 1024
  %157 = getelementptr inbounds float, float* %23, i64 %156
  %158 = bitcast float* %157 to <4 x float>*
  %wide.load130 = load <4 x float>, <4 x float>* %158, align 16, !tbaa !527
  %159 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load129, <4 x float> %wide.load130, <4 x float> %152)
  %160 = bitcast float* %94 to <4 x float>*
  store <4 x float> %159, <4 x float>* %160, align 16, !tbaa !530
  %index.next = add i64 %index, 4
  %161 = icmp eq i64 %index.next, 128
  br i1 %161, label %for_end20, label %vector.body, !prof !335, !llvm.loop !533

for_end17:                                        ; preds = %for_end20
  %indvars.iv.next97 = add nuw nsw i64 %indvars.iv96, 1
  %exitcond98.not = icmp eq i64 %indvars.iv.next97, 28
  br i1 %exitcond98.not, label %for_begin27.preheader, label %for_begin15.preheader, !prof !51

for_end20:                                        ; preds = %vector.body
  %indvars.iv.next94 = add nuw nsw i64 %indvars.iv93, 1
  %exitcond95.not = icmp eq i64 %indvars.iv.next94, 28
  br i1 %exitcond95.not, label %for_end17, label %for_begin18.preheader, !prof !51

for_begin30.preheader:                            ; preds = %for_begin27.preheader, %for_end32
  %indvars.iv79 = phi i64 [ 0, %for_begin27.preheader ], [ %indvars.iv.next80, %for_end32 ]
  %162 = mul nuw nsw i64 %indvars.iv79, 3584
  br label %for_begin33.preheader

for_begin36.preheader:                            ; preds = %for_end32
  %163 = bitcast i8* %3 to float*
  br label %for_begin39.preheader

for_begin33.preheader:                            ; preds = %for_begin30.preheader, %for_begin33.preheader
  %indvars.iv76 = phi i64 [ 0, %for_begin30.preheader ], [ %indvars.iv.next77, %for_begin33.preheader ]
  %164 = shl nsw i64 %indvars.iv76, 7
  %165 = add nuw nsw i64 %164, %162
  %166 = getelementptr inbounds float, float* %21, i64 %165
  %167 = bitcast float* %166 to <4 x float>*
  %wide.load141 = load <4 x float>, <4 x float>* %167, align 128, !tbaa !530
  %168 = getelementptr inbounds float, float* %166, i64 4
  %169 = bitcast float* %168 to <4 x float>*
  %wide.load142 = load <4 x float>, <4 x float>* %169, align 16, !tbaa !530
  %170 = fadd <4 x float> %wide.load139, %wide.load141
  %171 = fadd <4 x float> %wide.load140, %wide.load142
  %172 = bitcast float* %166 to <4 x float>*
  store <4 x float> %170, <4 x float>* %172, align 128, !tbaa !530
  %173 = bitcast float* %168 to <4 x float>*
  store <4 x float> %171, <4 x float>* %173, align 16, !tbaa !530
  %174 = or i64 %165, 8
  %175 = getelementptr inbounds float, float* %21, i64 %174
  %176 = bitcast float* %175 to <4 x float>*
  %wide.load141.1 = load <4 x float>, <4 x float>* %176, align 32, !tbaa !530
  %177 = getelementptr inbounds float, float* %175, i64 4
  %178 = bitcast float* %177 to <4 x float>*
  %wide.load142.1 = load <4 x float>, <4 x float>* %178, align 16, !tbaa !530
  %179 = fadd <4 x float> %wide.load139.1, %wide.load141.1
  %180 = fadd <4 x float> %wide.load140.1, %wide.load142.1
  %181 = bitcast float* %175 to <4 x float>*
  store <4 x float> %179, <4 x float>* %181, align 32, !tbaa !530
  %182 = bitcast float* %177 to <4 x float>*
  store <4 x float> %180, <4 x float>* %182, align 16, !tbaa !530
  %183 = or i64 %165, 16
  %184 = getelementptr inbounds float, float* %21, i64 %183
  %185 = bitcast float* %184 to <4 x float>*
  %wide.load141.2 = load <4 x float>, <4 x float>* %185, align 64, !tbaa !530
  %186 = getelementptr inbounds float, float* %184, i64 4
  %187 = bitcast float* %186 to <4 x float>*
  %wide.load142.2 = load <4 x float>, <4 x float>* %187, align 16, !tbaa !530
  %188 = fadd <4 x float> %wide.load139.2, %wide.load141.2
  %189 = fadd <4 x float> %wide.load140.2, %wide.load142.2
  %190 = bitcast float* %184 to <4 x float>*
  store <4 x float> %188, <4 x float>* %190, align 64, !tbaa !530
  %191 = bitcast float* %186 to <4 x float>*
  store <4 x float> %189, <4 x float>* %191, align 16, !tbaa !530
  %192 = or i64 %165, 24
  %193 = getelementptr inbounds float, float* %21, i64 %192
  %194 = bitcast float* %193 to <4 x float>*
  %wide.load141.3 = load <4 x float>, <4 x float>* %194, align 32, !tbaa !530
  %195 = getelementptr inbounds float, float* %193, i64 4
  %196 = bitcast float* %195 to <4 x float>*
  %wide.load142.3 = load <4 x float>, <4 x float>* %196, align 16, !tbaa !530
  %197 = fadd <4 x float> %wide.load139.3, %wide.load141.3
  %198 = fadd <4 x float> %wide.load140.3, %wide.load142.3
  %199 = bitcast float* %193 to <4 x float>*
  store <4 x float> %197, <4 x float>* %199, align 32, !tbaa !530
  %200 = bitcast float* %195 to <4 x float>*
  store <4 x float> %198, <4 x float>* %200, align 16, !tbaa !530
  %201 = or i64 %165, 32
  %202 = getelementptr inbounds float, float* %21, i64 %201
  %203 = bitcast float* %202 to <4 x float>*
  %wide.load141.4 = load <4 x float>, <4 x float>* %203, align 128, !tbaa !530
  %204 = getelementptr inbounds float, float* %202, i64 4
  %205 = bitcast float* %204 to <4 x float>*
  %wide.load142.4 = load <4 x float>, <4 x float>* %205, align 16, !tbaa !530
  %206 = fadd <4 x float> %wide.load139.4, %wide.load141.4
  %207 = fadd <4 x float> %wide.load140.4, %wide.load142.4
  %208 = bitcast float* %202 to <4 x float>*
  store <4 x float> %206, <4 x float>* %208, align 128, !tbaa !530
  %209 = bitcast float* %204 to <4 x float>*
  store <4 x float> %207, <4 x float>* %209, align 16, !tbaa !530
  %210 = or i64 %165, 40
  %211 = getelementptr inbounds float, float* %21, i64 %210
  %212 = bitcast float* %211 to <4 x float>*
  %wide.load141.5 = load <4 x float>, <4 x float>* %212, align 32, !tbaa !530
  %213 = getelementptr inbounds float, float* %211, i64 4
  %214 = bitcast float* %213 to <4 x float>*
  %wide.load142.5 = load <4 x float>, <4 x float>* %214, align 16, !tbaa !530
  %215 = fadd <4 x float> %wide.load139.5, %wide.load141.5
  %216 = fadd <4 x float> %wide.load140.5, %wide.load142.5
  %217 = bitcast float* %211 to <4 x float>*
  store <4 x float> %215, <4 x float>* %217, align 32, !tbaa !530
  %218 = bitcast float* %213 to <4 x float>*
  store <4 x float> %216, <4 x float>* %218, align 16, !tbaa !530
  %219 = or i64 %165, 48
  %220 = getelementptr inbounds float, float* %21, i64 %219
  %221 = bitcast float* %220 to <4 x float>*
  %wide.load141.6 = load <4 x float>, <4 x float>* %221, align 64, !tbaa !530
  %222 = getelementptr inbounds float, float* %220, i64 4
  %223 = bitcast float* %222 to <4 x float>*
  %wide.load142.6 = load <4 x float>, <4 x float>* %223, align 16, !tbaa !530
  %224 = fadd <4 x float> %wide.load139.6, %wide.load141.6
  %225 = fadd <4 x float> %wide.load140.6, %wide.load142.6
  %226 = bitcast float* %220 to <4 x float>*
  store <4 x float> %224, <4 x float>* %226, align 64, !tbaa !530
  %227 = bitcast float* %222 to <4 x float>*
  store <4 x float> %225, <4 x float>* %227, align 16, !tbaa !530
  %228 = or i64 %165, 56
  %229 = getelementptr inbounds float, float* %21, i64 %228
  %230 = bitcast float* %229 to <4 x float>*
  %wide.load141.7 = load <4 x float>, <4 x float>* %230, align 32, !tbaa !530
  %231 = getelementptr inbounds float, float* %229, i64 4
  %232 = bitcast float* %231 to <4 x float>*
  %wide.load142.7 = load <4 x float>, <4 x float>* %232, align 16, !tbaa !530
  %233 = fadd <4 x float> %wide.load139.7, %wide.load141.7
  %234 = fadd <4 x float> %wide.load140.7, %wide.load142.7
  %235 = bitcast float* %229 to <4 x float>*
  store <4 x float> %233, <4 x float>* %235, align 32, !tbaa !530
  %236 = bitcast float* %231 to <4 x float>*
  store <4 x float> %234, <4 x float>* %236, align 16, !tbaa !530
  %237 = or i64 %165, 64
  %238 = getelementptr inbounds float, float* %21, i64 %237
  %239 = bitcast float* %238 to <4 x float>*
  %wide.load141.8 = load <4 x float>, <4 x float>* %239, align 128, !tbaa !530
  %240 = getelementptr inbounds float, float* %238, i64 4
  %241 = bitcast float* %240 to <4 x float>*
  %wide.load142.8 = load <4 x float>, <4 x float>* %241, align 16, !tbaa !530
  %242 = fadd <4 x float> %wide.load139.8, %wide.load141.8
  %243 = fadd <4 x float> %wide.load140.8, %wide.load142.8
  %244 = bitcast float* %238 to <4 x float>*
  store <4 x float> %242, <4 x float>* %244, align 128, !tbaa !530
  %245 = bitcast float* %240 to <4 x float>*
  store <4 x float> %243, <4 x float>* %245, align 16, !tbaa !530
  %246 = or i64 %165, 72
  %247 = getelementptr inbounds float, float* %21, i64 %246
  %248 = bitcast float* %247 to <4 x float>*
  %wide.load141.9 = load <4 x float>, <4 x float>* %248, align 32, !tbaa !530
  %249 = getelementptr inbounds float, float* %247, i64 4
  %250 = bitcast float* %249 to <4 x float>*
  %wide.load142.9 = load <4 x float>, <4 x float>* %250, align 16, !tbaa !530
  %251 = fadd <4 x float> %wide.load139.9, %wide.load141.9
  %252 = fadd <4 x float> %wide.load140.9, %wide.load142.9
  %253 = bitcast float* %247 to <4 x float>*
  store <4 x float> %251, <4 x float>* %253, align 32, !tbaa !530
  %254 = bitcast float* %249 to <4 x float>*
  store <4 x float> %252, <4 x float>* %254, align 16, !tbaa !530
  %255 = or i64 %165, 80
  %256 = getelementptr inbounds float, float* %21, i64 %255
  %257 = bitcast float* %256 to <4 x float>*
  %wide.load141.10 = load <4 x float>, <4 x float>* %257, align 64, !tbaa !530
  %258 = getelementptr inbounds float, float* %256, i64 4
  %259 = bitcast float* %258 to <4 x float>*
  %wide.load142.10 = load <4 x float>, <4 x float>* %259, align 16, !tbaa !530
  %260 = fadd <4 x float> %wide.load139.10, %wide.load141.10
  %261 = fadd <4 x float> %wide.load140.10, %wide.load142.10
  %262 = bitcast float* %256 to <4 x float>*
  store <4 x float> %260, <4 x float>* %262, align 64, !tbaa !530
  %263 = bitcast float* %258 to <4 x float>*
  store <4 x float> %261, <4 x float>* %263, align 16, !tbaa !530
  %264 = or i64 %165, 88
  %265 = getelementptr inbounds float, float* %21, i64 %264
  %266 = bitcast float* %265 to <4 x float>*
  %wide.load141.11 = load <4 x float>, <4 x float>* %266, align 32, !tbaa !530
  %267 = getelementptr inbounds float, float* %265, i64 4
  %268 = bitcast float* %267 to <4 x float>*
  %wide.load142.11 = load <4 x float>, <4 x float>* %268, align 16, !tbaa !530
  %269 = fadd <4 x float> %wide.load139.11, %wide.load141.11
  %270 = fadd <4 x float> %wide.load140.11, %wide.load142.11
  %271 = bitcast float* %265 to <4 x float>*
  store <4 x float> %269, <4 x float>* %271, align 32, !tbaa !530
  %272 = bitcast float* %267 to <4 x float>*
  store <4 x float> %270, <4 x float>* %272, align 16, !tbaa !530
  %273 = or i64 %165, 96
  %274 = getelementptr inbounds float, float* %21, i64 %273
  %275 = bitcast float* %274 to <4 x float>*
  %wide.load141.12 = load <4 x float>, <4 x float>* %275, align 128, !tbaa !530
  %276 = getelementptr inbounds float, float* %274, i64 4
  %277 = bitcast float* %276 to <4 x float>*
  %wide.load142.12 = load <4 x float>, <4 x float>* %277, align 16, !tbaa !530
  %278 = fadd <4 x float> %wide.load139.12, %wide.load141.12
  %279 = fadd <4 x float> %wide.load140.12, %wide.load142.12
  %280 = bitcast float* %274 to <4 x float>*
  store <4 x float> %278, <4 x float>* %280, align 128, !tbaa !530
  %281 = bitcast float* %276 to <4 x float>*
  store <4 x float> %279, <4 x float>* %281, align 16, !tbaa !530
  %282 = or i64 %165, 104
  %283 = getelementptr inbounds float, float* %21, i64 %282
  %284 = bitcast float* %283 to <4 x float>*
  %wide.load141.13 = load <4 x float>, <4 x float>* %284, align 32, !tbaa !530
  %285 = getelementptr inbounds float, float* %283, i64 4
  %286 = bitcast float* %285 to <4 x float>*
  %wide.load142.13 = load <4 x float>, <4 x float>* %286, align 16, !tbaa !530
  %287 = fadd <4 x float> %wide.load139.13, %wide.load141.13
  %288 = fadd <4 x float> %wide.load140.13, %wide.load142.13
  %289 = bitcast float* %283 to <4 x float>*
  store <4 x float> %287, <4 x float>* %289, align 32, !tbaa !530
  %290 = bitcast float* %285 to <4 x float>*
  store <4 x float> %288, <4 x float>* %290, align 16, !tbaa !530
  %291 = or i64 %165, 112
  %292 = getelementptr inbounds float, float* %21, i64 %291
  %293 = bitcast float* %292 to <4 x float>*
  %wide.load141.14 = load <4 x float>, <4 x float>* %293, align 64, !tbaa !530
  %294 = getelementptr inbounds float, float* %292, i64 4
  %295 = bitcast float* %294 to <4 x float>*
  %wide.load142.14 = load <4 x float>, <4 x float>* %295, align 16, !tbaa !530
  %296 = fadd <4 x float> %wide.load139.14, %wide.load141.14
  %297 = fadd <4 x float> %wide.load140.14, %wide.load142.14
  %298 = bitcast float* %292 to <4 x float>*
  store <4 x float> %296, <4 x float>* %298, align 64, !tbaa !530
  %299 = bitcast float* %294 to <4 x float>*
  store <4 x float> %297, <4 x float>* %299, align 16, !tbaa !530
  %300 = or i64 %165, 120
  %301 = getelementptr inbounds float, float* %21, i64 %300
  %302 = bitcast float* %301 to <4 x float>*
  %wide.load141.15 = load <4 x float>, <4 x float>* %302, align 32, !tbaa !530
  %303 = getelementptr inbounds float, float* %301, i64 4
  %304 = bitcast float* %303 to <4 x float>*
  %wide.load142.15 = load <4 x float>, <4 x float>* %304, align 16, !tbaa !530
  %305 = fadd <4 x float> %wide.load139.15, %wide.load141.15
  %306 = fadd <4 x float> %wide.load140.15, %wide.load142.15
  %307 = bitcast float* %301 to <4 x float>*
  store <4 x float> %305, <4 x float>* %307, align 32, !tbaa !530
  %308 = bitcast float* %303 to <4 x float>*
  store <4 x float> %306, <4 x float>* %308, align 16, !tbaa !530
  %indvars.iv.next77 = add nuw nsw i64 %indvars.iv76, 1
  %exitcond78.not = icmp eq i64 %indvars.iv.next77, 28
  br i1 %exitcond78.not, label %for_end32, label %for_begin33.preheader, !prof !51

for_end32:                                        ; preds = %for_begin33.preheader
  %indvars.iv.next80 = add nuw nsw i64 %indvars.iv79, 1
  %exitcond81.not = icmp eq i64 %indvars.iv.next80, 28
  br i1 %exitcond81.not, label %for_begin36.preheader, label %for_begin30.preheader, !prof !51

for_begin39.preheader:                            ; preds = %for_begin36.preheader, %for_end41
  %indvars.iv70 = phi i64 [ 0, %for_begin36.preheader ], [ %indvars.iv.next71, %for_end41 ]
  %309 = mul nuw nsw i64 %indvars.iv70, 3584
  br label %for_begin42.preheader

for_end38:                                        ; preds = %for_end41
  %310 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %311 = tail call i32 %310(i32 1, i32 %4, i8* nonnull %9)
  %.not = icmp eq i32 %311, 0
  br i1 %.not, label %if_end46, label %if_then, !prof !51

for_begin42.preheader:                            ; preds = %for_begin39.preheader, %for_begin42.preheader
  %indvars.iv67 = phi i64 [ 0, %for_begin39.preheader ], [ %indvars.iv.next68, %for_begin42.preheader ]
  %312 = shl nsw i64 %indvars.iv67, 7
  %313 = add nuw nsw i64 %312, %309
  %314 = getelementptr inbounds float, float* %21, i64 %313
  %315 = bitcast float* %314 to <4 x float>*
  %wide.load151 = load <4 x float>, <4 x float>* %315, align 128, !tbaa !530
  %316 = getelementptr inbounds float, float* %314, i64 4
  %317 = bitcast float* %316 to <4 x float>*
  %wide.load152 = load <4 x float>, <4 x float>* %317, align 16, !tbaa !530
  %318 = fcmp olt <4 x float> %wide.load151, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %319 = fcmp olt <4 x float> %wide.load152, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %320 = select <4 x i1> %318, <4 x float> %wide.load151, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %321 = select <4 x i1> %319, <4 x float> %wide.load152, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %322 = fcmp ogt <4 x float> %320, zeroinitializer
  %323 = fcmp ogt <4 x float> %321, zeroinitializer
  %324 = select <4 x i1> %322, <4 x float> %320, <4 x float> zeroinitializer
  %325 = select <4 x i1> %323, <4 x float> %321, <4 x float> zeroinitializer
  %326 = getelementptr inbounds float, float* %163, i64 %313
  %327 = bitcast float* %326 to <4 x float>*
  store <4 x float> %324, <4 x float>* %327, align 128, !tbaa !534
  %328 = getelementptr inbounds float, float* %326, i64 4
  %329 = bitcast float* %328 to <4 x float>*
  store <4 x float> %325, <4 x float>* %329, align 16, !tbaa !534
  %330 = or i64 %313, 8
  %331 = getelementptr inbounds float, float* %21, i64 %330
  %332 = bitcast float* %331 to <4 x float>*
  %wide.load151.1 = load <4 x float>, <4 x float>* %332, align 32, !tbaa !530
  %333 = getelementptr inbounds float, float* %331, i64 4
  %334 = bitcast float* %333 to <4 x float>*
  %wide.load152.1 = load <4 x float>, <4 x float>* %334, align 16, !tbaa !530
  %335 = fcmp olt <4 x float> %wide.load151.1, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %336 = fcmp olt <4 x float> %wide.load152.1, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %337 = select <4 x i1> %335, <4 x float> %wide.load151.1, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %338 = select <4 x i1> %336, <4 x float> %wide.load152.1, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %339 = fcmp ogt <4 x float> %337, zeroinitializer
  %340 = fcmp ogt <4 x float> %338, zeroinitializer
  %341 = select <4 x i1> %339, <4 x float> %337, <4 x float> zeroinitializer
  %342 = select <4 x i1> %340, <4 x float> %338, <4 x float> zeroinitializer
  %343 = getelementptr inbounds float, float* %163, i64 %330
  %344 = bitcast float* %343 to <4 x float>*
  store <4 x float> %341, <4 x float>* %344, align 32, !tbaa !534
  %345 = getelementptr inbounds float, float* %343, i64 4
  %346 = bitcast float* %345 to <4 x float>*
  store <4 x float> %342, <4 x float>* %346, align 16, !tbaa !534
  %347 = or i64 %313, 16
  %348 = getelementptr inbounds float, float* %21, i64 %347
  %349 = bitcast float* %348 to <4 x float>*
  %wide.load151.2 = load <4 x float>, <4 x float>* %349, align 64, !tbaa !530
  %350 = getelementptr inbounds float, float* %348, i64 4
  %351 = bitcast float* %350 to <4 x float>*
  %wide.load152.2 = load <4 x float>, <4 x float>* %351, align 16, !tbaa !530
  %352 = fcmp olt <4 x float> %wide.load151.2, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %353 = fcmp olt <4 x float> %wide.load152.2, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %354 = select <4 x i1> %352, <4 x float> %wide.load151.2, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %355 = select <4 x i1> %353, <4 x float> %wide.load152.2, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %356 = fcmp ogt <4 x float> %354, zeroinitializer
  %357 = fcmp ogt <4 x float> %355, zeroinitializer
  %358 = select <4 x i1> %356, <4 x float> %354, <4 x float> zeroinitializer
  %359 = select <4 x i1> %357, <4 x float> %355, <4 x float> zeroinitializer
  %360 = getelementptr inbounds float, float* %163, i64 %347
  %361 = bitcast float* %360 to <4 x float>*
  store <4 x float> %358, <4 x float>* %361, align 64, !tbaa !534
  %362 = getelementptr inbounds float, float* %360, i64 4
  %363 = bitcast float* %362 to <4 x float>*
  store <4 x float> %359, <4 x float>* %363, align 16, !tbaa !534
  %364 = or i64 %313, 24
  %365 = getelementptr inbounds float, float* %21, i64 %364
  %366 = bitcast float* %365 to <4 x float>*
  %wide.load151.3 = load <4 x float>, <4 x float>* %366, align 32, !tbaa !530
  %367 = getelementptr inbounds float, float* %365, i64 4
  %368 = bitcast float* %367 to <4 x float>*
  %wide.load152.3 = load <4 x float>, <4 x float>* %368, align 16, !tbaa !530
  %369 = fcmp olt <4 x float> %wide.load151.3, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %370 = fcmp olt <4 x float> %wide.load152.3, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %371 = select <4 x i1> %369, <4 x float> %wide.load151.3, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %372 = select <4 x i1> %370, <4 x float> %wide.load152.3, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %373 = fcmp ogt <4 x float> %371, zeroinitializer
  %374 = fcmp ogt <4 x float> %372, zeroinitializer
  %375 = select <4 x i1> %373, <4 x float> %371, <4 x float> zeroinitializer
  %376 = select <4 x i1> %374, <4 x float> %372, <4 x float> zeroinitializer
  %377 = getelementptr inbounds float, float* %163, i64 %364
  %378 = bitcast float* %377 to <4 x float>*
  store <4 x float> %375, <4 x float>* %378, align 32, !tbaa !534
  %379 = getelementptr inbounds float, float* %377, i64 4
  %380 = bitcast float* %379 to <4 x float>*
  store <4 x float> %376, <4 x float>* %380, align 16, !tbaa !534
  %381 = or i64 %313, 32
  %382 = getelementptr inbounds float, float* %21, i64 %381
  %383 = bitcast float* %382 to <4 x float>*
  %wide.load151.4 = load <4 x float>, <4 x float>* %383, align 128, !tbaa !530
  %384 = getelementptr inbounds float, float* %382, i64 4
  %385 = bitcast float* %384 to <4 x float>*
  %wide.load152.4 = load <4 x float>, <4 x float>* %385, align 16, !tbaa !530
  %386 = fcmp olt <4 x float> %wide.load151.4, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %387 = fcmp olt <4 x float> %wide.load152.4, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %388 = select <4 x i1> %386, <4 x float> %wide.load151.4, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %389 = select <4 x i1> %387, <4 x float> %wide.load152.4, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %390 = fcmp ogt <4 x float> %388, zeroinitializer
  %391 = fcmp ogt <4 x float> %389, zeroinitializer
  %392 = select <4 x i1> %390, <4 x float> %388, <4 x float> zeroinitializer
  %393 = select <4 x i1> %391, <4 x float> %389, <4 x float> zeroinitializer
  %394 = getelementptr inbounds float, float* %163, i64 %381
  %395 = bitcast float* %394 to <4 x float>*
  store <4 x float> %392, <4 x float>* %395, align 128, !tbaa !534
  %396 = getelementptr inbounds float, float* %394, i64 4
  %397 = bitcast float* %396 to <4 x float>*
  store <4 x float> %393, <4 x float>* %397, align 16, !tbaa !534
  %398 = or i64 %313, 40
  %399 = getelementptr inbounds float, float* %21, i64 %398
  %400 = bitcast float* %399 to <4 x float>*
  %wide.load151.5 = load <4 x float>, <4 x float>* %400, align 32, !tbaa !530
  %401 = getelementptr inbounds float, float* %399, i64 4
  %402 = bitcast float* %401 to <4 x float>*
  %wide.load152.5 = load <4 x float>, <4 x float>* %402, align 16, !tbaa !530
  %403 = fcmp olt <4 x float> %wide.load151.5, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %404 = fcmp olt <4 x float> %wide.load152.5, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %405 = select <4 x i1> %403, <4 x float> %wide.load151.5, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %406 = select <4 x i1> %404, <4 x float> %wide.load152.5, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %407 = fcmp ogt <4 x float> %405, zeroinitializer
  %408 = fcmp ogt <4 x float> %406, zeroinitializer
  %409 = select <4 x i1> %407, <4 x float> %405, <4 x float> zeroinitializer
  %410 = select <4 x i1> %408, <4 x float> %406, <4 x float> zeroinitializer
  %411 = getelementptr inbounds float, float* %163, i64 %398
  %412 = bitcast float* %411 to <4 x float>*
  store <4 x float> %409, <4 x float>* %412, align 32, !tbaa !534
  %413 = getelementptr inbounds float, float* %411, i64 4
  %414 = bitcast float* %413 to <4 x float>*
  store <4 x float> %410, <4 x float>* %414, align 16, !tbaa !534
  %415 = or i64 %313, 48
  %416 = getelementptr inbounds float, float* %21, i64 %415
  %417 = bitcast float* %416 to <4 x float>*
  %wide.load151.6 = load <4 x float>, <4 x float>* %417, align 64, !tbaa !530
  %418 = getelementptr inbounds float, float* %416, i64 4
  %419 = bitcast float* %418 to <4 x float>*
  %wide.load152.6 = load <4 x float>, <4 x float>* %419, align 16, !tbaa !530
  %420 = fcmp olt <4 x float> %wide.load151.6, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %421 = fcmp olt <4 x float> %wide.load152.6, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %422 = select <4 x i1> %420, <4 x float> %wide.load151.6, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %423 = select <4 x i1> %421, <4 x float> %wide.load152.6, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %424 = fcmp ogt <4 x float> %422, zeroinitializer
  %425 = fcmp ogt <4 x float> %423, zeroinitializer
  %426 = select <4 x i1> %424, <4 x float> %422, <4 x float> zeroinitializer
  %427 = select <4 x i1> %425, <4 x float> %423, <4 x float> zeroinitializer
  %428 = getelementptr inbounds float, float* %163, i64 %415
  %429 = bitcast float* %428 to <4 x float>*
  store <4 x float> %426, <4 x float>* %429, align 64, !tbaa !534
  %430 = getelementptr inbounds float, float* %428, i64 4
  %431 = bitcast float* %430 to <4 x float>*
  store <4 x float> %427, <4 x float>* %431, align 16, !tbaa !534
  %432 = or i64 %313, 56
  %433 = getelementptr inbounds float, float* %21, i64 %432
  %434 = bitcast float* %433 to <4 x float>*
  %wide.load151.7 = load <4 x float>, <4 x float>* %434, align 32, !tbaa !530
  %435 = getelementptr inbounds float, float* %433, i64 4
  %436 = bitcast float* %435 to <4 x float>*
  %wide.load152.7 = load <4 x float>, <4 x float>* %436, align 16, !tbaa !530
  %437 = fcmp olt <4 x float> %wide.load151.7, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %438 = fcmp olt <4 x float> %wide.load152.7, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %439 = select <4 x i1> %437, <4 x float> %wide.load151.7, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %440 = select <4 x i1> %438, <4 x float> %wide.load152.7, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %441 = fcmp ogt <4 x float> %439, zeroinitializer
  %442 = fcmp ogt <4 x float> %440, zeroinitializer
  %443 = select <4 x i1> %441, <4 x float> %439, <4 x float> zeroinitializer
  %444 = select <4 x i1> %442, <4 x float> %440, <4 x float> zeroinitializer
  %445 = getelementptr inbounds float, float* %163, i64 %432
  %446 = bitcast float* %445 to <4 x float>*
  store <4 x float> %443, <4 x float>* %446, align 32, !tbaa !534
  %447 = getelementptr inbounds float, float* %445, i64 4
  %448 = bitcast float* %447 to <4 x float>*
  store <4 x float> %444, <4 x float>* %448, align 16, !tbaa !534
  %449 = or i64 %313, 64
  %450 = getelementptr inbounds float, float* %21, i64 %449
  %451 = bitcast float* %450 to <4 x float>*
  %wide.load151.8 = load <4 x float>, <4 x float>* %451, align 128, !tbaa !530
  %452 = getelementptr inbounds float, float* %450, i64 4
  %453 = bitcast float* %452 to <4 x float>*
  %wide.load152.8 = load <4 x float>, <4 x float>* %453, align 16, !tbaa !530
  %454 = fcmp olt <4 x float> %wide.load151.8, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %455 = fcmp olt <4 x float> %wide.load152.8, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %456 = select <4 x i1> %454, <4 x float> %wide.load151.8, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %457 = select <4 x i1> %455, <4 x float> %wide.load152.8, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %458 = fcmp ogt <4 x float> %456, zeroinitializer
  %459 = fcmp ogt <4 x float> %457, zeroinitializer
  %460 = select <4 x i1> %458, <4 x float> %456, <4 x float> zeroinitializer
  %461 = select <4 x i1> %459, <4 x float> %457, <4 x float> zeroinitializer
  %462 = getelementptr inbounds float, float* %163, i64 %449
  %463 = bitcast float* %462 to <4 x float>*
  store <4 x float> %460, <4 x float>* %463, align 128, !tbaa !534
  %464 = getelementptr inbounds float, float* %462, i64 4
  %465 = bitcast float* %464 to <4 x float>*
  store <4 x float> %461, <4 x float>* %465, align 16, !tbaa !534
  %466 = or i64 %313, 72
  %467 = getelementptr inbounds float, float* %21, i64 %466
  %468 = bitcast float* %467 to <4 x float>*
  %wide.load151.9 = load <4 x float>, <4 x float>* %468, align 32, !tbaa !530
  %469 = getelementptr inbounds float, float* %467, i64 4
  %470 = bitcast float* %469 to <4 x float>*
  %wide.load152.9 = load <4 x float>, <4 x float>* %470, align 16, !tbaa !530
  %471 = fcmp olt <4 x float> %wide.load151.9, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %472 = fcmp olt <4 x float> %wide.load152.9, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %473 = select <4 x i1> %471, <4 x float> %wide.load151.9, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %474 = select <4 x i1> %472, <4 x float> %wide.load152.9, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %475 = fcmp ogt <4 x float> %473, zeroinitializer
  %476 = fcmp ogt <4 x float> %474, zeroinitializer
  %477 = select <4 x i1> %475, <4 x float> %473, <4 x float> zeroinitializer
  %478 = select <4 x i1> %476, <4 x float> %474, <4 x float> zeroinitializer
  %479 = getelementptr inbounds float, float* %163, i64 %466
  %480 = bitcast float* %479 to <4 x float>*
  store <4 x float> %477, <4 x float>* %480, align 32, !tbaa !534
  %481 = getelementptr inbounds float, float* %479, i64 4
  %482 = bitcast float* %481 to <4 x float>*
  store <4 x float> %478, <4 x float>* %482, align 16, !tbaa !534
  %483 = or i64 %313, 80
  %484 = getelementptr inbounds float, float* %21, i64 %483
  %485 = bitcast float* %484 to <4 x float>*
  %wide.load151.10 = load <4 x float>, <4 x float>* %485, align 64, !tbaa !530
  %486 = getelementptr inbounds float, float* %484, i64 4
  %487 = bitcast float* %486 to <4 x float>*
  %wide.load152.10 = load <4 x float>, <4 x float>* %487, align 16, !tbaa !530
  %488 = fcmp olt <4 x float> %wide.load151.10, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %489 = fcmp olt <4 x float> %wide.load152.10, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %490 = select <4 x i1> %488, <4 x float> %wide.load151.10, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %491 = select <4 x i1> %489, <4 x float> %wide.load152.10, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %492 = fcmp ogt <4 x float> %490, zeroinitializer
  %493 = fcmp ogt <4 x float> %491, zeroinitializer
  %494 = select <4 x i1> %492, <4 x float> %490, <4 x float> zeroinitializer
  %495 = select <4 x i1> %493, <4 x float> %491, <4 x float> zeroinitializer
  %496 = getelementptr inbounds float, float* %163, i64 %483
  %497 = bitcast float* %496 to <4 x float>*
  store <4 x float> %494, <4 x float>* %497, align 64, !tbaa !534
  %498 = getelementptr inbounds float, float* %496, i64 4
  %499 = bitcast float* %498 to <4 x float>*
  store <4 x float> %495, <4 x float>* %499, align 16, !tbaa !534
  %500 = or i64 %313, 88
  %501 = getelementptr inbounds float, float* %21, i64 %500
  %502 = bitcast float* %501 to <4 x float>*
  %wide.load151.11 = load <4 x float>, <4 x float>* %502, align 32, !tbaa !530
  %503 = getelementptr inbounds float, float* %501, i64 4
  %504 = bitcast float* %503 to <4 x float>*
  %wide.load152.11 = load <4 x float>, <4 x float>* %504, align 16, !tbaa !530
  %505 = fcmp olt <4 x float> %wide.load151.11, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %506 = fcmp olt <4 x float> %wide.load152.11, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %507 = select <4 x i1> %505, <4 x float> %wide.load151.11, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %508 = select <4 x i1> %506, <4 x float> %wide.load152.11, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %509 = fcmp ogt <4 x float> %507, zeroinitializer
  %510 = fcmp ogt <4 x float> %508, zeroinitializer
  %511 = select <4 x i1> %509, <4 x float> %507, <4 x float> zeroinitializer
  %512 = select <4 x i1> %510, <4 x float> %508, <4 x float> zeroinitializer
  %513 = getelementptr inbounds float, float* %163, i64 %500
  %514 = bitcast float* %513 to <4 x float>*
  store <4 x float> %511, <4 x float>* %514, align 32, !tbaa !534
  %515 = getelementptr inbounds float, float* %513, i64 4
  %516 = bitcast float* %515 to <4 x float>*
  store <4 x float> %512, <4 x float>* %516, align 16, !tbaa !534
  %517 = or i64 %313, 96
  %518 = getelementptr inbounds float, float* %21, i64 %517
  %519 = bitcast float* %518 to <4 x float>*
  %wide.load151.12 = load <4 x float>, <4 x float>* %519, align 128, !tbaa !530
  %520 = getelementptr inbounds float, float* %518, i64 4
  %521 = bitcast float* %520 to <4 x float>*
  %wide.load152.12 = load <4 x float>, <4 x float>* %521, align 16, !tbaa !530
  %522 = fcmp olt <4 x float> %wide.load151.12, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %523 = fcmp olt <4 x float> %wide.load152.12, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %524 = select <4 x i1> %522, <4 x float> %wide.load151.12, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %525 = select <4 x i1> %523, <4 x float> %wide.load152.12, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %526 = fcmp ogt <4 x float> %524, zeroinitializer
  %527 = fcmp ogt <4 x float> %525, zeroinitializer
  %528 = select <4 x i1> %526, <4 x float> %524, <4 x float> zeroinitializer
  %529 = select <4 x i1> %527, <4 x float> %525, <4 x float> zeroinitializer
  %530 = getelementptr inbounds float, float* %163, i64 %517
  %531 = bitcast float* %530 to <4 x float>*
  store <4 x float> %528, <4 x float>* %531, align 128, !tbaa !534
  %532 = getelementptr inbounds float, float* %530, i64 4
  %533 = bitcast float* %532 to <4 x float>*
  store <4 x float> %529, <4 x float>* %533, align 16, !tbaa !534
  %534 = or i64 %313, 104
  %535 = getelementptr inbounds float, float* %21, i64 %534
  %536 = bitcast float* %535 to <4 x float>*
  %wide.load151.13 = load <4 x float>, <4 x float>* %536, align 32, !tbaa !530
  %537 = getelementptr inbounds float, float* %535, i64 4
  %538 = bitcast float* %537 to <4 x float>*
  %wide.load152.13 = load <4 x float>, <4 x float>* %538, align 16, !tbaa !530
  %539 = fcmp olt <4 x float> %wide.load151.13, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %540 = fcmp olt <4 x float> %wide.load152.13, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %541 = select <4 x i1> %539, <4 x float> %wide.load151.13, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %542 = select <4 x i1> %540, <4 x float> %wide.load152.13, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %543 = fcmp ogt <4 x float> %541, zeroinitializer
  %544 = fcmp ogt <4 x float> %542, zeroinitializer
  %545 = select <4 x i1> %543, <4 x float> %541, <4 x float> zeroinitializer
  %546 = select <4 x i1> %544, <4 x float> %542, <4 x float> zeroinitializer
  %547 = getelementptr inbounds float, float* %163, i64 %534
  %548 = bitcast float* %547 to <4 x float>*
  store <4 x float> %545, <4 x float>* %548, align 32, !tbaa !534
  %549 = getelementptr inbounds float, float* %547, i64 4
  %550 = bitcast float* %549 to <4 x float>*
  store <4 x float> %546, <4 x float>* %550, align 16, !tbaa !534
  %551 = or i64 %313, 112
  %552 = getelementptr inbounds float, float* %21, i64 %551
  %553 = bitcast float* %552 to <4 x float>*
  %wide.load151.14 = load <4 x float>, <4 x float>* %553, align 64, !tbaa !530
  %554 = getelementptr inbounds float, float* %552, i64 4
  %555 = bitcast float* %554 to <4 x float>*
  %wide.load152.14 = load <4 x float>, <4 x float>* %555, align 16, !tbaa !530
  %556 = fcmp olt <4 x float> %wide.load151.14, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %557 = fcmp olt <4 x float> %wide.load152.14, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %558 = select <4 x i1> %556, <4 x float> %wide.load151.14, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %559 = select <4 x i1> %557, <4 x float> %wide.load152.14, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %560 = fcmp ogt <4 x float> %558, zeroinitializer
  %561 = fcmp ogt <4 x float> %559, zeroinitializer
  %562 = select <4 x i1> %560, <4 x float> %558, <4 x float> zeroinitializer
  %563 = select <4 x i1> %561, <4 x float> %559, <4 x float> zeroinitializer
  %564 = getelementptr inbounds float, float* %163, i64 %551
  %565 = bitcast float* %564 to <4 x float>*
  store <4 x float> %562, <4 x float>* %565, align 64, !tbaa !534
  %566 = getelementptr inbounds float, float* %564, i64 4
  %567 = bitcast float* %566 to <4 x float>*
  store <4 x float> %563, <4 x float>* %567, align 16, !tbaa !534
  %568 = or i64 %313, 120
  %569 = getelementptr inbounds float, float* %21, i64 %568
  %570 = bitcast float* %569 to <4 x float>*
  %wide.load151.15 = load <4 x float>, <4 x float>* %570, align 32, !tbaa !530
  %571 = getelementptr inbounds float, float* %569, i64 4
  %572 = bitcast float* %571 to <4 x float>*
  %wide.load152.15 = load <4 x float>, <4 x float>* %572, align 16, !tbaa !530
  %573 = fcmp olt <4 x float> %wide.load151.15, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %574 = fcmp olt <4 x float> %wide.load152.15, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %575 = select <4 x i1> %573, <4 x float> %wide.load151.15, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %576 = select <4 x i1> %574, <4 x float> %wide.load152.15, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %577 = fcmp ogt <4 x float> %575, zeroinitializer
  %578 = fcmp ogt <4 x float> %576, zeroinitializer
  %579 = select <4 x i1> %577, <4 x float> %575, <4 x float> zeroinitializer
  %580 = select <4 x i1> %578, <4 x float> %576, <4 x float> zeroinitializer
  %581 = getelementptr inbounds float, float* %163, i64 %568
  %582 = bitcast float* %581 to <4 x float>*
  store <4 x float> %579, <4 x float>* %582, align 32, !tbaa !534
  %583 = getelementptr inbounds float, float* %581, i64 4
  %584 = bitcast float* %583 to <4 x float>*
  store <4 x float> %580, <4 x float>* %584, align 16, !tbaa !534
  %indvars.iv.next68 = add nuw nsw i64 %indvars.iv67, 1
  %exitcond69.not = icmp eq i64 %indvars.iv.next68, 28
  br i1 %exitcond69.not, label %for_end41, label %for_begin42.preheader, !prof !51

for_end41:                                        ; preds = %for_begin42.preheader
  %indvars.iv.next71 = add nuw nsw i64 %indvars.iv70, 1
  %exitcond72.not = icmp eq i64 %indvars.iv.next71, 28
  br i1 %exitcond72.not, label %for_end38, label %for_begin39.preheader, !prof !51

if_end46:                                         ; preds = %for_end38
  %585 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %586 = tail call i32 %585(i32 1, i32 %4, i8* nonnull %6)
  %.not57 = icmp ne i32 %586, 0
  %spec.select = sext i1 %.not57 to i32
  ret i32 %spec.select
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_13(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([88 x i8], [88 x i8]* @.str.77, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !537
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !551
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !553
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !556
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.78, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.79, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.80, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.81, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !558
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !572
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 56
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.66, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !574
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 56
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.67, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !577
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 128
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.68, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !579
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 401408, i32 7168, i32 128, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.69, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !591
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 3
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.51, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !605
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 3
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.52, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !607
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 128
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.70, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !610
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 1
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.54, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !612
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 384, i32 128, i32 1, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.71, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !624
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 128
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.72, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !638
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !652
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !666
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 56
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.82, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !668
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 56
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.83, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !671
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 128
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.75, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !673
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 401408, i32 7168, i32 128, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.84, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_13_compute_(i8* %31, i8* %39, i8* %45, i8* %51, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_13_compute_(i8* noalias nocapture readonly align 128 %0, i8* noalias nocapture readonly align 128 %1, i8* noalias nocapture readonly align 128 %2, i8* noalias nocapture align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 1722368, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %for_end38, %if_end, %entry
  ret i32 -1

if_end:                                           ; preds = %entry
  %8 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %9 = tail call i8* %8(i32 1, i32 %4, i64 1605632, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %9, i64 128) ]
  %10 = icmp eq i8* %9, null
  br i1 %10, label %if_then, label %for_begin4.preheader, !prof !5

for_begin4.preheader:                             ; preds = %if_end, %for_end6
  %indvar = phi i64 [ %indvar.next, %for_end6 ], [ 0, %if_end ]
  %11 = phi i32 [ %25, %for_end6 ], [ 0, %if_end ]
  %12 = mul nuw nsw i64 %indvar, 29696
  %13 = mul nuw nsw i64 %indvar, 28672
  %14 = add nsw i64 %13, -29184
  %.off = add nsw i32 %11, -1
  %15 = icmp ult i32 %.off, 56
  br i1 %15, label %for_begin7.preheader.us, label %for_begin7.preheader.preheader

for_begin7.preheader.preheader:                   ; preds = %for_begin4.preheader
  %scevgep104 = getelementptr i8, i8* %6, i64 %12
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(29696) %scevgep104, i8 0, i64 29696, i1 false)
  br label %for_end6

for_begin7.preheader.us:                          ; preds = %for_begin4.preheader, %for_end9.us
  %indvar105 = phi i64 [ %indvar.next106, %for_end9.us ], [ 0, %for_begin4.preheader ]
  %16 = phi i32 [ %21, %for_end9.us ], [ 0, %for_begin4.preheader ]
  %17 = shl nuw nsw i64 %indvar105, 9
  %18 = add nuw nsw i64 %12, %17
  %scevgep109 = getelementptr i8, i8* %6, i64 %18
  %.off58.us = add nsw i32 %16, -1
  %19 = icmp ult i32 %.off58.us, 56
  br i1 %19, label %for_body8.us.us.preheader, label %for_body8.us61.preheader

for_body8.us61.preheader:                         ; preds = %for_begin7.preheader.us
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(512) %scevgep109, i8 0, i64 512, i1 false)
  br label %for_end9.us

for_body8.us.us.preheader:                        ; preds = %for_begin7.preheader.us
  %20 = add nsw i64 %14, %17
  %scevgep110 = getelementptr i8, i8* %0, i64 %20
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(512) %scevgep109, i8* nonnull align 128 dereferenceable(512) %scevgep110, i64 512, i1 false)
  br label %for_end9.us

for_end9.us:                                      ; preds = %for_body8.us61.preheader, %for_body8.us.us.preheader
  %21 = add nuw nsw i32 %16, 1
  %indvar.next106 = add nuw nsw i64 %indvar105, 1
  %exitcond113.not = icmp eq i64 %indvar.next106, 58
  br i1 %exitcond113.not, label %for_end6, label %for_begin7.preheader.us, !prof !51

for_begin12.preheader:                            ; preds = %for_end6
  %22 = bitcast i8* %9 to float*
  %23 = bitcast i8* %6 to float*
  %24 = bitcast i8* %1 to float*
  br label %for_begin15.preheader

for_end6:                                         ; preds = %for_end9.us, %for_begin7.preheader.preheader
  %25 = add nuw nsw i32 %11, 1
  %indvar.next = add nuw nsw i64 %indvar, 1
  %exitcond114.not = icmp eq i64 %indvar.next, 58
  br i1 %exitcond114.not, label %for_begin12.preheader, label %for_begin4.preheader, !prof !51

for_begin15.preheader:                            ; preds = %for_begin12.preheader, %for_end17
  %indvars.iv97 = phi i64 [ 0, %for_begin12.preheader ], [ %28, %for_end17 ]
  %26 = mul nuw nsw i64 %indvars.iv97, 7168
  %27 = mul nuw nsw i64 %indvars.iv97, 7424
  %28 = add nuw nsw i64 %indvars.iv97, 1
  %29 = mul nuw nsw i64 %28, 7424
  %30 = mul i64 %indvars.iv97, 7424
  %31 = add i64 %30, 14848
  br label %for_begin18.preheader

for_begin27.preheader:                            ; preds = %for_end17
  %32 = bitcast i8* %2 to <4 x float>*
  %wide.load140 = load <4 x float>, <4 x float>* %32, align 128, !tbaa !685
  %33 = getelementptr inbounds i8, i8* %2, i64 16
  %34 = bitcast i8* %33 to <4 x float>*
  %wide.load141 = load <4 x float>, <4 x float>* %34, align 16, !tbaa !685
  %35 = getelementptr inbounds i8, i8* %2, i64 32
  %36 = bitcast i8* %35 to <4 x float>*
  %wide.load140.1 = load <4 x float>, <4 x float>* %36, align 32, !tbaa !685
  %37 = getelementptr inbounds i8, i8* %2, i64 48
  %38 = bitcast i8* %37 to <4 x float>*
  %wide.load141.1 = load <4 x float>, <4 x float>* %38, align 16, !tbaa !685
  %39 = getelementptr inbounds i8, i8* %2, i64 64
  %40 = bitcast i8* %39 to <4 x float>*
  %wide.load140.2 = load <4 x float>, <4 x float>* %40, align 64, !tbaa !685
  %41 = getelementptr inbounds i8, i8* %2, i64 80
  %42 = bitcast i8* %41 to <4 x float>*
  %wide.load141.2 = load <4 x float>, <4 x float>* %42, align 16, !tbaa !685
  %43 = getelementptr inbounds i8, i8* %2, i64 96
  %44 = bitcast i8* %43 to <4 x float>*
  %wide.load140.3 = load <4 x float>, <4 x float>* %44, align 32, !tbaa !685
  %45 = getelementptr inbounds i8, i8* %2, i64 112
  %46 = bitcast i8* %45 to <4 x float>*
  %wide.load141.3 = load <4 x float>, <4 x float>* %46, align 16, !tbaa !685
  %47 = getelementptr inbounds i8, i8* %2, i64 128
  %48 = bitcast i8* %47 to <4 x float>*
  %wide.load140.4 = load <4 x float>, <4 x float>* %48, align 128, !tbaa !685
  %49 = getelementptr inbounds i8, i8* %2, i64 144
  %50 = bitcast i8* %49 to <4 x float>*
  %wide.load141.4 = load <4 x float>, <4 x float>* %50, align 16, !tbaa !685
  %51 = getelementptr inbounds i8, i8* %2, i64 160
  %52 = bitcast i8* %51 to <4 x float>*
  %wide.load140.5 = load <4 x float>, <4 x float>* %52, align 32, !tbaa !685
  %53 = getelementptr inbounds i8, i8* %2, i64 176
  %54 = bitcast i8* %53 to <4 x float>*
  %wide.load141.5 = load <4 x float>, <4 x float>* %54, align 16, !tbaa !685
  %55 = getelementptr inbounds i8, i8* %2, i64 192
  %56 = bitcast i8* %55 to <4 x float>*
  %wide.load140.6 = load <4 x float>, <4 x float>* %56, align 64, !tbaa !685
  %57 = getelementptr inbounds i8, i8* %2, i64 208
  %58 = bitcast i8* %57 to <4 x float>*
  %wide.load141.6 = load <4 x float>, <4 x float>* %58, align 16, !tbaa !685
  %59 = getelementptr inbounds i8, i8* %2, i64 224
  %60 = bitcast i8* %59 to <4 x float>*
  %wide.load140.7 = load <4 x float>, <4 x float>* %60, align 32, !tbaa !685
  %61 = getelementptr inbounds i8, i8* %2, i64 240
  %62 = bitcast i8* %61 to <4 x float>*
  %wide.load141.7 = load <4 x float>, <4 x float>* %62, align 16, !tbaa !685
  %63 = getelementptr inbounds i8, i8* %2, i64 256
  %64 = bitcast i8* %63 to <4 x float>*
  %wide.load140.8 = load <4 x float>, <4 x float>* %64, align 128, !tbaa !685
  %65 = getelementptr inbounds i8, i8* %2, i64 272
  %66 = bitcast i8* %65 to <4 x float>*
  %wide.load141.8 = load <4 x float>, <4 x float>* %66, align 16, !tbaa !685
  %67 = getelementptr inbounds i8, i8* %2, i64 288
  %68 = bitcast i8* %67 to <4 x float>*
  %wide.load140.9 = load <4 x float>, <4 x float>* %68, align 32, !tbaa !685
  %69 = getelementptr inbounds i8, i8* %2, i64 304
  %70 = bitcast i8* %69 to <4 x float>*
  %wide.load141.9 = load <4 x float>, <4 x float>* %70, align 16, !tbaa !685
  %71 = getelementptr inbounds i8, i8* %2, i64 320
  %72 = bitcast i8* %71 to <4 x float>*
  %wide.load140.10 = load <4 x float>, <4 x float>* %72, align 64, !tbaa !685
  %73 = getelementptr inbounds i8, i8* %2, i64 336
  %74 = bitcast i8* %73 to <4 x float>*
  %wide.load141.10 = load <4 x float>, <4 x float>* %74, align 16, !tbaa !685
  %75 = getelementptr inbounds i8, i8* %2, i64 352
  %76 = bitcast i8* %75 to <4 x float>*
  %wide.load140.11 = load <4 x float>, <4 x float>* %76, align 32, !tbaa !685
  %77 = getelementptr inbounds i8, i8* %2, i64 368
  %78 = bitcast i8* %77 to <4 x float>*
  %wide.load141.11 = load <4 x float>, <4 x float>* %78, align 16, !tbaa !685
  %79 = getelementptr inbounds i8, i8* %2, i64 384
  %80 = bitcast i8* %79 to <4 x float>*
  %wide.load140.12 = load <4 x float>, <4 x float>* %80, align 128, !tbaa !685
  %81 = getelementptr inbounds i8, i8* %2, i64 400
  %82 = bitcast i8* %81 to <4 x float>*
  %wide.load141.12 = load <4 x float>, <4 x float>* %82, align 16, !tbaa !685
  %83 = getelementptr inbounds i8, i8* %2, i64 416
  %84 = bitcast i8* %83 to <4 x float>*
  %wide.load140.13 = load <4 x float>, <4 x float>* %84, align 32, !tbaa !685
  %85 = getelementptr inbounds i8, i8* %2, i64 432
  %86 = bitcast i8* %85 to <4 x float>*
  %wide.load141.13 = load <4 x float>, <4 x float>* %86, align 16, !tbaa !685
  %87 = getelementptr inbounds i8, i8* %2, i64 448
  %88 = bitcast i8* %87 to <4 x float>*
  %wide.load140.14 = load <4 x float>, <4 x float>* %88, align 64, !tbaa !685
  %89 = getelementptr inbounds i8, i8* %2, i64 464
  %90 = bitcast i8* %89 to <4 x float>*
  %wide.load141.14 = load <4 x float>, <4 x float>* %90, align 16, !tbaa !685
  %91 = getelementptr inbounds i8, i8* %2, i64 480
  %92 = bitcast i8* %91 to <4 x float>*
  %wide.load140.15 = load <4 x float>, <4 x float>* %92, align 32, !tbaa !685
  %93 = getelementptr inbounds i8, i8* %2, i64 496
  %94 = bitcast i8* %93 to <4 x float>*
  %wide.load141.15 = load <4 x float>, <4 x float>* %94, align 16, !tbaa !685
  br label %for_begin30.preheader

for_begin18.preheader:                            ; preds = %for_begin15.preheader, %for_end20
  %indvars.iv94 = phi i64 [ 0, %for_begin15.preheader ], [ %indvars.iv.next95, %for_end20 ]
  %95 = shl nsw i64 %indvars.iv94, 7
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_begin18.preheader
  %index = phi i64 [ 0, %for_begin18.preheader ], [ %index.next, %vector.body ]
  %96 = add nuw nsw i64 %index, %95
  %97 = add nuw nsw i64 %96, %26
  %98 = getelementptr inbounds float, float* %22, i64 %97
  %99 = add nuw nsw i64 %96, %27
  %100 = getelementptr inbounds float, float* %23, i64 %99
  %101 = bitcast float* %100 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %101, align 16, !tbaa !688
  %102 = getelementptr inbounds float, float* %24, i64 %index
  %103 = bitcast float* %102 to <4 x float>*
  %wide.load115 = load <4 x float>, <4 x float>* %103, align 16, !tbaa !691
  %104 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load, <4 x float> %wide.load115, <4 x float> zeroinitializer)
  %105 = add nuw nsw i64 %99, 128
  %106 = getelementptr inbounds float, float* %23, i64 %105
  %107 = bitcast float* %106 to <4 x float>*
  %wide.load116 = load <4 x float>, <4 x float>* %107, align 16, !tbaa !688
  %108 = add nuw nsw i64 %index, 128
  %109 = getelementptr inbounds float, float* %24, i64 %108
  %110 = bitcast float* %109 to <4 x float>*
  %wide.load117 = load <4 x float>, <4 x float>* %110, align 16, !tbaa !691
  %111 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load116, <4 x float> %wide.load117, <4 x float> %104)
  %112 = add nuw nsw i64 %99, 256
  %113 = getelementptr inbounds float, float* %23, i64 %112
  %114 = bitcast float* %113 to <4 x float>*
  %wide.load118 = load <4 x float>, <4 x float>* %114, align 16, !tbaa !688
  %115 = add nuw nsw i64 %index, 256
  %116 = getelementptr inbounds float, float* %24, i64 %115
  %117 = bitcast float* %116 to <4 x float>*
  %wide.load119 = load <4 x float>, <4 x float>* %117, align 16, !tbaa !691
  %118 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load118, <4 x float> %wide.load119, <4 x float> %111)
  %119 = add nuw nsw i64 %96, %29
  %120 = getelementptr inbounds float, float* %23, i64 %119
  %121 = bitcast float* %120 to <4 x float>*
  %wide.load120 = load <4 x float>, <4 x float>* %121, align 16, !tbaa !688
  %122 = add nuw nsw i64 %index, 384
  %123 = getelementptr inbounds float, float* %24, i64 %122
  %124 = bitcast float* %123 to <4 x float>*
  %wide.load121 = load <4 x float>, <4 x float>* %124, align 16, !tbaa !691
  %125 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load120, <4 x float> %wide.load121, <4 x float> %118)
  %126 = add nuw nsw i64 %119, 128
  %127 = getelementptr inbounds float, float* %23, i64 %126
  %128 = bitcast float* %127 to <4 x float>*
  %wide.load122 = load <4 x float>, <4 x float>* %128, align 16, !tbaa !688
  %129 = add nuw nsw i64 %index, 512
  %130 = getelementptr inbounds float, float* %24, i64 %129
  %131 = bitcast float* %130 to <4 x float>*
  %wide.load123 = load <4 x float>, <4 x float>* %131, align 16, !tbaa !691
  %132 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load122, <4 x float> %wide.load123, <4 x float> %125)
  %133 = add nuw nsw i64 %119, 256
  %134 = getelementptr inbounds float, float* %23, i64 %133
  %135 = bitcast float* %134 to <4 x float>*
  %wide.load124 = load <4 x float>, <4 x float>* %135, align 16, !tbaa !688
  %136 = add nuw nsw i64 %index, 640
  %137 = getelementptr inbounds float, float* %24, i64 %136
  %138 = bitcast float* %137 to <4 x float>*
  %wide.load125 = load <4 x float>, <4 x float>* %138, align 16, !tbaa !691
  %139 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load124, <4 x float> %wide.load125, <4 x float> %132)
  %140 = add nuw nsw i64 %96, %31
  %141 = getelementptr inbounds float, float* %23, i64 %140
  %142 = bitcast float* %141 to <4 x float>*
  %wide.load126 = load <4 x float>, <4 x float>* %142, align 16, !tbaa !688
  %143 = add nuw nsw i64 %index, 768
  %144 = getelementptr inbounds float, float* %24, i64 %143
  %145 = bitcast float* %144 to <4 x float>*
  %wide.load127 = load <4 x float>, <4 x float>* %145, align 16, !tbaa !691
  %146 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load126, <4 x float> %wide.load127, <4 x float> %139)
  %147 = add nuw nsw i64 %140, 128
  %148 = getelementptr inbounds float, float* %23, i64 %147
  %149 = bitcast float* %148 to <4 x float>*
  %wide.load128 = load <4 x float>, <4 x float>* %149, align 16, !tbaa !688
  %150 = add nuw nsw i64 %index, 896
  %151 = getelementptr inbounds float, float* %24, i64 %150
  %152 = bitcast float* %151 to <4 x float>*
  %wide.load129 = load <4 x float>, <4 x float>* %152, align 16, !tbaa !691
  %153 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load128, <4 x float> %wide.load129, <4 x float> %146)
  %154 = add nuw nsw i64 %140, 256
  %155 = getelementptr inbounds float, float* %23, i64 %154
  %156 = bitcast float* %155 to <4 x float>*
  %wide.load130 = load <4 x float>, <4 x float>* %156, align 16, !tbaa !688
  %157 = add nuw nsw i64 %index, 1024
  %158 = getelementptr inbounds float, float* %24, i64 %157
  %159 = bitcast float* %158 to <4 x float>*
  %wide.load131 = load <4 x float>, <4 x float>* %159, align 16, !tbaa !691
  %160 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load130, <4 x float> %wide.load131, <4 x float> %153)
  %161 = bitcast float* %98 to <4 x float>*
  store <4 x float> %160, <4 x float>* %161, align 16, !tbaa !694
  %index.next = add i64 %index, 4
  %162 = icmp eq i64 %index.next, 128
  br i1 %162, label %for_end20, label %vector.body, !prof !335, !llvm.loop !697

for_end17:                                        ; preds = %for_end20
  %exitcond99.not = icmp eq i64 %28, 56
  br i1 %exitcond99.not, label %for_begin27.preheader, label %for_begin15.preheader, !prof !51

for_end20:                                        ; preds = %vector.body
  %indvars.iv.next95 = add nuw nsw i64 %indvars.iv94, 1
  %exitcond96.not = icmp eq i64 %indvars.iv.next95, 56
  br i1 %exitcond96.not, label %for_end17, label %for_begin18.preheader, !prof !51

for_begin30.preheader:                            ; preds = %for_begin27.preheader, %for_end32
  %indvars.iv80 = phi i64 [ 0, %for_begin27.preheader ], [ %indvars.iv.next81, %for_end32 ]
  %163 = mul nuw nsw i64 %indvars.iv80, 7168
  br label %for_begin33.preheader

for_begin36.preheader:                            ; preds = %for_end32
  %164 = bitcast i8* %3 to float*
  br label %for_begin39.preheader

for_begin33.preheader:                            ; preds = %for_begin30.preheader, %for_begin33.preheader
  %indvars.iv77 = phi i64 [ 0, %for_begin30.preheader ], [ %indvars.iv.next78, %for_begin33.preheader ]
  %165 = shl nsw i64 %indvars.iv77, 7
  %166 = add nuw nsw i64 %165, %163
  %167 = getelementptr inbounds float, float* %22, i64 %166
  %168 = bitcast float* %167 to <4 x float>*
  %wide.load142 = load <4 x float>, <4 x float>* %168, align 128, !tbaa !694
  %169 = getelementptr inbounds float, float* %167, i64 4
  %170 = bitcast float* %169 to <4 x float>*
  %wide.load143 = load <4 x float>, <4 x float>* %170, align 16, !tbaa !694
  %171 = fadd <4 x float> %wide.load140, %wide.load142
  %172 = fadd <4 x float> %wide.load141, %wide.load143
  %173 = bitcast float* %167 to <4 x float>*
  store <4 x float> %171, <4 x float>* %173, align 128, !tbaa !694
  %174 = bitcast float* %169 to <4 x float>*
  store <4 x float> %172, <4 x float>* %174, align 16, !tbaa !694
  %175 = or i64 %166, 8
  %176 = getelementptr inbounds float, float* %22, i64 %175
  %177 = bitcast float* %176 to <4 x float>*
  %wide.load142.1 = load <4 x float>, <4 x float>* %177, align 32, !tbaa !694
  %178 = getelementptr inbounds float, float* %176, i64 4
  %179 = bitcast float* %178 to <4 x float>*
  %wide.load143.1 = load <4 x float>, <4 x float>* %179, align 16, !tbaa !694
  %180 = fadd <4 x float> %wide.load140.1, %wide.load142.1
  %181 = fadd <4 x float> %wide.load141.1, %wide.load143.1
  %182 = bitcast float* %176 to <4 x float>*
  store <4 x float> %180, <4 x float>* %182, align 32, !tbaa !694
  %183 = bitcast float* %178 to <4 x float>*
  store <4 x float> %181, <4 x float>* %183, align 16, !tbaa !694
  %184 = or i64 %166, 16
  %185 = getelementptr inbounds float, float* %22, i64 %184
  %186 = bitcast float* %185 to <4 x float>*
  %wide.load142.2 = load <4 x float>, <4 x float>* %186, align 64, !tbaa !694
  %187 = getelementptr inbounds float, float* %185, i64 4
  %188 = bitcast float* %187 to <4 x float>*
  %wide.load143.2 = load <4 x float>, <4 x float>* %188, align 16, !tbaa !694
  %189 = fadd <4 x float> %wide.load140.2, %wide.load142.2
  %190 = fadd <4 x float> %wide.load141.2, %wide.load143.2
  %191 = bitcast float* %185 to <4 x float>*
  store <4 x float> %189, <4 x float>* %191, align 64, !tbaa !694
  %192 = bitcast float* %187 to <4 x float>*
  store <4 x float> %190, <4 x float>* %192, align 16, !tbaa !694
  %193 = or i64 %166, 24
  %194 = getelementptr inbounds float, float* %22, i64 %193
  %195 = bitcast float* %194 to <4 x float>*
  %wide.load142.3 = load <4 x float>, <4 x float>* %195, align 32, !tbaa !694
  %196 = getelementptr inbounds float, float* %194, i64 4
  %197 = bitcast float* %196 to <4 x float>*
  %wide.load143.3 = load <4 x float>, <4 x float>* %197, align 16, !tbaa !694
  %198 = fadd <4 x float> %wide.load140.3, %wide.load142.3
  %199 = fadd <4 x float> %wide.load141.3, %wide.load143.3
  %200 = bitcast float* %194 to <4 x float>*
  store <4 x float> %198, <4 x float>* %200, align 32, !tbaa !694
  %201 = bitcast float* %196 to <4 x float>*
  store <4 x float> %199, <4 x float>* %201, align 16, !tbaa !694
  %202 = or i64 %166, 32
  %203 = getelementptr inbounds float, float* %22, i64 %202
  %204 = bitcast float* %203 to <4 x float>*
  %wide.load142.4 = load <4 x float>, <4 x float>* %204, align 128, !tbaa !694
  %205 = getelementptr inbounds float, float* %203, i64 4
  %206 = bitcast float* %205 to <4 x float>*
  %wide.load143.4 = load <4 x float>, <4 x float>* %206, align 16, !tbaa !694
  %207 = fadd <4 x float> %wide.load140.4, %wide.load142.4
  %208 = fadd <4 x float> %wide.load141.4, %wide.load143.4
  %209 = bitcast float* %203 to <4 x float>*
  store <4 x float> %207, <4 x float>* %209, align 128, !tbaa !694
  %210 = bitcast float* %205 to <4 x float>*
  store <4 x float> %208, <4 x float>* %210, align 16, !tbaa !694
  %211 = or i64 %166, 40
  %212 = getelementptr inbounds float, float* %22, i64 %211
  %213 = bitcast float* %212 to <4 x float>*
  %wide.load142.5 = load <4 x float>, <4 x float>* %213, align 32, !tbaa !694
  %214 = getelementptr inbounds float, float* %212, i64 4
  %215 = bitcast float* %214 to <4 x float>*
  %wide.load143.5 = load <4 x float>, <4 x float>* %215, align 16, !tbaa !694
  %216 = fadd <4 x float> %wide.load140.5, %wide.load142.5
  %217 = fadd <4 x float> %wide.load141.5, %wide.load143.5
  %218 = bitcast float* %212 to <4 x float>*
  store <4 x float> %216, <4 x float>* %218, align 32, !tbaa !694
  %219 = bitcast float* %214 to <4 x float>*
  store <4 x float> %217, <4 x float>* %219, align 16, !tbaa !694
  %220 = or i64 %166, 48
  %221 = getelementptr inbounds float, float* %22, i64 %220
  %222 = bitcast float* %221 to <4 x float>*
  %wide.load142.6 = load <4 x float>, <4 x float>* %222, align 64, !tbaa !694
  %223 = getelementptr inbounds float, float* %221, i64 4
  %224 = bitcast float* %223 to <4 x float>*
  %wide.load143.6 = load <4 x float>, <4 x float>* %224, align 16, !tbaa !694
  %225 = fadd <4 x float> %wide.load140.6, %wide.load142.6
  %226 = fadd <4 x float> %wide.load141.6, %wide.load143.6
  %227 = bitcast float* %221 to <4 x float>*
  store <4 x float> %225, <4 x float>* %227, align 64, !tbaa !694
  %228 = bitcast float* %223 to <4 x float>*
  store <4 x float> %226, <4 x float>* %228, align 16, !tbaa !694
  %229 = or i64 %166, 56
  %230 = getelementptr inbounds float, float* %22, i64 %229
  %231 = bitcast float* %230 to <4 x float>*
  %wide.load142.7 = load <4 x float>, <4 x float>* %231, align 32, !tbaa !694
  %232 = getelementptr inbounds float, float* %230, i64 4
  %233 = bitcast float* %232 to <4 x float>*
  %wide.load143.7 = load <4 x float>, <4 x float>* %233, align 16, !tbaa !694
  %234 = fadd <4 x float> %wide.load140.7, %wide.load142.7
  %235 = fadd <4 x float> %wide.load141.7, %wide.load143.7
  %236 = bitcast float* %230 to <4 x float>*
  store <4 x float> %234, <4 x float>* %236, align 32, !tbaa !694
  %237 = bitcast float* %232 to <4 x float>*
  store <4 x float> %235, <4 x float>* %237, align 16, !tbaa !694
  %238 = or i64 %166, 64
  %239 = getelementptr inbounds float, float* %22, i64 %238
  %240 = bitcast float* %239 to <4 x float>*
  %wide.load142.8 = load <4 x float>, <4 x float>* %240, align 128, !tbaa !694
  %241 = getelementptr inbounds float, float* %239, i64 4
  %242 = bitcast float* %241 to <4 x float>*
  %wide.load143.8 = load <4 x float>, <4 x float>* %242, align 16, !tbaa !694
  %243 = fadd <4 x float> %wide.load140.8, %wide.load142.8
  %244 = fadd <4 x float> %wide.load141.8, %wide.load143.8
  %245 = bitcast float* %239 to <4 x float>*
  store <4 x float> %243, <4 x float>* %245, align 128, !tbaa !694
  %246 = bitcast float* %241 to <4 x float>*
  store <4 x float> %244, <4 x float>* %246, align 16, !tbaa !694
  %247 = or i64 %166, 72
  %248 = getelementptr inbounds float, float* %22, i64 %247
  %249 = bitcast float* %248 to <4 x float>*
  %wide.load142.9 = load <4 x float>, <4 x float>* %249, align 32, !tbaa !694
  %250 = getelementptr inbounds float, float* %248, i64 4
  %251 = bitcast float* %250 to <4 x float>*
  %wide.load143.9 = load <4 x float>, <4 x float>* %251, align 16, !tbaa !694
  %252 = fadd <4 x float> %wide.load140.9, %wide.load142.9
  %253 = fadd <4 x float> %wide.load141.9, %wide.load143.9
  %254 = bitcast float* %248 to <4 x float>*
  store <4 x float> %252, <4 x float>* %254, align 32, !tbaa !694
  %255 = bitcast float* %250 to <4 x float>*
  store <4 x float> %253, <4 x float>* %255, align 16, !tbaa !694
  %256 = or i64 %166, 80
  %257 = getelementptr inbounds float, float* %22, i64 %256
  %258 = bitcast float* %257 to <4 x float>*
  %wide.load142.10 = load <4 x float>, <4 x float>* %258, align 64, !tbaa !694
  %259 = getelementptr inbounds float, float* %257, i64 4
  %260 = bitcast float* %259 to <4 x float>*
  %wide.load143.10 = load <4 x float>, <4 x float>* %260, align 16, !tbaa !694
  %261 = fadd <4 x float> %wide.load140.10, %wide.load142.10
  %262 = fadd <4 x float> %wide.load141.10, %wide.load143.10
  %263 = bitcast float* %257 to <4 x float>*
  store <4 x float> %261, <4 x float>* %263, align 64, !tbaa !694
  %264 = bitcast float* %259 to <4 x float>*
  store <4 x float> %262, <4 x float>* %264, align 16, !tbaa !694
  %265 = or i64 %166, 88
  %266 = getelementptr inbounds float, float* %22, i64 %265
  %267 = bitcast float* %266 to <4 x float>*
  %wide.load142.11 = load <4 x float>, <4 x float>* %267, align 32, !tbaa !694
  %268 = getelementptr inbounds float, float* %266, i64 4
  %269 = bitcast float* %268 to <4 x float>*
  %wide.load143.11 = load <4 x float>, <4 x float>* %269, align 16, !tbaa !694
  %270 = fadd <4 x float> %wide.load140.11, %wide.load142.11
  %271 = fadd <4 x float> %wide.load141.11, %wide.load143.11
  %272 = bitcast float* %266 to <4 x float>*
  store <4 x float> %270, <4 x float>* %272, align 32, !tbaa !694
  %273 = bitcast float* %268 to <4 x float>*
  store <4 x float> %271, <4 x float>* %273, align 16, !tbaa !694
  %274 = or i64 %166, 96
  %275 = getelementptr inbounds float, float* %22, i64 %274
  %276 = bitcast float* %275 to <4 x float>*
  %wide.load142.12 = load <4 x float>, <4 x float>* %276, align 128, !tbaa !694
  %277 = getelementptr inbounds float, float* %275, i64 4
  %278 = bitcast float* %277 to <4 x float>*
  %wide.load143.12 = load <4 x float>, <4 x float>* %278, align 16, !tbaa !694
  %279 = fadd <4 x float> %wide.load140.12, %wide.load142.12
  %280 = fadd <4 x float> %wide.load141.12, %wide.load143.12
  %281 = bitcast float* %275 to <4 x float>*
  store <4 x float> %279, <4 x float>* %281, align 128, !tbaa !694
  %282 = bitcast float* %277 to <4 x float>*
  store <4 x float> %280, <4 x float>* %282, align 16, !tbaa !694
  %283 = or i64 %166, 104
  %284 = getelementptr inbounds float, float* %22, i64 %283
  %285 = bitcast float* %284 to <4 x float>*
  %wide.load142.13 = load <4 x float>, <4 x float>* %285, align 32, !tbaa !694
  %286 = getelementptr inbounds float, float* %284, i64 4
  %287 = bitcast float* %286 to <4 x float>*
  %wide.load143.13 = load <4 x float>, <4 x float>* %287, align 16, !tbaa !694
  %288 = fadd <4 x float> %wide.load140.13, %wide.load142.13
  %289 = fadd <4 x float> %wide.load141.13, %wide.load143.13
  %290 = bitcast float* %284 to <4 x float>*
  store <4 x float> %288, <4 x float>* %290, align 32, !tbaa !694
  %291 = bitcast float* %286 to <4 x float>*
  store <4 x float> %289, <4 x float>* %291, align 16, !tbaa !694
  %292 = or i64 %166, 112
  %293 = getelementptr inbounds float, float* %22, i64 %292
  %294 = bitcast float* %293 to <4 x float>*
  %wide.load142.14 = load <4 x float>, <4 x float>* %294, align 64, !tbaa !694
  %295 = getelementptr inbounds float, float* %293, i64 4
  %296 = bitcast float* %295 to <4 x float>*
  %wide.load143.14 = load <4 x float>, <4 x float>* %296, align 16, !tbaa !694
  %297 = fadd <4 x float> %wide.load140.14, %wide.load142.14
  %298 = fadd <4 x float> %wide.load141.14, %wide.load143.14
  %299 = bitcast float* %293 to <4 x float>*
  store <4 x float> %297, <4 x float>* %299, align 64, !tbaa !694
  %300 = bitcast float* %295 to <4 x float>*
  store <4 x float> %298, <4 x float>* %300, align 16, !tbaa !694
  %301 = or i64 %166, 120
  %302 = getelementptr inbounds float, float* %22, i64 %301
  %303 = bitcast float* %302 to <4 x float>*
  %wide.load142.15 = load <4 x float>, <4 x float>* %303, align 32, !tbaa !694
  %304 = getelementptr inbounds float, float* %302, i64 4
  %305 = bitcast float* %304 to <4 x float>*
  %wide.load143.15 = load <4 x float>, <4 x float>* %305, align 16, !tbaa !694
  %306 = fadd <4 x float> %wide.load140.15, %wide.load142.15
  %307 = fadd <4 x float> %wide.load141.15, %wide.load143.15
  %308 = bitcast float* %302 to <4 x float>*
  store <4 x float> %306, <4 x float>* %308, align 32, !tbaa !694
  %309 = bitcast float* %304 to <4 x float>*
  store <4 x float> %307, <4 x float>* %309, align 16, !tbaa !694
  %indvars.iv.next78 = add nuw nsw i64 %indvars.iv77, 1
  %exitcond79.not = icmp eq i64 %indvars.iv.next78, 56
  br i1 %exitcond79.not, label %for_end32, label %for_begin33.preheader, !prof !51

for_end32:                                        ; preds = %for_begin33.preheader
  %indvars.iv.next81 = add nuw nsw i64 %indvars.iv80, 1
  %exitcond82.not = icmp eq i64 %indvars.iv.next81, 56
  br i1 %exitcond82.not, label %for_begin36.preheader, label %for_begin30.preheader, !prof !51

for_begin39.preheader:                            ; preds = %for_begin36.preheader, %for_end41
  %indvars.iv71 = phi i64 [ 0, %for_begin36.preheader ], [ %indvars.iv.next72, %for_end41 ]
  %310 = mul nuw nsw i64 %indvars.iv71, 7168
  br label %for_begin42.preheader

for_end38:                                        ; preds = %for_end41
  %311 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %312 = tail call i32 %311(i32 1, i32 %4, i8* nonnull %9)
  %.not = icmp eq i32 %312, 0
  br i1 %.not, label %if_end46, label %if_then, !prof !51

for_begin42.preheader:                            ; preds = %for_begin39.preheader, %for_begin42.preheader
  %indvars.iv68 = phi i64 [ 0, %for_begin39.preheader ], [ %indvars.iv.next69, %for_begin42.preheader ]
  %313 = shl nsw i64 %indvars.iv68, 7
  %314 = add nuw nsw i64 %313, %310
  %315 = getelementptr inbounds float, float* %22, i64 %314
  %316 = bitcast float* %315 to <4 x float>*
  %wide.load152 = load <4 x float>, <4 x float>* %316, align 128, !tbaa !694
  %317 = getelementptr inbounds float, float* %315, i64 4
  %318 = bitcast float* %317 to <4 x float>*
  %wide.load153 = load <4 x float>, <4 x float>* %318, align 16, !tbaa !694
  %319 = fcmp olt <4 x float> %wide.load152, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %320 = fcmp olt <4 x float> %wide.load153, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %321 = select <4 x i1> %319, <4 x float> %wide.load152, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %322 = select <4 x i1> %320, <4 x float> %wide.load153, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %323 = fcmp ogt <4 x float> %321, zeroinitializer
  %324 = fcmp ogt <4 x float> %322, zeroinitializer
  %325 = select <4 x i1> %323, <4 x float> %321, <4 x float> zeroinitializer
  %326 = select <4 x i1> %324, <4 x float> %322, <4 x float> zeroinitializer
  %327 = getelementptr inbounds float, float* %164, i64 %314
  %328 = bitcast float* %327 to <4 x float>*
  store <4 x float> %325, <4 x float>* %328, align 128, !tbaa !698
  %329 = getelementptr inbounds float, float* %327, i64 4
  %330 = bitcast float* %329 to <4 x float>*
  store <4 x float> %326, <4 x float>* %330, align 16, !tbaa !698
  %331 = or i64 %314, 8
  %332 = getelementptr inbounds float, float* %22, i64 %331
  %333 = bitcast float* %332 to <4 x float>*
  %wide.load152.1 = load <4 x float>, <4 x float>* %333, align 32, !tbaa !694
  %334 = getelementptr inbounds float, float* %332, i64 4
  %335 = bitcast float* %334 to <4 x float>*
  %wide.load153.1 = load <4 x float>, <4 x float>* %335, align 16, !tbaa !694
  %336 = fcmp olt <4 x float> %wide.load152.1, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %337 = fcmp olt <4 x float> %wide.load153.1, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %338 = select <4 x i1> %336, <4 x float> %wide.load152.1, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %339 = select <4 x i1> %337, <4 x float> %wide.load153.1, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %340 = fcmp ogt <4 x float> %338, zeroinitializer
  %341 = fcmp ogt <4 x float> %339, zeroinitializer
  %342 = select <4 x i1> %340, <4 x float> %338, <4 x float> zeroinitializer
  %343 = select <4 x i1> %341, <4 x float> %339, <4 x float> zeroinitializer
  %344 = getelementptr inbounds float, float* %164, i64 %331
  %345 = bitcast float* %344 to <4 x float>*
  store <4 x float> %342, <4 x float>* %345, align 32, !tbaa !698
  %346 = getelementptr inbounds float, float* %344, i64 4
  %347 = bitcast float* %346 to <4 x float>*
  store <4 x float> %343, <4 x float>* %347, align 16, !tbaa !698
  %348 = or i64 %314, 16
  %349 = getelementptr inbounds float, float* %22, i64 %348
  %350 = bitcast float* %349 to <4 x float>*
  %wide.load152.2 = load <4 x float>, <4 x float>* %350, align 64, !tbaa !694
  %351 = getelementptr inbounds float, float* %349, i64 4
  %352 = bitcast float* %351 to <4 x float>*
  %wide.load153.2 = load <4 x float>, <4 x float>* %352, align 16, !tbaa !694
  %353 = fcmp olt <4 x float> %wide.load152.2, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %354 = fcmp olt <4 x float> %wide.load153.2, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %355 = select <4 x i1> %353, <4 x float> %wide.load152.2, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %356 = select <4 x i1> %354, <4 x float> %wide.load153.2, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %357 = fcmp ogt <4 x float> %355, zeroinitializer
  %358 = fcmp ogt <4 x float> %356, zeroinitializer
  %359 = select <4 x i1> %357, <4 x float> %355, <4 x float> zeroinitializer
  %360 = select <4 x i1> %358, <4 x float> %356, <4 x float> zeroinitializer
  %361 = getelementptr inbounds float, float* %164, i64 %348
  %362 = bitcast float* %361 to <4 x float>*
  store <4 x float> %359, <4 x float>* %362, align 64, !tbaa !698
  %363 = getelementptr inbounds float, float* %361, i64 4
  %364 = bitcast float* %363 to <4 x float>*
  store <4 x float> %360, <4 x float>* %364, align 16, !tbaa !698
  %365 = or i64 %314, 24
  %366 = getelementptr inbounds float, float* %22, i64 %365
  %367 = bitcast float* %366 to <4 x float>*
  %wide.load152.3 = load <4 x float>, <4 x float>* %367, align 32, !tbaa !694
  %368 = getelementptr inbounds float, float* %366, i64 4
  %369 = bitcast float* %368 to <4 x float>*
  %wide.load153.3 = load <4 x float>, <4 x float>* %369, align 16, !tbaa !694
  %370 = fcmp olt <4 x float> %wide.load152.3, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %371 = fcmp olt <4 x float> %wide.load153.3, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %372 = select <4 x i1> %370, <4 x float> %wide.load152.3, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %373 = select <4 x i1> %371, <4 x float> %wide.load153.3, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %374 = fcmp ogt <4 x float> %372, zeroinitializer
  %375 = fcmp ogt <4 x float> %373, zeroinitializer
  %376 = select <4 x i1> %374, <4 x float> %372, <4 x float> zeroinitializer
  %377 = select <4 x i1> %375, <4 x float> %373, <4 x float> zeroinitializer
  %378 = getelementptr inbounds float, float* %164, i64 %365
  %379 = bitcast float* %378 to <4 x float>*
  store <4 x float> %376, <4 x float>* %379, align 32, !tbaa !698
  %380 = getelementptr inbounds float, float* %378, i64 4
  %381 = bitcast float* %380 to <4 x float>*
  store <4 x float> %377, <4 x float>* %381, align 16, !tbaa !698
  %382 = or i64 %314, 32
  %383 = getelementptr inbounds float, float* %22, i64 %382
  %384 = bitcast float* %383 to <4 x float>*
  %wide.load152.4 = load <4 x float>, <4 x float>* %384, align 128, !tbaa !694
  %385 = getelementptr inbounds float, float* %383, i64 4
  %386 = bitcast float* %385 to <4 x float>*
  %wide.load153.4 = load <4 x float>, <4 x float>* %386, align 16, !tbaa !694
  %387 = fcmp olt <4 x float> %wide.load152.4, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %388 = fcmp olt <4 x float> %wide.load153.4, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %389 = select <4 x i1> %387, <4 x float> %wide.load152.4, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %390 = select <4 x i1> %388, <4 x float> %wide.load153.4, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %391 = fcmp ogt <4 x float> %389, zeroinitializer
  %392 = fcmp ogt <4 x float> %390, zeroinitializer
  %393 = select <4 x i1> %391, <4 x float> %389, <4 x float> zeroinitializer
  %394 = select <4 x i1> %392, <4 x float> %390, <4 x float> zeroinitializer
  %395 = getelementptr inbounds float, float* %164, i64 %382
  %396 = bitcast float* %395 to <4 x float>*
  store <4 x float> %393, <4 x float>* %396, align 128, !tbaa !698
  %397 = getelementptr inbounds float, float* %395, i64 4
  %398 = bitcast float* %397 to <4 x float>*
  store <4 x float> %394, <4 x float>* %398, align 16, !tbaa !698
  %399 = or i64 %314, 40
  %400 = getelementptr inbounds float, float* %22, i64 %399
  %401 = bitcast float* %400 to <4 x float>*
  %wide.load152.5 = load <4 x float>, <4 x float>* %401, align 32, !tbaa !694
  %402 = getelementptr inbounds float, float* %400, i64 4
  %403 = bitcast float* %402 to <4 x float>*
  %wide.load153.5 = load <4 x float>, <4 x float>* %403, align 16, !tbaa !694
  %404 = fcmp olt <4 x float> %wide.load152.5, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %405 = fcmp olt <4 x float> %wide.load153.5, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %406 = select <4 x i1> %404, <4 x float> %wide.load152.5, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %407 = select <4 x i1> %405, <4 x float> %wide.load153.5, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %408 = fcmp ogt <4 x float> %406, zeroinitializer
  %409 = fcmp ogt <4 x float> %407, zeroinitializer
  %410 = select <4 x i1> %408, <4 x float> %406, <4 x float> zeroinitializer
  %411 = select <4 x i1> %409, <4 x float> %407, <4 x float> zeroinitializer
  %412 = getelementptr inbounds float, float* %164, i64 %399
  %413 = bitcast float* %412 to <4 x float>*
  store <4 x float> %410, <4 x float>* %413, align 32, !tbaa !698
  %414 = getelementptr inbounds float, float* %412, i64 4
  %415 = bitcast float* %414 to <4 x float>*
  store <4 x float> %411, <4 x float>* %415, align 16, !tbaa !698
  %416 = or i64 %314, 48
  %417 = getelementptr inbounds float, float* %22, i64 %416
  %418 = bitcast float* %417 to <4 x float>*
  %wide.load152.6 = load <4 x float>, <4 x float>* %418, align 64, !tbaa !694
  %419 = getelementptr inbounds float, float* %417, i64 4
  %420 = bitcast float* %419 to <4 x float>*
  %wide.load153.6 = load <4 x float>, <4 x float>* %420, align 16, !tbaa !694
  %421 = fcmp olt <4 x float> %wide.load152.6, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %422 = fcmp olt <4 x float> %wide.load153.6, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %423 = select <4 x i1> %421, <4 x float> %wide.load152.6, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %424 = select <4 x i1> %422, <4 x float> %wide.load153.6, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %425 = fcmp ogt <4 x float> %423, zeroinitializer
  %426 = fcmp ogt <4 x float> %424, zeroinitializer
  %427 = select <4 x i1> %425, <4 x float> %423, <4 x float> zeroinitializer
  %428 = select <4 x i1> %426, <4 x float> %424, <4 x float> zeroinitializer
  %429 = getelementptr inbounds float, float* %164, i64 %416
  %430 = bitcast float* %429 to <4 x float>*
  store <4 x float> %427, <4 x float>* %430, align 64, !tbaa !698
  %431 = getelementptr inbounds float, float* %429, i64 4
  %432 = bitcast float* %431 to <4 x float>*
  store <4 x float> %428, <4 x float>* %432, align 16, !tbaa !698
  %433 = or i64 %314, 56
  %434 = getelementptr inbounds float, float* %22, i64 %433
  %435 = bitcast float* %434 to <4 x float>*
  %wide.load152.7 = load <4 x float>, <4 x float>* %435, align 32, !tbaa !694
  %436 = getelementptr inbounds float, float* %434, i64 4
  %437 = bitcast float* %436 to <4 x float>*
  %wide.load153.7 = load <4 x float>, <4 x float>* %437, align 16, !tbaa !694
  %438 = fcmp olt <4 x float> %wide.load152.7, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %439 = fcmp olt <4 x float> %wide.load153.7, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %440 = select <4 x i1> %438, <4 x float> %wide.load152.7, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %441 = select <4 x i1> %439, <4 x float> %wide.load153.7, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %442 = fcmp ogt <4 x float> %440, zeroinitializer
  %443 = fcmp ogt <4 x float> %441, zeroinitializer
  %444 = select <4 x i1> %442, <4 x float> %440, <4 x float> zeroinitializer
  %445 = select <4 x i1> %443, <4 x float> %441, <4 x float> zeroinitializer
  %446 = getelementptr inbounds float, float* %164, i64 %433
  %447 = bitcast float* %446 to <4 x float>*
  store <4 x float> %444, <4 x float>* %447, align 32, !tbaa !698
  %448 = getelementptr inbounds float, float* %446, i64 4
  %449 = bitcast float* %448 to <4 x float>*
  store <4 x float> %445, <4 x float>* %449, align 16, !tbaa !698
  %450 = or i64 %314, 64
  %451 = getelementptr inbounds float, float* %22, i64 %450
  %452 = bitcast float* %451 to <4 x float>*
  %wide.load152.8 = load <4 x float>, <4 x float>* %452, align 128, !tbaa !694
  %453 = getelementptr inbounds float, float* %451, i64 4
  %454 = bitcast float* %453 to <4 x float>*
  %wide.load153.8 = load <4 x float>, <4 x float>* %454, align 16, !tbaa !694
  %455 = fcmp olt <4 x float> %wide.load152.8, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %456 = fcmp olt <4 x float> %wide.load153.8, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %457 = select <4 x i1> %455, <4 x float> %wide.load152.8, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %458 = select <4 x i1> %456, <4 x float> %wide.load153.8, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %459 = fcmp ogt <4 x float> %457, zeroinitializer
  %460 = fcmp ogt <4 x float> %458, zeroinitializer
  %461 = select <4 x i1> %459, <4 x float> %457, <4 x float> zeroinitializer
  %462 = select <4 x i1> %460, <4 x float> %458, <4 x float> zeroinitializer
  %463 = getelementptr inbounds float, float* %164, i64 %450
  %464 = bitcast float* %463 to <4 x float>*
  store <4 x float> %461, <4 x float>* %464, align 128, !tbaa !698
  %465 = getelementptr inbounds float, float* %463, i64 4
  %466 = bitcast float* %465 to <4 x float>*
  store <4 x float> %462, <4 x float>* %466, align 16, !tbaa !698
  %467 = or i64 %314, 72
  %468 = getelementptr inbounds float, float* %22, i64 %467
  %469 = bitcast float* %468 to <4 x float>*
  %wide.load152.9 = load <4 x float>, <4 x float>* %469, align 32, !tbaa !694
  %470 = getelementptr inbounds float, float* %468, i64 4
  %471 = bitcast float* %470 to <4 x float>*
  %wide.load153.9 = load <4 x float>, <4 x float>* %471, align 16, !tbaa !694
  %472 = fcmp olt <4 x float> %wide.load152.9, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %473 = fcmp olt <4 x float> %wide.load153.9, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %474 = select <4 x i1> %472, <4 x float> %wide.load152.9, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %475 = select <4 x i1> %473, <4 x float> %wide.load153.9, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %476 = fcmp ogt <4 x float> %474, zeroinitializer
  %477 = fcmp ogt <4 x float> %475, zeroinitializer
  %478 = select <4 x i1> %476, <4 x float> %474, <4 x float> zeroinitializer
  %479 = select <4 x i1> %477, <4 x float> %475, <4 x float> zeroinitializer
  %480 = getelementptr inbounds float, float* %164, i64 %467
  %481 = bitcast float* %480 to <4 x float>*
  store <4 x float> %478, <4 x float>* %481, align 32, !tbaa !698
  %482 = getelementptr inbounds float, float* %480, i64 4
  %483 = bitcast float* %482 to <4 x float>*
  store <4 x float> %479, <4 x float>* %483, align 16, !tbaa !698
  %484 = or i64 %314, 80
  %485 = getelementptr inbounds float, float* %22, i64 %484
  %486 = bitcast float* %485 to <4 x float>*
  %wide.load152.10 = load <4 x float>, <4 x float>* %486, align 64, !tbaa !694
  %487 = getelementptr inbounds float, float* %485, i64 4
  %488 = bitcast float* %487 to <4 x float>*
  %wide.load153.10 = load <4 x float>, <4 x float>* %488, align 16, !tbaa !694
  %489 = fcmp olt <4 x float> %wide.load152.10, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %490 = fcmp olt <4 x float> %wide.load153.10, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %491 = select <4 x i1> %489, <4 x float> %wide.load152.10, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %492 = select <4 x i1> %490, <4 x float> %wide.load153.10, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %493 = fcmp ogt <4 x float> %491, zeroinitializer
  %494 = fcmp ogt <4 x float> %492, zeroinitializer
  %495 = select <4 x i1> %493, <4 x float> %491, <4 x float> zeroinitializer
  %496 = select <4 x i1> %494, <4 x float> %492, <4 x float> zeroinitializer
  %497 = getelementptr inbounds float, float* %164, i64 %484
  %498 = bitcast float* %497 to <4 x float>*
  store <4 x float> %495, <4 x float>* %498, align 64, !tbaa !698
  %499 = getelementptr inbounds float, float* %497, i64 4
  %500 = bitcast float* %499 to <4 x float>*
  store <4 x float> %496, <4 x float>* %500, align 16, !tbaa !698
  %501 = or i64 %314, 88
  %502 = getelementptr inbounds float, float* %22, i64 %501
  %503 = bitcast float* %502 to <4 x float>*
  %wide.load152.11 = load <4 x float>, <4 x float>* %503, align 32, !tbaa !694
  %504 = getelementptr inbounds float, float* %502, i64 4
  %505 = bitcast float* %504 to <4 x float>*
  %wide.load153.11 = load <4 x float>, <4 x float>* %505, align 16, !tbaa !694
  %506 = fcmp olt <4 x float> %wide.load152.11, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %507 = fcmp olt <4 x float> %wide.load153.11, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %508 = select <4 x i1> %506, <4 x float> %wide.load152.11, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %509 = select <4 x i1> %507, <4 x float> %wide.load153.11, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %510 = fcmp ogt <4 x float> %508, zeroinitializer
  %511 = fcmp ogt <4 x float> %509, zeroinitializer
  %512 = select <4 x i1> %510, <4 x float> %508, <4 x float> zeroinitializer
  %513 = select <4 x i1> %511, <4 x float> %509, <4 x float> zeroinitializer
  %514 = getelementptr inbounds float, float* %164, i64 %501
  %515 = bitcast float* %514 to <4 x float>*
  store <4 x float> %512, <4 x float>* %515, align 32, !tbaa !698
  %516 = getelementptr inbounds float, float* %514, i64 4
  %517 = bitcast float* %516 to <4 x float>*
  store <4 x float> %513, <4 x float>* %517, align 16, !tbaa !698
  %518 = or i64 %314, 96
  %519 = getelementptr inbounds float, float* %22, i64 %518
  %520 = bitcast float* %519 to <4 x float>*
  %wide.load152.12 = load <4 x float>, <4 x float>* %520, align 128, !tbaa !694
  %521 = getelementptr inbounds float, float* %519, i64 4
  %522 = bitcast float* %521 to <4 x float>*
  %wide.load153.12 = load <4 x float>, <4 x float>* %522, align 16, !tbaa !694
  %523 = fcmp olt <4 x float> %wide.load152.12, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %524 = fcmp olt <4 x float> %wide.load153.12, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %525 = select <4 x i1> %523, <4 x float> %wide.load152.12, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %526 = select <4 x i1> %524, <4 x float> %wide.load153.12, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %527 = fcmp ogt <4 x float> %525, zeroinitializer
  %528 = fcmp ogt <4 x float> %526, zeroinitializer
  %529 = select <4 x i1> %527, <4 x float> %525, <4 x float> zeroinitializer
  %530 = select <4 x i1> %528, <4 x float> %526, <4 x float> zeroinitializer
  %531 = getelementptr inbounds float, float* %164, i64 %518
  %532 = bitcast float* %531 to <4 x float>*
  store <4 x float> %529, <4 x float>* %532, align 128, !tbaa !698
  %533 = getelementptr inbounds float, float* %531, i64 4
  %534 = bitcast float* %533 to <4 x float>*
  store <4 x float> %530, <4 x float>* %534, align 16, !tbaa !698
  %535 = or i64 %314, 104
  %536 = getelementptr inbounds float, float* %22, i64 %535
  %537 = bitcast float* %536 to <4 x float>*
  %wide.load152.13 = load <4 x float>, <4 x float>* %537, align 32, !tbaa !694
  %538 = getelementptr inbounds float, float* %536, i64 4
  %539 = bitcast float* %538 to <4 x float>*
  %wide.load153.13 = load <4 x float>, <4 x float>* %539, align 16, !tbaa !694
  %540 = fcmp olt <4 x float> %wide.load152.13, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %541 = fcmp olt <4 x float> %wide.load153.13, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %542 = select <4 x i1> %540, <4 x float> %wide.load152.13, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %543 = select <4 x i1> %541, <4 x float> %wide.load153.13, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %544 = fcmp ogt <4 x float> %542, zeroinitializer
  %545 = fcmp ogt <4 x float> %543, zeroinitializer
  %546 = select <4 x i1> %544, <4 x float> %542, <4 x float> zeroinitializer
  %547 = select <4 x i1> %545, <4 x float> %543, <4 x float> zeroinitializer
  %548 = getelementptr inbounds float, float* %164, i64 %535
  %549 = bitcast float* %548 to <4 x float>*
  store <4 x float> %546, <4 x float>* %549, align 32, !tbaa !698
  %550 = getelementptr inbounds float, float* %548, i64 4
  %551 = bitcast float* %550 to <4 x float>*
  store <4 x float> %547, <4 x float>* %551, align 16, !tbaa !698
  %552 = or i64 %314, 112
  %553 = getelementptr inbounds float, float* %22, i64 %552
  %554 = bitcast float* %553 to <4 x float>*
  %wide.load152.14 = load <4 x float>, <4 x float>* %554, align 64, !tbaa !694
  %555 = getelementptr inbounds float, float* %553, i64 4
  %556 = bitcast float* %555 to <4 x float>*
  %wide.load153.14 = load <4 x float>, <4 x float>* %556, align 16, !tbaa !694
  %557 = fcmp olt <4 x float> %wide.load152.14, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %558 = fcmp olt <4 x float> %wide.load153.14, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %559 = select <4 x i1> %557, <4 x float> %wide.load152.14, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %560 = select <4 x i1> %558, <4 x float> %wide.load153.14, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %561 = fcmp ogt <4 x float> %559, zeroinitializer
  %562 = fcmp ogt <4 x float> %560, zeroinitializer
  %563 = select <4 x i1> %561, <4 x float> %559, <4 x float> zeroinitializer
  %564 = select <4 x i1> %562, <4 x float> %560, <4 x float> zeroinitializer
  %565 = getelementptr inbounds float, float* %164, i64 %552
  %566 = bitcast float* %565 to <4 x float>*
  store <4 x float> %563, <4 x float>* %566, align 64, !tbaa !698
  %567 = getelementptr inbounds float, float* %565, i64 4
  %568 = bitcast float* %567 to <4 x float>*
  store <4 x float> %564, <4 x float>* %568, align 16, !tbaa !698
  %569 = or i64 %314, 120
  %570 = getelementptr inbounds float, float* %22, i64 %569
  %571 = bitcast float* %570 to <4 x float>*
  %wide.load152.15 = load <4 x float>, <4 x float>* %571, align 32, !tbaa !694
  %572 = getelementptr inbounds float, float* %570, i64 4
  %573 = bitcast float* %572 to <4 x float>*
  %wide.load153.15 = load <4 x float>, <4 x float>* %573, align 16, !tbaa !694
  %574 = fcmp olt <4 x float> %wide.load152.15, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %575 = fcmp olt <4 x float> %wide.load153.15, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %576 = select <4 x i1> %574, <4 x float> %wide.load152.15, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %577 = select <4 x i1> %575, <4 x float> %wide.load153.15, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %578 = fcmp ogt <4 x float> %576, zeroinitializer
  %579 = fcmp ogt <4 x float> %577, zeroinitializer
  %580 = select <4 x i1> %578, <4 x float> %576, <4 x float> zeroinitializer
  %581 = select <4 x i1> %579, <4 x float> %577, <4 x float> zeroinitializer
  %582 = getelementptr inbounds float, float* %164, i64 %569
  %583 = bitcast float* %582 to <4 x float>*
  store <4 x float> %580, <4 x float>* %583, align 32, !tbaa !698
  %584 = getelementptr inbounds float, float* %582, i64 4
  %585 = bitcast float* %584 to <4 x float>*
  store <4 x float> %581, <4 x float>* %585, align 16, !tbaa !698
  %indvars.iv.next69 = add nuw nsw i64 %indvars.iv68, 1
  %exitcond70.not = icmp eq i64 %indvars.iv.next69, 56
  br i1 %exitcond70.not, label %for_end41, label %for_begin42.preheader, !prof !51

for_end41:                                        ; preds = %for_begin42.preheader
  %indvars.iv.next72 = add nuw nsw i64 %indvars.iv71, 1
  %exitcond73.not = icmp eq i64 %indvars.iv.next72, 56
  br i1 %exitcond73.not, label %for_end38, label %for_begin39.preheader, !prof !51

if_end46:                                         ; preds = %for_end38
  %586 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %587 = tail call i32 %586(i32 1, i32 %4, i8* nonnull %6)
  %.not57 = icmp ne i32 %587, 0
  %spec.select = sext i1 %.not57 to i32
  ret i32 %spec.select
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_8(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.85, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !701
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !715
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !717
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !720
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.86, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.87, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.88, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.89, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !722
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !736
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 28
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.90, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !738
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 28
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.91, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !741
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 256
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.92, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !743
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 200704, i32 7168, i32 256, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.93, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !755
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 1
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.16, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !769
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 1
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.17, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !771
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 256
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.94, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !774
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 256
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.95, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !776
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 65536, i32 65536, i32 256, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.96, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !788
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 256
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.97, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !802
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !816
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !830
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 28
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.73, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !832
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 28
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.74, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !835
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 256
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.98, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !837
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 200704, i32 7168, i32 256, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.99, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_8_compute_(i8* %31, i8* %39, i8* %51, i8* %45, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_8_compute_(i8* noalias align 128 %0, i8* noalias align 128 %1, i8* noalias align 128 %2, i8* noalias align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 802816, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %call_end, %if_end, %entry
  %merge = phi i32 [ -1, %entry ], [ %13, %if_end ], [ %22, %call_end ]
  ret i32 %merge

if_end:                                           ; preds = %entry
  %8 = alloca %6, align 8
  %9 = getelementptr inbounds %6, %6* %8, i64 0, i32 0
  store i8* %6, i8** %9, align 8
  %10 = getelementptr inbounds %6, %6* %8, i64 0, i32 1
  store i8* %0, i8** %10, align 8
  %11 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %12 = bitcast %6* %8 to i8*
  %13 = call i32 %11(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.100, i8* nonnull %12, i32 0)
  %14 = icmp eq i32 %13, 0
  br i1 %14, label %call_end, label %if_then, !prof !5

call_end:                                         ; preds = %if_end
  %15 = alloca %7, align 8
  %16 = getelementptr inbounds %7, %7* %15, i64 0, i32 0
  store i8* %6, i8** %16, align 8
  %17 = getelementptr inbounds %7, %7* %15, i64 0, i32 1
  store i8* %1, i8** %17, align 8
  %18 = getelementptr inbounds %7, %7* %15, i64 0, i32 2
  store i8* %2, i8** %18, align 8
  %19 = getelementptr inbounds %7, %7* %15, i64 0, i32 3
  store i8* %3, i8** %19, align 8
  %20 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %21 = bitcast %7* %15 to i8*
  %22 = call i32 %20(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.101, i8* nonnull %21, i32 0)
  %23 = icmp eq i32 %22, 0
  br i1 %23, label %call_end2, label %if_then, !prof !5

call_end2:                                        ; preds = %call_end
  %24 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %25 = call i32 %24(i32 1, i32 %4, i8* nonnull %6)
  %.not = icmp ne i32 %25, 0
  %spec.select = sext i1 %.not to i32
  ret i32 %spec.select
}

; Function Attrs: nofree norecurse nounwind
define private i32 @__tvm_parallel_lambda.100(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #2 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %9 = load i32, i32* %8, align 4
  %10 = add nsw i32 %9, 27
  %11 = sdiv i32 %10, %9
  %12 = add nsw i32 %0, 1
  %13 = mul nsw i32 %11, %12
  %14 = icmp slt i32 %13, 28
  %15 = select i1 %14, i32 %13, i32 28
  %16 = mul nsw i32 %11, %0
  %17 = icmp slt i32 %16, 28
  %18 = select i1 %17, i32 %16, i32 28
  %19 = icmp slt i32 %18, %15
  br i1 %19, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %20 = sext i32 %18 to i64
  %wide.trip.count = sext i32 %15 to i64
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end3
  %indvars.iv13 = phi i64 [ %20, %for_begin1.preheader.preheader ], [ %indvars.iv.next14, %for_end3 ]
  %21 = mul nsw i64 %indvars.iv13, 7168
  br label %for_begin4.preheader

for_end:                                          ; preds = %for_end3, %entry
  ret i32 0

for_begin4.preheader:                             ; preds = %for_begin1.preheader, %for_begin4.preheader
  %indvars.iv10 = phi i64 [ 0, %for_begin1.preheader ], [ %indvars.iv.next11, %for_begin4.preheader ]
  %22 = shl nsw i64 %indvars.iv10, 8
  %23 = add nsw i64 %22, %21
  %24 = getelementptr inbounds float, float* %7, i64 %23
  %25 = bitcast float* %24 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %25, align 4, !tbaa !849
  %26 = getelementptr inbounds float, float* %24, i64 4
  %27 = bitcast float* %26 to <4 x float>*
  %wide.load16 = load <4 x float>, <4 x float>* %27, align 4, !tbaa !849
  %28 = getelementptr inbounds float, float* %4, i64 %23
  %29 = bitcast float* %28 to <4 x float>*
  store <4 x float> %wide.load, <4 x float>* %29, align 4, !tbaa !852
  %30 = getelementptr inbounds float, float* %28, i64 4
  %31 = bitcast float* %30 to <4 x float>*
  store <4 x float> %wide.load16, <4 x float>* %31, align 4, !tbaa !852
  %32 = or i64 %23, 8
  %33 = getelementptr inbounds float, float* %7, i64 %32
  %34 = bitcast float* %33 to <4 x float>*
  %wide.load.1 = load <4 x float>, <4 x float>* %34, align 4, !tbaa !849
  %35 = getelementptr inbounds float, float* %33, i64 4
  %36 = bitcast float* %35 to <4 x float>*
  %wide.load16.1 = load <4 x float>, <4 x float>* %36, align 4, !tbaa !849
  %37 = getelementptr inbounds float, float* %4, i64 %32
  %38 = bitcast float* %37 to <4 x float>*
  store <4 x float> %wide.load.1, <4 x float>* %38, align 4, !tbaa !852
  %39 = getelementptr inbounds float, float* %37, i64 4
  %40 = bitcast float* %39 to <4 x float>*
  store <4 x float> %wide.load16.1, <4 x float>* %40, align 4, !tbaa !852
  %41 = or i64 %23, 16
  %42 = getelementptr inbounds float, float* %7, i64 %41
  %43 = bitcast float* %42 to <4 x float>*
  %wide.load.2 = load <4 x float>, <4 x float>* %43, align 4, !tbaa !849
  %44 = getelementptr inbounds float, float* %42, i64 4
  %45 = bitcast float* %44 to <4 x float>*
  %wide.load16.2 = load <4 x float>, <4 x float>* %45, align 4, !tbaa !849
  %46 = getelementptr inbounds float, float* %4, i64 %41
  %47 = bitcast float* %46 to <4 x float>*
  store <4 x float> %wide.load.2, <4 x float>* %47, align 4, !tbaa !852
  %48 = getelementptr inbounds float, float* %46, i64 4
  %49 = bitcast float* %48 to <4 x float>*
  store <4 x float> %wide.load16.2, <4 x float>* %49, align 4, !tbaa !852
  %50 = or i64 %23, 24
  %51 = getelementptr inbounds float, float* %7, i64 %50
  %52 = bitcast float* %51 to <4 x float>*
  %wide.load.3 = load <4 x float>, <4 x float>* %52, align 4, !tbaa !849
  %53 = getelementptr inbounds float, float* %51, i64 4
  %54 = bitcast float* %53 to <4 x float>*
  %wide.load16.3 = load <4 x float>, <4 x float>* %54, align 4, !tbaa !849
  %55 = getelementptr inbounds float, float* %4, i64 %50
  %56 = bitcast float* %55 to <4 x float>*
  store <4 x float> %wide.load.3, <4 x float>* %56, align 4, !tbaa !852
  %57 = getelementptr inbounds float, float* %55, i64 4
  %58 = bitcast float* %57 to <4 x float>*
  store <4 x float> %wide.load16.3, <4 x float>* %58, align 4, !tbaa !852
  %59 = or i64 %23, 32
  %60 = getelementptr inbounds float, float* %7, i64 %59
  %61 = bitcast float* %60 to <4 x float>*
  %wide.load.4 = load <4 x float>, <4 x float>* %61, align 4, !tbaa !849
  %62 = getelementptr inbounds float, float* %60, i64 4
  %63 = bitcast float* %62 to <4 x float>*
  %wide.load16.4 = load <4 x float>, <4 x float>* %63, align 4, !tbaa !849
  %64 = getelementptr inbounds float, float* %4, i64 %59
  %65 = bitcast float* %64 to <4 x float>*
  store <4 x float> %wide.load.4, <4 x float>* %65, align 4, !tbaa !852
  %66 = getelementptr inbounds float, float* %64, i64 4
  %67 = bitcast float* %66 to <4 x float>*
  store <4 x float> %wide.load16.4, <4 x float>* %67, align 4, !tbaa !852
  %68 = or i64 %23, 40
  %69 = getelementptr inbounds float, float* %7, i64 %68
  %70 = bitcast float* %69 to <4 x float>*
  %wide.load.5 = load <4 x float>, <4 x float>* %70, align 4, !tbaa !849
  %71 = getelementptr inbounds float, float* %69, i64 4
  %72 = bitcast float* %71 to <4 x float>*
  %wide.load16.5 = load <4 x float>, <4 x float>* %72, align 4, !tbaa !849
  %73 = getelementptr inbounds float, float* %4, i64 %68
  %74 = bitcast float* %73 to <4 x float>*
  store <4 x float> %wide.load.5, <4 x float>* %74, align 4, !tbaa !852
  %75 = getelementptr inbounds float, float* %73, i64 4
  %76 = bitcast float* %75 to <4 x float>*
  store <4 x float> %wide.load16.5, <4 x float>* %76, align 4, !tbaa !852
  %77 = or i64 %23, 48
  %78 = getelementptr inbounds float, float* %7, i64 %77
  %79 = bitcast float* %78 to <4 x float>*
  %wide.load.6 = load <4 x float>, <4 x float>* %79, align 4, !tbaa !849
  %80 = getelementptr inbounds float, float* %78, i64 4
  %81 = bitcast float* %80 to <4 x float>*
  %wide.load16.6 = load <4 x float>, <4 x float>* %81, align 4, !tbaa !849
  %82 = getelementptr inbounds float, float* %4, i64 %77
  %83 = bitcast float* %82 to <4 x float>*
  store <4 x float> %wide.load.6, <4 x float>* %83, align 4, !tbaa !852
  %84 = getelementptr inbounds float, float* %82, i64 4
  %85 = bitcast float* %84 to <4 x float>*
  store <4 x float> %wide.load16.6, <4 x float>* %85, align 4, !tbaa !852
  %86 = or i64 %23, 56
  %87 = getelementptr inbounds float, float* %7, i64 %86
  %88 = bitcast float* %87 to <4 x float>*
  %wide.load.7 = load <4 x float>, <4 x float>* %88, align 4, !tbaa !849
  %89 = getelementptr inbounds float, float* %87, i64 4
  %90 = bitcast float* %89 to <4 x float>*
  %wide.load16.7 = load <4 x float>, <4 x float>* %90, align 4, !tbaa !849
  %91 = getelementptr inbounds float, float* %4, i64 %86
  %92 = bitcast float* %91 to <4 x float>*
  store <4 x float> %wide.load.7, <4 x float>* %92, align 4, !tbaa !852
  %93 = getelementptr inbounds float, float* %91, i64 4
  %94 = bitcast float* %93 to <4 x float>*
  store <4 x float> %wide.load16.7, <4 x float>* %94, align 4, !tbaa !852
  %95 = or i64 %23, 64
  %96 = getelementptr inbounds float, float* %7, i64 %95
  %97 = bitcast float* %96 to <4 x float>*
  %wide.load.8 = load <4 x float>, <4 x float>* %97, align 4, !tbaa !849
  %98 = getelementptr inbounds float, float* %96, i64 4
  %99 = bitcast float* %98 to <4 x float>*
  %wide.load16.8 = load <4 x float>, <4 x float>* %99, align 4, !tbaa !849
  %100 = getelementptr inbounds float, float* %4, i64 %95
  %101 = bitcast float* %100 to <4 x float>*
  store <4 x float> %wide.load.8, <4 x float>* %101, align 4, !tbaa !852
  %102 = getelementptr inbounds float, float* %100, i64 4
  %103 = bitcast float* %102 to <4 x float>*
  store <4 x float> %wide.load16.8, <4 x float>* %103, align 4, !tbaa !852
  %104 = or i64 %23, 72
  %105 = getelementptr inbounds float, float* %7, i64 %104
  %106 = bitcast float* %105 to <4 x float>*
  %wide.load.9 = load <4 x float>, <4 x float>* %106, align 4, !tbaa !849
  %107 = getelementptr inbounds float, float* %105, i64 4
  %108 = bitcast float* %107 to <4 x float>*
  %wide.load16.9 = load <4 x float>, <4 x float>* %108, align 4, !tbaa !849
  %109 = getelementptr inbounds float, float* %4, i64 %104
  %110 = bitcast float* %109 to <4 x float>*
  store <4 x float> %wide.load.9, <4 x float>* %110, align 4, !tbaa !852
  %111 = getelementptr inbounds float, float* %109, i64 4
  %112 = bitcast float* %111 to <4 x float>*
  store <4 x float> %wide.load16.9, <4 x float>* %112, align 4, !tbaa !852
  %113 = or i64 %23, 80
  %114 = getelementptr inbounds float, float* %7, i64 %113
  %115 = bitcast float* %114 to <4 x float>*
  %wide.load.10 = load <4 x float>, <4 x float>* %115, align 4, !tbaa !849
  %116 = getelementptr inbounds float, float* %114, i64 4
  %117 = bitcast float* %116 to <4 x float>*
  %wide.load16.10 = load <4 x float>, <4 x float>* %117, align 4, !tbaa !849
  %118 = getelementptr inbounds float, float* %4, i64 %113
  %119 = bitcast float* %118 to <4 x float>*
  store <4 x float> %wide.load.10, <4 x float>* %119, align 4, !tbaa !852
  %120 = getelementptr inbounds float, float* %118, i64 4
  %121 = bitcast float* %120 to <4 x float>*
  store <4 x float> %wide.load16.10, <4 x float>* %121, align 4, !tbaa !852
  %122 = or i64 %23, 88
  %123 = getelementptr inbounds float, float* %7, i64 %122
  %124 = bitcast float* %123 to <4 x float>*
  %wide.load.11 = load <4 x float>, <4 x float>* %124, align 4, !tbaa !849
  %125 = getelementptr inbounds float, float* %123, i64 4
  %126 = bitcast float* %125 to <4 x float>*
  %wide.load16.11 = load <4 x float>, <4 x float>* %126, align 4, !tbaa !849
  %127 = getelementptr inbounds float, float* %4, i64 %122
  %128 = bitcast float* %127 to <4 x float>*
  store <4 x float> %wide.load.11, <4 x float>* %128, align 4, !tbaa !852
  %129 = getelementptr inbounds float, float* %127, i64 4
  %130 = bitcast float* %129 to <4 x float>*
  store <4 x float> %wide.load16.11, <4 x float>* %130, align 4, !tbaa !852
  %131 = or i64 %23, 96
  %132 = getelementptr inbounds float, float* %7, i64 %131
  %133 = bitcast float* %132 to <4 x float>*
  %wide.load.12 = load <4 x float>, <4 x float>* %133, align 4, !tbaa !849
  %134 = getelementptr inbounds float, float* %132, i64 4
  %135 = bitcast float* %134 to <4 x float>*
  %wide.load16.12 = load <4 x float>, <4 x float>* %135, align 4, !tbaa !849
  %136 = getelementptr inbounds float, float* %4, i64 %131
  %137 = bitcast float* %136 to <4 x float>*
  store <4 x float> %wide.load.12, <4 x float>* %137, align 4, !tbaa !852
  %138 = getelementptr inbounds float, float* %136, i64 4
  %139 = bitcast float* %138 to <4 x float>*
  store <4 x float> %wide.load16.12, <4 x float>* %139, align 4, !tbaa !852
  %140 = or i64 %23, 104
  %141 = getelementptr inbounds float, float* %7, i64 %140
  %142 = bitcast float* %141 to <4 x float>*
  %wide.load.13 = load <4 x float>, <4 x float>* %142, align 4, !tbaa !849
  %143 = getelementptr inbounds float, float* %141, i64 4
  %144 = bitcast float* %143 to <4 x float>*
  %wide.load16.13 = load <4 x float>, <4 x float>* %144, align 4, !tbaa !849
  %145 = getelementptr inbounds float, float* %4, i64 %140
  %146 = bitcast float* %145 to <4 x float>*
  store <4 x float> %wide.load.13, <4 x float>* %146, align 4, !tbaa !852
  %147 = getelementptr inbounds float, float* %145, i64 4
  %148 = bitcast float* %147 to <4 x float>*
  store <4 x float> %wide.load16.13, <4 x float>* %148, align 4, !tbaa !852
  %149 = or i64 %23, 112
  %150 = getelementptr inbounds float, float* %7, i64 %149
  %151 = bitcast float* %150 to <4 x float>*
  %wide.load.14 = load <4 x float>, <4 x float>* %151, align 4, !tbaa !849
  %152 = getelementptr inbounds float, float* %150, i64 4
  %153 = bitcast float* %152 to <4 x float>*
  %wide.load16.14 = load <4 x float>, <4 x float>* %153, align 4, !tbaa !849
  %154 = getelementptr inbounds float, float* %4, i64 %149
  %155 = bitcast float* %154 to <4 x float>*
  store <4 x float> %wide.load.14, <4 x float>* %155, align 4, !tbaa !852
  %156 = getelementptr inbounds float, float* %154, i64 4
  %157 = bitcast float* %156 to <4 x float>*
  store <4 x float> %wide.load16.14, <4 x float>* %157, align 4, !tbaa !852
  %158 = or i64 %23, 120
  %159 = getelementptr inbounds float, float* %7, i64 %158
  %160 = bitcast float* %159 to <4 x float>*
  %wide.load.15 = load <4 x float>, <4 x float>* %160, align 4, !tbaa !849
  %161 = getelementptr inbounds float, float* %159, i64 4
  %162 = bitcast float* %161 to <4 x float>*
  %wide.load16.15 = load <4 x float>, <4 x float>* %162, align 4, !tbaa !849
  %163 = getelementptr inbounds float, float* %4, i64 %158
  %164 = bitcast float* %163 to <4 x float>*
  store <4 x float> %wide.load.15, <4 x float>* %164, align 4, !tbaa !852
  %165 = getelementptr inbounds float, float* %163, i64 4
  %166 = bitcast float* %165 to <4 x float>*
  store <4 x float> %wide.load16.15, <4 x float>* %166, align 4, !tbaa !852
  %167 = or i64 %23, 128
  %168 = getelementptr inbounds float, float* %7, i64 %167
  %169 = bitcast float* %168 to <4 x float>*
  %wide.load.16 = load <4 x float>, <4 x float>* %169, align 4, !tbaa !849
  %170 = getelementptr inbounds float, float* %168, i64 4
  %171 = bitcast float* %170 to <4 x float>*
  %wide.load16.16 = load <4 x float>, <4 x float>* %171, align 4, !tbaa !849
  %172 = getelementptr inbounds float, float* %4, i64 %167
  %173 = bitcast float* %172 to <4 x float>*
  store <4 x float> %wide.load.16, <4 x float>* %173, align 4, !tbaa !852
  %174 = getelementptr inbounds float, float* %172, i64 4
  %175 = bitcast float* %174 to <4 x float>*
  store <4 x float> %wide.load16.16, <4 x float>* %175, align 4, !tbaa !852
  %176 = or i64 %23, 136
  %177 = getelementptr inbounds float, float* %7, i64 %176
  %178 = bitcast float* %177 to <4 x float>*
  %wide.load.17 = load <4 x float>, <4 x float>* %178, align 4, !tbaa !849
  %179 = getelementptr inbounds float, float* %177, i64 4
  %180 = bitcast float* %179 to <4 x float>*
  %wide.load16.17 = load <4 x float>, <4 x float>* %180, align 4, !tbaa !849
  %181 = getelementptr inbounds float, float* %4, i64 %176
  %182 = bitcast float* %181 to <4 x float>*
  store <4 x float> %wide.load.17, <4 x float>* %182, align 4, !tbaa !852
  %183 = getelementptr inbounds float, float* %181, i64 4
  %184 = bitcast float* %183 to <4 x float>*
  store <4 x float> %wide.load16.17, <4 x float>* %184, align 4, !tbaa !852
  %185 = or i64 %23, 144
  %186 = getelementptr inbounds float, float* %7, i64 %185
  %187 = bitcast float* %186 to <4 x float>*
  %wide.load.18 = load <4 x float>, <4 x float>* %187, align 4, !tbaa !849
  %188 = getelementptr inbounds float, float* %186, i64 4
  %189 = bitcast float* %188 to <4 x float>*
  %wide.load16.18 = load <4 x float>, <4 x float>* %189, align 4, !tbaa !849
  %190 = getelementptr inbounds float, float* %4, i64 %185
  %191 = bitcast float* %190 to <4 x float>*
  store <4 x float> %wide.load.18, <4 x float>* %191, align 4, !tbaa !852
  %192 = getelementptr inbounds float, float* %190, i64 4
  %193 = bitcast float* %192 to <4 x float>*
  store <4 x float> %wide.load16.18, <4 x float>* %193, align 4, !tbaa !852
  %194 = or i64 %23, 152
  %195 = getelementptr inbounds float, float* %7, i64 %194
  %196 = bitcast float* %195 to <4 x float>*
  %wide.load.19 = load <4 x float>, <4 x float>* %196, align 4, !tbaa !849
  %197 = getelementptr inbounds float, float* %195, i64 4
  %198 = bitcast float* %197 to <4 x float>*
  %wide.load16.19 = load <4 x float>, <4 x float>* %198, align 4, !tbaa !849
  %199 = getelementptr inbounds float, float* %4, i64 %194
  %200 = bitcast float* %199 to <4 x float>*
  store <4 x float> %wide.load.19, <4 x float>* %200, align 4, !tbaa !852
  %201 = getelementptr inbounds float, float* %199, i64 4
  %202 = bitcast float* %201 to <4 x float>*
  store <4 x float> %wide.load16.19, <4 x float>* %202, align 4, !tbaa !852
  %203 = or i64 %23, 160
  %204 = getelementptr inbounds float, float* %7, i64 %203
  %205 = bitcast float* %204 to <4 x float>*
  %wide.load.20 = load <4 x float>, <4 x float>* %205, align 4, !tbaa !849
  %206 = getelementptr inbounds float, float* %204, i64 4
  %207 = bitcast float* %206 to <4 x float>*
  %wide.load16.20 = load <4 x float>, <4 x float>* %207, align 4, !tbaa !849
  %208 = getelementptr inbounds float, float* %4, i64 %203
  %209 = bitcast float* %208 to <4 x float>*
  store <4 x float> %wide.load.20, <4 x float>* %209, align 4, !tbaa !852
  %210 = getelementptr inbounds float, float* %208, i64 4
  %211 = bitcast float* %210 to <4 x float>*
  store <4 x float> %wide.load16.20, <4 x float>* %211, align 4, !tbaa !852
  %212 = or i64 %23, 168
  %213 = getelementptr inbounds float, float* %7, i64 %212
  %214 = bitcast float* %213 to <4 x float>*
  %wide.load.21 = load <4 x float>, <4 x float>* %214, align 4, !tbaa !849
  %215 = getelementptr inbounds float, float* %213, i64 4
  %216 = bitcast float* %215 to <4 x float>*
  %wide.load16.21 = load <4 x float>, <4 x float>* %216, align 4, !tbaa !849
  %217 = getelementptr inbounds float, float* %4, i64 %212
  %218 = bitcast float* %217 to <4 x float>*
  store <4 x float> %wide.load.21, <4 x float>* %218, align 4, !tbaa !852
  %219 = getelementptr inbounds float, float* %217, i64 4
  %220 = bitcast float* %219 to <4 x float>*
  store <4 x float> %wide.load16.21, <4 x float>* %220, align 4, !tbaa !852
  %221 = or i64 %23, 176
  %222 = getelementptr inbounds float, float* %7, i64 %221
  %223 = bitcast float* %222 to <4 x float>*
  %wide.load.22 = load <4 x float>, <4 x float>* %223, align 4, !tbaa !849
  %224 = getelementptr inbounds float, float* %222, i64 4
  %225 = bitcast float* %224 to <4 x float>*
  %wide.load16.22 = load <4 x float>, <4 x float>* %225, align 4, !tbaa !849
  %226 = getelementptr inbounds float, float* %4, i64 %221
  %227 = bitcast float* %226 to <4 x float>*
  store <4 x float> %wide.load.22, <4 x float>* %227, align 4, !tbaa !852
  %228 = getelementptr inbounds float, float* %226, i64 4
  %229 = bitcast float* %228 to <4 x float>*
  store <4 x float> %wide.load16.22, <4 x float>* %229, align 4, !tbaa !852
  %230 = or i64 %23, 184
  %231 = getelementptr inbounds float, float* %7, i64 %230
  %232 = bitcast float* %231 to <4 x float>*
  %wide.load.23 = load <4 x float>, <4 x float>* %232, align 4, !tbaa !849
  %233 = getelementptr inbounds float, float* %231, i64 4
  %234 = bitcast float* %233 to <4 x float>*
  %wide.load16.23 = load <4 x float>, <4 x float>* %234, align 4, !tbaa !849
  %235 = getelementptr inbounds float, float* %4, i64 %230
  %236 = bitcast float* %235 to <4 x float>*
  store <4 x float> %wide.load.23, <4 x float>* %236, align 4, !tbaa !852
  %237 = getelementptr inbounds float, float* %235, i64 4
  %238 = bitcast float* %237 to <4 x float>*
  store <4 x float> %wide.load16.23, <4 x float>* %238, align 4, !tbaa !852
  %239 = or i64 %23, 192
  %240 = getelementptr inbounds float, float* %7, i64 %239
  %241 = bitcast float* %240 to <4 x float>*
  %wide.load.24 = load <4 x float>, <4 x float>* %241, align 4, !tbaa !849
  %242 = getelementptr inbounds float, float* %240, i64 4
  %243 = bitcast float* %242 to <4 x float>*
  %wide.load16.24 = load <4 x float>, <4 x float>* %243, align 4, !tbaa !849
  %244 = getelementptr inbounds float, float* %4, i64 %239
  %245 = bitcast float* %244 to <4 x float>*
  store <4 x float> %wide.load.24, <4 x float>* %245, align 4, !tbaa !852
  %246 = getelementptr inbounds float, float* %244, i64 4
  %247 = bitcast float* %246 to <4 x float>*
  store <4 x float> %wide.load16.24, <4 x float>* %247, align 4, !tbaa !852
  %248 = or i64 %23, 200
  %249 = getelementptr inbounds float, float* %7, i64 %248
  %250 = bitcast float* %249 to <4 x float>*
  %wide.load.25 = load <4 x float>, <4 x float>* %250, align 4, !tbaa !849
  %251 = getelementptr inbounds float, float* %249, i64 4
  %252 = bitcast float* %251 to <4 x float>*
  %wide.load16.25 = load <4 x float>, <4 x float>* %252, align 4, !tbaa !849
  %253 = getelementptr inbounds float, float* %4, i64 %248
  %254 = bitcast float* %253 to <4 x float>*
  store <4 x float> %wide.load.25, <4 x float>* %254, align 4, !tbaa !852
  %255 = getelementptr inbounds float, float* %253, i64 4
  %256 = bitcast float* %255 to <4 x float>*
  store <4 x float> %wide.load16.25, <4 x float>* %256, align 4, !tbaa !852
  %257 = or i64 %23, 208
  %258 = getelementptr inbounds float, float* %7, i64 %257
  %259 = bitcast float* %258 to <4 x float>*
  %wide.load.26 = load <4 x float>, <4 x float>* %259, align 4, !tbaa !849
  %260 = getelementptr inbounds float, float* %258, i64 4
  %261 = bitcast float* %260 to <4 x float>*
  %wide.load16.26 = load <4 x float>, <4 x float>* %261, align 4, !tbaa !849
  %262 = getelementptr inbounds float, float* %4, i64 %257
  %263 = bitcast float* %262 to <4 x float>*
  store <4 x float> %wide.load.26, <4 x float>* %263, align 4, !tbaa !852
  %264 = getelementptr inbounds float, float* %262, i64 4
  %265 = bitcast float* %264 to <4 x float>*
  store <4 x float> %wide.load16.26, <4 x float>* %265, align 4, !tbaa !852
  %266 = or i64 %23, 216
  %267 = getelementptr inbounds float, float* %7, i64 %266
  %268 = bitcast float* %267 to <4 x float>*
  %wide.load.27 = load <4 x float>, <4 x float>* %268, align 4, !tbaa !849
  %269 = getelementptr inbounds float, float* %267, i64 4
  %270 = bitcast float* %269 to <4 x float>*
  %wide.load16.27 = load <4 x float>, <4 x float>* %270, align 4, !tbaa !849
  %271 = getelementptr inbounds float, float* %4, i64 %266
  %272 = bitcast float* %271 to <4 x float>*
  store <4 x float> %wide.load.27, <4 x float>* %272, align 4, !tbaa !852
  %273 = getelementptr inbounds float, float* %271, i64 4
  %274 = bitcast float* %273 to <4 x float>*
  store <4 x float> %wide.load16.27, <4 x float>* %274, align 4, !tbaa !852
  %275 = or i64 %23, 224
  %276 = getelementptr inbounds float, float* %7, i64 %275
  %277 = bitcast float* %276 to <4 x float>*
  %wide.load.28 = load <4 x float>, <4 x float>* %277, align 4, !tbaa !849
  %278 = getelementptr inbounds float, float* %276, i64 4
  %279 = bitcast float* %278 to <4 x float>*
  %wide.load16.28 = load <4 x float>, <4 x float>* %279, align 4, !tbaa !849
  %280 = getelementptr inbounds float, float* %4, i64 %275
  %281 = bitcast float* %280 to <4 x float>*
  store <4 x float> %wide.load.28, <4 x float>* %281, align 4, !tbaa !852
  %282 = getelementptr inbounds float, float* %280, i64 4
  %283 = bitcast float* %282 to <4 x float>*
  store <4 x float> %wide.load16.28, <4 x float>* %283, align 4, !tbaa !852
  %284 = or i64 %23, 232
  %285 = getelementptr inbounds float, float* %7, i64 %284
  %286 = bitcast float* %285 to <4 x float>*
  %wide.load.29 = load <4 x float>, <4 x float>* %286, align 4, !tbaa !849
  %287 = getelementptr inbounds float, float* %285, i64 4
  %288 = bitcast float* %287 to <4 x float>*
  %wide.load16.29 = load <4 x float>, <4 x float>* %288, align 4, !tbaa !849
  %289 = getelementptr inbounds float, float* %4, i64 %284
  %290 = bitcast float* %289 to <4 x float>*
  store <4 x float> %wide.load.29, <4 x float>* %290, align 4, !tbaa !852
  %291 = getelementptr inbounds float, float* %289, i64 4
  %292 = bitcast float* %291 to <4 x float>*
  store <4 x float> %wide.load16.29, <4 x float>* %292, align 4, !tbaa !852
  %293 = or i64 %23, 240
  %294 = getelementptr inbounds float, float* %7, i64 %293
  %295 = bitcast float* %294 to <4 x float>*
  %wide.load.30 = load <4 x float>, <4 x float>* %295, align 4, !tbaa !849
  %296 = getelementptr inbounds float, float* %294, i64 4
  %297 = bitcast float* %296 to <4 x float>*
  %wide.load16.30 = load <4 x float>, <4 x float>* %297, align 4, !tbaa !849
  %298 = getelementptr inbounds float, float* %4, i64 %293
  %299 = bitcast float* %298 to <4 x float>*
  store <4 x float> %wide.load.30, <4 x float>* %299, align 4, !tbaa !852
  %300 = getelementptr inbounds float, float* %298, i64 4
  %301 = bitcast float* %300 to <4 x float>*
  store <4 x float> %wide.load16.30, <4 x float>* %301, align 4, !tbaa !852
  %302 = or i64 %23, 248
  %303 = getelementptr inbounds float, float* %7, i64 %302
  %304 = bitcast float* %303 to <4 x float>*
  %wide.load.31 = load <4 x float>, <4 x float>* %304, align 4, !tbaa !849
  %305 = getelementptr inbounds float, float* %303, i64 4
  %306 = bitcast float* %305 to <4 x float>*
  %wide.load16.31 = load <4 x float>, <4 x float>* %306, align 4, !tbaa !849
  %307 = getelementptr inbounds float, float* %4, i64 %302
  %308 = bitcast float* %307 to <4 x float>*
  store <4 x float> %wide.load.31, <4 x float>* %308, align 4, !tbaa !852
  %309 = getelementptr inbounds float, float* %307, i64 4
  %310 = bitcast float* %309 to <4 x float>*
  store <4 x float> %wide.load16.31, <4 x float>* %310, align 4, !tbaa !852
  %indvars.iv.next11 = add nuw nsw i64 %indvars.iv10, 1
  %exitcond12.not = icmp eq i64 %indvars.iv.next11, 28
  br i1 %exitcond12.not, label %for_end3, label %for_begin4.preheader, !prof !51

for_end3:                                         ; preds = %for_begin4.preheader
  %indvars.iv.next14 = add nsw i64 %indvars.iv13, 1
  %exitcond15.not = icmp eq i64 %indvars.iv.next14, %wide.trip.count
  br i1 %exitcond15.not, label %for_end, label %for_begin1.preheader, !prof !51
}

; Function Attrs: nofree nounwind
define private i32 @__tvm_parallel_lambda.101(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #3 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds i8, i8* %2, i64 16
  %9 = bitcast i8* %8 to float**
  %10 = load float*, float** %9, align 8
  %11 = getelementptr inbounds i8, i8* %2, i64 24
  %12 = bitcast i8* %11 to float**
  %13 = load float*, float** %12, align 8
  %14 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %15 = load i32, i32* %14, align 4
  %16 = add nsw i32 %15, 783
  %17 = sdiv i32 %16, %15
  %18 = add nsw i32 %0, 1
  %19 = mul nsw i32 %17, %18
  %20 = icmp slt i32 %19, 784
  %21 = select i1 %20, i32 %19, i32 784
  %22 = mul nsw i32 %17, %0
  %23 = icmp slt i32 %22, 784
  %24 = select i1 %23, i32 %22, i32 784
  %25 = icmp slt i32 %24, %21
  br i1 %25, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %26 = sext i32 %24 to i64
  %wide.trip.count = sext i32 %21 to i64
  %27 = bitcast float* %13 to <64 x float>*
  %28 = load <64 x float>, <64 x float>* %27, align 128, !tbaa !855
  %29 = getelementptr inbounds float, float* %13, i64 64
  %30 = bitcast float* %29 to <64 x float>*
  %31 = load <64 x float>, <64 x float>* %30, align 128, !tbaa !855
  %32 = getelementptr inbounds float, float* %13, i64 128
  %33 = bitcast float* %32 to <64 x float>*
  %34 = load <64 x float>, <64 x float>* %33, align 128, !tbaa !855
  %35 = getelementptr inbounds float, float* %13, i64 192
  %36 = bitcast float* %35 to <64 x float>*
  %37 = load <64 x float>, <64 x float>* %36, align 128, !tbaa !855
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end6.3
  %indvars.iv14 = phi i64 [ %26, %for_begin1.preheader.preheader ], [ %indvars.iv.next15, %for_end6.3 ]
  %38 = phi i32 [ %24, %for_begin1.preheader.preheader ], [ %158, %for_end6.3 ]
  %39 = shl nsw i32 %38, 8
  %40 = sext i32 %39 to i64
  br label %for_body5

for_end:                                          ; preds = %for_end6.3, %entry
  ret i32 0

for_body5:                                        ; preds = %for_body5, %for_begin1.preheader
  %indvars.iv = phi i64 [ 0, %for_begin1.preheader ], [ %indvars.iv.next.128, %for_body5 ]
  %.010 = phi <64 x float> [ zeroinitializer, %for_begin1.preheader ], [ %60, %for_body5 ]
  %41 = add nsw i64 %indvars.iv, %40
  %42 = getelementptr inbounds float, float* %4, i64 %41
  %43 = load float, float* %42, align 4, !tbaa !852
  %44 = insertelement <64 x float> undef, float %43, i32 0
  %45 = shufflevector <64 x float> %44, <64 x float> undef, <64 x i32> zeroinitializer
  %46 = shl nuw nsw i64 %indvars.iv, 8
  %47 = getelementptr inbounds float, float* %7, i64 %46
  %48 = bitcast float* %47 to <64 x float>*
  %49 = load <64 x float>, <64 x float>* %48, align 128, !tbaa !858
  %50 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %45, <64 x float> %49, <64 x float> %.010)
  %indvars.iv.next = or i64 %indvars.iv, 1
  %51 = add nsw i64 %indvars.iv.next, %40
  %52 = getelementptr inbounds float, float* %4, i64 %51
  %53 = load float, float* %52, align 4, !tbaa !852
  %54 = insertelement <64 x float> undef, float %53, i32 0
  %55 = shufflevector <64 x float> %54, <64 x float> undef, <64 x i32> zeroinitializer
  %56 = shl nuw nsw i64 %indvars.iv.next, 8
  %57 = getelementptr inbounds float, float* %7, i64 %56
  %58 = bitcast float* %57 to <64 x float>*
  %59 = load <64 x float>, <64 x float>* %58, align 128, !tbaa !858
  %60 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %55, <64 x float> %59, <64 x float> %50)
  %indvars.iv.next.128 = add nuw nsw i64 %indvars.iv, 2
  %exitcond.not.1 = icmp eq i64 %indvars.iv.next.128, 256
  br i1 %exitcond.not.1, label %for_end6, label %for_body5, !prof !51

for_end6:                                         ; preds = %for_body5
  %61 = fadd <64 x float> %60, %28
  %62 = fcmp olt <64 x float> %61, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %63 = select <64 x i1> %62, <64 x float> %61, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %64 = fcmp ogt <64 x float> %63, zeroinitializer
  %65 = select <64 x i1> %64, <64 x float> %63, <64 x float> zeroinitializer
  %66 = getelementptr inbounds float, float* %10, i64 %40
  %67 = bitcast float* %66 to <64 x float>*
  store <64 x float> %65, <64 x float>* %67, align 128, !tbaa !861
  br label %for_body5.1

for_body5.1:                                      ; preds = %for_body5.1, %for_end6
  %indvars.iv.1 = phi i64 [ 0, %for_end6 ], [ %indvars.iv.next.1.1, %for_body5.1 ]
  %.010.1 = phi <64 x float> [ zeroinitializer, %for_end6 ], [ %89, %for_body5.1 ]
  %68 = add nsw i64 %indvars.iv.1, %40
  %69 = getelementptr inbounds float, float* %4, i64 %68
  %70 = load float, float* %69, align 4, !tbaa !852
  %71 = insertelement <64 x float> undef, float %70, i32 0
  %72 = shufflevector <64 x float> %71, <64 x float> undef, <64 x i32> zeroinitializer
  %73 = shl nuw nsw i64 %indvars.iv.1, 8
  %74 = or i64 %73, 64
  %75 = getelementptr inbounds float, float* %7, i64 %74
  %76 = bitcast float* %75 to <64 x float>*
  %77 = load <64 x float>, <64 x float>* %76, align 128, !tbaa !858
  %78 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %72, <64 x float> %77, <64 x float> %.010.1)
  %indvars.iv.next.1 = or i64 %indvars.iv.1, 1
  %79 = add nsw i64 %indvars.iv.next.1, %40
  %80 = getelementptr inbounds float, float* %4, i64 %79
  %81 = load float, float* %80, align 4, !tbaa !852
  %82 = insertelement <64 x float> undef, float %81, i32 0
  %83 = shufflevector <64 x float> %82, <64 x float> undef, <64 x i32> zeroinitializer
  %84 = shl nuw nsw i64 %indvars.iv.next.1, 8
  %85 = or i64 %84, 64
  %86 = getelementptr inbounds float, float* %7, i64 %85
  %87 = bitcast float* %86 to <64 x float>*
  %88 = load <64 x float>, <64 x float>* %87, align 128, !tbaa !858
  %89 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %83, <64 x float> %88, <64 x float> %78)
  %indvars.iv.next.1.1 = add nuw nsw i64 %indvars.iv.1, 2
  %exitcond.1.not.1 = icmp eq i64 %indvars.iv.next.1.1, 256
  br i1 %exitcond.1.not.1, label %for_end6.1, label %for_body5.1, !prof !51

for_end6.1:                                       ; preds = %for_body5.1
  %90 = or i64 %40, 64
  %91 = fadd <64 x float> %89, %31
  %92 = fcmp olt <64 x float> %91, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %93 = select <64 x i1> %92, <64 x float> %91, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %94 = fcmp ogt <64 x float> %93, zeroinitializer
  %95 = select <64 x i1> %94, <64 x float> %93, <64 x float> zeroinitializer
  %96 = getelementptr inbounds float, float* %10, i64 %90
  %97 = bitcast float* %96 to <64 x float>*
  store <64 x float> %95, <64 x float>* %97, align 128, !tbaa !861
  br label %for_body5.2

for_body5.2:                                      ; preds = %for_body5.2, %for_end6.1
  %indvars.iv.2 = phi i64 [ 0, %for_end6.1 ], [ %indvars.iv.next.2.1, %for_body5.2 ]
  %.010.2 = phi <64 x float> [ zeroinitializer, %for_end6.1 ], [ %119, %for_body5.2 ]
  %98 = add nsw i64 %indvars.iv.2, %40
  %99 = getelementptr inbounds float, float* %4, i64 %98
  %100 = load float, float* %99, align 4, !tbaa !852
  %101 = insertelement <64 x float> undef, float %100, i32 0
  %102 = shufflevector <64 x float> %101, <64 x float> undef, <64 x i32> zeroinitializer
  %103 = shl nuw nsw i64 %indvars.iv.2, 8
  %104 = or i64 %103, 128
  %105 = getelementptr inbounds float, float* %7, i64 %104
  %106 = bitcast float* %105 to <64 x float>*
  %107 = load <64 x float>, <64 x float>* %106, align 128, !tbaa !858
  %108 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %102, <64 x float> %107, <64 x float> %.010.2)
  %indvars.iv.next.2 = or i64 %indvars.iv.2, 1
  %109 = add nsw i64 %indvars.iv.next.2, %40
  %110 = getelementptr inbounds float, float* %4, i64 %109
  %111 = load float, float* %110, align 4, !tbaa !852
  %112 = insertelement <64 x float> undef, float %111, i32 0
  %113 = shufflevector <64 x float> %112, <64 x float> undef, <64 x i32> zeroinitializer
  %114 = shl nuw nsw i64 %indvars.iv.next.2, 8
  %115 = or i64 %114, 128
  %116 = getelementptr inbounds float, float* %7, i64 %115
  %117 = bitcast float* %116 to <64 x float>*
  %118 = load <64 x float>, <64 x float>* %117, align 128, !tbaa !858
  %119 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %113, <64 x float> %118, <64 x float> %108)
  %indvars.iv.next.2.1 = add nuw nsw i64 %indvars.iv.2, 2
  %exitcond.2.not.1 = icmp eq i64 %indvars.iv.next.2.1, 256
  br i1 %exitcond.2.not.1, label %for_end6.2, label %for_body5.2, !prof !51

for_end6.2:                                       ; preds = %for_body5.2
  %120 = or i64 %40, 128
  %121 = fadd <64 x float> %119, %34
  %122 = fcmp olt <64 x float> %121, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %123 = select <64 x i1> %122, <64 x float> %121, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %124 = fcmp ogt <64 x float> %123, zeroinitializer
  %125 = select <64 x i1> %124, <64 x float> %123, <64 x float> zeroinitializer
  %126 = getelementptr inbounds float, float* %10, i64 %120
  %127 = bitcast float* %126 to <64 x float>*
  store <64 x float> %125, <64 x float>* %127, align 128, !tbaa !861
  br label %for_body5.3

for_body5.3:                                      ; preds = %for_body5.3, %for_end6.2
  %indvars.iv.3 = phi i64 [ 0, %for_end6.2 ], [ %indvars.iv.next.3.1, %for_body5.3 ]
  %.010.3 = phi <64 x float> [ zeroinitializer, %for_end6.2 ], [ %149, %for_body5.3 ]
  %128 = add nsw i64 %indvars.iv.3, %40
  %129 = getelementptr inbounds float, float* %4, i64 %128
  %130 = load float, float* %129, align 4, !tbaa !852
  %131 = insertelement <64 x float> undef, float %130, i32 0
  %132 = shufflevector <64 x float> %131, <64 x float> undef, <64 x i32> zeroinitializer
  %133 = shl nuw nsw i64 %indvars.iv.3, 8
  %134 = or i64 %133, 192
  %135 = getelementptr inbounds float, float* %7, i64 %134
  %136 = bitcast float* %135 to <64 x float>*
  %137 = load <64 x float>, <64 x float>* %136, align 128, !tbaa !858
  %138 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %132, <64 x float> %137, <64 x float> %.010.3)
  %indvars.iv.next.3 = or i64 %indvars.iv.3, 1
  %139 = add nsw i64 %indvars.iv.next.3, %40
  %140 = getelementptr inbounds float, float* %4, i64 %139
  %141 = load float, float* %140, align 4, !tbaa !852
  %142 = insertelement <64 x float> undef, float %141, i32 0
  %143 = shufflevector <64 x float> %142, <64 x float> undef, <64 x i32> zeroinitializer
  %144 = shl nuw nsw i64 %indvars.iv.next.3, 8
  %145 = or i64 %144, 192
  %146 = getelementptr inbounds float, float* %7, i64 %145
  %147 = bitcast float* %146 to <64 x float>*
  %148 = load <64 x float>, <64 x float>* %147, align 128, !tbaa !858
  %149 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %143, <64 x float> %148, <64 x float> %138)
  %indvars.iv.next.3.1 = add nuw nsw i64 %indvars.iv.3, 2
  %exitcond.3.not.1 = icmp eq i64 %indvars.iv.next.3.1, 256
  br i1 %exitcond.3.not.1, label %for_end6.3, label %for_body5.3, !prof !51

for_end6.3:                                       ; preds = %for_body5.3
  %150 = or i64 %40, 192
  %151 = fadd <64 x float> %149, %37
  %152 = fcmp olt <64 x float> %151, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %153 = select <64 x i1> %152, <64 x float> %151, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %154 = fcmp ogt <64 x float> %153, zeroinitializer
  %155 = select <64 x i1> %154, <64 x float> %153, <64 x float> zeroinitializer
  %156 = getelementptr inbounds float, float* %10, i64 %150
  %157 = bitcast float* %156 to <64 x float>*
  store <64 x float> %155, <64 x float>* %157, align 128, !tbaa !861
  %indvars.iv.next15 = add nsw i64 %indvars.iv14, 1
  %158 = add nsw i32 %38, 1
  %exitcond16.not = icmp eq i64 %indvars.iv.next15, %wide.trip.count
  br i1 %exitcond16.not, label %for_end, label %for_begin1.preheader, !prof !51
}

define dllexport i32 @fused_nn_softmax(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 2
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([69 x i8], [69 x i8]* @.str.102, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !864
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !878
  %18 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %19 = load i8*, i8** %18, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %19, i64 128) ]
  %20 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %21 = load i64*, i64** %20, align 8
  %22 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %23 = load i64*, i64** %22, align 8
  %24 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %25 = load i32, i32* %24, align 4
  %26 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %27 = load i8*, i8** %26, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %27, i64 128) ]
  %28 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %29 = load i64*, i64** %28, align 8
  %30 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %31 = load i64*, i64** %30, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %32 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %32(i8* getelementptr inbounds ([144 x i8], [144 x i8]* @.str.103, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %33 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %33(i8* getelementptr inbounds ([144 x i8], [144 x i8]* @.str.104, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %35 = load i32, i32* %34, align 4
  %36 = icmp eq i32 %35, 2
  br i1 %36, label %assert_end8, label %assert_fail5, !prof !5

assert_fail5:                                     ; preds = %assert_end4
  %37 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %37(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.105, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end4
  %38 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %39 = load i16, i16* %38, align 2
  %40 = icmp eq i16 %39, 1
  %41 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %42 = load i8, i8* %41, align 1
  %43 = icmp eq i8 %42, 32
  %44 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %45 = load i8, i8* %44, align 1
  %46 = icmp eq i8 %45, 2
  %47 = and i1 %43, %46
  %48 = and i1 %40, %47
  br i1 %48, label %assert_end10, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %49 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %49(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end10:                                     ; preds = %assert_end8
  %50 = load i64, i64* %21, align 8, !tbaa !880
  %51 = trunc i64 %50 to i32
  %52 = icmp eq i32 %51, 1
  br i1 %52, label %assert_end12, label %assert_fail11, !prof !5

assert_fail11:                                    ; preds = %assert_end10
  %53 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %53(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end10
  %54 = getelementptr inbounds i64, i64* %21, i64 1
  %55 = load i64, i64* %54, align 8, !tbaa !894
  %56 = trunc i64 %55 to i32
  %57 = icmp eq i32 %56, 1001
  br i1 %57, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.106, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %.not = icmp eq i64* %23, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end14
  %59 = load i64, i64* %23, align 8, !tbaa !896
  %60 = trunc i64 %59 to i32
  %61 = icmp eq i32 %60, 1001
  %62 = getelementptr inbounds i64, i64* %23, i64 1
  %63 = load i64, i64* %62, align 8, !tbaa !910
  %64 = trunc i64 %63 to i32
  %65 = icmp eq i32 %64, 1
  %66 = and i1 %61, %65
  br i1 %66, label %if_end, label %assert_fail15, !prof !5

if_end:                                           ; preds = %if_then, %assert_end14
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %68 = load i64, i64* %67, align 8
  %69 = icmp eq i64 %68, 0
  br i1 %69, label %assert_end18, label %assert_fail17, !prof !5

assert_fail15:                                    ; preds = %if_then
  %70 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %70(i8* getelementptr inbounds ([125 x i8], [125 x i8]* @.str.107, i64 0, i64 0))
  ret i32 -1

assert_fail17:                                    ; preds = %if_end
  %71 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %71(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %if_end
  %72 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %73 = load i32, i32* %72, align 4
  %74 = icmp eq i32 %73, 1
  br i1 %74, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %76 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %77 = load i32, i32* %76, align 4
  %78 = icmp eq i32 %77, 2
  br i1 %78, label %assert_end24, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.108, i64 0, i64 0))
  ret i32 -1

assert_end24:                                     ; preds = %assert_end20
  %80 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %81 = load i16, i16* %80, align 2
  %82 = icmp eq i16 %81, 1
  %83 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %84 = load i8, i8* %83, align 1
  %85 = icmp eq i8 %84, 32
  %86 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %87 = load i8, i8* %86, align 1
  %88 = icmp eq i8 %87, 2
  %89 = and i1 %85, %88
  %90 = and i1 %82, %89
  br i1 %90, label %assert_end26, label %assert_fail25, !prof !5

assert_fail25:                                    ; preds = %assert_end24
  %91 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %91(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %assert_end24
  %92 = load i64, i64* %29, align 8, !tbaa !912
  %93 = trunc i64 %92 to i32
  %94 = icmp eq i32 %93, 1
  br i1 %94, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %95 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %95(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.16, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %96 = getelementptr inbounds i64, i64* %29, i64 1
  %97 = load i64, i64* %96, align 8, !tbaa !926
  %98 = trunc i64 %97 to i32
  %99 = icmp eq i32 %98, 1001
  br i1 %99, label %assert_end30, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %100 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %100(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.109, i64 0, i64 0))
  ret i32 -1

assert_end30:                                     ; preds = %assert_end28
  %.not41 = icmp eq i64* %31, null
  br i1 %.not41, label %if_end32, label %if_then31, !prof !51

if_then31:                                        ; preds = %assert_end30
  %101 = load i64, i64* %31, align 8, !tbaa !928
  %102 = trunc i64 %101 to i32
  %103 = icmp eq i32 %102, 1001
  %104 = getelementptr inbounds i64, i64* %31, i64 1
  %105 = load i64, i64* %104, align 8, !tbaa !942
  %106 = trunc i64 %105 to i32
  %107 = icmp eq i32 %106, 1
  %108 = and i1 %103, %107
  br i1 %108, label %if_end32, label %assert_fail33, !prof !5

if_end32:                                         ; preds = %if_then31, %assert_end30
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %110 = load i64, i64* %109, align 8
  %111 = icmp eq i64 %110, 0
  br i1 %111, label %assert_end36, label %assert_fail35, !prof !5

assert_fail33:                                    ; preds = %if_then31
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([125 x i8], [125 x i8]* @.str.110, i64 0, i64 0))
  ret i32 -1

assert_fail35:                                    ; preds = %if_end32
  %113 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %113(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %if_end32
  %114 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %115 = load i32, i32* %114, align 4
  %116 = icmp eq i32 %115, 1
  br i1 %116, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %117 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %117(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %118 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %119 = load i32, i32* %118, align 4
  %120 = icmp eq i32 %25, %119
  br i1 %120, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %121 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %121(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %122 = tail call fastcc i32 @fused_nn_softmax_compute_(i8* %19, i8* %27, i32 %25)
  ret i32 %122
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_softmax_compute_(i8* noalias nocapture readonly align 128 %0, i8* noalias nocapture align 128 %1, i32 %2) unnamed_addr #1 {
entry:
  %3 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %4 = tail call i8* %3(i32 1, i32 %2, i64 4004, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %4, i64 128) ]
  %5 = icmp eq i8* %4, null
  br i1 %5, label %if_then, label %for_begin.preheader, !prof !5

for_begin.preheader:                              ; preds = %entry
  %6 = bitcast i8* %0 to float*
  br label %for_body

if_then:                                          ; preds = %entry
  ret i32 -1

for_begin1.preheader:                             ; preds = %for_body
  %7 = bitcast i8* %4 to float*
  %broadcast.splatinsert = insertelement <4 x float> poison, float %18, i32 0
  %broadcast.splat = shufflevector <4 x float> %broadcast.splatinsert, <4 x float> poison, <4 x i32> zeroinitializer
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_begin1.preheader
  %index = phi i64 [ 0, %for_begin1.preheader ], [ %index.next, %vector.body ]
  %8 = getelementptr inbounds float, float* %6, i64 %index
  %9 = bitcast float* %8 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %9, align 16, !tbaa !944
  %10 = fsub <4 x float> %wide.load, %broadcast.splat
  %11 = call <4 x float> @llvm.exp.v4f32(<4 x float> %10)
  %12 = getelementptr inbounds float, float* %7, i64 %index
  %13 = bitcast float* %12 to <4 x float>*
  store <4 x float> %11, <4 x float>* %13, align 16, !tbaa !947
  %index.next = add i64 %index, 4
  %14 = icmp eq i64 %index.next, 1000
  br i1 %14, label %for_body2, label %vector.body, !prof !335, !llvm.loop !950

for_body:                                         ; preds = %for_body.1, %for_begin.preheader
  %indvars.iv31 = phi i64 [ 0, %for_begin.preheader ], [ %indvars.iv.next32.3, %for_body.1 ]
  %.023 = phi float [ 0xC7EFFFFFE0000000, %for_begin.preheader ], [ %78, %for_body.1 ]
  %15 = getelementptr inbounds float, float* %6, i64 %indvars.iv31
  %16 = load float, float* %15, align 16, !tbaa !944
  %17 = fcmp ogt float %.023, %16
  %18 = select i1 %17, float %.023, float %16
  %indvars.iv.next32 = or i64 %indvars.iv31, 1
  %exitcond33.not = icmp eq i64 %indvars.iv.next32, 1001
  br i1 %exitcond33.not, label %for_begin1.preheader, label %for_body.1, !prof !51

for_body2:                                        ; preds = %vector.body
  %19 = getelementptr inbounds i8, i8* %0, i64 4000
  %20 = bitcast i8* %19 to float*
  %21 = load float, float* %20, align 32, !tbaa !944
  %22 = fsub float %21, %18
  %23 = tail call float @llvm.exp.f32(float %22)
  %24 = getelementptr inbounds i8, i8* %4, i64 4000
  %25 = bitcast i8* %24 to float*
  store float %23, float* %25, align 32, !tbaa !947
  br label %for_body5

for_begin7.preheader:                             ; preds = %for_body5
  %26 = bitcast i8* %1 to float*
  %broadcast.splatinsert45 = insertelement <4 x float> poison, float %58, i32 0
  %broadcast.splat46 = shufflevector <4 x float> %broadcast.splatinsert45, <4 x float> poison, <4 x i32> zeroinitializer
  br label %vector.body38

vector.body38:                                    ; preds = %vector.body38, %for_begin7.preheader
  %index40 = phi i64 [ 0, %for_begin7.preheader ], [ %index.next41.1, %vector.body38 ]
  %27 = getelementptr inbounds float, float* %7, i64 %index40
  %28 = bitcast float* %27 to <4 x float>*
  %wide.load44 = load <4 x float>, <4 x float>* %28, align 32, !tbaa !947
  %29 = fdiv <4 x float> %wide.load44, %broadcast.splat46
  %30 = getelementptr inbounds float, float* %26, i64 %index40
  %31 = bitcast float* %30 to <4 x float>*
  store <4 x float> %29, <4 x float>* %31, align 32, !tbaa !951
  %index.next41 = or i64 %index40, 4
  %32 = getelementptr inbounds float, float* %7, i64 %index.next41
  %33 = bitcast float* %32 to <4 x float>*
  %wide.load44.1 = load <4 x float>, <4 x float>* %33, align 16, !tbaa !947
  %34 = fdiv <4 x float> %wide.load44.1, %broadcast.splat46
  %35 = getelementptr inbounds float, float* %26, i64 %index.next41
  %36 = bitcast float* %35 to <4 x float>*
  store <4 x float> %34, <4 x float>* %36, align 16, !tbaa !951
  %index.next41.1 = add nuw nsw i64 %index40, 8
  %37 = icmp eq i64 %index.next41.1, 1000
  br i1 %37, label %for_body8, label %vector.body38, !prof !335, !llvm.loop !954

for_body5:                                        ; preds = %for_body5, %for_body2
  %indvars.iv25 = phi i64 [ 0, %for_body2 ], [ %indvars.iv.next26.6, %for_body5 ]
  %.01622 = phi float [ 0.000000e+00, %for_body2 ], [ %58, %for_body5 ]
  %38 = getelementptr inbounds float, float* %7, i64 %indvars.iv25
  %39 = load float, float* %38, align 4, !tbaa !947
  %40 = fadd float %.01622, %39
  %indvars.iv.next26 = add nuw nsw i64 %indvars.iv25, 1
  %41 = getelementptr inbounds float, float* %7, i64 %indvars.iv.next26
  %42 = load float, float* %41, align 4, !tbaa !947
  %43 = fadd float %40, %42
  %indvars.iv.next26.1 = add nuw nsw i64 %indvars.iv25, 2
  %44 = getelementptr inbounds float, float* %7, i64 %indvars.iv.next26.1
  %45 = load float, float* %44, align 4, !tbaa !947
  %46 = fadd float %43, %45
  %indvars.iv.next26.2 = add nuw nsw i64 %indvars.iv25, 3
  %47 = getelementptr inbounds float, float* %7, i64 %indvars.iv.next26.2
  %48 = load float, float* %47, align 4, !tbaa !947
  %49 = fadd float %46, %48
  %indvars.iv.next26.3 = add nuw nsw i64 %indvars.iv25, 4
  %50 = getelementptr inbounds float, float* %7, i64 %indvars.iv.next26.3
  %51 = load float, float* %50, align 4, !tbaa !947
  %52 = fadd float %49, %51
  %indvars.iv.next26.4 = add nuw nsw i64 %indvars.iv25, 5
  %53 = getelementptr inbounds float, float* %7, i64 %indvars.iv.next26.4
  %54 = load float, float* %53, align 4, !tbaa !947
  %55 = fadd float %52, %54
  %indvars.iv.next26.5 = add nuw nsw i64 %indvars.iv25, 6
  %56 = getelementptr inbounds float, float* %7, i64 %indvars.iv.next26.5
  %57 = load float, float* %56, align 4, !tbaa !947
  %58 = fadd float %55, %57
  %indvars.iv.next26.6 = add nuw nsw i64 %indvars.iv25, 7
  %exitcond27.not.6 = icmp eq i64 %indvars.iv.next26.6, 1001
  br i1 %exitcond27.not.6, label %for_begin7.preheader, label %for_body5, !prof !51

for_body8:                                        ; preds = %vector.body38
  %59 = getelementptr inbounds i8, i8* %4, i64 4000
  %60 = bitcast i8* %59 to float*
  %61 = load float, float* %60, align 32, !tbaa !947
  %62 = fdiv float %61, %58
  %63 = getelementptr inbounds i8, i8* %1, i64 4000
  %64 = bitcast i8* %63 to float*
  store float %62, float* %64, align 32, !tbaa !951
  %65 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %66 = tail call i32 %65(i32 1, i32 %2, i8* nonnull %4)
  %.not = icmp ne i32 %66, 0
  %spec.select = sext i1 %.not to i32
  ret i32 %spec.select

for_body.1:                                       ; preds = %for_body
  %67 = getelementptr inbounds float, float* %6, i64 %indvars.iv.next32
  %68 = load float, float* %67, align 4, !tbaa !944
  %69 = fcmp ogt float %18, %68
  %70 = select i1 %69, float %18, float %68
  %indvars.iv.next32.1 = or i64 %indvars.iv31, 2
  %71 = getelementptr inbounds float, float* %6, i64 %indvars.iv.next32.1
  %72 = load float, float* %71, align 8, !tbaa !944
  %73 = fcmp ogt float %70, %72
  %74 = select i1 %73, float %70, float %72
  %indvars.iv.next32.2 = or i64 %indvars.iv31, 3
  %75 = getelementptr inbounds float, float* %6, i64 %indvars.iv.next32.2
  %76 = load float, float* %75, align 4, !tbaa !944
  %77 = fcmp ogt float %74, %76
  %78 = select i1 %77, float %74, float %76
  %indvars.iv.next32.3 = add nuw nsw i64 %indvars.iv31, 4
  br label %for_body
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare float @llvm.exp.f32(float) #4

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_7(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.111, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !955
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !969
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !971
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !974
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.112, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.113, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.114, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.115, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !976
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !990
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 28
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.90, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !992
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 28
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.91, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !995
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 256
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.92, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !997
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 200704, i32 7168, i32 256, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.93, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !1009
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 3
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.51, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !1023
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 3
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.52, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !1025
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 256
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.94, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !1028
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 1
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.54, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !1030
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 768, i32 256, i32 1, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.116, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !1042
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 256
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.97, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !1056
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !1070
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !1084
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 14
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.57, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !1086
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 14
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.58, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !1089
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 256
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.98, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !1091
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 50176, i32 3584, i32 256, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.117, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_7_compute_(i8* %31, i8* %39, i8* %45, i8* %51, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_7_compute_(i8* noalias nocapture readonly align 128 %0, i8* noalias nocapture readonly align 128 %1, i8* noalias nocapture readonly align 128 %2, i8* noalias nocapture align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 861184, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %for_end38, %if_end, %entry
  ret i32 -1

if_end:                                           ; preds = %entry
  %8 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %9 = tail call i8* %8(i32 1, i32 %4, i64 200704, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %9, i64 128) ]
  %10 = icmp eq i8* %9, null
  br i1 %10, label %if_then, label %for_begin4.preheader, !prof !5

for_begin4.preheader:                             ; preds = %if_end, %for_end6
  %indvar = phi i64 [ %indvar.next, %for_end6 ], [ 0, %if_end ]
  %11 = phi i32 [ %24, %for_end6 ], [ 0, %if_end ]
  %12 = mul nuw nsw i64 %indvar, 29696
  %13 = mul nuw nsw i64 %indvar, 28672
  %14 = icmp ult i32 %11, 28
  br i1 %14, label %for_begin7.preheader.us, label %for_begin7.preheader.preheader

for_begin7.preheader.preheader:                   ; preds = %for_begin4.preheader
  %scevgep103 = getelementptr i8, i8* %6, i64 %12
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(29696) %scevgep103, i8 0, i64 29696, i1 false)
  br label %for_end6

for_begin7.preheader.us:                          ; preds = %for_begin4.preheader, %for_end9.us
  %indvar104 = phi i64 [ %indvar.next105, %for_end9.us ], [ 0, %for_begin4.preheader ]
  %15 = phi i32 [ %20, %for_end9.us ], [ 0, %for_begin4.preheader ]
  %16 = shl nuw nsw i64 %indvar104, 10
  %17 = add nuw nsw i64 %12, %16
  %scevgep108 = getelementptr i8, i8* %6, i64 %17
  %18 = icmp ult i32 %15, 28
  br i1 %18, label %for_body8.us.us.preheader, label %for_body8.us60.preheader

for_body8.us60.preheader:                         ; preds = %for_begin7.preheader.us
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(1024) %scevgep108, i8 0, i64 1024, i1 false)
  br label %for_end9.us

for_body8.us.us.preheader:                        ; preds = %for_begin7.preheader.us
  %19 = add nuw nsw i64 %13, %16
  %scevgep109 = getelementptr i8, i8* %0, i64 %19
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(1024) %scevgep108, i8* nonnull align 128 dereferenceable(1024) %scevgep109, i64 1024, i1 false)
  br label %for_end9.us

for_end9.us:                                      ; preds = %for_body8.us60.preheader, %for_body8.us.us.preheader
  %20 = add nuw nsw i32 %15, 1
  %indvar.next105 = add nuw nsw i64 %indvar104, 1
  %exitcond112.not = icmp eq i64 %indvar.next105, 29
  br i1 %exitcond112.not, label %for_end6, label %for_begin7.preheader.us, !prof !51

for_begin12.preheader:                            ; preds = %for_end6
  %21 = bitcast i8* %9 to float*
  %22 = bitcast i8* %6 to float*
  %23 = bitcast i8* %1 to float*
  br label %for_begin15.preheader

for_end6:                                         ; preds = %for_end9.us, %for_begin7.preheader.preheader
  %24 = add nuw nsw i32 %11, 1
  %indvar.next = add nuw nsw i64 %indvar, 1
  %exitcond113.not = icmp eq i64 %indvar.next, 29
  br i1 %exitcond113.not, label %for_begin12.preheader, label %for_begin4.preheader, !prof !51

for_begin15.preheader:                            ; preds = %for_begin12.preheader, %for_end17
  %indvars.iv96 = phi i64 [ 0, %for_begin12.preheader ], [ %indvars.iv.next97, %for_end17 ]
  %25 = mul nuw nsw i64 %indvars.iv96, 3584
  %26 = mul nuw nsw i64 %indvars.iv96, 14848
  br label %for_begin18.preheader

for_begin27.preheader:                            ; preds = %for_end17
  %27 = bitcast i8* %2 to float*
  br label %for_begin30.preheader

for_begin18.preheader:                            ; preds = %for_begin15.preheader, %for_end20
  %indvars.iv93 = phi i64 [ 0, %for_begin15.preheader ], [ %indvars.iv.next94, %for_end20 ]
  %28 = shl nsw i64 %indvars.iv93, 8
  %29 = add nuw nsw i64 %28, %25
  %30 = shl nsw i64 %indvars.iv93, 9
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_begin18.preheader
  %index = phi i64 [ 0, %for_begin18.preheader ], [ %index.next, %vector.body ]
  %31 = add nuw nsw i64 %29, %index
  %32 = getelementptr inbounds float, float* %21, i64 %31
  %33 = add nuw nsw i64 %index, %26
  %34 = add nuw nsw i64 %33, %30
  %35 = getelementptr inbounds float, float* %22, i64 %34
  %36 = bitcast float* %35 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %36, align 16, !tbaa !1103
  %37 = getelementptr inbounds float, float* %23, i64 %index
  %38 = bitcast float* %37 to <4 x float>*
  %wide.load114 = load <4 x float>, <4 x float>* %38, align 16, !tbaa !1106
  %39 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load, <4 x float> %wide.load114, <4 x float> zeroinitializer)
  %40 = add nuw nsw i64 %index, 256
  %41 = add nuw nsw i64 %40, %26
  %42 = add nuw nsw i64 %41, %30
  %43 = getelementptr inbounds float, float* %22, i64 %42
  %44 = bitcast float* %43 to <4 x float>*
  %wide.load115 = load <4 x float>, <4 x float>* %44, align 16, !tbaa !1103
  %45 = getelementptr inbounds float, float* %23, i64 %40
  %46 = bitcast float* %45 to <4 x float>*
  %wide.load116 = load <4 x float>, <4 x float>* %46, align 16, !tbaa !1106
  %47 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load115, <4 x float> %wide.load116, <4 x float> %39)
  %48 = add nuw nsw i64 %index, 512
  %49 = add nuw nsw i64 %48, %26
  %50 = add nuw nsw i64 %49, %30
  %51 = getelementptr inbounds float, float* %22, i64 %50
  %52 = bitcast float* %51 to <4 x float>*
  %wide.load117 = load <4 x float>, <4 x float>* %52, align 16, !tbaa !1103
  %53 = getelementptr inbounds float, float* %23, i64 %48
  %54 = bitcast float* %53 to <4 x float>*
  %wide.load118 = load <4 x float>, <4 x float>* %54, align 16, !tbaa !1106
  %55 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load117, <4 x float> %wide.load118, <4 x float> %47)
  %56 = add nuw nsw i64 %34, 7424
  %57 = getelementptr inbounds float, float* %22, i64 %56
  %58 = bitcast float* %57 to <4 x float>*
  %wide.load119 = load <4 x float>, <4 x float>* %58, align 16, !tbaa !1103
  %59 = add nuw nsw i64 %index, 768
  %60 = getelementptr inbounds float, float* %23, i64 %59
  %61 = bitcast float* %60 to <4 x float>*
  %wide.load120 = load <4 x float>, <4 x float>* %61, align 16, !tbaa !1106
  %62 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load119, <4 x float> %wide.load120, <4 x float> %55)
  %63 = add nuw nsw i64 %42, 7424
  %64 = getelementptr inbounds float, float* %22, i64 %63
  %65 = bitcast float* %64 to <4 x float>*
  %wide.load121 = load <4 x float>, <4 x float>* %65, align 16, !tbaa !1103
  %66 = add nuw nsw i64 %index, 1024
  %67 = getelementptr inbounds float, float* %23, i64 %66
  %68 = bitcast float* %67 to <4 x float>*
  %wide.load122 = load <4 x float>, <4 x float>* %68, align 16, !tbaa !1106
  %69 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load121, <4 x float> %wide.load122, <4 x float> %62)
  %70 = add nuw nsw i64 %50, 7424
  %71 = getelementptr inbounds float, float* %22, i64 %70
  %72 = bitcast float* %71 to <4 x float>*
  %wide.load123 = load <4 x float>, <4 x float>* %72, align 16, !tbaa !1103
  %73 = add nuw nsw i64 %index, 1280
  %74 = getelementptr inbounds float, float* %23, i64 %73
  %75 = bitcast float* %74 to <4 x float>*
  %wide.load124 = load <4 x float>, <4 x float>* %75, align 16, !tbaa !1106
  %76 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load123, <4 x float> %wide.load124, <4 x float> %69)
  %77 = add nuw nsw i64 %34, 14848
  %78 = getelementptr inbounds float, float* %22, i64 %77
  %79 = bitcast float* %78 to <4 x float>*
  %wide.load125 = load <4 x float>, <4 x float>* %79, align 16, !tbaa !1103
  %80 = add nuw nsw i64 %index, 1536
  %81 = getelementptr inbounds float, float* %23, i64 %80
  %82 = bitcast float* %81 to <4 x float>*
  %wide.load126 = load <4 x float>, <4 x float>* %82, align 16, !tbaa !1106
  %83 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load125, <4 x float> %wide.load126, <4 x float> %76)
  %84 = add nuw nsw i64 %42, 14848
  %85 = getelementptr inbounds float, float* %22, i64 %84
  %86 = bitcast float* %85 to <4 x float>*
  %wide.load127 = load <4 x float>, <4 x float>* %86, align 16, !tbaa !1103
  %87 = add nuw nsw i64 %index, 1792
  %88 = getelementptr inbounds float, float* %23, i64 %87
  %89 = bitcast float* %88 to <4 x float>*
  %wide.load128 = load <4 x float>, <4 x float>* %89, align 16, !tbaa !1106
  %90 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load127, <4 x float> %wide.load128, <4 x float> %83)
  %91 = add nuw nsw i64 %50, 14848
  %92 = getelementptr inbounds float, float* %22, i64 %91
  %93 = bitcast float* %92 to <4 x float>*
  %wide.load129 = load <4 x float>, <4 x float>* %93, align 16, !tbaa !1103
  %94 = add nuw nsw i64 %index, 2048
  %95 = getelementptr inbounds float, float* %23, i64 %94
  %96 = bitcast float* %95 to <4 x float>*
  %wide.load130 = load <4 x float>, <4 x float>* %96, align 16, !tbaa !1106
  %97 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load129, <4 x float> %wide.load130, <4 x float> %90)
  %98 = bitcast float* %32 to <4 x float>*
  store <4 x float> %97, <4 x float>* %98, align 16, !tbaa !1109
  %index.next = add i64 %index, 4
  %99 = icmp eq i64 %index.next, 256
  br i1 %99, label %for_end20, label %vector.body, !prof !335, !llvm.loop !1112

for_end17:                                        ; preds = %for_end20
  %indvars.iv.next97 = add nuw nsw i64 %indvars.iv96, 1
  %exitcond98.not = icmp eq i64 %indvars.iv.next97, 14
  br i1 %exitcond98.not, label %for_begin27.preheader, label %for_begin15.preheader, !prof !51

for_end20:                                        ; preds = %vector.body
  %indvars.iv.next94 = add nuw nsw i64 %indvars.iv93, 1
  %exitcond95.not = icmp eq i64 %indvars.iv.next94, 14
  br i1 %exitcond95.not, label %for_end17, label %for_begin18.preheader, !prof !51

for_begin30.preheader:                            ; preds = %for_begin27.preheader, %for_end35.13
  %indvars.iv79 = phi i64 [ 0, %for_begin27.preheader ], [ %indvars.iv.next80, %for_end35.13 ]
  %100 = mul nuw nsw i64 %indvars.iv79, 3584
  br label %vector.body289

vector.body289:                                   ; preds = %vector.body289, %for_begin30.preheader
  %index291 = phi i64 [ 0, %for_begin30.preheader ], [ %index.next292.1, %vector.body289 ]
  %101 = add nuw nsw i64 %100, %index291
  %102 = getelementptr inbounds float, float* %27, i64 %index291
  %103 = bitcast float* %102 to <4 x float>*
  %wide.load295 = load <4 x float>, <4 x float>* %103, align 64, !tbaa !1113
  %104 = getelementptr inbounds float, float* %102, i64 4
  %105 = bitcast float* %104 to <4 x float>*
  %wide.load296 = load <4 x float>, <4 x float>* %105, align 16, !tbaa !1113
  %106 = getelementptr inbounds float, float* %21, i64 %101
  %107 = bitcast float* %106 to <4 x float>*
  %wide.load297 = load <4 x float>, <4 x float>* %107, align 64, !tbaa !1109
  %108 = getelementptr inbounds float, float* %106, i64 4
  %109 = bitcast float* %108 to <4 x float>*
  %wide.load298 = load <4 x float>, <4 x float>* %109, align 16, !tbaa !1109
  %110 = fadd <4 x float> %wide.load295, %wide.load297
  %111 = fadd <4 x float> %wide.load296, %wide.load298
  %112 = bitcast float* %106 to <4 x float>*
  store <4 x float> %110, <4 x float>* %112, align 64, !tbaa !1109
  %113 = bitcast float* %108 to <4 x float>*
  store <4 x float> %111, <4 x float>* %113, align 16, !tbaa !1109
  %index.next292 = or i64 %index291, 8
  %114 = add nuw nsw i64 %100, %index.next292
  %115 = getelementptr inbounds float, float* %27, i64 %index.next292
  %116 = bitcast float* %115 to <4 x float>*
  %wide.load295.1 = load <4 x float>, <4 x float>* %116, align 32, !tbaa !1113
  %117 = getelementptr inbounds float, float* %115, i64 4
  %118 = bitcast float* %117 to <4 x float>*
  %wide.load296.1 = load <4 x float>, <4 x float>* %118, align 16, !tbaa !1113
  %119 = getelementptr inbounds float, float* %21, i64 %114
  %120 = bitcast float* %119 to <4 x float>*
  %wide.load297.1 = load <4 x float>, <4 x float>* %120, align 32, !tbaa !1109
  %121 = getelementptr inbounds float, float* %119, i64 4
  %122 = bitcast float* %121 to <4 x float>*
  %wide.load298.1 = load <4 x float>, <4 x float>* %122, align 16, !tbaa !1109
  %123 = fadd <4 x float> %wide.load295.1, %wide.load297.1
  %124 = fadd <4 x float> %wide.load296.1, %wide.load298.1
  %125 = bitcast float* %119 to <4 x float>*
  store <4 x float> %123, <4 x float>* %125, align 32, !tbaa !1109
  %126 = bitcast float* %121 to <4 x float>*
  store <4 x float> %124, <4 x float>* %126, align 16, !tbaa !1109
  %index.next292.1 = add nuw nsw i64 %index291, 16
  %127 = icmp eq i64 %index.next292.1, 256
  br i1 %127, label %for_end35, label %vector.body289, !prof !341, !llvm.loop !1116

for_begin36.preheader:                            ; preds = %for_end35.13
  %128 = bitcast i8* %3 to float*
  br label %for_begin39.preheader

for_end35:                                        ; preds = %vector.body289
  %129 = or i64 %100, 256
  br label %vector.body277

vector.body277:                                   ; preds = %vector.body277, %for_end35
  %index279 = phi i64 [ 0, %for_end35 ], [ %index.next280.1, %vector.body277 ]
  %130 = add nuw nsw i64 %129, %index279
  %131 = getelementptr inbounds float, float* %27, i64 %index279
  %132 = bitcast float* %131 to <4 x float>*
  %wide.load283 = load <4 x float>, <4 x float>* %132, align 64, !tbaa !1113
  %133 = getelementptr inbounds float, float* %131, i64 4
  %134 = bitcast float* %133 to <4 x float>*
  %wide.load284 = load <4 x float>, <4 x float>* %134, align 16, !tbaa !1113
  %135 = getelementptr inbounds float, float* %21, i64 %130
  %136 = bitcast float* %135 to <4 x float>*
  %wide.load285 = load <4 x float>, <4 x float>* %136, align 64, !tbaa !1109
  %137 = getelementptr inbounds float, float* %135, i64 4
  %138 = bitcast float* %137 to <4 x float>*
  %wide.load286 = load <4 x float>, <4 x float>* %138, align 16, !tbaa !1109
  %139 = fadd <4 x float> %wide.load283, %wide.load285
  %140 = fadd <4 x float> %wide.load284, %wide.load286
  %141 = bitcast float* %135 to <4 x float>*
  store <4 x float> %139, <4 x float>* %141, align 64, !tbaa !1109
  %142 = bitcast float* %137 to <4 x float>*
  store <4 x float> %140, <4 x float>* %142, align 16, !tbaa !1109
  %index.next280 = or i64 %index279, 8
  %143 = add nuw nsw i64 %129, %index.next280
  %144 = getelementptr inbounds float, float* %27, i64 %index.next280
  %145 = bitcast float* %144 to <4 x float>*
  %wide.load283.1 = load <4 x float>, <4 x float>* %145, align 32, !tbaa !1113
  %146 = getelementptr inbounds float, float* %144, i64 4
  %147 = bitcast float* %146 to <4 x float>*
  %wide.load284.1 = load <4 x float>, <4 x float>* %147, align 16, !tbaa !1113
  %148 = getelementptr inbounds float, float* %21, i64 %143
  %149 = bitcast float* %148 to <4 x float>*
  %wide.load285.1 = load <4 x float>, <4 x float>* %149, align 32, !tbaa !1109
  %150 = getelementptr inbounds float, float* %148, i64 4
  %151 = bitcast float* %150 to <4 x float>*
  %wide.load286.1 = load <4 x float>, <4 x float>* %151, align 16, !tbaa !1109
  %152 = fadd <4 x float> %wide.load283.1, %wide.load285.1
  %153 = fadd <4 x float> %wide.load284.1, %wide.load286.1
  %154 = bitcast float* %148 to <4 x float>*
  store <4 x float> %152, <4 x float>* %154, align 32, !tbaa !1109
  %155 = bitcast float* %150 to <4 x float>*
  store <4 x float> %153, <4 x float>* %155, align 16, !tbaa !1109
  %index.next280.1 = add nuw nsw i64 %index279, 16
  %156 = icmp eq i64 %index.next280.1, 256
  br i1 %156, label %for_end35.1, label %vector.body277, !prof !341, !llvm.loop !1117

for_begin39.preheader:                            ; preds = %for_begin36.preheader, %for_end44.13
  %indvars.iv70 = phi i64 [ 0, %for_begin36.preheader ], [ %indvars.iv.next71, %for_end44.13 ]
  %157 = mul nuw nsw i64 %indvars.iv70, 3584
  br label %vector.body431

vector.body431:                                   ; preds = %vector.body431, %for_begin39.preheader
  %index433 = phi i64 [ 0, %for_begin39.preheader ], [ %index.next434, %vector.body431 ]
  %158 = add nuw nsw i64 %157, %index433
  %159 = getelementptr inbounds float, float* %21, i64 %158
  %160 = bitcast float* %159 to <4 x float>*
  %wide.load437 = load <4 x float>, <4 x float>* %160, align 32, !tbaa !1109
  %161 = getelementptr inbounds float, float* %159, i64 4
  %162 = bitcast float* %161 to <4 x float>*
  %wide.load438 = load <4 x float>, <4 x float>* %162, align 16, !tbaa !1109
  %163 = fcmp olt <4 x float> %wide.load437, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %164 = fcmp olt <4 x float> %wide.load438, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %165 = select <4 x i1> %163, <4 x float> %wide.load437, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %166 = select <4 x i1> %164, <4 x float> %wide.load438, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %167 = fcmp ogt <4 x float> %165, zeroinitializer
  %168 = fcmp ogt <4 x float> %166, zeroinitializer
  %169 = select <4 x i1> %167, <4 x float> %165, <4 x float> zeroinitializer
  %170 = select <4 x i1> %168, <4 x float> %166, <4 x float> zeroinitializer
  %171 = getelementptr inbounds float, float* %128, i64 %158
  %172 = bitcast float* %171 to <4 x float>*
  store <4 x float> %169, <4 x float>* %172, align 32, !tbaa !1118
  %173 = getelementptr inbounds float, float* %171, i64 4
  %174 = bitcast float* %173 to <4 x float>*
  store <4 x float> %170, <4 x float>* %174, align 16, !tbaa !1118
  %index.next434 = add i64 %index433, 8
  %175 = icmp eq i64 %index.next434, 256
  br i1 %175, label %for_end44, label %vector.body431, !prof !341, !llvm.loop !1121

for_end38:                                        ; preds = %for_end44.13
  %176 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %177 = tail call i32 %176(i32 1, i32 %4, i8* nonnull %9)
  %.not = icmp eq i32 %177, 0
  br i1 %.not, label %if_end46, label %if_then, !prof !51

for_end44:                                        ; preds = %vector.body431
  %178 = or i64 %157, 256
  br label %vector.body421

vector.body421:                                   ; preds = %vector.body421, %for_end44
  %index423 = phi i64 [ 0, %for_end44 ], [ %index.next424, %vector.body421 ]
  %179 = add nuw nsw i64 %178, %index423
  %180 = getelementptr inbounds float, float* %21, i64 %179
  %181 = bitcast float* %180 to <4 x float>*
  %wide.load427 = load <4 x float>, <4 x float>* %181, align 32, !tbaa !1109
  %182 = getelementptr inbounds float, float* %180, i64 4
  %183 = bitcast float* %182 to <4 x float>*
  %wide.load428 = load <4 x float>, <4 x float>* %183, align 16, !tbaa !1109
  %184 = fcmp olt <4 x float> %wide.load427, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %185 = fcmp olt <4 x float> %wide.load428, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %186 = select <4 x i1> %184, <4 x float> %wide.load427, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %187 = select <4 x i1> %185, <4 x float> %wide.load428, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %188 = fcmp ogt <4 x float> %186, zeroinitializer
  %189 = fcmp ogt <4 x float> %187, zeroinitializer
  %190 = select <4 x i1> %188, <4 x float> %186, <4 x float> zeroinitializer
  %191 = select <4 x i1> %189, <4 x float> %187, <4 x float> zeroinitializer
  %192 = getelementptr inbounds float, float* %128, i64 %179
  %193 = bitcast float* %192 to <4 x float>*
  store <4 x float> %190, <4 x float>* %193, align 32, !tbaa !1118
  %194 = getelementptr inbounds float, float* %192, i64 4
  %195 = bitcast float* %194 to <4 x float>*
  store <4 x float> %191, <4 x float>* %195, align 16, !tbaa !1118
  %index.next424 = add i64 %index423, 8
  %196 = icmp eq i64 %index.next424, 256
  br i1 %196, label %for_end44.1, label %vector.body421, !prof !341, !llvm.loop !1122

if_end46:                                         ; preds = %for_end38
  %197 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %198 = tail call i32 %197(i32 1, i32 %4, i8* nonnull %6)
  %.not57 = icmp ne i32 %198, 0
  %spec.select = sext i1 %.not57 to i32
  ret i32 %spec.select

for_end44.1:                                      ; preds = %vector.body421
  %199 = add nuw nsw i64 %157, 512
  br label %vector.body411

vector.body411:                                   ; preds = %vector.body411, %for_end44.1
  %index413 = phi i64 [ 0, %for_end44.1 ], [ %index.next414, %vector.body411 ]
  %200 = add nuw nsw i64 %199, %index413
  %201 = getelementptr inbounds float, float* %21, i64 %200
  %202 = bitcast float* %201 to <4 x float>*
  %wide.load417 = load <4 x float>, <4 x float>* %202, align 32, !tbaa !1109
  %203 = getelementptr inbounds float, float* %201, i64 4
  %204 = bitcast float* %203 to <4 x float>*
  %wide.load418 = load <4 x float>, <4 x float>* %204, align 16, !tbaa !1109
  %205 = fcmp olt <4 x float> %wide.load417, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %206 = fcmp olt <4 x float> %wide.load418, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %207 = select <4 x i1> %205, <4 x float> %wide.load417, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %208 = select <4 x i1> %206, <4 x float> %wide.load418, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %209 = fcmp ogt <4 x float> %207, zeroinitializer
  %210 = fcmp ogt <4 x float> %208, zeroinitializer
  %211 = select <4 x i1> %209, <4 x float> %207, <4 x float> zeroinitializer
  %212 = select <4 x i1> %210, <4 x float> %208, <4 x float> zeroinitializer
  %213 = getelementptr inbounds float, float* %128, i64 %200
  %214 = bitcast float* %213 to <4 x float>*
  store <4 x float> %211, <4 x float>* %214, align 32, !tbaa !1118
  %215 = getelementptr inbounds float, float* %213, i64 4
  %216 = bitcast float* %215 to <4 x float>*
  store <4 x float> %212, <4 x float>* %216, align 16, !tbaa !1118
  %index.next414 = add i64 %index413, 8
  %217 = icmp eq i64 %index.next414, 256
  br i1 %217, label %for_end44.2, label %vector.body411, !prof !341, !llvm.loop !1123

for_end44.2:                                      ; preds = %vector.body411
  %218 = add nuw nsw i64 %157, 768
  br label %vector.body401

vector.body401:                                   ; preds = %vector.body401, %for_end44.2
  %index403 = phi i64 [ 0, %for_end44.2 ], [ %index.next404, %vector.body401 ]
  %219 = add nuw nsw i64 %218, %index403
  %220 = getelementptr inbounds float, float* %21, i64 %219
  %221 = bitcast float* %220 to <4 x float>*
  %wide.load407 = load <4 x float>, <4 x float>* %221, align 32, !tbaa !1109
  %222 = getelementptr inbounds float, float* %220, i64 4
  %223 = bitcast float* %222 to <4 x float>*
  %wide.load408 = load <4 x float>, <4 x float>* %223, align 16, !tbaa !1109
  %224 = fcmp olt <4 x float> %wide.load407, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %225 = fcmp olt <4 x float> %wide.load408, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %226 = select <4 x i1> %224, <4 x float> %wide.load407, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %227 = select <4 x i1> %225, <4 x float> %wide.load408, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %228 = fcmp ogt <4 x float> %226, zeroinitializer
  %229 = fcmp ogt <4 x float> %227, zeroinitializer
  %230 = select <4 x i1> %228, <4 x float> %226, <4 x float> zeroinitializer
  %231 = select <4 x i1> %229, <4 x float> %227, <4 x float> zeroinitializer
  %232 = getelementptr inbounds float, float* %128, i64 %219
  %233 = bitcast float* %232 to <4 x float>*
  store <4 x float> %230, <4 x float>* %233, align 32, !tbaa !1118
  %234 = getelementptr inbounds float, float* %232, i64 4
  %235 = bitcast float* %234 to <4 x float>*
  store <4 x float> %231, <4 x float>* %235, align 16, !tbaa !1118
  %index.next404 = add i64 %index403, 8
  %236 = icmp eq i64 %index.next404, 256
  br i1 %236, label %for_end44.3, label %vector.body401, !prof !341, !llvm.loop !1124

for_end44.3:                                      ; preds = %vector.body401
  %237 = add nuw nsw i64 %157, 1024
  br label %vector.body391

vector.body391:                                   ; preds = %vector.body391, %for_end44.3
  %index393 = phi i64 [ 0, %for_end44.3 ], [ %index.next394, %vector.body391 ]
  %238 = add nuw nsw i64 %237, %index393
  %239 = getelementptr inbounds float, float* %21, i64 %238
  %240 = bitcast float* %239 to <4 x float>*
  %wide.load397 = load <4 x float>, <4 x float>* %240, align 32, !tbaa !1109
  %241 = getelementptr inbounds float, float* %239, i64 4
  %242 = bitcast float* %241 to <4 x float>*
  %wide.load398 = load <4 x float>, <4 x float>* %242, align 16, !tbaa !1109
  %243 = fcmp olt <4 x float> %wide.load397, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %244 = fcmp olt <4 x float> %wide.load398, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %245 = select <4 x i1> %243, <4 x float> %wide.load397, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %246 = select <4 x i1> %244, <4 x float> %wide.load398, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %247 = fcmp ogt <4 x float> %245, zeroinitializer
  %248 = fcmp ogt <4 x float> %246, zeroinitializer
  %249 = select <4 x i1> %247, <4 x float> %245, <4 x float> zeroinitializer
  %250 = select <4 x i1> %248, <4 x float> %246, <4 x float> zeroinitializer
  %251 = getelementptr inbounds float, float* %128, i64 %238
  %252 = bitcast float* %251 to <4 x float>*
  store <4 x float> %249, <4 x float>* %252, align 32, !tbaa !1118
  %253 = getelementptr inbounds float, float* %251, i64 4
  %254 = bitcast float* %253 to <4 x float>*
  store <4 x float> %250, <4 x float>* %254, align 16, !tbaa !1118
  %index.next394 = add i64 %index393, 8
  %255 = icmp eq i64 %index.next394, 256
  br i1 %255, label %for_end44.4, label %vector.body391, !prof !341, !llvm.loop !1125

for_end44.4:                                      ; preds = %vector.body391
  %256 = add nuw nsw i64 %157, 1280
  br label %vector.body381

vector.body381:                                   ; preds = %vector.body381, %for_end44.4
  %index383 = phi i64 [ 0, %for_end44.4 ], [ %index.next384, %vector.body381 ]
  %257 = add nuw nsw i64 %256, %index383
  %258 = getelementptr inbounds float, float* %21, i64 %257
  %259 = bitcast float* %258 to <4 x float>*
  %wide.load387 = load <4 x float>, <4 x float>* %259, align 32, !tbaa !1109
  %260 = getelementptr inbounds float, float* %258, i64 4
  %261 = bitcast float* %260 to <4 x float>*
  %wide.load388 = load <4 x float>, <4 x float>* %261, align 16, !tbaa !1109
  %262 = fcmp olt <4 x float> %wide.load387, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %263 = fcmp olt <4 x float> %wide.load388, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %264 = select <4 x i1> %262, <4 x float> %wide.load387, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %265 = select <4 x i1> %263, <4 x float> %wide.load388, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %266 = fcmp ogt <4 x float> %264, zeroinitializer
  %267 = fcmp ogt <4 x float> %265, zeroinitializer
  %268 = select <4 x i1> %266, <4 x float> %264, <4 x float> zeroinitializer
  %269 = select <4 x i1> %267, <4 x float> %265, <4 x float> zeroinitializer
  %270 = getelementptr inbounds float, float* %128, i64 %257
  %271 = bitcast float* %270 to <4 x float>*
  store <4 x float> %268, <4 x float>* %271, align 32, !tbaa !1118
  %272 = getelementptr inbounds float, float* %270, i64 4
  %273 = bitcast float* %272 to <4 x float>*
  store <4 x float> %269, <4 x float>* %273, align 16, !tbaa !1118
  %index.next384 = add i64 %index383, 8
  %274 = icmp eq i64 %index.next384, 256
  br i1 %274, label %for_end44.5, label %vector.body381, !prof !341, !llvm.loop !1126

for_end44.5:                                      ; preds = %vector.body381
  %275 = add nuw nsw i64 %157, 1536
  br label %vector.body371

vector.body371:                                   ; preds = %vector.body371, %for_end44.5
  %index373 = phi i64 [ 0, %for_end44.5 ], [ %index.next374, %vector.body371 ]
  %276 = add nuw nsw i64 %275, %index373
  %277 = getelementptr inbounds float, float* %21, i64 %276
  %278 = bitcast float* %277 to <4 x float>*
  %wide.load377 = load <4 x float>, <4 x float>* %278, align 32, !tbaa !1109
  %279 = getelementptr inbounds float, float* %277, i64 4
  %280 = bitcast float* %279 to <4 x float>*
  %wide.load378 = load <4 x float>, <4 x float>* %280, align 16, !tbaa !1109
  %281 = fcmp olt <4 x float> %wide.load377, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %282 = fcmp olt <4 x float> %wide.load378, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %283 = select <4 x i1> %281, <4 x float> %wide.load377, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %284 = select <4 x i1> %282, <4 x float> %wide.load378, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %285 = fcmp ogt <4 x float> %283, zeroinitializer
  %286 = fcmp ogt <4 x float> %284, zeroinitializer
  %287 = select <4 x i1> %285, <4 x float> %283, <4 x float> zeroinitializer
  %288 = select <4 x i1> %286, <4 x float> %284, <4 x float> zeroinitializer
  %289 = getelementptr inbounds float, float* %128, i64 %276
  %290 = bitcast float* %289 to <4 x float>*
  store <4 x float> %287, <4 x float>* %290, align 32, !tbaa !1118
  %291 = getelementptr inbounds float, float* %289, i64 4
  %292 = bitcast float* %291 to <4 x float>*
  store <4 x float> %288, <4 x float>* %292, align 16, !tbaa !1118
  %index.next374 = add i64 %index373, 8
  %293 = icmp eq i64 %index.next374, 256
  br i1 %293, label %for_end44.6, label %vector.body371, !prof !341, !llvm.loop !1127

for_end44.6:                                      ; preds = %vector.body371
  %294 = add nuw nsw i64 %157, 1792
  br label %vector.body361

vector.body361:                                   ; preds = %vector.body361, %for_end44.6
  %index363 = phi i64 [ 0, %for_end44.6 ], [ %index.next364, %vector.body361 ]
  %295 = add nuw nsw i64 %294, %index363
  %296 = getelementptr inbounds float, float* %21, i64 %295
  %297 = bitcast float* %296 to <4 x float>*
  %wide.load367 = load <4 x float>, <4 x float>* %297, align 32, !tbaa !1109
  %298 = getelementptr inbounds float, float* %296, i64 4
  %299 = bitcast float* %298 to <4 x float>*
  %wide.load368 = load <4 x float>, <4 x float>* %299, align 16, !tbaa !1109
  %300 = fcmp olt <4 x float> %wide.load367, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %301 = fcmp olt <4 x float> %wide.load368, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %302 = select <4 x i1> %300, <4 x float> %wide.load367, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %303 = select <4 x i1> %301, <4 x float> %wide.load368, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %304 = fcmp ogt <4 x float> %302, zeroinitializer
  %305 = fcmp ogt <4 x float> %303, zeroinitializer
  %306 = select <4 x i1> %304, <4 x float> %302, <4 x float> zeroinitializer
  %307 = select <4 x i1> %305, <4 x float> %303, <4 x float> zeroinitializer
  %308 = getelementptr inbounds float, float* %128, i64 %295
  %309 = bitcast float* %308 to <4 x float>*
  store <4 x float> %306, <4 x float>* %309, align 32, !tbaa !1118
  %310 = getelementptr inbounds float, float* %308, i64 4
  %311 = bitcast float* %310 to <4 x float>*
  store <4 x float> %307, <4 x float>* %311, align 16, !tbaa !1118
  %index.next364 = add i64 %index363, 8
  %312 = icmp eq i64 %index.next364, 256
  br i1 %312, label %for_end44.7, label %vector.body361, !prof !341, !llvm.loop !1128

for_end44.7:                                      ; preds = %vector.body361
  %313 = add nuw nsw i64 %157, 2048
  br label %vector.body351

vector.body351:                                   ; preds = %vector.body351, %for_end44.7
  %index353 = phi i64 [ 0, %for_end44.7 ], [ %index.next354, %vector.body351 ]
  %314 = add nuw nsw i64 %313, %index353
  %315 = getelementptr inbounds float, float* %21, i64 %314
  %316 = bitcast float* %315 to <4 x float>*
  %wide.load357 = load <4 x float>, <4 x float>* %316, align 32, !tbaa !1109
  %317 = getelementptr inbounds float, float* %315, i64 4
  %318 = bitcast float* %317 to <4 x float>*
  %wide.load358 = load <4 x float>, <4 x float>* %318, align 16, !tbaa !1109
  %319 = fcmp olt <4 x float> %wide.load357, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %320 = fcmp olt <4 x float> %wide.load358, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %321 = select <4 x i1> %319, <4 x float> %wide.load357, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %322 = select <4 x i1> %320, <4 x float> %wide.load358, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %323 = fcmp ogt <4 x float> %321, zeroinitializer
  %324 = fcmp ogt <4 x float> %322, zeroinitializer
  %325 = select <4 x i1> %323, <4 x float> %321, <4 x float> zeroinitializer
  %326 = select <4 x i1> %324, <4 x float> %322, <4 x float> zeroinitializer
  %327 = getelementptr inbounds float, float* %128, i64 %314
  %328 = bitcast float* %327 to <4 x float>*
  store <4 x float> %325, <4 x float>* %328, align 32, !tbaa !1118
  %329 = getelementptr inbounds float, float* %327, i64 4
  %330 = bitcast float* %329 to <4 x float>*
  store <4 x float> %326, <4 x float>* %330, align 16, !tbaa !1118
  %index.next354 = add i64 %index353, 8
  %331 = icmp eq i64 %index.next354, 256
  br i1 %331, label %for_end44.8, label %vector.body351, !prof !341, !llvm.loop !1129

for_end44.8:                                      ; preds = %vector.body351
  %332 = add nuw nsw i64 %157, 2304
  br label %vector.body341

vector.body341:                                   ; preds = %vector.body341, %for_end44.8
  %index343 = phi i64 [ 0, %for_end44.8 ], [ %index.next344, %vector.body341 ]
  %333 = add nuw nsw i64 %332, %index343
  %334 = getelementptr inbounds float, float* %21, i64 %333
  %335 = bitcast float* %334 to <4 x float>*
  %wide.load347 = load <4 x float>, <4 x float>* %335, align 32, !tbaa !1109
  %336 = getelementptr inbounds float, float* %334, i64 4
  %337 = bitcast float* %336 to <4 x float>*
  %wide.load348 = load <4 x float>, <4 x float>* %337, align 16, !tbaa !1109
  %338 = fcmp olt <4 x float> %wide.load347, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %339 = fcmp olt <4 x float> %wide.load348, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %340 = select <4 x i1> %338, <4 x float> %wide.load347, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %341 = select <4 x i1> %339, <4 x float> %wide.load348, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %342 = fcmp ogt <4 x float> %340, zeroinitializer
  %343 = fcmp ogt <4 x float> %341, zeroinitializer
  %344 = select <4 x i1> %342, <4 x float> %340, <4 x float> zeroinitializer
  %345 = select <4 x i1> %343, <4 x float> %341, <4 x float> zeroinitializer
  %346 = getelementptr inbounds float, float* %128, i64 %333
  %347 = bitcast float* %346 to <4 x float>*
  store <4 x float> %344, <4 x float>* %347, align 32, !tbaa !1118
  %348 = getelementptr inbounds float, float* %346, i64 4
  %349 = bitcast float* %348 to <4 x float>*
  store <4 x float> %345, <4 x float>* %349, align 16, !tbaa !1118
  %index.next344 = add i64 %index343, 8
  %350 = icmp eq i64 %index.next344, 256
  br i1 %350, label %for_end44.9, label %vector.body341, !prof !341, !llvm.loop !1130

for_end44.9:                                      ; preds = %vector.body341
  %351 = add nuw nsw i64 %157, 2560
  br label %vector.body331

vector.body331:                                   ; preds = %vector.body331, %for_end44.9
  %index333 = phi i64 [ 0, %for_end44.9 ], [ %index.next334, %vector.body331 ]
  %352 = add nuw nsw i64 %351, %index333
  %353 = getelementptr inbounds float, float* %21, i64 %352
  %354 = bitcast float* %353 to <4 x float>*
  %wide.load337 = load <4 x float>, <4 x float>* %354, align 32, !tbaa !1109
  %355 = getelementptr inbounds float, float* %353, i64 4
  %356 = bitcast float* %355 to <4 x float>*
  %wide.load338 = load <4 x float>, <4 x float>* %356, align 16, !tbaa !1109
  %357 = fcmp olt <4 x float> %wide.load337, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %358 = fcmp olt <4 x float> %wide.load338, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %359 = select <4 x i1> %357, <4 x float> %wide.load337, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %360 = select <4 x i1> %358, <4 x float> %wide.load338, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %361 = fcmp ogt <4 x float> %359, zeroinitializer
  %362 = fcmp ogt <4 x float> %360, zeroinitializer
  %363 = select <4 x i1> %361, <4 x float> %359, <4 x float> zeroinitializer
  %364 = select <4 x i1> %362, <4 x float> %360, <4 x float> zeroinitializer
  %365 = getelementptr inbounds float, float* %128, i64 %352
  %366 = bitcast float* %365 to <4 x float>*
  store <4 x float> %363, <4 x float>* %366, align 32, !tbaa !1118
  %367 = getelementptr inbounds float, float* %365, i64 4
  %368 = bitcast float* %367 to <4 x float>*
  store <4 x float> %364, <4 x float>* %368, align 16, !tbaa !1118
  %index.next334 = add i64 %index333, 8
  %369 = icmp eq i64 %index.next334, 256
  br i1 %369, label %for_end44.10, label %vector.body331, !prof !341, !llvm.loop !1131

for_end44.10:                                     ; preds = %vector.body331
  %370 = add nuw nsw i64 %157, 2816
  br label %vector.body321

vector.body321:                                   ; preds = %vector.body321, %for_end44.10
  %index323 = phi i64 [ 0, %for_end44.10 ], [ %index.next324, %vector.body321 ]
  %371 = add nuw nsw i64 %370, %index323
  %372 = getelementptr inbounds float, float* %21, i64 %371
  %373 = bitcast float* %372 to <4 x float>*
  %wide.load327 = load <4 x float>, <4 x float>* %373, align 32, !tbaa !1109
  %374 = getelementptr inbounds float, float* %372, i64 4
  %375 = bitcast float* %374 to <4 x float>*
  %wide.load328 = load <4 x float>, <4 x float>* %375, align 16, !tbaa !1109
  %376 = fcmp olt <4 x float> %wide.load327, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %377 = fcmp olt <4 x float> %wide.load328, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %378 = select <4 x i1> %376, <4 x float> %wide.load327, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %379 = select <4 x i1> %377, <4 x float> %wide.load328, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %380 = fcmp ogt <4 x float> %378, zeroinitializer
  %381 = fcmp ogt <4 x float> %379, zeroinitializer
  %382 = select <4 x i1> %380, <4 x float> %378, <4 x float> zeroinitializer
  %383 = select <4 x i1> %381, <4 x float> %379, <4 x float> zeroinitializer
  %384 = getelementptr inbounds float, float* %128, i64 %371
  %385 = bitcast float* %384 to <4 x float>*
  store <4 x float> %382, <4 x float>* %385, align 32, !tbaa !1118
  %386 = getelementptr inbounds float, float* %384, i64 4
  %387 = bitcast float* %386 to <4 x float>*
  store <4 x float> %383, <4 x float>* %387, align 16, !tbaa !1118
  %index.next324 = add i64 %index323, 8
  %388 = icmp eq i64 %index.next324, 256
  br i1 %388, label %for_end44.11, label %vector.body321, !prof !341, !llvm.loop !1132

for_end44.11:                                     ; preds = %vector.body321
  %389 = add nuw nsw i64 %157, 3072
  br label %vector.body311

vector.body311:                                   ; preds = %vector.body311, %for_end44.11
  %index313 = phi i64 [ 0, %for_end44.11 ], [ %index.next314, %vector.body311 ]
  %390 = add nuw nsw i64 %389, %index313
  %391 = getelementptr inbounds float, float* %21, i64 %390
  %392 = bitcast float* %391 to <4 x float>*
  %wide.load317 = load <4 x float>, <4 x float>* %392, align 32, !tbaa !1109
  %393 = getelementptr inbounds float, float* %391, i64 4
  %394 = bitcast float* %393 to <4 x float>*
  %wide.load318 = load <4 x float>, <4 x float>* %394, align 16, !tbaa !1109
  %395 = fcmp olt <4 x float> %wide.load317, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %396 = fcmp olt <4 x float> %wide.load318, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %397 = select <4 x i1> %395, <4 x float> %wide.load317, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %398 = select <4 x i1> %396, <4 x float> %wide.load318, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %399 = fcmp ogt <4 x float> %397, zeroinitializer
  %400 = fcmp ogt <4 x float> %398, zeroinitializer
  %401 = select <4 x i1> %399, <4 x float> %397, <4 x float> zeroinitializer
  %402 = select <4 x i1> %400, <4 x float> %398, <4 x float> zeroinitializer
  %403 = getelementptr inbounds float, float* %128, i64 %390
  %404 = bitcast float* %403 to <4 x float>*
  store <4 x float> %401, <4 x float>* %404, align 32, !tbaa !1118
  %405 = getelementptr inbounds float, float* %403, i64 4
  %406 = bitcast float* %405 to <4 x float>*
  store <4 x float> %402, <4 x float>* %406, align 16, !tbaa !1118
  %index.next314 = add i64 %index313, 8
  %407 = icmp eq i64 %index.next314, 256
  br i1 %407, label %for_end44.12, label %vector.body311, !prof !341, !llvm.loop !1133

for_end44.12:                                     ; preds = %vector.body311
  %408 = add nuw nsw i64 %157, 3328
  br label %vector.body301

vector.body301:                                   ; preds = %vector.body301, %for_end44.12
  %index303 = phi i64 [ 0, %for_end44.12 ], [ %index.next304, %vector.body301 ]
  %409 = add nuw nsw i64 %408, %index303
  %410 = getelementptr inbounds float, float* %21, i64 %409
  %411 = bitcast float* %410 to <4 x float>*
  %wide.load307 = load <4 x float>, <4 x float>* %411, align 32, !tbaa !1109
  %412 = getelementptr inbounds float, float* %410, i64 4
  %413 = bitcast float* %412 to <4 x float>*
  %wide.load308 = load <4 x float>, <4 x float>* %413, align 16, !tbaa !1109
  %414 = fcmp olt <4 x float> %wide.load307, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %415 = fcmp olt <4 x float> %wide.load308, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %416 = select <4 x i1> %414, <4 x float> %wide.load307, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %417 = select <4 x i1> %415, <4 x float> %wide.load308, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %418 = fcmp ogt <4 x float> %416, zeroinitializer
  %419 = fcmp ogt <4 x float> %417, zeroinitializer
  %420 = select <4 x i1> %418, <4 x float> %416, <4 x float> zeroinitializer
  %421 = select <4 x i1> %419, <4 x float> %417, <4 x float> zeroinitializer
  %422 = getelementptr inbounds float, float* %128, i64 %409
  %423 = bitcast float* %422 to <4 x float>*
  store <4 x float> %420, <4 x float>* %423, align 32, !tbaa !1118
  %424 = getelementptr inbounds float, float* %422, i64 4
  %425 = bitcast float* %424 to <4 x float>*
  store <4 x float> %421, <4 x float>* %425, align 16, !tbaa !1118
  %index.next304 = add i64 %index303, 8
  %426 = icmp eq i64 %index.next304, 256
  br i1 %426, label %for_end44.13, label %vector.body301, !prof !341, !llvm.loop !1134

for_end44.13:                                     ; preds = %vector.body301
  %indvars.iv.next71 = add nuw nsw i64 %indvars.iv70, 1
  %exitcond72.not = icmp eq i64 %indvars.iv.next71, 14
  br i1 %exitcond72.not, label %for_end38, label %for_begin39.preheader, !prof !51

for_end35.1:                                      ; preds = %vector.body277
  %427 = add nuw nsw i64 %100, 512
  br label %vector.body265

vector.body265:                                   ; preds = %vector.body265, %for_end35.1
  %index267 = phi i64 [ 0, %for_end35.1 ], [ %index.next268.1, %vector.body265 ]
  %428 = add nuw nsw i64 %427, %index267
  %429 = getelementptr inbounds float, float* %27, i64 %index267
  %430 = bitcast float* %429 to <4 x float>*
  %wide.load271 = load <4 x float>, <4 x float>* %430, align 64, !tbaa !1113
  %431 = getelementptr inbounds float, float* %429, i64 4
  %432 = bitcast float* %431 to <4 x float>*
  %wide.load272 = load <4 x float>, <4 x float>* %432, align 16, !tbaa !1113
  %433 = getelementptr inbounds float, float* %21, i64 %428
  %434 = bitcast float* %433 to <4 x float>*
  %wide.load273 = load <4 x float>, <4 x float>* %434, align 64, !tbaa !1109
  %435 = getelementptr inbounds float, float* %433, i64 4
  %436 = bitcast float* %435 to <4 x float>*
  %wide.load274 = load <4 x float>, <4 x float>* %436, align 16, !tbaa !1109
  %437 = fadd <4 x float> %wide.load271, %wide.load273
  %438 = fadd <4 x float> %wide.load272, %wide.load274
  %439 = bitcast float* %433 to <4 x float>*
  store <4 x float> %437, <4 x float>* %439, align 64, !tbaa !1109
  %440 = bitcast float* %435 to <4 x float>*
  store <4 x float> %438, <4 x float>* %440, align 16, !tbaa !1109
  %index.next268 = or i64 %index267, 8
  %441 = add nuw nsw i64 %427, %index.next268
  %442 = getelementptr inbounds float, float* %27, i64 %index.next268
  %443 = bitcast float* %442 to <4 x float>*
  %wide.load271.1 = load <4 x float>, <4 x float>* %443, align 32, !tbaa !1113
  %444 = getelementptr inbounds float, float* %442, i64 4
  %445 = bitcast float* %444 to <4 x float>*
  %wide.load272.1 = load <4 x float>, <4 x float>* %445, align 16, !tbaa !1113
  %446 = getelementptr inbounds float, float* %21, i64 %441
  %447 = bitcast float* %446 to <4 x float>*
  %wide.load273.1 = load <4 x float>, <4 x float>* %447, align 32, !tbaa !1109
  %448 = getelementptr inbounds float, float* %446, i64 4
  %449 = bitcast float* %448 to <4 x float>*
  %wide.load274.1 = load <4 x float>, <4 x float>* %449, align 16, !tbaa !1109
  %450 = fadd <4 x float> %wide.load271.1, %wide.load273.1
  %451 = fadd <4 x float> %wide.load272.1, %wide.load274.1
  %452 = bitcast float* %446 to <4 x float>*
  store <4 x float> %450, <4 x float>* %452, align 32, !tbaa !1109
  %453 = bitcast float* %448 to <4 x float>*
  store <4 x float> %451, <4 x float>* %453, align 16, !tbaa !1109
  %index.next268.1 = add nuw nsw i64 %index267, 16
  %454 = icmp eq i64 %index.next268.1, 256
  br i1 %454, label %for_end35.2, label %vector.body265, !prof !341, !llvm.loop !1135

for_end35.2:                                      ; preds = %vector.body265
  %455 = add nuw nsw i64 %100, 768
  br label %vector.body253

vector.body253:                                   ; preds = %vector.body253, %for_end35.2
  %index255 = phi i64 [ 0, %for_end35.2 ], [ %index.next256.1, %vector.body253 ]
  %456 = add nuw nsw i64 %455, %index255
  %457 = getelementptr inbounds float, float* %27, i64 %index255
  %458 = bitcast float* %457 to <4 x float>*
  %wide.load259 = load <4 x float>, <4 x float>* %458, align 64, !tbaa !1113
  %459 = getelementptr inbounds float, float* %457, i64 4
  %460 = bitcast float* %459 to <4 x float>*
  %wide.load260 = load <4 x float>, <4 x float>* %460, align 16, !tbaa !1113
  %461 = getelementptr inbounds float, float* %21, i64 %456
  %462 = bitcast float* %461 to <4 x float>*
  %wide.load261 = load <4 x float>, <4 x float>* %462, align 64, !tbaa !1109
  %463 = getelementptr inbounds float, float* %461, i64 4
  %464 = bitcast float* %463 to <4 x float>*
  %wide.load262 = load <4 x float>, <4 x float>* %464, align 16, !tbaa !1109
  %465 = fadd <4 x float> %wide.load259, %wide.load261
  %466 = fadd <4 x float> %wide.load260, %wide.load262
  %467 = bitcast float* %461 to <4 x float>*
  store <4 x float> %465, <4 x float>* %467, align 64, !tbaa !1109
  %468 = bitcast float* %463 to <4 x float>*
  store <4 x float> %466, <4 x float>* %468, align 16, !tbaa !1109
  %index.next256 = or i64 %index255, 8
  %469 = add nuw nsw i64 %455, %index.next256
  %470 = getelementptr inbounds float, float* %27, i64 %index.next256
  %471 = bitcast float* %470 to <4 x float>*
  %wide.load259.1 = load <4 x float>, <4 x float>* %471, align 32, !tbaa !1113
  %472 = getelementptr inbounds float, float* %470, i64 4
  %473 = bitcast float* %472 to <4 x float>*
  %wide.load260.1 = load <4 x float>, <4 x float>* %473, align 16, !tbaa !1113
  %474 = getelementptr inbounds float, float* %21, i64 %469
  %475 = bitcast float* %474 to <4 x float>*
  %wide.load261.1 = load <4 x float>, <4 x float>* %475, align 32, !tbaa !1109
  %476 = getelementptr inbounds float, float* %474, i64 4
  %477 = bitcast float* %476 to <4 x float>*
  %wide.load262.1 = load <4 x float>, <4 x float>* %477, align 16, !tbaa !1109
  %478 = fadd <4 x float> %wide.load259.1, %wide.load261.1
  %479 = fadd <4 x float> %wide.load260.1, %wide.load262.1
  %480 = bitcast float* %474 to <4 x float>*
  store <4 x float> %478, <4 x float>* %480, align 32, !tbaa !1109
  %481 = bitcast float* %476 to <4 x float>*
  store <4 x float> %479, <4 x float>* %481, align 16, !tbaa !1109
  %index.next256.1 = add nuw nsw i64 %index255, 16
  %482 = icmp eq i64 %index.next256.1, 256
  br i1 %482, label %for_end35.3, label %vector.body253, !prof !341, !llvm.loop !1136

for_end35.3:                                      ; preds = %vector.body253
  %483 = add nuw nsw i64 %100, 1024
  br label %vector.body241

vector.body241:                                   ; preds = %vector.body241, %for_end35.3
  %index243 = phi i64 [ 0, %for_end35.3 ], [ %index.next244.1, %vector.body241 ]
  %484 = add nuw nsw i64 %483, %index243
  %485 = getelementptr inbounds float, float* %27, i64 %index243
  %486 = bitcast float* %485 to <4 x float>*
  %wide.load247 = load <4 x float>, <4 x float>* %486, align 64, !tbaa !1113
  %487 = getelementptr inbounds float, float* %485, i64 4
  %488 = bitcast float* %487 to <4 x float>*
  %wide.load248 = load <4 x float>, <4 x float>* %488, align 16, !tbaa !1113
  %489 = getelementptr inbounds float, float* %21, i64 %484
  %490 = bitcast float* %489 to <4 x float>*
  %wide.load249 = load <4 x float>, <4 x float>* %490, align 64, !tbaa !1109
  %491 = getelementptr inbounds float, float* %489, i64 4
  %492 = bitcast float* %491 to <4 x float>*
  %wide.load250 = load <4 x float>, <4 x float>* %492, align 16, !tbaa !1109
  %493 = fadd <4 x float> %wide.load247, %wide.load249
  %494 = fadd <4 x float> %wide.load248, %wide.load250
  %495 = bitcast float* %489 to <4 x float>*
  store <4 x float> %493, <4 x float>* %495, align 64, !tbaa !1109
  %496 = bitcast float* %491 to <4 x float>*
  store <4 x float> %494, <4 x float>* %496, align 16, !tbaa !1109
  %index.next244 = or i64 %index243, 8
  %497 = add nuw nsw i64 %483, %index.next244
  %498 = getelementptr inbounds float, float* %27, i64 %index.next244
  %499 = bitcast float* %498 to <4 x float>*
  %wide.load247.1 = load <4 x float>, <4 x float>* %499, align 32, !tbaa !1113
  %500 = getelementptr inbounds float, float* %498, i64 4
  %501 = bitcast float* %500 to <4 x float>*
  %wide.load248.1 = load <4 x float>, <4 x float>* %501, align 16, !tbaa !1113
  %502 = getelementptr inbounds float, float* %21, i64 %497
  %503 = bitcast float* %502 to <4 x float>*
  %wide.load249.1 = load <4 x float>, <4 x float>* %503, align 32, !tbaa !1109
  %504 = getelementptr inbounds float, float* %502, i64 4
  %505 = bitcast float* %504 to <4 x float>*
  %wide.load250.1 = load <4 x float>, <4 x float>* %505, align 16, !tbaa !1109
  %506 = fadd <4 x float> %wide.load247.1, %wide.load249.1
  %507 = fadd <4 x float> %wide.load248.1, %wide.load250.1
  %508 = bitcast float* %502 to <4 x float>*
  store <4 x float> %506, <4 x float>* %508, align 32, !tbaa !1109
  %509 = bitcast float* %504 to <4 x float>*
  store <4 x float> %507, <4 x float>* %509, align 16, !tbaa !1109
  %index.next244.1 = add nuw nsw i64 %index243, 16
  %510 = icmp eq i64 %index.next244.1, 256
  br i1 %510, label %for_end35.4, label %vector.body241, !prof !341, !llvm.loop !1137

for_end35.4:                                      ; preds = %vector.body241
  %511 = add nuw nsw i64 %100, 1280
  br label %vector.body229

vector.body229:                                   ; preds = %vector.body229, %for_end35.4
  %index231 = phi i64 [ 0, %for_end35.4 ], [ %index.next232.1, %vector.body229 ]
  %512 = add nuw nsw i64 %511, %index231
  %513 = getelementptr inbounds float, float* %27, i64 %index231
  %514 = bitcast float* %513 to <4 x float>*
  %wide.load235 = load <4 x float>, <4 x float>* %514, align 64, !tbaa !1113
  %515 = getelementptr inbounds float, float* %513, i64 4
  %516 = bitcast float* %515 to <4 x float>*
  %wide.load236 = load <4 x float>, <4 x float>* %516, align 16, !tbaa !1113
  %517 = getelementptr inbounds float, float* %21, i64 %512
  %518 = bitcast float* %517 to <4 x float>*
  %wide.load237 = load <4 x float>, <4 x float>* %518, align 64, !tbaa !1109
  %519 = getelementptr inbounds float, float* %517, i64 4
  %520 = bitcast float* %519 to <4 x float>*
  %wide.load238 = load <4 x float>, <4 x float>* %520, align 16, !tbaa !1109
  %521 = fadd <4 x float> %wide.load235, %wide.load237
  %522 = fadd <4 x float> %wide.load236, %wide.load238
  %523 = bitcast float* %517 to <4 x float>*
  store <4 x float> %521, <4 x float>* %523, align 64, !tbaa !1109
  %524 = bitcast float* %519 to <4 x float>*
  store <4 x float> %522, <4 x float>* %524, align 16, !tbaa !1109
  %index.next232 = or i64 %index231, 8
  %525 = add nuw nsw i64 %511, %index.next232
  %526 = getelementptr inbounds float, float* %27, i64 %index.next232
  %527 = bitcast float* %526 to <4 x float>*
  %wide.load235.1 = load <4 x float>, <4 x float>* %527, align 32, !tbaa !1113
  %528 = getelementptr inbounds float, float* %526, i64 4
  %529 = bitcast float* %528 to <4 x float>*
  %wide.load236.1 = load <4 x float>, <4 x float>* %529, align 16, !tbaa !1113
  %530 = getelementptr inbounds float, float* %21, i64 %525
  %531 = bitcast float* %530 to <4 x float>*
  %wide.load237.1 = load <4 x float>, <4 x float>* %531, align 32, !tbaa !1109
  %532 = getelementptr inbounds float, float* %530, i64 4
  %533 = bitcast float* %532 to <4 x float>*
  %wide.load238.1 = load <4 x float>, <4 x float>* %533, align 16, !tbaa !1109
  %534 = fadd <4 x float> %wide.load235.1, %wide.load237.1
  %535 = fadd <4 x float> %wide.load236.1, %wide.load238.1
  %536 = bitcast float* %530 to <4 x float>*
  store <4 x float> %534, <4 x float>* %536, align 32, !tbaa !1109
  %537 = bitcast float* %532 to <4 x float>*
  store <4 x float> %535, <4 x float>* %537, align 16, !tbaa !1109
  %index.next232.1 = add nuw nsw i64 %index231, 16
  %538 = icmp eq i64 %index.next232.1, 256
  br i1 %538, label %for_end35.5, label %vector.body229, !prof !341, !llvm.loop !1138

for_end35.5:                                      ; preds = %vector.body229
  %539 = add nuw nsw i64 %100, 1536
  br label %vector.body217

vector.body217:                                   ; preds = %vector.body217, %for_end35.5
  %index219 = phi i64 [ 0, %for_end35.5 ], [ %index.next220.1, %vector.body217 ]
  %540 = add nuw nsw i64 %539, %index219
  %541 = getelementptr inbounds float, float* %27, i64 %index219
  %542 = bitcast float* %541 to <4 x float>*
  %wide.load223 = load <4 x float>, <4 x float>* %542, align 64, !tbaa !1113
  %543 = getelementptr inbounds float, float* %541, i64 4
  %544 = bitcast float* %543 to <4 x float>*
  %wide.load224 = load <4 x float>, <4 x float>* %544, align 16, !tbaa !1113
  %545 = getelementptr inbounds float, float* %21, i64 %540
  %546 = bitcast float* %545 to <4 x float>*
  %wide.load225 = load <4 x float>, <4 x float>* %546, align 64, !tbaa !1109
  %547 = getelementptr inbounds float, float* %545, i64 4
  %548 = bitcast float* %547 to <4 x float>*
  %wide.load226 = load <4 x float>, <4 x float>* %548, align 16, !tbaa !1109
  %549 = fadd <4 x float> %wide.load223, %wide.load225
  %550 = fadd <4 x float> %wide.load224, %wide.load226
  %551 = bitcast float* %545 to <4 x float>*
  store <4 x float> %549, <4 x float>* %551, align 64, !tbaa !1109
  %552 = bitcast float* %547 to <4 x float>*
  store <4 x float> %550, <4 x float>* %552, align 16, !tbaa !1109
  %index.next220 = or i64 %index219, 8
  %553 = add nuw nsw i64 %539, %index.next220
  %554 = getelementptr inbounds float, float* %27, i64 %index.next220
  %555 = bitcast float* %554 to <4 x float>*
  %wide.load223.1 = load <4 x float>, <4 x float>* %555, align 32, !tbaa !1113
  %556 = getelementptr inbounds float, float* %554, i64 4
  %557 = bitcast float* %556 to <4 x float>*
  %wide.load224.1 = load <4 x float>, <4 x float>* %557, align 16, !tbaa !1113
  %558 = getelementptr inbounds float, float* %21, i64 %553
  %559 = bitcast float* %558 to <4 x float>*
  %wide.load225.1 = load <4 x float>, <4 x float>* %559, align 32, !tbaa !1109
  %560 = getelementptr inbounds float, float* %558, i64 4
  %561 = bitcast float* %560 to <4 x float>*
  %wide.load226.1 = load <4 x float>, <4 x float>* %561, align 16, !tbaa !1109
  %562 = fadd <4 x float> %wide.load223.1, %wide.load225.1
  %563 = fadd <4 x float> %wide.load224.1, %wide.load226.1
  %564 = bitcast float* %558 to <4 x float>*
  store <4 x float> %562, <4 x float>* %564, align 32, !tbaa !1109
  %565 = bitcast float* %560 to <4 x float>*
  store <4 x float> %563, <4 x float>* %565, align 16, !tbaa !1109
  %index.next220.1 = add nuw nsw i64 %index219, 16
  %566 = icmp eq i64 %index.next220.1, 256
  br i1 %566, label %for_end35.6, label %vector.body217, !prof !341, !llvm.loop !1139

for_end35.6:                                      ; preds = %vector.body217
  %567 = add nuw nsw i64 %100, 1792
  br label %vector.body205

vector.body205:                                   ; preds = %vector.body205, %for_end35.6
  %index207 = phi i64 [ 0, %for_end35.6 ], [ %index.next208.1, %vector.body205 ]
  %568 = add nuw nsw i64 %567, %index207
  %569 = getelementptr inbounds float, float* %27, i64 %index207
  %570 = bitcast float* %569 to <4 x float>*
  %wide.load211 = load <4 x float>, <4 x float>* %570, align 64, !tbaa !1113
  %571 = getelementptr inbounds float, float* %569, i64 4
  %572 = bitcast float* %571 to <4 x float>*
  %wide.load212 = load <4 x float>, <4 x float>* %572, align 16, !tbaa !1113
  %573 = getelementptr inbounds float, float* %21, i64 %568
  %574 = bitcast float* %573 to <4 x float>*
  %wide.load213 = load <4 x float>, <4 x float>* %574, align 64, !tbaa !1109
  %575 = getelementptr inbounds float, float* %573, i64 4
  %576 = bitcast float* %575 to <4 x float>*
  %wide.load214 = load <4 x float>, <4 x float>* %576, align 16, !tbaa !1109
  %577 = fadd <4 x float> %wide.load211, %wide.load213
  %578 = fadd <4 x float> %wide.load212, %wide.load214
  %579 = bitcast float* %573 to <4 x float>*
  store <4 x float> %577, <4 x float>* %579, align 64, !tbaa !1109
  %580 = bitcast float* %575 to <4 x float>*
  store <4 x float> %578, <4 x float>* %580, align 16, !tbaa !1109
  %index.next208 = or i64 %index207, 8
  %581 = add nuw nsw i64 %567, %index.next208
  %582 = getelementptr inbounds float, float* %27, i64 %index.next208
  %583 = bitcast float* %582 to <4 x float>*
  %wide.load211.1 = load <4 x float>, <4 x float>* %583, align 32, !tbaa !1113
  %584 = getelementptr inbounds float, float* %582, i64 4
  %585 = bitcast float* %584 to <4 x float>*
  %wide.load212.1 = load <4 x float>, <4 x float>* %585, align 16, !tbaa !1113
  %586 = getelementptr inbounds float, float* %21, i64 %581
  %587 = bitcast float* %586 to <4 x float>*
  %wide.load213.1 = load <4 x float>, <4 x float>* %587, align 32, !tbaa !1109
  %588 = getelementptr inbounds float, float* %586, i64 4
  %589 = bitcast float* %588 to <4 x float>*
  %wide.load214.1 = load <4 x float>, <4 x float>* %589, align 16, !tbaa !1109
  %590 = fadd <4 x float> %wide.load211.1, %wide.load213.1
  %591 = fadd <4 x float> %wide.load212.1, %wide.load214.1
  %592 = bitcast float* %586 to <4 x float>*
  store <4 x float> %590, <4 x float>* %592, align 32, !tbaa !1109
  %593 = bitcast float* %588 to <4 x float>*
  store <4 x float> %591, <4 x float>* %593, align 16, !tbaa !1109
  %index.next208.1 = add nuw nsw i64 %index207, 16
  %594 = icmp eq i64 %index.next208.1, 256
  br i1 %594, label %for_end35.7, label %vector.body205, !prof !341, !llvm.loop !1140

for_end35.7:                                      ; preds = %vector.body205
  %595 = add nuw nsw i64 %100, 2048
  br label %vector.body193

vector.body193:                                   ; preds = %vector.body193, %for_end35.7
  %index195 = phi i64 [ 0, %for_end35.7 ], [ %index.next196.1, %vector.body193 ]
  %596 = add nuw nsw i64 %595, %index195
  %597 = getelementptr inbounds float, float* %27, i64 %index195
  %598 = bitcast float* %597 to <4 x float>*
  %wide.load199 = load <4 x float>, <4 x float>* %598, align 64, !tbaa !1113
  %599 = getelementptr inbounds float, float* %597, i64 4
  %600 = bitcast float* %599 to <4 x float>*
  %wide.load200 = load <4 x float>, <4 x float>* %600, align 16, !tbaa !1113
  %601 = getelementptr inbounds float, float* %21, i64 %596
  %602 = bitcast float* %601 to <4 x float>*
  %wide.load201 = load <4 x float>, <4 x float>* %602, align 64, !tbaa !1109
  %603 = getelementptr inbounds float, float* %601, i64 4
  %604 = bitcast float* %603 to <4 x float>*
  %wide.load202 = load <4 x float>, <4 x float>* %604, align 16, !tbaa !1109
  %605 = fadd <4 x float> %wide.load199, %wide.load201
  %606 = fadd <4 x float> %wide.load200, %wide.load202
  %607 = bitcast float* %601 to <4 x float>*
  store <4 x float> %605, <4 x float>* %607, align 64, !tbaa !1109
  %608 = bitcast float* %603 to <4 x float>*
  store <4 x float> %606, <4 x float>* %608, align 16, !tbaa !1109
  %index.next196 = or i64 %index195, 8
  %609 = add nuw nsw i64 %595, %index.next196
  %610 = getelementptr inbounds float, float* %27, i64 %index.next196
  %611 = bitcast float* %610 to <4 x float>*
  %wide.load199.1 = load <4 x float>, <4 x float>* %611, align 32, !tbaa !1113
  %612 = getelementptr inbounds float, float* %610, i64 4
  %613 = bitcast float* %612 to <4 x float>*
  %wide.load200.1 = load <4 x float>, <4 x float>* %613, align 16, !tbaa !1113
  %614 = getelementptr inbounds float, float* %21, i64 %609
  %615 = bitcast float* %614 to <4 x float>*
  %wide.load201.1 = load <4 x float>, <4 x float>* %615, align 32, !tbaa !1109
  %616 = getelementptr inbounds float, float* %614, i64 4
  %617 = bitcast float* %616 to <4 x float>*
  %wide.load202.1 = load <4 x float>, <4 x float>* %617, align 16, !tbaa !1109
  %618 = fadd <4 x float> %wide.load199.1, %wide.load201.1
  %619 = fadd <4 x float> %wide.load200.1, %wide.load202.1
  %620 = bitcast float* %614 to <4 x float>*
  store <4 x float> %618, <4 x float>* %620, align 32, !tbaa !1109
  %621 = bitcast float* %616 to <4 x float>*
  store <4 x float> %619, <4 x float>* %621, align 16, !tbaa !1109
  %index.next196.1 = add nuw nsw i64 %index195, 16
  %622 = icmp eq i64 %index.next196.1, 256
  br i1 %622, label %for_end35.8, label %vector.body193, !prof !341, !llvm.loop !1141

for_end35.8:                                      ; preds = %vector.body193
  %623 = add nuw nsw i64 %100, 2304
  br label %vector.body181

vector.body181:                                   ; preds = %vector.body181, %for_end35.8
  %index183 = phi i64 [ 0, %for_end35.8 ], [ %index.next184.1, %vector.body181 ]
  %624 = add nuw nsw i64 %623, %index183
  %625 = getelementptr inbounds float, float* %27, i64 %index183
  %626 = bitcast float* %625 to <4 x float>*
  %wide.load187 = load <4 x float>, <4 x float>* %626, align 64, !tbaa !1113
  %627 = getelementptr inbounds float, float* %625, i64 4
  %628 = bitcast float* %627 to <4 x float>*
  %wide.load188 = load <4 x float>, <4 x float>* %628, align 16, !tbaa !1113
  %629 = getelementptr inbounds float, float* %21, i64 %624
  %630 = bitcast float* %629 to <4 x float>*
  %wide.load189 = load <4 x float>, <4 x float>* %630, align 64, !tbaa !1109
  %631 = getelementptr inbounds float, float* %629, i64 4
  %632 = bitcast float* %631 to <4 x float>*
  %wide.load190 = load <4 x float>, <4 x float>* %632, align 16, !tbaa !1109
  %633 = fadd <4 x float> %wide.load187, %wide.load189
  %634 = fadd <4 x float> %wide.load188, %wide.load190
  %635 = bitcast float* %629 to <4 x float>*
  store <4 x float> %633, <4 x float>* %635, align 64, !tbaa !1109
  %636 = bitcast float* %631 to <4 x float>*
  store <4 x float> %634, <4 x float>* %636, align 16, !tbaa !1109
  %index.next184 = or i64 %index183, 8
  %637 = add nuw nsw i64 %623, %index.next184
  %638 = getelementptr inbounds float, float* %27, i64 %index.next184
  %639 = bitcast float* %638 to <4 x float>*
  %wide.load187.1 = load <4 x float>, <4 x float>* %639, align 32, !tbaa !1113
  %640 = getelementptr inbounds float, float* %638, i64 4
  %641 = bitcast float* %640 to <4 x float>*
  %wide.load188.1 = load <4 x float>, <4 x float>* %641, align 16, !tbaa !1113
  %642 = getelementptr inbounds float, float* %21, i64 %637
  %643 = bitcast float* %642 to <4 x float>*
  %wide.load189.1 = load <4 x float>, <4 x float>* %643, align 32, !tbaa !1109
  %644 = getelementptr inbounds float, float* %642, i64 4
  %645 = bitcast float* %644 to <4 x float>*
  %wide.load190.1 = load <4 x float>, <4 x float>* %645, align 16, !tbaa !1109
  %646 = fadd <4 x float> %wide.load187.1, %wide.load189.1
  %647 = fadd <4 x float> %wide.load188.1, %wide.load190.1
  %648 = bitcast float* %642 to <4 x float>*
  store <4 x float> %646, <4 x float>* %648, align 32, !tbaa !1109
  %649 = bitcast float* %644 to <4 x float>*
  store <4 x float> %647, <4 x float>* %649, align 16, !tbaa !1109
  %index.next184.1 = add nuw nsw i64 %index183, 16
  %650 = icmp eq i64 %index.next184.1, 256
  br i1 %650, label %for_end35.9, label %vector.body181, !prof !341, !llvm.loop !1142

for_end35.9:                                      ; preds = %vector.body181
  %651 = add nuw nsw i64 %100, 2560
  br label %vector.body169

vector.body169:                                   ; preds = %vector.body169, %for_end35.9
  %index171 = phi i64 [ 0, %for_end35.9 ], [ %index.next172.1, %vector.body169 ]
  %652 = add nuw nsw i64 %651, %index171
  %653 = getelementptr inbounds float, float* %27, i64 %index171
  %654 = bitcast float* %653 to <4 x float>*
  %wide.load175 = load <4 x float>, <4 x float>* %654, align 64, !tbaa !1113
  %655 = getelementptr inbounds float, float* %653, i64 4
  %656 = bitcast float* %655 to <4 x float>*
  %wide.load176 = load <4 x float>, <4 x float>* %656, align 16, !tbaa !1113
  %657 = getelementptr inbounds float, float* %21, i64 %652
  %658 = bitcast float* %657 to <4 x float>*
  %wide.load177 = load <4 x float>, <4 x float>* %658, align 64, !tbaa !1109
  %659 = getelementptr inbounds float, float* %657, i64 4
  %660 = bitcast float* %659 to <4 x float>*
  %wide.load178 = load <4 x float>, <4 x float>* %660, align 16, !tbaa !1109
  %661 = fadd <4 x float> %wide.load175, %wide.load177
  %662 = fadd <4 x float> %wide.load176, %wide.load178
  %663 = bitcast float* %657 to <4 x float>*
  store <4 x float> %661, <4 x float>* %663, align 64, !tbaa !1109
  %664 = bitcast float* %659 to <4 x float>*
  store <4 x float> %662, <4 x float>* %664, align 16, !tbaa !1109
  %index.next172 = or i64 %index171, 8
  %665 = add nuw nsw i64 %651, %index.next172
  %666 = getelementptr inbounds float, float* %27, i64 %index.next172
  %667 = bitcast float* %666 to <4 x float>*
  %wide.load175.1 = load <4 x float>, <4 x float>* %667, align 32, !tbaa !1113
  %668 = getelementptr inbounds float, float* %666, i64 4
  %669 = bitcast float* %668 to <4 x float>*
  %wide.load176.1 = load <4 x float>, <4 x float>* %669, align 16, !tbaa !1113
  %670 = getelementptr inbounds float, float* %21, i64 %665
  %671 = bitcast float* %670 to <4 x float>*
  %wide.load177.1 = load <4 x float>, <4 x float>* %671, align 32, !tbaa !1109
  %672 = getelementptr inbounds float, float* %670, i64 4
  %673 = bitcast float* %672 to <4 x float>*
  %wide.load178.1 = load <4 x float>, <4 x float>* %673, align 16, !tbaa !1109
  %674 = fadd <4 x float> %wide.load175.1, %wide.load177.1
  %675 = fadd <4 x float> %wide.load176.1, %wide.load178.1
  %676 = bitcast float* %670 to <4 x float>*
  store <4 x float> %674, <4 x float>* %676, align 32, !tbaa !1109
  %677 = bitcast float* %672 to <4 x float>*
  store <4 x float> %675, <4 x float>* %677, align 16, !tbaa !1109
  %index.next172.1 = add nuw nsw i64 %index171, 16
  %678 = icmp eq i64 %index.next172.1, 256
  br i1 %678, label %for_end35.10, label %vector.body169, !prof !341, !llvm.loop !1143

for_end35.10:                                     ; preds = %vector.body169
  %679 = add nuw nsw i64 %100, 2816
  br label %vector.body157

vector.body157:                                   ; preds = %vector.body157, %for_end35.10
  %index159 = phi i64 [ 0, %for_end35.10 ], [ %index.next160.1, %vector.body157 ]
  %680 = add nuw nsw i64 %679, %index159
  %681 = getelementptr inbounds float, float* %27, i64 %index159
  %682 = bitcast float* %681 to <4 x float>*
  %wide.load163 = load <4 x float>, <4 x float>* %682, align 64, !tbaa !1113
  %683 = getelementptr inbounds float, float* %681, i64 4
  %684 = bitcast float* %683 to <4 x float>*
  %wide.load164 = load <4 x float>, <4 x float>* %684, align 16, !tbaa !1113
  %685 = getelementptr inbounds float, float* %21, i64 %680
  %686 = bitcast float* %685 to <4 x float>*
  %wide.load165 = load <4 x float>, <4 x float>* %686, align 64, !tbaa !1109
  %687 = getelementptr inbounds float, float* %685, i64 4
  %688 = bitcast float* %687 to <4 x float>*
  %wide.load166 = load <4 x float>, <4 x float>* %688, align 16, !tbaa !1109
  %689 = fadd <4 x float> %wide.load163, %wide.load165
  %690 = fadd <4 x float> %wide.load164, %wide.load166
  %691 = bitcast float* %685 to <4 x float>*
  store <4 x float> %689, <4 x float>* %691, align 64, !tbaa !1109
  %692 = bitcast float* %687 to <4 x float>*
  store <4 x float> %690, <4 x float>* %692, align 16, !tbaa !1109
  %index.next160 = or i64 %index159, 8
  %693 = add nuw nsw i64 %679, %index.next160
  %694 = getelementptr inbounds float, float* %27, i64 %index.next160
  %695 = bitcast float* %694 to <4 x float>*
  %wide.load163.1 = load <4 x float>, <4 x float>* %695, align 32, !tbaa !1113
  %696 = getelementptr inbounds float, float* %694, i64 4
  %697 = bitcast float* %696 to <4 x float>*
  %wide.load164.1 = load <4 x float>, <4 x float>* %697, align 16, !tbaa !1113
  %698 = getelementptr inbounds float, float* %21, i64 %693
  %699 = bitcast float* %698 to <4 x float>*
  %wide.load165.1 = load <4 x float>, <4 x float>* %699, align 32, !tbaa !1109
  %700 = getelementptr inbounds float, float* %698, i64 4
  %701 = bitcast float* %700 to <4 x float>*
  %wide.load166.1 = load <4 x float>, <4 x float>* %701, align 16, !tbaa !1109
  %702 = fadd <4 x float> %wide.load163.1, %wide.load165.1
  %703 = fadd <4 x float> %wide.load164.1, %wide.load166.1
  %704 = bitcast float* %698 to <4 x float>*
  store <4 x float> %702, <4 x float>* %704, align 32, !tbaa !1109
  %705 = bitcast float* %700 to <4 x float>*
  store <4 x float> %703, <4 x float>* %705, align 16, !tbaa !1109
  %index.next160.1 = add nuw nsw i64 %index159, 16
  %706 = icmp eq i64 %index.next160.1, 256
  br i1 %706, label %for_end35.11, label %vector.body157, !prof !341, !llvm.loop !1144

for_end35.11:                                     ; preds = %vector.body157
  %707 = add nuw nsw i64 %100, 3072
  br label %vector.body145

vector.body145:                                   ; preds = %vector.body145, %for_end35.11
  %index147 = phi i64 [ 0, %for_end35.11 ], [ %index.next148.1, %vector.body145 ]
  %708 = add nuw nsw i64 %707, %index147
  %709 = getelementptr inbounds float, float* %27, i64 %index147
  %710 = bitcast float* %709 to <4 x float>*
  %wide.load151 = load <4 x float>, <4 x float>* %710, align 64, !tbaa !1113
  %711 = getelementptr inbounds float, float* %709, i64 4
  %712 = bitcast float* %711 to <4 x float>*
  %wide.load152 = load <4 x float>, <4 x float>* %712, align 16, !tbaa !1113
  %713 = getelementptr inbounds float, float* %21, i64 %708
  %714 = bitcast float* %713 to <4 x float>*
  %wide.load153 = load <4 x float>, <4 x float>* %714, align 64, !tbaa !1109
  %715 = getelementptr inbounds float, float* %713, i64 4
  %716 = bitcast float* %715 to <4 x float>*
  %wide.load154 = load <4 x float>, <4 x float>* %716, align 16, !tbaa !1109
  %717 = fadd <4 x float> %wide.load151, %wide.load153
  %718 = fadd <4 x float> %wide.load152, %wide.load154
  %719 = bitcast float* %713 to <4 x float>*
  store <4 x float> %717, <4 x float>* %719, align 64, !tbaa !1109
  %720 = bitcast float* %715 to <4 x float>*
  store <4 x float> %718, <4 x float>* %720, align 16, !tbaa !1109
  %index.next148 = or i64 %index147, 8
  %721 = add nuw nsw i64 %707, %index.next148
  %722 = getelementptr inbounds float, float* %27, i64 %index.next148
  %723 = bitcast float* %722 to <4 x float>*
  %wide.load151.1 = load <4 x float>, <4 x float>* %723, align 32, !tbaa !1113
  %724 = getelementptr inbounds float, float* %722, i64 4
  %725 = bitcast float* %724 to <4 x float>*
  %wide.load152.1 = load <4 x float>, <4 x float>* %725, align 16, !tbaa !1113
  %726 = getelementptr inbounds float, float* %21, i64 %721
  %727 = bitcast float* %726 to <4 x float>*
  %wide.load153.1 = load <4 x float>, <4 x float>* %727, align 32, !tbaa !1109
  %728 = getelementptr inbounds float, float* %726, i64 4
  %729 = bitcast float* %728 to <4 x float>*
  %wide.load154.1 = load <4 x float>, <4 x float>* %729, align 16, !tbaa !1109
  %730 = fadd <4 x float> %wide.load151.1, %wide.load153.1
  %731 = fadd <4 x float> %wide.load152.1, %wide.load154.1
  %732 = bitcast float* %726 to <4 x float>*
  store <4 x float> %730, <4 x float>* %732, align 32, !tbaa !1109
  %733 = bitcast float* %728 to <4 x float>*
  store <4 x float> %731, <4 x float>* %733, align 16, !tbaa !1109
  %index.next148.1 = add nuw nsw i64 %index147, 16
  %734 = icmp eq i64 %index.next148.1, 256
  br i1 %734, label %for_end35.12, label %vector.body145, !prof !341, !llvm.loop !1145

for_end35.12:                                     ; preds = %vector.body145
  %735 = add nuw nsw i64 %100, 3328
  br label %vector.body133

vector.body133:                                   ; preds = %vector.body133, %for_end35.12
  %index135 = phi i64 [ 0, %for_end35.12 ], [ %index.next136.1, %vector.body133 ]
  %736 = add nuw nsw i64 %735, %index135
  %737 = getelementptr inbounds float, float* %27, i64 %index135
  %738 = bitcast float* %737 to <4 x float>*
  %wide.load139 = load <4 x float>, <4 x float>* %738, align 64, !tbaa !1113
  %739 = getelementptr inbounds float, float* %737, i64 4
  %740 = bitcast float* %739 to <4 x float>*
  %wide.load140 = load <4 x float>, <4 x float>* %740, align 16, !tbaa !1113
  %741 = getelementptr inbounds float, float* %21, i64 %736
  %742 = bitcast float* %741 to <4 x float>*
  %wide.load141 = load <4 x float>, <4 x float>* %742, align 64, !tbaa !1109
  %743 = getelementptr inbounds float, float* %741, i64 4
  %744 = bitcast float* %743 to <4 x float>*
  %wide.load142 = load <4 x float>, <4 x float>* %744, align 16, !tbaa !1109
  %745 = fadd <4 x float> %wide.load139, %wide.load141
  %746 = fadd <4 x float> %wide.load140, %wide.load142
  %747 = bitcast float* %741 to <4 x float>*
  store <4 x float> %745, <4 x float>* %747, align 64, !tbaa !1109
  %748 = bitcast float* %743 to <4 x float>*
  store <4 x float> %746, <4 x float>* %748, align 16, !tbaa !1109
  %index.next136 = or i64 %index135, 8
  %749 = add nuw nsw i64 %735, %index.next136
  %750 = getelementptr inbounds float, float* %27, i64 %index.next136
  %751 = bitcast float* %750 to <4 x float>*
  %wide.load139.1 = load <4 x float>, <4 x float>* %751, align 32, !tbaa !1113
  %752 = getelementptr inbounds float, float* %750, i64 4
  %753 = bitcast float* %752 to <4 x float>*
  %wide.load140.1 = load <4 x float>, <4 x float>* %753, align 16, !tbaa !1113
  %754 = getelementptr inbounds float, float* %21, i64 %749
  %755 = bitcast float* %754 to <4 x float>*
  %wide.load141.1 = load <4 x float>, <4 x float>* %755, align 32, !tbaa !1109
  %756 = getelementptr inbounds float, float* %754, i64 4
  %757 = bitcast float* %756 to <4 x float>*
  %wide.load142.1 = load <4 x float>, <4 x float>* %757, align 16, !tbaa !1109
  %758 = fadd <4 x float> %wide.load139.1, %wide.load141.1
  %759 = fadd <4 x float> %wide.load140.1, %wide.load142.1
  %760 = bitcast float* %754 to <4 x float>*
  store <4 x float> %758, <4 x float>* %760, align 32, !tbaa !1109
  %761 = bitcast float* %756 to <4 x float>*
  store <4 x float> %759, <4 x float>* %761, align 16, !tbaa !1109
  %index.next136.1 = add nuw nsw i64 %index135, 16
  %762 = icmp eq i64 %index.next136.1, 256
  br i1 %762, label %for_end35.13, label %vector.body133, !prof !341, !llvm.loop !1146

for_end35.13:                                     ; preds = %vector.body133
  %indvars.iv.next80 = add nuw nsw i64 %indvars.iv79, 1
  %exitcond81.not = icmp eq i64 %indvars.iv.next80, 14
  br i1 %exitcond81.not, label %for_begin36.preheader, label %for_begin30.preheader, !prof !51
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.118, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !1147
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !1161
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !1163
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !1166
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([160 x i8], [160 x i8]* @.str.119, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([160 x i8], [160 x i8]* @.str.120, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([160 x i8], [160 x i8]* @.str.121, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([160 x i8], [160 x i8]* @.str.122, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !1168
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !1182
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 7
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.123, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !1184
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 7
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.124, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !1187
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 1024
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.125, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !1189
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 50176, i32 7168, i32 1024, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.126, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !1201
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 1
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.16, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !1215
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 1
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.17, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !1217
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 1024
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.127, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !1220
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 1024
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.128, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !1222
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 1048576, i32 1048576, i32 1024, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([207 x i8], [207 x i8]* @.str.129, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !1234
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 1024
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.130, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !1248
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !1262
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !1276
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 7
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.131, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !1278
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 7
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.132, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !1281
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 1024
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.133, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !1283
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 50176, i32 7168, i32 1024, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.134, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_compute_(i8* %31, i8* %39, i8* %51, i8* %45, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_compute_(i8* noalias align 128 %0, i8* noalias align 128 %1, i8* noalias align 128 %2, i8* noalias align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 200704, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %call_end, %if_end, %entry
  %merge = phi i32 [ -1, %entry ], [ %13, %if_end ], [ %22, %call_end ]
  ret i32 %merge

if_end:                                           ; preds = %entry
  %8 = alloca %8, align 8
  %9 = getelementptr inbounds %8, %8* %8, i64 0, i32 0
  store i8* %6, i8** %9, align 8
  %10 = getelementptr inbounds %8, %8* %8, i64 0, i32 1
  store i8* %0, i8** %10, align 8
  %11 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %12 = bitcast %8* %8 to i8*
  %13 = call i32 %11(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.135, i8* nonnull %12, i32 0)
  %14 = icmp eq i32 %13, 0
  br i1 %14, label %call_end, label %if_then, !prof !5

call_end:                                         ; preds = %if_end
  %15 = alloca %9, align 8
  %16 = getelementptr inbounds %9, %9* %15, i64 0, i32 0
  store i8* %6, i8** %16, align 8
  %17 = getelementptr inbounds %9, %9* %15, i64 0, i32 1
  store i8* %1, i8** %17, align 8
  %18 = getelementptr inbounds %9, %9* %15, i64 0, i32 2
  store i8* %2, i8** %18, align 8
  %19 = getelementptr inbounds %9, %9* %15, i64 0, i32 3
  store i8* %3, i8** %19, align 8
  %20 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %21 = bitcast %9* %15 to i8*
  %22 = call i32 %20(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.136, i8* nonnull %21, i32 0)
  %23 = icmp eq i32 %22, 0
  br i1 %23, label %call_end2, label %if_then, !prof !5

call_end2:                                        ; preds = %call_end
  %24 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %25 = call i32 %24(i32 1, i32 %4, i8* nonnull %6)
  %.not = icmp ne i32 %25, 0
  %spec.select = sext i1 %.not to i32
  ret i32 %spec.select
}

; Function Attrs: nofree norecurse nounwind
define private i32 @__tvm_parallel_lambda.135(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #2 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %9 = load i32, i32* %8, align 4
  %10 = add nsw i32 %9, 6
  %11 = sdiv i32 %10, %9
  %12 = add nsw i32 %0, 1
  %13 = mul nsw i32 %11, %12
  %14 = icmp slt i32 %13, 7
  %15 = select i1 %14, i32 %13, i32 7
  %16 = mul nsw i32 %11, %0
  %17 = icmp slt i32 %16, 7
  %18 = select i1 %17, i32 %16, i32 7
  %19 = icmp slt i32 %18, %15
  br i1 %19, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %20 = sext i32 %18 to i64
  %wide.trip.count = sext i32 %15 to i64
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end6.6
  %indvars.iv13 = phi i64 [ %20, %for_begin1.preheader.preheader ], [ %indvars.iv.next14, %for_end6.6 ]
  %21 = mul nsw i64 %indvars.iv13, 7168
  br label %vector.body69

vector.body69:                                    ; preds = %vector.body69, %for_begin1.preheader
  %index71 = phi i64 [ 0, %for_begin1.preheader ], [ %index.next72.3, %vector.body69 ]
  %22 = add nsw i64 %21, %index71
  %23 = getelementptr inbounds float, float* %7, i64 %22
  %24 = bitcast float* %23 to <4 x float>*
  %wide.load75 = load <4 x float>, <4 x float>* %24, align 4, !tbaa !1295
  %25 = getelementptr inbounds float, float* %23, i64 4
  %26 = bitcast float* %25 to <4 x float>*
  %wide.load76 = load <4 x float>, <4 x float>* %26, align 4, !tbaa !1295
  %27 = getelementptr inbounds float, float* %4, i64 %22
  %28 = bitcast float* %27 to <4 x float>*
  store <4 x float> %wide.load75, <4 x float>* %28, align 4, !tbaa !1298
  %29 = getelementptr inbounds float, float* %27, i64 4
  %30 = bitcast float* %29 to <4 x float>*
  store <4 x float> %wide.load76, <4 x float>* %30, align 4, !tbaa !1298
  %index.next72 = or i64 %index71, 8
  %31 = add nsw i64 %21, %index.next72
  %32 = getelementptr inbounds float, float* %7, i64 %31
  %33 = bitcast float* %32 to <4 x float>*
  %wide.load75.1 = load <4 x float>, <4 x float>* %33, align 4, !tbaa !1295
  %34 = getelementptr inbounds float, float* %32, i64 4
  %35 = bitcast float* %34 to <4 x float>*
  %wide.load76.1 = load <4 x float>, <4 x float>* %35, align 4, !tbaa !1295
  %36 = getelementptr inbounds float, float* %4, i64 %31
  %37 = bitcast float* %36 to <4 x float>*
  store <4 x float> %wide.load75.1, <4 x float>* %37, align 4, !tbaa !1298
  %38 = getelementptr inbounds float, float* %36, i64 4
  %39 = bitcast float* %38 to <4 x float>*
  store <4 x float> %wide.load76.1, <4 x float>* %39, align 4, !tbaa !1298
  %index.next72.1 = or i64 %index71, 16
  %40 = add nsw i64 %21, %index.next72.1
  %41 = getelementptr inbounds float, float* %7, i64 %40
  %42 = bitcast float* %41 to <4 x float>*
  %wide.load75.2 = load <4 x float>, <4 x float>* %42, align 4, !tbaa !1295
  %43 = getelementptr inbounds float, float* %41, i64 4
  %44 = bitcast float* %43 to <4 x float>*
  %wide.load76.2 = load <4 x float>, <4 x float>* %44, align 4, !tbaa !1295
  %45 = getelementptr inbounds float, float* %4, i64 %40
  %46 = bitcast float* %45 to <4 x float>*
  store <4 x float> %wide.load75.2, <4 x float>* %46, align 4, !tbaa !1298
  %47 = getelementptr inbounds float, float* %45, i64 4
  %48 = bitcast float* %47 to <4 x float>*
  store <4 x float> %wide.load76.2, <4 x float>* %48, align 4, !tbaa !1298
  %index.next72.2 = or i64 %index71, 24
  %49 = add nsw i64 %21, %index.next72.2
  %50 = getelementptr inbounds float, float* %7, i64 %49
  %51 = bitcast float* %50 to <4 x float>*
  %wide.load75.3 = load <4 x float>, <4 x float>* %51, align 4, !tbaa !1295
  %52 = getelementptr inbounds float, float* %50, i64 4
  %53 = bitcast float* %52 to <4 x float>*
  %wide.load76.3 = load <4 x float>, <4 x float>* %53, align 4, !tbaa !1295
  %54 = getelementptr inbounds float, float* %4, i64 %49
  %55 = bitcast float* %54 to <4 x float>*
  store <4 x float> %wide.load75.3, <4 x float>* %55, align 4, !tbaa !1298
  %56 = getelementptr inbounds float, float* %54, i64 4
  %57 = bitcast float* %56 to <4 x float>*
  store <4 x float> %wide.load76.3, <4 x float>* %57, align 4, !tbaa !1298
  %index.next72.3 = add nuw nsw i64 %index71, 32
  %58 = icmp eq i64 %index.next72.3, 1024
  br i1 %58, label %for_end6, label %vector.body69, !prof !341, !llvm.loop !1301

for_end:                                          ; preds = %for_end6.6, %entry
  ret i32 0

for_end6:                                         ; preds = %vector.body69
  %59 = add nsw i64 %21, 1024
  br label %vector.body59

vector.body59:                                    ; preds = %vector.body59, %for_end6
  %index61 = phi i64 [ 0, %for_end6 ], [ %index.next62.3, %vector.body59 ]
  %60 = add nsw i64 %59, %index61
  %61 = getelementptr inbounds float, float* %7, i64 %60
  %62 = bitcast float* %61 to <4 x float>*
  %wide.load65 = load <4 x float>, <4 x float>* %62, align 4, !tbaa !1295
  %63 = getelementptr inbounds float, float* %61, i64 4
  %64 = bitcast float* %63 to <4 x float>*
  %wide.load66 = load <4 x float>, <4 x float>* %64, align 4, !tbaa !1295
  %65 = getelementptr inbounds float, float* %4, i64 %60
  %66 = bitcast float* %65 to <4 x float>*
  store <4 x float> %wide.load65, <4 x float>* %66, align 4, !tbaa !1298
  %67 = getelementptr inbounds float, float* %65, i64 4
  %68 = bitcast float* %67 to <4 x float>*
  store <4 x float> %wide.load66, <4 x float>* %68, align 4, !tbaa !1298
  %index.next62 = or i64 %index61, 8
  %69 = add nsw i64 %59, %index.next62
  %70 = getelementptr inbounds float, float* %7, i64 %69
  %71 = bitcast float* %70 to <4 x float>*
  %wide.load65.1 = load <4 x float>, <4 x float>* %71, align 4, !tbaa !1295
  %72 = getelementptr inbounds float, float* %70, i64 4
  %73 = bitcast float* %72 to <4 x float>*
  %wide.load66.1 = load <4 x float>, <4 x float>* %73, align 4, !tbaa !1295
  %74 = getelementptr inbounds float, float* %4, i64 %69
  %75 = bitcast float* %74 to <4 x float>*
  store <4 x float> %wide.load65.1, <4 x float>* %75, align 4, !tbaa !1298
  %76 = getelementptr inbounds float, float* %74, i64 4
  %77 = bitcast float* %76 to <4 x float>*
  store <4 x float> %wide.load66.1, <4 x float>* %77, align 4, !tbaa !1298
  %index.next62.1 = or i64 %index61, 16
  %78 = add nsw i64 %59, %index.next62.1
  %79 = getelementptr inbounds float, float* %7, i64 %78
  %80 = bitcast float* %79 to <4 x float>*
  %wide.load65.2 = load <4 x float>, <4 x float>* %80, align 4, !tbaa !1295
  %81 = getelementptr inbounds float, float* %79, i64 4
  %82 = bitcast float* %81 to <4 x float>*
  %wide.load66.2 = load <4 x float>, <4 x float>* %82, align 4, !tbaa !1295
  %83 = getelementptr inbounds float, float* %4, i64 %78
  %84 = bitcast float* %83 to <4 x float>*
  store <4 x float> %wide.load65.2, <4 x float>* %84, align 4, !tbaa !1298
  %85 = getelementptr inbounds float, float* %83, i64 4
  %86 = bitcast float* %85 to <4 x float>*
  store <4 x float> %wide.load66.2, <4 x float>* %86, align 4, !tbaa !1298
  %index.next62.2 = or i64 %index61, 24
  %87 = add nsw i64 %59, %index.next62.2
  %88 = getelementptr inbounds float, float* %7, i64 %87
  %89 = bitcast float* %88 to <4 x float>*
  %wide.load65.3 = load <4 x float>, <4 x float>* %89, align 4, !tbaa !1295
  %90 = getelementptr inbounds float, float* %88, i64 4
  %91 = bitcast float* %90 to <4 x float>*
  %wide.load66.3 = load <4 x float>, <4 x float>* %91, align 4, !tbaa !1295
  %92 = getelementptr inbounds float, float* %4, i64 %87
  %93 = bitcast float* %92 to <4 x float>*
  store <4 x float> %wide.load65.3, <4 x float>* %93, align 4, !tbaa !1298
  %94 = getelementptr inbounds float, float* %92, i64 4
  %95 = bitcast float* %94 to <4 x float>*
  store <4 x float> %wide.load66.3, <4 x float>* %95, align 4, !tbaa !1298
  %index.next62.3 = add nuw nsw i64 %index61, 32
  %96 = icmp eq i64 %index.next62.3, 1024
  br i1 %96, label %for_end6.1, label %vector.body59, !prof !341, !llvm.loop !1302

for_end6.1:                                       ; preds = %vector.body59
  %97 = add nsw i64 %21, 2048
  br label %vector.body49

vector.body49:                                    ; preds = %vector.body49, %for_end6.1
  %index51 = phi i64 [ 0, %for_end6.1 ], [ %index.next52.3, %vector.body49 ]
  %98 = add nsw i64 %97, %index51
  %99 = getelementptr inbounds float, float* %7, i64 %98
  %100 = bitcast float* %99 to <4 x float>*
  %wide.load55 = load <4 x float>, <4 x float>* %100, align 4, !tbaa !1295
  %101 = getelementptr inbounds float, float* %99, i64 4
  %102 = bitcast float* %101 to <4 x float>*
  %wide.load56 = load <4 x float>, <4 x float>* %102, align 4, !tbaa !1295
  %103 = getelementptr inbounds float, float* %4, i64 %98
  %104 = bitcast float* %103 to <4 x float>*
  store <4 x float> %wide.load55, <4 x float>* %104, align 4, !tbaa !1298
  %105 = getelementptr inbounds float, float* %103, i64 4
  %106 = bitcast float* %105 to <4 x float>*
  store <4 x float> %wide.load56, <4 x float>* %106, align 4, !tbaa !1298
  %index.next52 = or i64 %index51, 8
  %107 = add nsw i64 %97, %index.next52
  %108 = getelementptr inbounds float, float* %7, i64 %107
  %109 = bitcast float* %108 to <4 x float>*
  %wide.load55.1 = load <4 x float>, <4 x float>* %109, align 4, !tbaa !1295
  %110 = getelementptr inbounds float, float* %108, i64 4
  %111 = bitcast float* %110 to <4 x float>*
  %wide.load56.1 = load <4 x float>, <4 x float>* %111, align 4, !tbaa !1295
  %112 = getelementptr inbounds float, float* %4, i64 %107
  %113 = bitcast float* %112 to <4 x float>*
  store <4 x float> %wide.load55.1, <4 x float>* %113, align 4, !tbaa !1298
  %114 = getelementptr inbounds float, float* %112, i64 4
  %115 = bitcast float* %114 to <4 x float>*
  store <4 x float> %wide.load56.1, <4 x float>* %115, align 4, !tbaa !1298
  %index.next52.1 = or i64 %index51, 16
  %116 = add nsw i64 %97, %index.next52.1
  %117 = getelementptr inbounds float, float* %7, i64 %116
  %118 = bitcast float* %117 to <4 x float>*
  %wide.load55.2 = load <4 x float>, <4 x float>* %118, align 4, !tbaa !1295
  %119 = getelementptr inbounds float, float* %117, i64 4
  %120 = bitcast float* %119 to <4 x float>*
  %wide.load56.2 = load <4 x float>, <4 x float>* %120, align 4, !tbaa !1295
  %121 = getelementptr inbounds float, float* %4, i64 %116
  %122 = bitcast float* %121 to <4 x float>*
  store <4 x float> %wide.load55.2, <4 x float>* %122, align 4, !tbaa !1298
  %123 = getelementptr inbounds float, float* %121, i64 4
  %124 = bitcast float* %123 to <4 x float>*
  store <4 x float> %wide.load56.2, <4 x float>* %124, align 4, !tbaa !1298
  %index.next52.2 = or i64 %index51, 24
  %125 = add nsw i64 %97, %index.next52.2
  %126 = getelementptr inbounds float, float* %7, i64 %125
  %127 = bitcast float* %126 to <4 x float>*
  %wide.load55.3 = load <4 x float>, <4 x float>* %127, align 4, !tbaa !1295
  %128 = getelementptr inbounds float, float* %126, i64 4
  %129 = bitcast float* %128 to <4 x float>*
  %wide.load56.3 = load <4 x float>, <4 x float>* %129, align 4, !tbaa !1295
  %130 = getelementptr inbounds float, float* %4, i64 %125
  %131 = bitcast float* %130 to <4 x float>*
  store <4 x float> %wide.load55.3, <4 x float>* %131, align 4, !tbaa !1298
  %132 = getelementptr inbounds float, float* %130, i64 4
  %133 = bitcast float* %132 to <4 x float>*
  store <4 x float> %wide.load56.3, <4 x float>* %133, align 4, !tbaa !1298
  %index.next52.3 = add nuw nsw i64 %index51, 32
  %134 = icmp eq i64 %index.next52.3, 1024
  br i1 %134, label %for_end6.2, label %vector.body49, !prof !341, !llvm.loop !1303

for_end6.2:                                       ; preds = %vector.body49
  %135 = add nsw i64 %21, 3072
  br label %vector.body39

vector.body39:                                    ; preds = %vector.body39, %for_end6.2
  %index41 = phi i64 [ 0, %for_end6.2 ], [ %index.next42.3, %vector.body39 ]
  %136 = add nsw i64 %135, %index41
  %137 = getelementptr inbounds float, float* %7, i64 %136
  %138 = bitcast float* %137 to <4 x float>*
  %wide.load45 = load <4 x float>, <4 x float>* %138, align 4, !tbaa !1295
  %139 = getelementptr inbounds float, float* %137, i64 4
  %140 = bitcast float* %139 to <4 x float>*
  %wide.load46 = load <4 x float>, <4 x float>* %140, align 4, !tbaa !1295
  %141 = getelementptr inbounds float, float* %4, i64 %136
  %142 = bitcast float* %141 to <4 x float>*
  store <4 x float> %wide.load45, <4 x float>* %142, align 4, !tbaa !1298
  %143 = getelementptr inbounds float, float* %141, i64 4
  %144 = bitcast float* %143 to <4 x float>*
  store <4 x float> %wide.load46, <4 x float>* %144, align 4, !tbaa !1298
  %index.next42 = or i64 %index41, 8
  %145 = add nsw i64 %135, %index.next42
  %146 = getelementptr inbounds float, float* %7, i64 %145
  %147 = bitcast float* %146 to <4 x float>*
  %wide.load45.1 = load <4 x float>, <4 x float>* %147, align 4, !tbaa !1295
  %148 = getelementptr inbounds float, float* %146, i64 4
  %149 = bitcast float* %148 to <4 x float>*
  %wide.load46.1 = load <4 x float>, <4 x float>* %149, align 4, !tbaa !1295
  %150 = getelementptr inbounds float, float* %4, i64 %145
  %151 = bitcast float* %150 to <4 x float>*
  store <4 x float> %wide.load45.1, <4 x float>* %151, align 4, !tbaa !1298
  %152 = getelementptr inbounds float, float* %150, i64 4
  %153 = bitcast float* %152 to <4 x float>*
  store <4 x float> %wide.load46.1, <4 x float>* %153, align 4, !tbaa !1298
  %index.next42.1 = or i64 %index41, 16
  %154 = add nsw i64 %135, %index.next42.1
  %155 = getelementptr inbounds float, float* %7, i64 %154
  %156 = bitcast float* %155 to <4 x float>*
  %wide.load45.2 = load <4 x float>, <4 x float>* %156, align 4, !tbaa !1295
  %157 = getelementptr inbounds float, float* %155, i64 4
  %158 = bitcast float* %157 to <4 x float>*
  %wide.load46.2 = load <4 x float>, <4 x float>* %158, align 4, !tbaa !1295
  %159 = getelementptr inbounds float, float* %4, i64 %154
  %160 = bitcast float* %159 to <4 x float>*
  store <4 x float> %wide.load45.2, <4 x float>* %160, align 4, !tbaa !1298
  %161 = getelementptr inbounds float, float* %159, i64 4
  %162 = bitcast float* %161 to <4 x float>*
  store <4 x float> %wide.load46.2, <4 x float>* %162, align 4, !tbaa !1298
  %index.next42.2 = or i64 %index41, 24
  %163 = add nsw i64 %135, %index.next42.2
  %164 = getelementptr inbounds float, float* %7, i64 %163
  %165 = bitcast float* %164 to <4 x float>*
  %wide.load45.3 = load <4 x float>, <4 x float>* %165, align 4, !tbaa !1295
  %166 = getelementptr inbounds float, float* %164, i64 4
  %167 = bitcast float* %166 to <4 x float>*
  %wide.load46.3 = load <4 x float>, <4 x float>* %167, align 4, !tbaa !1295
  %168 = getelementptr inbounds float, float* %4, i64 %163
  %169 = bitcast float* %168 to <4 x float>*
  store <4 x float> %wide.load45.3, <4 x float>* %169, align 4, !tbaa !1298
  %170 = getelementptr inbounds float, float* %168, i64 4
  %171 = bitcast float* %170 to <4 x float>*
  store <4 x float> %wide.load46.3, <4 x float>* %171, align 4, !tbaa !1298
  %index.next42.3 = add nuw nsw i64 %index41, 32
  %172 = icmp eq i64 %index.next42.3, 1024
  br i1 %172, label %for_end6.3, label %vector.body39, !prof !341, !llvm.loop !1304

for_end6.3:                                       ; preds = %vector.body39
  %173 = add nsw i64 %21, 4096
  br label %vector.body29

vector.body29:                                    ; preds = %vector.body29, %for_end6.3
  %index31 = phi i64 [ 0, %for_end6.3 ], [ %index.next32.3, %vector.body29 ]
  %174 = add nsw i64 %173, %index31
  %175 = getelementptr inbounds float, float* %7, i64 %174
  %176 = bitcast float* %175 to <4 x float>*
  %wide.load35 = load <4 x float>, <4 x float>* %176, align 4, !tbaa !1295
  %177 = getelementptr inbounds float, float* %175, i64 4
  %178 = bitcast float* %177 to <4 x float>*
  %wide.load36 = load <4 x float>, <4 x float>* %178, align 4, !tbaa !1295
  %179 = getelementptr inbounds float, float* %4, i64 %174
  %180 = bitcast float* %179 to <4 x float>*
  store <4 x float> %wide.load35, <4 x float>* %180, align 4, !tbaa !1298
  %181 = getelementptr inbounds float, float* %179, i64 4
  %182 = bitcast float* %181 to <4 x float>*
  store <4 x float> %wide.load36, <4 x float>* %182, align 4, !tbaa !1298
  %index.next32 = or i64 %index31, 8
  %183 = add nsw i64 %173, %index.next32
  %184 = getelementptr inbounds float, float* %7, i64 %183
  %185 = bitcast float* %184 to <4 x float>*
  %wide.load35.1 = load <4 x float>, <4 x float>* %185, align 4, !tbaa !1295
  %186 = getelementptr inbounds float, float* %184, i64 4
  %187 = bitcast float* %186 to <4 x float>*
  %wide.load36.1 = load <4 x float>, <4 x float>* %187, align 4, !tbaa !1295
  %188 = getelementptr inbounds float, float* %4, i64 %183
  %189 = bitcast float* %188 to <4 x float>*
  store <4 x float> %wide.load35.1, <4 x float>* %189, align 4, !tbaa !1298
  %190 = getelementptr inbounds float, float* %188, i64 4
  %191 = bitcast float* %190 to <4 x float>*
  store <4 x float> %wide.load36.1, <4 x float>* %191, align 4, !tbaa !1298
  %index.next32.1 = or i64 %index31, 16
  %192 = add nsw i64 %173, %index.next32.1
  %193 = getelementptr inbounds float, float* %7, i64 %192
  %194 = bitcast float* %193 to <4 x float>*
  %wide.load35.2 = load <4 x float>, <4 x float>* %194, align 4, !tbaa !1295
  %195 = getelementptr inbounds float, float* %193, i64 4
  %196 = bitcast float* %195 to <4 x float>*
  %wide.load36.2 = load <4 x float>, <4 x float>* %196, align 4, !tbaa !1295
  %197 = getelementptr inbounds float, float* %4, i64 %192
  %198 = bitcast float* %197 to <4 x float>*
  store <4 x float> %wide.load35.2, <4 x float>* %198, align 4, !tbaa !1298
  %199 = getelementptr inbounds float, float* %197, i64 4
  %200 = bitcast float* %199 to <4 x float>*
  store <4 x float> %wide.load36.2, <4 x float>* %200, align 4, !tbaa !1298
  %index.next32.2 = or i64 %index31, 24
  %201 = add nsw i64 %173, %index.next32.2
  %202 = getelementptr inbounds float, float* %7, i64 %201
  %203 = bitcast float* %202 to <4 x float>*
  %wide.load35.3 = load <4 x float>, <4 x float>* %203, align 4, !tbaa !1295
  %204 = getelementptr inbounds float, float* %202, i64 4
  %205 = bitcast float* %204 to <4 x float>*
  %wide.load36.3 = load <4 x float>, <4 x float>* %205, align 4, !tbaa !1295
  %206 = getelementptr inbounds float, float* %4, i64 %201
  %207 = bitcast float* %206 to <4 x float>*
  store <4 x float> %wide.load35.3, <4 x float>* %207, align 4, !tbaa !1298
  %208 = getelementptr inbounds float, float* %206, i64 4
  %209 = bitcast float* %208 to <4 x float>*
  store <4 x float> %wide.load36.3, <4 x float>* %209, align 4, !tbaa !1298
  %index.next32.3 = add nuw nsw i64 %index31, 32
  %210 = icmp eq i64 %index.next32.3, 1024
  br i1 %210, label %for_end6.4, label %vector.body29, !prof !341, !llvm.loop !1305

for_end6.4:                                       ; preds = %vector.body29
  %211 = add nsw i64 %21, 5120
  br label %vector.body19

vector.body19:                                    ; preds = %vector.body19, %for_end6.4
  %index21 = phi i64 [ 0, %for_end6.4 ], [ %index.next22.3, %vector.body19 ]
  %212 = add nsw i64 %211, %index21
  %213 = getelementptr inbounds float, float* %7, i64 %212
  %214 = bitcast float* %213 to <4 x float>*
  %wide.load25 = load <4 x float>, <4 x float>* %214, align 4, !tbaa !1295
  %215 = getelementptr inbounds float, float* %213, i64 4
  %216 = bitcast float* %215 to <4 x float>*
  %wide.load26 = load <4 x float>, <4 x float>* %216, align 4, !tbaa !1295
  %217 = getelementptr inbounds float, float* %4, i64 %212
  %218 = bitcast float* %217 to <4 x float>*
  store <4 x float> %wide.load25, <4 x float>* %218, align 4, !tbaa !1298
  %219 = getelementptr inbounds float, float* %217, i64 4
  %220 = bitcast float* %219 to <4 x float>*
  store <4 x float> %wide.load26, <4 x float>* %220, align 4, !tbaa !1298
  %index.next22 = or i64 %index21, 8
  %221 = add nsw i64 %211, %index.next22
  %222 = getelementptr inbounds float, float* %7, i64 %221
  %223 = bitcast float* %222 to <4 x float>*
  %wide.load25.1 = load <4 x float>, <4 x float>* %223, align 4, !tbaa !1295
  %224 = getelementptr inbounds float, float* %222, i64 4
  %225 = bitcast float* %224 to <4 x float>*
  %wide.load26.1 = load <4 x float>, <4 x float>* %225, align 4, !tbaa !1295
  %226 = getelementptr inbounds float, float* %4, i64 %221
  %227 = bitcast float* %226 to <4 x float>*
  store <4 x float> %wide.load25.1, <4 x float>* %227, align 4, !tbaa !1298
  %228 = getelementptr inbounds float, float* %226, i64 4
  %229 = bitcast float* %228 to <4 x float>*
  store <4 x float> %wide.load26.1, <4 x float>* %229, align 4, !tbaa !1298
  %index.next22.1 = or i64 %index21, 16
  %230 = add nsw i64 %211, %index.next22.1
  %231 = getelementptr inbounds float, float* %7, i64 %230
  %232 = bitcast float* %231 to <4 x float>*
  %wide.load25.2 = load <4 x float>, <4 x float>* %232, align 4, !tbaa !1295
  %233 = getelementptr inbounds float, float* %231, i64 4
  %234 = bitcast float* %233 to <4 x float>*
  %wide.load26.2 = load <4 x float>, <4 x float>* %234, align 4, !tbaa !1295
  %235 = getelementptr inbounds float, float* %4, i64 %230
  %236 = bitcast float* %235 to <4 x float>*
  store <4 x float> %wide.load25.2, <4 x float>* %236, align 4, !tbaa !1298
  %237 = getelementptr inbounds float, float* %235, i64 4
  %238 = bitcast float* %237 to <4 x float>*
  store <4 x float> %wide.load26.2, <4 x float>* %238, align 4, !tbaa !1298
  %index.next22.2 = or i64 %index21, 24
  %239 = add nsw i64 %211, %index.next22.2
  %240 = getelementptr inbounds float, float* %7, i64 %239
  %241 = bitcast float* %240 to <4 x float>*
  %wide.load25.3 = load <4 x float>, <4 x float>* %241, align 4, !tbaa !1295
  %242 = getelementptr inbounds float, float* %240, i64 4
  %243 = bitcast float* %242 to <4 x float>*
  %wide.load26.3 = load <4 x float>, <4 x float>* %243, align 4, !tbaa !1295
  %244 = getelementptr inbounds float, float* %4, i64 %239
  %245 = bitcast float* %244 to <4 x float>*
  store <4 x float> %wide.load25.3, <4 x float>* %245, align 4, !tbaa !1298
  %246 = getelementptr inbounds float, float* %244, i64 4
  %247 = bitcast float* %246 to <4 x float>*
  store <4 x float> %wide.load26.3, <4 x float>* %247, align 4, !tbaa !1298
  %index.next22.3 = add nuw nsw i64 %index21, 32
  %248 = icmp eq i64 %index.next22.3, 1024
  br i1 %248, label %for_end6.5, label %vector.body19, !prof !341, !llvm.loop !1306

for_end6.5:                                       ; preds = %vector.body19
  %249 = add nsw i64 %21, 6144
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_end6.5
  %index = phi i64 [ 0, %for_end6.5 ], [ %index.next.3, %vector.body ]
  %250 = add nsw i64 %249, %index
  %251 = getelementptr inbounds float, float* %7, i64 %250
  %252 = bitcast float* %251 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %252, align 4, !tbaa !1295
  %253 = getelementptr inbounds float, float* %251, i64 4
  %254 = bitcast float* %253 to <4 x float>*
  %wide.load16 = load <4 x float>, <4 x float>* %254, align 4, !tbaa !1295
  %255 = getelementptr inbounds float, float* %4, i64 %250
  %256 = bitcast float* %255 to <4 x float>*
  store <4 x float> %wide.load, <4 x float>* %256, align 4, !tbaa !1298
  %257 = getelementptr inbounds float, float* %255, i64 4
  %258 = bitcast float* %257 to <4 x float>*
  store <4 x float> %wide.load16, <4 x float>* %258, align 4, !tbaa !1298
  %index.next = or i64 %index, 8
  %259 = add nsw i64 %249, %index.next
  %260 = getelementptr inbounds float, float* %7, i64 %259
  %261 = bitcast float* %260 to <4 x float>*
  %wide.load.1 = load <4 x float>, <4 x float>* %261, align 4, !tbaa !1295
  %262 = getelementptr inbounds float, float* %260, i64 4
  %263 = bitcast float* %262 to <4 x float>*
  %wide.load16.1 = load <4 x float>, <4 x float>* %263, align 4, !tbaa !1295
  %264 = getelementptr inbounds float, float* %4, i64 %259
  %265 = bitcast float* %264 to <4 x float>*
  store <4 x float> %wide.load.1, <4 x float>* %265, align 4, !tbaa !1298
  %266 = getelementptr inbounds float, float* %264, i64 4
  %267 = bitcast float* %266 to <4 x float>*
  store <4 x float> %wide.load16.1, <4 x float>* %267, align 4, !tbaa !1298
  %index.next.1 = or i64 %index, 16
  %268 = add nsw i64 %249, %index.next.1
  %269 = getelementptr inbounds float, float* %7, i64 %268
  %270 = bitcast float* %269 to <4 x float>*
  %wide.load.2 = load <4 x float>, <4 x float>* %270, align 4, !tbaa !1295
  %271 = getelementptr inbounds float, float* %269, i64 4
  %272 = bitcast float* %271 to <4 x float>*
  %wide.load16.2 = load <4 x float>, <4 x float>* %272, align 4, !tbaa !1295
  %273 = getelementptr inbounds float, float* %4, i64 %268
  %274 = bitcast float* %273 to <4 x float>*
  store <4 x float> %wide.load.2, <4 x float>* %274, align 4, !tbaa !1298
  %275 = getelementptr inbounds float, float* %273, i64 4
  %276 = bitcast float* %275 to <4 x float>*
  store <4 x float> %wide.load16.2, <4 x float>* %276, align 4, !tbaa !1298
  %index.next.2 = or i64 %index, 24
  %277 = add nsw i64 %249, %index.next.2
  %278 = getelementptr inbounds float, float* %7, i64 %277
  %279 = bitcast float* %278 to <4 x float>*
  %wide.load.3 = load <4 x float>, <4 x float>* %279, align 4, !tbaa !1295
  %280 = getelementptr inbounds float, float* %278, i64 4
  %281 = bitcast float* %280 to <4 x float>*
  %wide.load16.3 = load <4 x float>, <4 x float>* %281, align 4, !tbaa !1295
  %282 = getelementptr inbounds float, float* %4, i64 %277
  %283 = bitcast float* %282 to <4 x float>*
  store <4 x float> %wide.load.3, <4 x float>* %283, align 4, !tbaa !1298
  %284 = getelementptr inbounds float, float* %282, i64 4
  %285 = bitcast float* %284 to <4 x float>*
  store <4 x float> %wide.load16.3, <4 x float>* %285, align 4, !tbaa !1298
  %index.next.3 = add nuw nsw i64 %index, 32
  %286 = icmp eq i64 %index.next.3, 1024
  br i1 %286, label %for_end6.6, label %vector.body, !prof !341, !llvm.loop !1307

for_end6.6:                                       ; preds = %vector.body
  %indvars.iv.next14 = add nsw i64 %indvars.iv13, 1
  %exitcond15.not = icmp eq i64 %indvars.iv.next14, %wide.trip.count
  br i1 %exitcond15.not, label %for_end, label %for_begin1.preheader, !prof !51
}

; Function Attrs: nofree nounwind
define private i32 @__tvm_parallel_lambda.136(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #3 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds i8, i8* %2, i64 16
  %9 = bitcast i8* %8 to float**
  %10 = load float*, float** %9, align 8
  %11 = getelementptr inbounds i8, i8* %2, i64 24
  %12 = bitcast i8* %11 to float**
  %13 = load float*, float** %12, align 8
  %14 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %15 = load i32, i32* %14, align 4
  %16 = add nsw i32 %15, 48
  %17 = sdiv i32 %16, %15
  %18 = add nsw i32 %0, 1
  %19 = mul nsw i32 %17, %18
  %20 = icmp slt i32 %19, 49
  %21 = select i1 %20, i32 %19, i32 49
  %22 = mul nsw i32 %17, %0
  %23 = icmp slt i32 %22, 49
  %24 = select i1 %23, i32 %22, i32 49
  %25 = icmp slt i32 %24, %21
  br i1 %25, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %26 = sext i32 %24 to i64
  %wide.trip.count = sext i32 %21 to i64
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end3
  %indvars.iv14 = phi i64 [ %26, %for_begin1.preheader.preheader ], [ %indvars.iv.next15, %for_end3 ]
  %27 = phi i32 [ %24, %for_begin1.preheader.preheader ], [ %31, %for_end3 ]
  %28 = shl nsw i32 %27, 10
  %29 = sext i32 %28 to i64
  br label %for_begin4.preheader

for_end:                                          ; preds = %for_end3, %entry
  ret i32 0

for_begin4.preheader:                             ; preds = %for_begin1.preheader, %for_end6
  %indvars.iv11 = phi i64 [ 0, %for_begin1.preheader ], [ %indvars.iv.next12, %for_end6 ]
  %30 = shl nsw i64 %indvars.iv11, 6
  br label %for_body5

for_end3:                                         ; preds = %for_end6
  %indvars.iv.next15 = add nsw i64 %indvars.iv14, 1
  %31 = add nsw i32 %27, 1
  %exitcond16.not = icmp eq i64 %indvars.iv.next15, %wide.trip.count
  br i1 %exitcond16.not, label %for_end, label %for_begin1.preheader, !prof !51

for_body5:                                        ; preds = %for_body5, %for_begin4.preheader
  %indvars.iv = phi i64 [ 0, %for_begin4.preheader ], [ %indvars.iv.next.1, %for_body5 ]
  %.010 = phi <64 x float> [ zeroinitializer, %for_begin4.preheader ], [ %53, %for_body5 ]
  %32 = add nsw i64 %indvars.iv, %29
  %33 = getelementptr inbounds float, float* %4, i64 %32
  %34 = load float, float* %33, align 4, !tbaa !1298
  %35 = insertelement <64 x float> undef, float %34, i32 0
  %36 = shufflevector <64 x float> %35, <64 x float> undef, <64 x i32> zeroinitializer
  %37 = shl nuw nsw i64 %indvars.iv, 10
  %38 = add nuw nsw i64 %37, %30
  %39 = getelementptr inbounds float, float* %7, i64 %38
  %40 = bitcast float* %39 to <64 x float>*
  %41 = load <64 x float>, <64 x float>* %40, align 128, !tbaa !1308
  %42 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %36, <64 x float> %41, <64 x float> %.010)
  %indvars.iv.next = or i64 %indvars.iv, 1
  %43 = add nsw i64 %indvars.iv.next, %29
  %44 = getelementptr inbounds float, float* %4, i64 %43
  %45 = load float, float* %44, align 4, !tbaa !1298
  %46 = insertelement <64 x float> undef, float %45, i32 0
  %47 = shufflevector <64 x float> %46, <64 x float> undef, <64 x i32> zeroinitializer
  %48 = shl nuw nsw i64 %indvars.iv.next, 10
  %49 = add nuw nsw i64 %48, %30
  %50 = getelementptr inbounds float, float* %7, i64 %49
  %51 = bitcast float* %50 to <64 x float>*
  %52 = load <64 x float>, <64 x float>* %51, align 128, !tbaa !1308
  %53 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %47, <64 x float> %52, <64 x float> %42)
  %indvars.iv.next.1 = add nuw nsw i64 %indvars.iv, 2
  %exitcond.not.1 = icmp eq i64 %indvars.iv.next.1, 1024
  br i1 %exitcond.not.1, label %for_end6, label %for_body5, !prof !51

for_end6:                                         ; preds = %for_body5
  %54 = add nsw i64 %30, %29
  %55 = getelementptr inbounds float, float* %13, i64 %30
  %56 = bitcast float* %55 to <64 x float>*
  %57 = load <64 x float>, <64 x float>* %56, align 128, !tbaa !1311
  %58 = fadd <64 x float> %53, %57
  %59 = fcmp olt <64 x float> %58, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %60 = select <64 x i1> %59, <64 x float> %58, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %61 = fcmp ogt <64 x float> %60, zeroinitializer
  %62 = select <64 x i1> %61, <64 x float> %60, <64 x float> zeroinitializer
  %63 = getelementptr inbounds float, float* %10, i64 %54
  %64 = bitcast float* %63 to <64 x float>*
  store <64 x float> %62, <64 x float>* %64, align 128, !tbaa !1314
  %indvars.iv.next12 = add nuw nsw i64 %indvars.iv11, 1
  %exitcond13.not = icmp eq i64 %indvars.iv.next12, 16
  br i1 %exitcond13.not, label %for_end3, label %for_begin4.preheader, !prof !51
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_18(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([88 x i8], [88 x i8]* @.str.137, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !1317
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !1331
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !1333
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !1336
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.138, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.139, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.140, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.141, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !1338
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !1352
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 224
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.142, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !1354
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 224
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.143, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !1357
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 3
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.144, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !1359
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 150528, i32 672, i32 3, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([199 x i8], [199 x i8]* @.str.145, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !1371
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 3
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.51, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !1385
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 3
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.52, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !1387
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 3
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.146, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !1390
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 32
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.147, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !1392
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 288, i32 96, i32 32, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.148, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !1404
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 32
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.149, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !1418
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !1432
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !1446
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 112
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.34, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !1448
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 112
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.35, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !1451
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 32
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.150, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !1453
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 401408, i32 3584, i32 32, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.151, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_18_compute_(i8* %31, i8* %39, i8* %51, i8* %45, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_18_compute_(i8* noalias align 128 %0, i8* noalias align 128 %1, i8* noalias align 128 %2, i8* noalias align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 607500, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %call_end, %if_end, %entry
  %merge = phi i32 [ -1, %entry ], [ %13, %if_end ], [ %22, %call_end ]
  ret i32 %merge

if_end:                                           ; preds = %entry
  %8 = alloca %10, align 8
  %9 = getelementptr inbounds %10, %10* %8, i64 0, i32 0
  store i8* %6, i8** %9, align 8
  %10 = getelementptr inbounds %10, %10* %8, i64 0, i32 1
  store i8* %0, i8** %10, align 8
  %11 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %12 = bitcast %10* %8 to i8*
  %13 = call i32 %11(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.152, i8* nonnull %12, i32 0)
  %14 = icmp eq i32 %13, 0
  br i1 %14, label %call_end, label %if_then, !prof !5

call_end:                                         ; preds = %if_end
  %15 = alloca %11, align 8
  %16 = getelementptr inbounds %11, %11* %15, i64 0, i32 0
  store i8* %6, i8** %16, align 8
  %17 = getelementptr inbounds %11, %11* %15, i64 0, i32 1
  store i8* %1, i8** %17, align 8
  %18 = getelementptr inbounds %11, %11* %15, i64 0, i32 2
  store i8* %2, i8** %18, align 8
  %19 = getelementptr inbounds %11, %11* %15, i64 0, i32 3
  store i8* %3, i8** %19, align 8
  %20 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %21 = bitcast %11* %15 to i8*
  %22 = call i32 %20(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.153, i8* nonnull %21, i32 0)
  %23 = icmp eq i32 %22, 0
  br i1 %23, label %call_end2, label %if_then, !prof !5

call_end2:                                        ; preds = %call_end
  %24 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %25 = call i32 %24(i32 1, i32 %4, i8* nonnull %6)
  %.not = icmp ne i32 %25, 0
  %spec.select = sext i1 %.not to i32
  ret i32 %spec.select
}

; Function Attrs: nofree norecurse nounwind
define private i32 @__tvm_parallel_lambda.152(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #2 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %9 = load i32, i32* %8, align 4
  %10 = add nsw i32 %9, 224
  %11 = sdiv i32 %10, %9
  %12 = add nsw i32 %0, 1
  %13 = mul nsw i32 %11, %12
  %14 = icmp slt i32 %13, 225
  %15 = select i1 %14, i32 %13, i32 225
  %16 = mul nsw i32 %11, %0
  %17 = icmp slt i32 %16, 225
  %18 = select i1 %17, i32 %16, i32 225
  %19 = icmp slt i32 %18, %15
  br i1 %19, label %for_begin1.preheader, label %for_end, !prof !5

for_begin1.preheader:                             ; preds = %entry, %for_end3
  %20 = phi i32 [ %75, %for_end3 ], [ %18, %entry ]
  %21 = mul nsw i32 %20, 675
  %22 = icmp slt i32 %20, 224
  %23 = mul nsw i32 %20, 672
  br i1 %22, label %for_begin4.preheader.us, label %for_begin4.preheader

for_begin4.preheader.us:                          ; preds = %for_begin1.preheader, %for_end6.us
  %indvars.iv21 = phi i64 [ %indvars.iv.next22, %for_end6.us ], [ 0, %for_begin1.preheader ]
  %24 = mul nuw nsw i64 %indvars.iv21, 3
  %25 = icmp ult i64 %indvars.iv21, 224
  %26 = trunc i64 %24 to i32
  %27 = add i32 %21, %26
  br i1 %25, label %for_body5.us.us.preheader, label %for_body5.us8.preheader

for_body5.us8.preheader:                          ; preds = %for_begin4.preheader.us
  %28 = sext i32 %27 to i64
  %29 = getelementptr inbounds float, float* %4, i64 %28
  store float 0.000000e+00, float* %29, align 4, !tbaa !1465
  %30 = trunc i64 %24 to i32
  %31 = add i32 %30, 1
  %32 = add i32 %31, %21
  %33 = sext i32 %32 to i64
  %34 = getelementptr inbounds float, float* %4, i64 %33
  store float 0.000000e+00, float* %34, align 4, !tbaa !1465
  %35 = trunc i64 %24 to i32
  %36 = add i32 %35, 2
  br label %for_end6.us

for_body5.us.us.preheader:                        ; preds = %for_begin4.preheader.us
  %37 = add i32 %23, %26
  %38 = sext i32 %37 to i64
  %39 = getelementptr inbounds float, float* %7, i64 %38
  %40 = load float, float* %39, align 4, !tbaa !1468
  %41 = sext i32 %27 to i64
  %42 = getelementptr inbounds float, float* %4, i64 %41
  store float %40, float* %42, align 4, !tbaa !1465
  %43 = trunc i64 %24 to i32
  %44 = add i32 %43, 1
  %45 = add i32 %44, %21
  %46 = add i32 %44, %23
  %47 = sext i32 %46 to i64
  %48 = getelementptr inbounds float, float* %7, i64 %47
  %49 = load float, float* %48, align 4, !tbaa !1468
  %50 = sext i32 %45 to i64
  %51 = getelementptr inbounds float, float* %4, i64 %50
  store float %49, float* %51, align 4, !tbaa !1465
  %52 = trunc i64 %24 to i32
  %53 = add i32 %52, 2
  %54 = add i32 %53, %23
  %55 = sext i32 %54 to i64
  %56 = getelementptr inbounds float, float* %7, i64 %55
  %57 = load float, float* %56, align 4, !tbaa !1468
  br label %for_end6.us

for_end6.us:                                      ; preds = %for_body5.us8.preheader, %for_body5.us.us.preheader
  %.pn = phi i32 [ %36, %for_body5.us8.preheader ], [ %53, %for_body5.us.us.preheader ]
  %.sink = phi float [ 0.000000e+00, %for_body5.us8.preheader ], [ %57, %for_body5.us.us.preheader ]
  %.sink29 = add i32 %.pn, %21
  %58 = sext i32 %.sink29 to i64
  %59 = getelementptr inbounds float, float* %4, i64 %58
  store float %.sink, float* %59, align 4, !tbaa !1465
  %indvars.iv.next22 = add nuw nsw i64 %indvars.iv21, 1
  %exitcond23.not = icmp eq i64 %indvars.iv.next22, 225
  br i1 %exitcond23.not, label %for_end3, label %for_begin4.preheader.us, !prof !51

for_end:                                          ; preds = %for_end3, %entry
  ret i32 0

for_begin4.preheader:                             ; preds = %for_begin1.preheader, %for_begin4.preheader
  %indvars.iv = phi i64 [ %indvars.iv.next, %for_begin4.preheader ], [ 0, %for_begin1.preheader ]
  %60 = mul nuw nsw i64 %indvars.iv, 3
  %61 = trunc i64 %60 to i32
  %62 = add i32 %21, %61
  %63 = sext i32 %62 to i64
  %64 = getelementptr inbounds float, float* %4, i64 %63
  store float 0.000000e+00, float* %64, align 4, !tbaa !1465
  %65 = trunc i64 %60 to i32
  %66 = add i32 %65, 1
  %67 = add i32 %66, %21
  %68 = sext i32 %67 to i64
  %69 = getelementptr inbounds float, float* %4, i64 %68
  store float 0.000000e+00, float* %69, align 4, !tbaa !1465
  %70 = trunc i64 %60 to i32
  %71 = add i32 %70, 2
  %72 = add i32 %71, %21
  %73 = sext i32 %72 to i64
  %74 = getelementptr inbounds float, float* %4, i64 %73
  store float 0.000000e+00, float* %74, align 4, !tbaa !1465
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, 225
  br i1 %exitcond.not, label %for_end3, label %for_begin4.preheader, !prof !51

for_end3:                                         ; preds = %for_begin4.preheader, %for_end6.us
  %75 = add nsw i32 %20, 1
  %exitcond24.not = icmp eq i32 %75, %15
  br i1 %exitcond24.not, label %for_end, label %for_begin1.preheader, !prof !51
}

; Function Attrs: nofree nounwind
define private i32 @__tvm_parallel_lambda.153(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #3 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds i8, i8* %2, i64 16
  %9 = bitcast i8* %8 to float**
  %10 = load float*, float** %9, align 8
  %11 = getelementptr inbounds i8, i8* %2, i64 24
  %12 = bitcast i8* %11 to float**
  %13 = load float*, float** %12, align 8
  %14 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %15 = load i32, i32* %14, align 4
  %16 = add nsw i32 %15, 12543
  %17 = sdiv i32 %16, %15
  %18 = add nsw i32 %0, 1
  %19 = mul nsw i32 %17, %18
  %20 = icmp slt i32 %19, 12544
  %21 = select i1 %20, i32 %19, i32 12544
  %22 = mul nsw i32 %17, %0
  %23 = icmp slt i32 %22, 12544
  %24 = select i1 %23, i32 %22, i32 12544
  %25 = icmp slt i32 %24, %21
  br i1 %25, label %for_begin1.preheader, label %for_end, !prof !5

for_begin1.preheader:                             ; preds = %entry, %for_end3
  %26 = phi i32 [ %260, %for_end3 ], [ %24, %entry ]
  %27 = srem i32 %26, 112
  %28 = mul nsw i32 %27, 6
  %29 = sdiv i32 %26, 112
  %30 = mul nsw i32 %29, 1350
  %31 = add i32 %28, %30
  %32 = shl nsw i32 %26, 5
  %33 = add i32 %31, 1358
  %34 = sext i32 %33 to i64
  %35 = getelementptr inbounds float, float* %4, i64 %34
  %36 = load float, float* %35, align 4, !tbaa !1465
  %37 = add i32 %31, 1357
  %38 = sext i32 %37 to i64
  %39 = getelementptr inbounds float, float* %4, i64 %38
  %40 = load float, float* %39, align 4, !tbaa !1465
  %41 = add i32 %31, 1356
  %42 = sext i32 %41 to i64
  %43 = getelementptr inbounds float, float* %4, i64 %42
  %44 = load float, float* %43, align 4, !tbaa !1465
  %45 = add i32 %31, 1355
  %46 = sext i32 %45 to i64
  %47 = getelementptr inbounds float, float* %4, i64 %46
  %48 = load float, float* %47, align 4, !tbaa !1465
  %49 = add i32 %31, 1354
  %50 = sext i32 %49 to i64
  %51 = getelementptr inbounds float, float* %4, i64 %50
  %52 = load float, float* %51, align 4, !tbaa !1465
  %53 = add i32 %31, 1353
  %54 = sext i32 %53 to i64
  %55 = getelementptr inbounds float, float* %4, i64 %54
  %56 = load float, float* %55, align 4, !tbaa !1465
  %57 = add i32 %31, 1352
  %58 = sext i32 %57 to i64
  %59 = getelementptr inbounds float, float* %4, i64 %58
  %60 = load float, float* %59, align 4, !tbaa !1465
  %61 = add i32 %31, 1351
  %62 = sext i32 %61 to i64
  %63 = getelementptr inbounds float, float* %4, i64 %62
  %64 = load float, float* %63, align 4, !tbaa !1465
  %65 = add i32 %31, 1350
  %66 = sext i32 %65 to i64
  %67 = getelementptr inbounds float, float* %4, i64 %66
  %68 = load float, float* %67, align 4, !tbaa !1465
  %69 = add i32 %31, 683
  %70 = sext i32 %69 to i64
  %71 = getelementptr inbounds float, float* %4, i64 %70
  %72 = load float, float* %71, align 4, !tbaa !1465
  %73 = add i32 %31, 682
  %74 = sext i32 %73 to i64
  %75 = getelementptr inbounds float, float* %4, i64 %74
  %76 = load float, float* %75, align 4, !tbaa !1465
  %77 = add i32 %31, 681
  %78 = sext i32 %77 to i64
  %79 = getelementptr inbounds float, float* %4, i64 %78
  %80 = load float, float* %79, align 4, !tbaa !1465
  %81 = add i32 %31, 680
  %82 = sext i32 %81 to i64
  %83 = getelementptr inbounds float, float* %4, i64 %82
  %84 = load float, float* %83, align 4, !tbaa !1465
  %85 = add i32 %31, 679
  %86 = sext i32 %85 to i64
  %87 = getelementptr inbounds float, float* %4, i64 %86
  %88 = load float, float* %87, align 4, !tbaa !1465
  %89 = add i32 %31, 678
  %90 = sext i32 %89 to i64
  %91 = getelementptr inbounds float, float* %4, i64 %90
  %92 = load float, float* %91, align 4, !tbaa !1465
  %93 = add i32 %31, 677
  %94 = sext i32 %93 to i64
  %95 = getelementptr inbounds float, float* %4, i64 %94
  %96 = load float, float* %95, align 4, !tbaa !1465
  %97 = add i32 %31, 676
  %98 = sext i32 %97 to i64
  %99 = getelementptr inbounds float, float* %4, i64 %98
  %100 = load float, float* %99, align 4, !tbaa !1465
  %101 = add i32 %31, 675
  %102 = sext i32 %101 to i64
  %103 = getelementptr inbounds float, float* %4, i64 %102
  %104 = load float, float* %103, align 4, !tbaa !1465
  %105 = add i32 %31, 8
  %106 = sext i32 %105 to i64
  %107 = getelementptr inbounds float, float* %4, i64 %106
  %108 = load float, float* %107, align 4, !tbaa !1465
  %109 = add i32 %31, 7
  %110 = sext i32 %109 to i64
  %111 = getelementptr inbounds float, float* %4, i64 %110
  %112 = load float, float* %111, align 4, !tbaa !1465
  %113 = add i32 %31, 6
  %114 = sext i32 %113 to i64
  %115 = getelementptr inbounds float, float* %4, i64 %114
  %116 = load float, float* %115, align 4, !tbaa !1465
  %117 = add i32 %31, 5
  %118 = sext i32 %117 to i64
  %119 = getelementptr inbounds float, float* %4, i64 %118
  %120 = load float, float* %119, align 4, !tbaa !1465
  %121 = add i32 %31, 4
  %122 = sext i32 %121 to i64
  %123 = getelementptr inbounds float, float* %4, i64 %122
  %124 = load float, float* %123, align 4, !tbaa !1465
  %125 = add i32 %31, 3
  %126 = sext i32 %125 to i64
  %127 = getelementptr inbounds float, float* %4, i64 %126
  %128 = load float, float* %127, align 4, !tbaa !1465
  %129 = add i32 %31, 2
  %130 = sext i32 %129 to i64
  %131 = getelementptr inbounds float, float* %4, i64 %130
  %132 = load float, float* %131, align 4, !tbaa !1465
  %133 = or i32 %31, 1
  %134 = sext i32 %133 to i64
  %135 = getelementptr inbounds float, float* %4, i64 %134
  %136 = load float, float* %135, align 4, !tbaa !1465
  %137 = sext i32 %31 to i64
  %138 = getelementptr inbounds float, float* %4, i64 %137
  %139 = load float, float* %138, align 4, !tbaa !1465
  %broadcast.splatinsert = insertelement <4 x float> poison, float %139, i32 0
  %broadcast.splat = shufflevector <4 x float> %broadcast.splatinsert, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert23 = insertelement <4 x float> poison, float %136, i32 0
  %broadcast.splat24 = shufflevector <4 x float> %broadcast.splatinsert23, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert26 = insertelement <4 x float> poison, float %132, i32 0
  %broadcast.splat27 = shufflevector <4 x float> %broadcast.splatinsert26, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert29 = insertelement <4 x float> poison, float %128, i32 0
  %broadcast.splat30 = shufflevector <4 x float> %broadcast.splatinsert29, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert32 = insertelement <4 x float> poison, float %124, i32 0
  %broadcast.splat33 = shufflevector <4 x float> %broadcast.splatinsert32, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert35 = insertelement <4 x float> poison, float %120, i32 0
  %broadcast.splat36 = shufflevector <4 x float> %broadcast.splatinsert35, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert38 = insertelement <4 x float> poison, float %116, i32 0
  %broadcast.splat39 = shufflevector <4 x float> %broadcast.splatinsert38, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert41 = insertelement <4 x float> poison, float %112, i32 0
  %broadcast.splat42 = shufflevector <4 x float> %broadcast.splatinsert41, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert44 = insertelement <4 x float> poison, float %108, i32 0
  %broadcast.splat45 = shufflevector <4 x float> %broadcast.splatinsert44, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert47 = insertelement <4 x float> poison, float %104, i32 0
  %broadcast.splat48 = shufflevector <4 x float> %broadcast.splatinsert47, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert50 = insertelement <4 x float> poison, float %100, i32 0
  %broadcast.splat51 = shufflevector <4 x float> %broadcast.splatinsert50, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert53 = insertelement <4 x float> poison, float %96, i32 0
  %broadcast.splat54 = shufflevector <4 x float> %broadcast.splatinsert53, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert56 = insertelement <4 x float> poison, float %92, i32 0
  %broadcast.splat57 = shufflevector <4 x float> %broadcast.splatinsert56, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert59 = insertelement <4 x float> poison, float %88, i32 0
  %broadcast.splat60 = shufflevector <4 x float> %broadcast.splatinsert59, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert62 = insertelement <4 x float> poison, float %84, i32 0
  %broadcast.splat63 = shufflevector <4 x float> %broadcast.splatinsert62, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert65 = insertelement <4 x float> poison, float %80, i32 0
  %broadcast.splat66 = shufflevector <4 x float> %broadcast.splatinsert65, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert68 = insertelement <4 x float> poison, float %76, i32 0
  %broadcast.splat69 = shufflevector <4 x float> %broadcast.splatinsert68, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert71 = insertelement <4 x float> poison, float %72, i32 0
  %broadcast.splat72 = shufflevector <4 x float> %broadcast.splatinsert71, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert74 = insertelement <4 x float> poison, float %68, i32 0
  %broadcast.splat75 = shufflevector <4 x float> %broadcast.splatinsert74, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert77 = insertelement <4 x float> poison, float %64, i32 0
  %broadcast.splat78 = shufflevector <4 x float> %broadcast.splatinsert77, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert80 = insertelement <4 x float> poison, float %60, i32 0
  %broadcast.splat81 = shufflevector <4 x float> %broadcast.splatinsert80, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert83 = insertelement <4 x float> poison, float %56, i32 0
  %broadcast.splat84 = shufflevector <4 x float> %broadcast.splatinsert83, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert86 = insertelement <4 x float> poison, float %52, i32 0
  %broadcast.splat87 = shufflevector <4 x float> %broadcast.splatinsert86, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert89 = insertelement <4 x float> poison, float %48, i32 0
  %broadcast.splat90 = shufflevector <4 x float> %broadcast.splatinsert89, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert92 = insertelement <4 x float> poison, float %44, i32 0
  %broadcast.splat93 = shufflevector <4 x float> %broadcast.splatinsert92, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert95 = insertelement <4 x float> poison, float %40, i32 0
  %broadcast.splat96 = shufflevector <4 x float> %broadcast.splatinsert95, <4 x float> poison, <4 x i32> zeroinitializer
  %broadcast.splatinsert98 = insertelement <4 x float> poison, float %36, i32 0
  %broadcast.splat99 = shufflevector <4 x float> %broadcast.splatinsert98, <4 x float> poison, <4 x i32> zeroinitializer
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_begin1.preheader
  %index = phi i64 [ 0, %for_begin1.preheader ], [ %index.next, %vector.body ]
  %140 = getelementptr inbounds float, float* %7, i64 %index
  %141 = bitcast float* %140 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %141, align 4, !tbaa !1471
  %142 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat, <4 x float> %wide.load, <4 x float> zeroinitializer)
  %143 = add nuw nsw i64 %index, 32
  %144 = getelementptr inbounds float, float* %7, i64 %143
  %145 = bitcast float* %144 to <4 x float>*
  %wide.load22 = load <4 x float>, <4 x float>* %145, align 4, !tbaa !1471
  %146 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat24, <4 x float> %wide.load22, <4 x float> %142)
  %147 = add nuw nsw i64 %index, 64
  %148 = getelementptr inbounds float, float* %7, i64 %147
  %149 = bitcast float* %148 to <4 x float>*
  %wide.load25 = load <4 x float>, <4 x float>* %149, align 4, !tbaa !1471
  %150 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat27, <4 x float> %wide.load25, <4 x float> %146)
  %151 = add nuw nsw i64 %index, 96
  %152 = getelementptr inbounds float, float* %7, i64 %151
  %153 = bitcast float* %152 to <4 x float>*
  %wide.load28 = load <4 x float>, <4 x float>* %153, align 4, !tbaa !1471
  %154 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat30, <4 x float> %wide.load28, <4 x float> %150)
  %155 = add nuw nsw i64 %index, 128
  %156 = getelementptr inbounds float, float* %7, i64 %155
  %157 = bitcast float* %156 to <4 x float>*
  %wide.load31 = load <4 x float>, <4 x float>* %157, align 4, !tbaa !1471
  %158 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat33, <4 x float> %wide.load31, <4 x float> %154)
  %159 = add nuw nsw i64 %index, 160
  %160 = getelementptr inbounds float, float* %7, i64 %159
  %161 = bitcast float* %160 to <4 x float>*
  %wide.load34 = load <4 x float>, <4 x float>* %161, align 4, !tbaa !1471
  %162 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat36, <4 x float> %wide.load34, <4 x float> %158)
  %163 = add nuw nsw i64 %index, 192
  %164 = getelementptr inbounds float, float* %7, i64 %163
  %165 = bitcast float* %164 to <4 x float>*
  %wide.load37 = load <4 x float>, <4 x float>* %165, align 4, !tbaa !1471
  %166 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat39, <4 x float> %wide.load37, <4 x float> %162)
  %167 = add nuw nsw i64 %index, 224
  %168 = getelementptr inbounds float, float* %7, i64 %167
  %169 = bitcast float* %168 to <4 x float>*
  %wide.load40 = load <4 x float>, <4 x float>* %169, align 4, !tbaa !1471
  %170 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat42, <4 x float> %wide.load40, <4 x float> %166)
  %171 = add nuw nsw i64 %index, 256
  %172 = getelementptr inbounds float, float* %7, i64 %171
  %173 = bitcast float* %172 to <4 x float>*
  %wide.load43 = load <4 x float>, <4 x float>* %173, align 4, !tbaa !1471
  %174 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat45, <4 x float> %wide.load43, <4 x float> %170)
  %175 = add nuw nsw i64 %index, 288
  %176 = getelementptr inbounds float, float* %7, i64 %175
  %177 = bitcast float* %176 to <4 x float>*
  %wide.load46 = load <4 x float>, <4 x float>* %177, align 4, !tbaa !1471
  %178 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat48, <4 x float> %wide.load46, <4 x float> %174)
  %179 = add nuw nsw i64 %index, 320
  %180 = getelementptr inbounds float, float* %7, i64 %179
  %181 = bitcast float* %180 to <4 x float>*
  %wide.load49 = load <4 x float>, <4 x float>* %181, align 4, !tbaa !1471
  %182 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat51, <4 x float> %wide.load49, <4 x float> %178)
  %183 = add nuw nsw i64 %index, 352
  %184 = getelementptr inbounds float, float* %7, i64 %183
  %185 = bitcast float* %184 to <4 x float>*
  %wide.load52 = load <4 x float>, <4 x float>* %185, align 4, !tbaa !1471
  %186 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat54, <4 x float> %wide.load52, <4 x float> %182)
  %187 = add nuw nsw i64 %index, 384
  %188 = getelementptr inbounds float, float* %7, i64 %187
  %189 = bitcast float* %188 to <4 x float>*
  %wide.load55 = load <4 x float>, <4 x float>* %189, align 4, !tbaa !1471
  %190 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat57, <4 x float> %wide.load55, <4 x float> %186)
  %191 = add nuw nsw i64 %index, 416
  %192 = getelementptr inbounds float, float* %7, i64 %191
  %193 = bitcast float* %192 to <4 x float>*
  %wide.load58 = load <4 x float>, <4 x float>* %193, align 4, !tbaa !1471
  %194 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat60, <4 x float> %wide.load58, <4 x float> %190)
  %195 = add nuw nsw i64 %index, 448
  %196 = getelementptr inbounds float, float* %7, i64 %195
  %197 = bitcast float* %196 to <4 x float>*
  %wide.load61 = load <4 x float>, <4 x float>* %197, align 4, !tbaa !1471
  %198 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat63, <4 x float> %wide.load61, <4 x float> %194)
  %199 = add nuw nsw i64 %index, 480
  %200 = getelementptr inbounds float, float* %7, i64 %199
  %201 = bitcast float* %200 to <4 x float>*
  %wide.load64 = load <4 x float>, <4 x float>* %201, align 4, !tbaa !1471
  %202 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat66, <4 x float> %wide.load64, <4 x float> %198)
  %203 = add nuw nsw i64 %index, 512
  %204 = getelementptr inbounds float, float* %7, i64 %203
  %205 = bitcast float* %204 to <4 x float>*
  %wide.load67 = load <4 x float>, <4 x float>* %205, align 4, !tbaa !1471
  %206 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat69, <4 x float> %wide.load67, <4 x float> %202)
  %207 = add nuw nsw i64 %index, 544
  %208 = getelementptr inbounds float, float* %7, i64 %207
  %209 = bitcast float* %208 to <4 x float>*
  %wide.load70 = load <4 x float>, <4 x float>* %209, align 4, !tbaa !1471
  %210 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat72, <4 x float> %wide.load70, <4 x float> %206)
  %211 = add nuw nsw i64 %index, 576
  %212 = getelementptr inbounds float, float* %7, i64 %211
  %213 = bitcast float* %212 to <4 x float>*
  %wide.load73 = load <4 x float>, <4 x float>* %213, align 4, !tbaa !1471
  %214 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat75, <4 x float> %wide.load73, <4 x float> %210)
  %215 = add nuw nsw i64 %index, 608
  %216 = getelementptr inbounds float, float* %7, i64 %215
  %217 = bitcast float* %216 to <4 x float>*
  %wide.load76 = load <4 x float>, <4 x float>* %217, align 4, !tbaa !1471
  %218 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat78, <4 x float> %wide.load76, <4 x float> %214)
  %219 = add nuw nsw i64 %index, 640
  %220 = getelementptr inbounds float, float* %7, i64 %219
  %221 = bitcast float* %220 to <4 x float>*
  %wide.load79 = load <4 x float>, <4 x float>* %221, align 4, !tbaa !1471
  %222 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat81, <4 x float> %wide.load79, <4 x float> %218)
  %223 = add nuw nsw i64 %index, 672
  %224 = getelementptr inbounds float, float* %7, i64 %223
  %225 = bitcast float* %224 to <4 x float>*
  %wide.load82 = load <4 x float>, <4 x float>* %225, align 4, !tbaa !1471
  %226 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat84, <4 x float> %wide.load82, <4 x float> %222)
  %227 = add nuw nsw i64 %index, 704
  %228 = getelementptr inbounds float, float* %7, i64 %227
  %229 = bitcast float* %228 to <4 x float>*
  %wide.load85 = load <4 x float>, <4 x float>* %229, align 4, !tbaa !1471
  %230 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat87, <4 x float> %wide.load85, <4 x float> %226)
  %231 = add nuw nsw i64 %index, 736
  %232 = getelementptr inbounds float, float* %7, i64 %231
  %233 = bitcast float* %232 to <4 x float>*
  %wide.load88 = load <4 x float>, <4 x float>* %233, align 4, !tbaa !1471
  %234 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat90, <4 x float> %wide.load88, <4 x float> %230)
  %235 = add nuw nsw i64 %index, 768
  %236 = getelementptr inbounds float, float* %7, i64 %235
  %237 = bitcast float* %236 to <4 x float>*
  %wide.load91 = load <4 x float>, <4 x float>* %237, align 4, !tbaa !1471
  %238 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat93, <4 x float> %wide.load91, <4 x float> %234)
  %239 = add nuw nsw i64 %index, 800
  %240 = getelementptr inbounds float, float* %7, i64 %239
  %241 = bitcast float* %240 to <4 x float>*
  %wide.load94 = load <4 x float>, <4 x float>* %241, align 4, !tbaa !1471
  %242 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat96, <4 x float> %wide.load94, <4 x float> %238)
  %243 = add nuw nsw i64 %index, 832
  %244 = getelementptr inbounds float, float* %7, i64 %243
  %245 = bitcast float* %244 to <4 x float>*
  %wide.load97 = load <4 x float>, <4 x float>* %245, align 4, !tbaa !1471
  %246 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %broadcast.splat99, <4 x float> %wide.load97, <4 x float> %242)
  %247 = trunc i64 %index to i32
  %248 = add nsw i32 %32, %247
  %249 = getelementptr inbounds float, float* %13, i64 %index
  %250 = bitcast float* %249 to <4 x float>*
  %wide.load100 = load <4 x float>, <4 x float>* %250, align 4, !tbaa !1474
  %251 = fadd <4 x float> %246, %wide.load100
  %252 = fcmp olt <4 x float> %251, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %253 = select <4 x i1> %252, <4 x float> %251, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %254 = fcmp ogt <4 x float> %253, zeroinitializer
  %255 = select <4 x i1> %254, <4 x float> %253, <4 x float> zeroinitializer
  %256 = sext i32 %248 to i64
  %257 = getelementptr inbounds float, float* %10, i64 %256
  %258 = bitcast float* %257 to <4 x float>*
  store <4 x float> %255, <4 x float>* %258, align 4, !tbaa !1477
  %index.next = add i64 %index, 4
  %259 = icmp eq i64 %index.next, 32
  br i1 %259, label %for_end3, label %vector.body, !prof !335, !llvm.loop !1480

for_end:                                          ; preds = %for_end3, %entry
  ret i32 0

for_end3:                                         ; preds = %vector.body
  %260 = add nsw i32 %26, 1
  %exitcond21.not = icmp eq i32 %260, %21
  br i1 %exitcond21.not, label %for_end, label %for_begin1.preheader, !prof !51
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_12(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([88 x i8], [88 x i8]* @.str.154, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !1481
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !1495
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !1497
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !1500
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.155, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.156, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.157, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.158, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !1502
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !1516
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 56
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.66, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !1518
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 56
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.67, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !1521
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 128
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.68, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !1523
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 401408, i32 7168, i32 128, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.69, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !1535
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 1
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.16, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !1549
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 1
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.17, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !1551
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 128
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.70, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !1554
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 128
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.159, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !1556
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 16384, i32 16384, i32 128, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.160, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !1568
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 128
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.72, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !1582
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !1596
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !1610
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 56
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.82, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !1612
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 56
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.83, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !1615
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 128
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.75, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !1617
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 401408, i32 7168, i32 128, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.84, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_12_compute_(i8* %31, i8* %39, i8* %51, i8* %45, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_12_compute_(i8* noalias align 128 %0, i8* noalias align 128 %1, i8* noalias align 128 %2, i8* noalias align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 1605632, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %call_end, %if_end, %entry
  %merge = phi i32 [ -1, %entry ], [ %13, %if_end ], [ %22, %call_end ]
  ret i32 %merge

if_end:                                           ; preds = %entry
  %8 = alloca %12, align 8
  %9 = getelementptr inbounds %12, %12* %8, i64 0, i32 0
  store i8* %6, i8** %9, align 8
  %10 = getelementptr inbounds %12, %12* %8, i64 0, i32 1
  store i8* %0, i8** %10, align 8
  %11 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %12 = bitcast %12* %8 to i8*
  %13 = call i32 %11(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.161, i8* nonnull %12, i32 0)
  %14 = icmp eq i32 %13, 0
  br i1 %14, label %call_end, label %if_then, !prof !5

call_end:                                         ; preds = %if_end
  %15 = alloca %13, align 8
  %16 = getelementptr inbounds %13, %13* %15, i64 0, i32 0
  store i8* %6, i8** %16, align 8
  %17 = getelementptr inbounds %13, %13* %15, i64 0, i32 1
  store i8* %1, i8** %17, align 8
  %18 = getelementptr inbounds %13, %13* %15, i64 0, i32 2
  store i8* %2, i8** %18, align 8
  %19 = getelementptr inbounds %13, %13* %15, i64 0, i32 3
  store i8* %3, i8** %19, align 8
  %20 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %21 = bitcast %13* %15 to i8*
  %22 = call i32 %20(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.162, i8* nonnull %21, i32 0)
  %23 = icmp eq i32 %22, 0
  br i1 %23, label %call_end2, label %if_then, !prof !5

call_end2:                                        ; preds = %call_end
  %24 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %25 = call i32 %24(i32 1, i32 %4, i8* nonnull %6)
  %.not = icmp ne i32 %25, 0
  %spec.select = sext i1 %.not to i32
  ret i32 %spec.select
}

; Function Attrs: nofree norecurse nounwind
define private i32 @__tvm_parallel_lambda.161(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #2 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %9 = load i32, i32* %8, align 4
  %10 = add nsw i32 %9, 55
  %11 = sdiv i32 %10, %9
  %12 = add nsw i32 %0, 1
  %13 = mul nsw i32 %11, %12
  %14 = icmp slt i32 %13, 56
  %15 = select i1 %14, i32 %13, i32 56
  %16 = mul nsw i32 %11, %0
  %17 = icmp slt i32 %16, 56
  %18 = select i1 %17, i32 %16, i32 56
  %19 = icmp slt i32 %18, %15
  br i1 %19, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %20 = sext i32 %18 to i64
  %wide.trip.count = sext i32 %15 to i64
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end3
  %indvars.iv13 = phi i64 [ %20, %for_begin1.preheader.preheader ], [ %indvars.iv.next14, %for_end3 ]
  %21 = mul nsw i64 %indvars.iv13, 7168
  br label %for_begin4.preheader

for_end:                                          ; preds = %for_end3, %entry
  ret i32 0

for_begin4.preheader:                             ; preds = %for_begin1.preheader, %for_begin4.preheader
  %indvars.iv10 = phi i64 [ 0, %for_begin1.preheader ], [ %indvars.iv.next11, %for_begin4.preheader ]
  %22 = shl nsw i64 %indvars.iv10, 7
  %23 = add nsw i64 %22, %21
  %24 = getelementptr inbounds float, float* %7, i64 %23
  %25 = bitcast float* %24 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %25, align 4, !tbaa !1629
  %26 = getelementptr inbounds float, float* %24, i64 4
  %27 = bitcast float* %26 to <4 x float>*
  %wide.load16 = load <4 x float>, <4 x float>* %27, align 4, !tbaa !1629
  %28 = getelementptr inbounds float, float* %4, i64 %23
  %29 = bitcast float* %28 to <4 x float>*
  store <4 x float> %wide.load, <4 x float>* %29, align 4, !tbaa !1632
  %30 = getelementptr inbounds float, float* %28, i64 4
  %31 = bitcast float* %30 to <4 x float>*
  store <4 x float> %wide.load16, <4 x float>* %31, align 4, !tbaa !1632
  %32 = or i64 %23, 8
  %33 = getelementptr inbounds float, float* %7, i64 %32
  %34 = bitcast float* %33 to <4 x float>*
  %wide.load.1 = load <4 x float>, <4 x float>* %34, align 4, !tbaa !1629
  %35 = getelementptr inbounds float, float* %33, i64 4
  %36 = bitcast float* %35 to <4 x float>*
  %wide.load16.1 = load <4 x float>, <4 x float>* %36, align 4, !tbaa !1629
  %37 = getelementptr inbounds float, float* %4, i64 %32
  %38 = bitcast float* %37 to <4 x float>*
  store <4 x float> %wide.load.1, <4 x float>* %38, align 4, !tbaa !1632
  %39 = getelementptr inbounds float, float* %37, i64 4
  %40 = bitcast float* %39 to <4 x float>*
  store <4 x float> %wide.load16.1, <4 x float>* %40, align 4, !tbaa !1632
  %41 = or i64 %23, 16
  %42 = getelementptr inbounds float, float* %7, i64 %41
  %43 = bitcast float* %42 to <4 x float>*
  %wide.load.2 = load <4 x float>, <4 x float>* %43, align 4, !tbaa !1629
  %44 = getelementptr inbounds float, float* %42, i64 4
  %45 = bitcast float* %44 to <4 x float>*
  %wide.load16.2 = load <4 x float>, <4 x float>* %45, align 4, !tbaa !1629
  %46 = getelementptr inbounds float, float* %4, i64 %41
  %47 = bitcast float* %46 to <4 x float>*
  store <4 x float> %wide.load.2, <4 x float>* %47, align 4, !tbaa !1632
  %48 = getelementptr inbounds float, float* %46, i64 4
  %49 = bitcast float* %48 to <4 x float>*
  store <4 x float> %wide.load16.2, <4 x float>* %49, align 4, !tbaa !1632
  %50 = or i64 %23, 24
  %51 = getelementptr inbounds float, float* %7, i64 %50
  %52 = bitcast float* %51 to <4 x float>*
  %wide.load.3 = load <4 x float>, <4 x float>* %52, align 4, !tbaa !1629
  %53 = getelementptr inbounds float, float* %51, i64 4
  %54 = bitcast float* %53 to <4 x float>*
  %wide.load16.3 = load <4 x float>, <4 x float>* %54, align 4, !tbaa !1629
  %55 = getelementptr inbounds float, float* %4, i64 %50
  %56 = bitcast float* %55 to <4 x float>*
  store <4 x float> %wide.load.3, <4 x float>* %56, align 4, !tbaa !1632
  %57 = getelementptr inbounds float, float* %55, i64 4
  %58 = bitcast float* %57 to <4 x float>*
  store <4 x float> %wide.load16.3, <4 x float>* %58, align 4, !tbaa !1632
  %59 = or i64 %23, 32
  %60 = getelementptr inbounds float, float* %7, i64 %59
  %61 = bitcast float* %60 to <4 x float>*
  %wide.load.4 = load <4 x float>, <4 x float>* %61, align 4, !tbaa !1629
  %62 = getelementptr inbounds float, float* %60, i64 4
  %63 = bitcast float* %62 to <4 x float>*
  %wide.load16.4 = load <4 x float>, <4 x float>* %63, align 4, !tbaa !1629
  %64 = getelementptr inbounds float, float* %4, i64 %59
  %65 = bitcast float* %64 to <4 x float>*
  store <4 x float> %wide.load.4, <4 x float>* %65, align 4, !tbaa !1632
  %66 = getelementptr inbounds float, float* %64, i64 4
  %67 = bitcast float* %66 to <4 x float>*
  store <4 x float> %wide.load16.4, <4 x float>* %67, align 4, !tbaa !1632
  %68 = or i64 %23, 40
  %69 = getelementptr inbounds float, float* %7, i64 %68
  %70 = bitcast float* %69 to <4 x float>*
  %wide.load.5 = load <4 x float>, <4 x float>* %70, align 4, !tbaa !1629
  %71 = getelementptr inbounds float, float* %69, i64 4
  %72 = bitcast float* %71 to <4 x float>*
  %wide.load16.5 = load <4 x float>, <4 x float>* %72, align 4, !tbaa !1629
  %73 = getelementptr inbounds float, float* %4, i64 %68
  %74 = bitcast float* %73 to <4 x float>*
  store <4 x float> %wide.load.5, <4 x float>* %74, align 4, !tbaa !1632
  %75 = getelementptr inbounds float, float* %73, i64 4
  %76 = bitcast float* %75 to <4 x float>*
  store <4 x float> %wide.load16.5, <4 x float>* %76, align 4, !tbaa !1632
  %77 = or i64 %23, 48
  %78 = getelementptr inbounds float, float* %7, i64 %77
  %79 = bitcast float* %78 to <4 x float>*
  %wide.load.6 = load <4 x float>, <4 x float>* %79, align 4, !tbaa !1629
  %80 = getelementptr inbounds float, float* %78, i64 4
  %81 = bitcast float* %80 to <4 x float>*
  %wide.load16.6 = load <4 x float>, <4 x float>* %81, align 4, !tbaa !1629
  %82 = getelementptr inbounds float, float* %4, i64 %77
  %83 = bitcast float* %82 to <4 x float>*
  store <4 x float> %wide.load.6, <4 x float>* %83, align 4, !tbaa !1632
  %84 = getelementptr inbounds float, float* %82, i64 4
  %85 = bitcast float* %84 to <4 x float>*
  store <4 x float> %wide.load16.6, <4 x float>* %85, align 4, !tbaa !1632
  %86 = or i64 %23, 56
  %87 = getelementptr inbounds float, float* %7, i64 %86
  %88 = bitcast float* %87 to <4 x float>*
  %wide.load.7 = load <4 x float>, <4 x float>* %88, align 4, !tbaa !1629
  %89 = getelementptr inbounds float, float* %87, i64 4
  %90 = bitcast float* %89 to <4 x float>*
  %wide.load16.7 = load <4 x float>, <4 x float>* %90, align 4, !tbaa !1629
  %91 = getelementptr inbounds float, float* %4, i64 %86
  %92 = bitcast float* %91 to <4 x float>*
  store <4 x float> %wide.load.7, <4 x float>* %92, align 4, !tbaa !1632
  %93 = getelementptr inbounds float, float* %91, i64 4
  %94 = bitcast float* %93 to <4 x float>*
  store <4 x float> %wide.load16.7, <4 x float>* %94, align 4, !tbaa !1632
  %95 = or i64 %23, 64
  %96 = getelementptr inbounds float, float* %7, i64 %95
  %97 = bitcast float* %96 to <4 x float>*
  %wide.load.8 = load <4 x float>, <4 x float>* %97, align 4, !tbaa !1629
  %98 = getelementptr inbounds float, float* %96, i64 4
  %99 = bitcast float* %98 to <4 x float>*
  %wide.load16.8 = load <4 x float>, <4 x float>* %99, align 4, !tbaa !1629
  %100 = getelementptr inbounds float, float* %4, i64 %95
  %101 = bitcast float* %100 to <4 x float>*
  store <4 x float> %wide.load.8, <4 x float>* %101, align 4, !tbaa !1632
  %102 = getelementptr inbounds float, float* %100, i64 4
  %103 = bitcast float* %102 to <4 x float>*
  store <4 x float> %wide.load16.8, <4 x float>* %103, align 4, !tbaa !1632
  %104 = or i64 %23, 72
  %105 = getelementptr inbounds float, float* %7, i64 %104
  %106 = bitcast float* %105 to <4 x float>*
  %wide.load.9 = load <4 x float>, <4 x float>* %106, align 4, !tbaa !1629
  %107 = getelementptr inbounds float, float* %105, i64 4
  %108 = bitcast float* %107 to <4 x float>*
  %wide.load16.9 = load <4 x float>, <4 x float>* %108, align 4, !tbaa !1629
  %109 = getelementptr inbounds float, float* %4, i64 %104
  %110 = bitcast float* %109 to <4 x float>*
  store <4 x float> %wide.load.9, <4 x float>* %110, align 4, !tbaa !1632
  %111 = getelementptr inbounds float, float* %109, i64 4
  %112 = bitcast float* %111 to <4 x float>*
  store <4 x float> %wide.load16.9, <4 x float>* %112, align 4, !tbaa !1632
  %113 = or i64 %23, 80
  %114 = getelementptr inbounds float, float* %7, i64 %113
  %115 = bitcast float* %114 to <4 x float>*
  %wide.load.10 = load <4 x float>, <4 x float>* %115, align 4, !tbaa !1629
  %116 = getelementptr inbounds float, float* %114, i64 4
  %117 = bitcast float* %116 to <4 x float>*
  %wide.load16.10 = load <4 x float>, <4 x float>* %117, align 4, !tbaa !1629
  %118 = getelementptr inbounds float, float* %4, i64 %113
  %119 = bitcast float* %118 to <4 x float>*
  store <4 x float> %wide.load.10, <4 x float>* %119, align 4, !tbaa !1632
  %120 = getelementptr inbounds float, float* %118, i64 4
  %121 = bitcast float* %120 to <4 x float>*
  store <4 x float> %wide.load16.10, <4 x float>* %121, align 4, !tbaa !1632
  %122 = or i64 %23, 88
  %123 = getelementptr inbounds float, float* %7, i64 %122
  %124 = bitcast float* %123 to <4 x float>*
  %wide.load.11 = load <4 x float>, <4 x float>* %124, align 4, !tbaa !1629
  %125 = getelementptr inbounds float, float* %123, i64 4
  %126 = bitcast float* %125 to <4 x float>*
  %wide.load16.11 = load <4 x float>, <4 x float>* %126, align 4, !tbaa !1629
  %127 = getelementptr inbounds float, float* %4, i64 %122
  %128 = bitcast float* %127 to <4 x float>*
  store <4 x float> %wide.load.11, <4 x float>* %128, align 4, !tbaa !1632
  %129 = getelementptr inbounds float, float* %127, i64 4
  %130 = bitcast float* %129 to <4 x float>*
  store <4 x float> %wide.load16.11, <4 x float>* %130, align 4, !tbaa !1632
  %131 = or i64 %23, 96
  %132 = getelementptr inbounds float, float* %7, i64 %131
  %133 = bitcast float* %132 to <4 x float>*
  %wide.load.12 = load <4 x float>, <4 x float>* %133, align 4, !tbaa !1629
  %134 = getelementptr inbounds float, float* %132, i64 4
  %135 = bitcast float* %134 to <4 x float>*
  %wide.load16.12 = load <4 x float>, <4 x float>* %135, align 4, !tbaa !1629
  %136 = getelementptr inbounds float, float* %4, i64 %131
  %137 = bitcast float* %136 to <4 x float>*
  store <4 x float> %wide.load.12, <4 x float>* %137, align 4, !tbaa !1632
  %138 = getelementptr inbounds float, float* %136, i64 4
  %139 = bitcast float* %138 to <4 x float>*
  store <4 x float> %wide.load16.12, <4 x float>* %139, align 4, !tbaa !1632
  %140 = or i64 %23, 104
  %141 = getelementptr inbounds float, float* %7, i64 %140
  %142 = bitcast float* %141 to <4 x float>*
  %wide.load.13 = load <4 x float>, <4 x float>* %142, align 4, !tbaa !1629
  %143 = getelementptr inbounds float, float* %141, i64 4
  %144 = bitcast float* %143 to <4 x float>*
  %wide.load16.13 = load <4 x float>, <4 x float>* %144, align 4, !tbaa !1629
  %145 = getelementptr inbounds float, float* %4, i64 %140
  %146 = bitcast float* %145 to <4 x float>*
  store <4 x float> %wide.load.13, <4 x float>* %146, align 4, !tbaa !1632
  %147 = getelementptr inbounds float, float* %145, i64 4
  %148 = bitcast float* %147 to <4 x float>*
  store <4 x float> %wide.load16.13, <4 x float>* %148, align 4, !tbaa !1632
  %149 = or i64 %23, 112
  %150 = getelementptr inbounds float, float* %7, i64 %149
  %151 = bitcast float* %150 to <4 x float>*
  %wide.load.14 = load <4 x float>, <4 x float>* %151, align 4, !tbaa !1629
  %152 = getelementptr inbounds float, float* %150, i64 4
  %153 = bitcast float* %152 to <4 x float>*
  %wide.load16.14 = load <4 x float>, <4 x float>* %153, align 4, !tbaa !1629
  %154 = getelementptr inbounds float, float* %4, i64 %149
  %155 = bitcast float* %154 to <4 x float>*
  store <4 x float> %wide.load.14, <4 x float>* %155, align 4, !tbaa !1632
  %156 = getelementptr inbounds float, float* %154, i64 4
  %157 = bitcast float* %156 to <4 x float>*
  store <4 x float> %wide.load16.14, <4 x float>* %157, align 4, !tbaa !1632
  %158 = or i64 %23, 120
  %159 = getelementptr inbounds float, float* %7, i64 %158
  %160 = bitcast float* %159 to <4 x float>*
  %wide.load.15 = load <4 x float>, <4 x float>* %160, align 4, !tbaa !1629
  %161 = getelementptr inbounds float, float* %159, i64 4
  %162 = bitcast float* %161 to <4 x float>*
  %wide.load16.15 = load <4 x float>, <4 x float>* %162, align 4, !tbaa !1629
  %163 = getelementptr inbounds float, float* %4, i64 %158
  %164 = bitcast float* %163 to <4 x float>*
  store <4 x float> %wide.load.15, <4 x float>* %164, align 4, !tbaa !1632
  %165 = getelementptr inbounds float, float* %163, i64 4
  %166 = bitcast float* %165 to <4 x float>*
  store <4 x float> %wide.load16.15, <4 x float>* %166, align 4, !tbaa !1632
  %indvars.iv.next11 = add nuw nsw i64 %indvars.iv10, 1
  %exitcond12.not = icmp eq i64 %indvars.iv.next11, 56
  br i1 %exitcond12.not, label %for_end3, label %for_begin4.preheader, !prof !51

for_end3:                                         ; preds = %for_begin4.preheader
  %indvars.iv.next14 = add nsw i64 %indvars.iv13, 1
  %exitcond15.not = icmp eq i64 %indvars.iv.next14, %wide.trip.count
  br i1 %exitcond15.not, label %for_end, label %for_begin1.preheader, !prof !51
}

; Function Attrs: nofree nounwind
define private i32 @__tvm_parallel_lambda.162(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #3 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds i8, i8* %2, i64 16
  %9 = bitcast i8* %8 to float**
  %10 = load float*, float** %9, align 8
  %11 = getelementptr inbounds i8, i8* %2, i64 24
  %12 = bitcast i8* %11 to float**
  %13 = load float*, float** %12, align 8
  %14 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %15 = load i32, i32* %14, align 4
  %16 = add nsw i32 %15, 3135
  %17 = sdiv i32 %16, %15
  %18 = add nsw i32 %0, 1
  %19 = mul nsw i32 %17, %18
  %20 = icmp slt i32 %19, 3136
  %21 = select i1 %20, i32 %19, i32 3136
  %22 = mul nsw i32 %17, %0
  %23 = icmp slt i32 %22, 3136
  %24 = select i1 %23, i32 %22, i32 3136
  %25 = icmp slt i32 %24, %21
  br i1 %25, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %26 = sext i32 %24 to i64
  %wide.trip.count = sext i32 %21 to i64
  %27 = bitcast float* %13 to <64 x float>*
  %28 = load <64 x float>, <64 x float>* %27, align 128, !tbaa !1635
  %29 = getelementptr inbounds float, float* %13, i64 64
  %30 = bitcast float* %29 to <64 x float>*
  %31 = load <64 x float>, <64 x float>* %30, align 128, !tbaa !1635
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end6.1
  %indvars.iv13 = phi i64 [ %26, %for_begin1.preheader.preheader ], [ %indvars.iv.next14, %for_end6.1 ]
  %32 = phi i32 [ %24, %for_begin1.preheader.preheader ], [ %92, %for_end6.1 ]
  %33 = shl nsw i32 %32, 7
  %34 = sext i32 %33 to i64
  br label %for_body5

for_end:                                          ; preds = %for_end6.1, %entry
  ret i32 0

for_body5:                                        ; preds = %for_body5, %for_begin1.preheader
  %indvars.iv = phi i64 [ 0, %for_begin1.preheader ], [ %indvars.iv.next.121, %for_body5 ]
  %.010 = phi <64 x float> [ zeroinitializer, %for_begin1.preheader ], [ %54, %for_body5 ]
  %35 = add nsw i64 %indvars.iv, %34
  %36 = getelementptr inbounds float, float* %4, i64 %35
  %37 = load float, float* %36, align 4, !tbaa !1632
  %38 = insertelement <64 x float> undef, float %37, i32 0
  %39 = shufflevector <64 x float> %38, <64 x float> undef, <64 x i32> zeroinitializer
  %40 = shl nuw nsw i64 %indvars.iv, 7
  %41 = getelementptr inbounds float, float* %7, i64 %40
  %42 = bitcast float* %41 to <64 x float>*
  %43 = load <64 x float>, <64 x float>* %42, align 128, !tbaa !1638
  %44 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %39, <64 x float> %43, <64 x float> %.010)
  %indvars.iv.next = or i64 %indvars.iv, 1
  %45 = add nsw i64 %indvars.iv.next, %34
  %46 = getelementptr inbounds float, float* %4, i64 %45
  %47 = load float, float* %46, align 4, !tbaa !1632
  %48 = insertelement <64 x float> undef, float %47, i32 0
  %49 = shufflevector <64 x float> %48, <64 x float> undef, <64 x i32> zeroinitializer
  %50 = shl nuw nsw i64 %indvars.iv.next, 7
  %51 = getelementptr inbounds float, float* %7, i64 %50
  %52 = bitcast float* %51 to <64 x float>*
  %53 = load <64 x float>, <64 x float>* %52, align 128, !tbaa !1638
  %54 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %49, <64 x float> %53, <64 x float> %44)
  %indvars.iv.next.121 = add nuw nsw i64 %indvars.iv, 2
  %exitcond.not.1 = icmp eq i64 %indvars.iv.next.121, 128
  br i1 %exitcond.not.1, label %for_end6, label %for_body5, !prof !51

for_end6:                                         ; preds = %for_body5
  %55 = fadd <64 x float> %54, %28
  %56 = fcmp olt <64 x float> %55, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %57 = select <64 x i1> %56, <64 x float> %55, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %58 = fcmp ogt <64 x float> %57, zeroinitializer
  %59 = select <64 x i1> %58, <64 x float> %57, <64 x float> zeroinitializer
  %60 = getelementptr inbounds float, float* %10, i64 %34
  %61 = bitcast float* %60 to <64 x float>*
  store <64 x float> %59, <64 x float>* %61, align 128, !tbaa !1641
  br label %for_body5.1

for_body5.1:                                      ; preds = %for_body5.1, %for_end6
  %indvars.iv.1 = phi i64 [ 0, %for_end6 ], [ %indvars.iv.next.1.1, %for_body5.1 ]
  %.010.1 = phi <64 x float> [ zeroinitializer, %for_end6 ], [ %83, %for_body5.1 ]
  %62 = add nsw i64 %indvars.iv.1, %34
  %63 = getelementptr inbounds float, float* %4, i64 %62
  %64 = load float, float* %63, align 4, !tbaa !1632
  %65 = insertelement <64 x float> undef, float %64, i32 0
  %66 = shufflevector <64 x float> %65, <64 x float> undef, <64 x i32> zeroinitializer
  %67 = shl nuw nsw i64 %indvars.iv.1, 7
  %68 = or i64 %67, 64
  %69 = getelementptr inbounds float, float* %7, i64 %68
  %70 = bitcast float* %69 to <64 x float>*
  %71 = load <64 x float>, <64 x float>* %70, align 128, !tbaa !1638
  %72 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %66, <64 x float> %71, <64 x float> %.010.1)
  %indvars.iv.next.1 = or i64 %indvars.iv.1, 1
  %73 = add nsw i64 %indvars.iv.next.1, %34
  %74 = getelementptr inbounds float, float* %4, i64 %73
  %75 = load float, float* %74, align 4, !tbaa !1632
  %76 = insertelement <64 x float> undef, float %75, i32 0
  %77 = shufflevector <64 x float> %76, <64 x float> undef, <64 x i32> zeroinitializer
  %78 = shl nuw nsw i64 %indvars.iv.next.1, 7
  %79 = or i64 %78, 64
  %80 = getelementptr inbounds float, float* %7, i64 %79
  %81 = bitcast float* %80 to <64 x float>*
  %82 = load <64 x float>, <64 x float>* %81, align 128, !tbaa !1638
  %83 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %77, <64 x float> %82, <64 x float> %72)
  %indvars.iv.next.1.1 = add nuw nsw i64 %indvars.iv.1, 2
  %exitcond.1.not.1 = icmp eq i64 %indvars.iv.next.1.1, 128
  br i1 %exitcond.1.not.1, label %for_end6.1, label %for_body5.1, !prof !51

for_end6.1:                                       ; preds = %for_body5.1
  %84 = or i64 %34, 64
  %85 = fadd <64 x float> %83, %31
  %86 = fcmp olt <64 x float> %85, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %87 = select <64 x i1> %86, <64 x float> %85, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %88 = fcmp ogt <64 x float> %87, zeroinitializer
  %89 = select <64 x i1> %88, <64 x float> %87, <64 x float> zeroinitializer
  %90 = getelementptr inbounds float, float* %10, i64 %84
  %91 = bitcast float* %90 to <64 x float>*
  store <64 x float> %89, <64 x float>* %91, align 128, !tbaa !1641
  %indvars.iv.next14 = add nsw i64 %indvars.iv13, 1
  %92 = add nsw i32 %32, 1
  %exitcond15.not = icmp eq i64 %indvars.iv.next14, %wide.trip.count
  br i1 %exitcond15.not, label %for_end, label %for_begin1.preheader, !prof !51
}

define dllexport i32 @fused_reshape(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 2
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([66 x i8], [66 x i8]* @.str.163, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !1644
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !1658
  %18 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %19 = load i8*, i8** %18, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %19, i64 128) ]
  %20 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %21 = load i64*, i64** %20, align 8
  %22 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %23 = load i64*, i64** %22, align 8
  %24 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %25 = load i32, i32* %24, align 4
  %26 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %27 = load i8*, i8** %26, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %27, i64 128) ]
  %28 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %29 = load i64*, i64** %28, align 8
  %30 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %31 = load i64*, i64** %30, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %32 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %32(i8* getelementptr inbounds ([141 x i8], [141 x i8]* @.str.164, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %33 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %33(i8* getelementptr inbounds ([141 x i8], [141 x i8]* @.str.165, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %35 = load i32, i32* %34, align 4
  %36 = icmp eq i32 %35, 4
  br i1 %36, label %assert_end8, label %assert_fail5, !prof !5

assert_fail5:                                     ; preds = %assert_end4
  %37 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %37(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end4
  %38 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %39 = load i16, i16* %38, align 2
  %40 = icmp eq i16 %39, 1
  %41 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %42 = load i8, i8* %41, align 1
  %43 = icmp eq i8 %42, 32
  %44 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %45 = load i8, i8* %44, align 1
  %46 = icmp eq i8 %45, 2
  %47 = and i1 %43, %46
  %48 = and i1 %40, %47
  br i1 %48, label %assert_end10, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %49 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %49(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end10:                                     ; preds = %assert_end8
  %50 = load i64, i64* %21, align 8, !tbaa !1660
  %51 = trunc i64 %50 to i32
  %52 = icmp eq i32 %51, 1
  br i1 %52, label %assert_end12, label %assert_fail11, !prof !5

assert_fail11:                                    ; preds = %assert_end10
  %53 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %53(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end10
  %54 = getelementptr inbounds i64, i64* %21, i64 1
  %55 = load i64, i64* %54, align 8, !tbaa !1674
  %56 = trunc i64 %55 to i32
  %57 = icmp eq i32 %56, 1
  br i1 %57, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.166, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %59 = getelementptr inbounds i64, i64* %21, i64 2
  %60 = load i64, i64* %59, align 8, !tbaa !1676
  %61 = trunc i64 %60 to i32
  %62 = icmp eq i32 %61, 1
  br i1 %62, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.167, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %64 = getelementptr inbounds i64, i64* %21, i64 3
  %65 = load i64, i64* %64, align 8, !tbaa !1679
  %66 = trunc i64 %65 to i32
  %67 = icmp eq i32 %66, 1001
  br i1 %67, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %68 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %68(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.168, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %.not = icmp eq i64* %23, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end18
  %69 = bitcast i64* %23 to <4 x i64>*
  %70 = load <4 x i64>, <4 x i64>* %69, align 8, !tbaa !1681
  %71 = trunc <4 x i64> %70 to <4 x i32>
  %72 = icmp eq <4 x i32> %71, <i32 1001, i32 1001, i32 1001, i32 1>
  %73 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %72)
  br i1 %73, label %if_end, label %assert_fail19, !prof !5

if_end:                                           ; preds = %if_then, %assert_end18
  %74 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %75 = load i64, i64* %74, align 8
  %76 = icmp eq i64 %75, 0
  br i1 %76, label %assert_end22, label %assert_fail21, !prof !5

assert_fail19:                                    ; preds = %if_then
  %77 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %77(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.169, i64 0, i64 0))
  ret i32 -1

assert_fail21:                                    ; preds = %if_end
  %78 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %78(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %if_end
  %79 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %80 = load i32, i32* %79, align 4
  %81 = icmp eq i32 %80, 1
  br i1 %81, label %assert_end24, label %assert_fail23, !prof !5

assert_fail23:                                    ; preds = %assert_end22
  %82 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %82(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end24:                                     ; preds = %assert_end22
  %83 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %84 = load i32, i32* %83, align 4
  %85 = icmp eq i32 %84, 2
  br i1 %85, label %assert_end28, label %assert_fail25, !prof !5

assert_fail25:                                    ; preds = %assert_end24
  %86 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %86(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.108, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end24
  %87 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %88 = load i16, i16* %87, align 2
  %89 = icmp eq i16 %88, 1
  %90 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %91 = load i8, i8* %90, align 1
  %92 = icmp eq i8 %91, 32
  %93 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %94 = load i8, i8* %93, align 1
  %95 = icmp eq i8 %94, 2
  %96 = and i1 %92, %95
  %97 = and i1 %89, %96
  br i1 %97, label %assert_end30, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %98 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %98(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end30:                                     ; preds = %assert_end28
  %99 = load i64, i64* %29, align 8, !tbaa !1693
  %100 = trunc i64 %99 to i32
  %101 = icmp eq i32 %100, 1
  br i1 %101, label %assert_end32, label %assert_fail31, !prof !5

assert_fail31:                                    ; preds = %assert_end30
  %102 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %102(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.16, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end30
  %103 = getelementptr inbounds i64, i64* %29, i64 1
  %104 = load i64, i64* %103, align 8, !tbaa !1707
  %105 = trunc i64 %104 to i32
  %106 = icmp eq i32 %105, 1001
  br i1 %106, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %107 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %107(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.109, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %.not45 = icmp eq i64* %31, null
  br i1 %.not45, label %if_end36, label %if_then35, !prof !51

if_then35:                                        ; preds = %assert_end34
  %108 = load i64, i64* %31, align 8, !tbaa !1709
  %109 = trunc i64 %108 to i32
  %110 = icmp eq i32 %109, 1001
  %111 = getelementptr inbounds i64, i64* %31, i64 1
  %112 = load i64, i64* %111, align 8, !tbaa !1723
  %113 = trunc i64 %112 to i32
  %114 = icmp eq i32 %113, 1
  %115 = and i1 %110, %114
  br i1 %115, label %if_end36, label %assert_fail37, !prof !5

if_end36:                                         ; preds = %if_then35, %assert_end34
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %117 = load i64, i64* %116, align 8
  %118 = icmp eq i64 %117, 0
  br i1 %118, label %assert_end40, label %assert_fail39, !prof !5

assert_fail37:                                    ; preds = %if_then35
  %119 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %119(i8* getelementptr inbounds ([125 x i8], [125 x i8]* @.str.110, i64 0, i64 0))
  ret i32 -1

assert_fail39:                                    ; preds = %if_end36
  %120 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %120(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %if_end36
  %121 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %122 = load i32, i32* %121, align 4
  %123 = icmp eq i32 %122, 1
  br i1 %123, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %125 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %126 = load i32, i32* %125, align 4
  %127 = icmp eq i32 %25, %126
  br i1 %127, label %assert_end44, label %assert_fail43, !prof !5

assert_fail43:                                    ; preds = %assert_end42
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end44:                                     ; preds = %assert_end42
  tail call fastcc void @fused_reshape_compute_(i8* %27, i8* %19)
  ret i32 0
}

; Function Attrs: nofree noinline norecurse nounwind
define private fastcc void @fused_reshape_compute_(i8* noalias nocapture align 128 %0, i8* noalias nocapture readonly align 128 %1) unnamed_addr #5 {
entry:
  %2 = bitcast i8* %1 to float*
  %3 = bitcast i8* %0 to float*
  br label %if_end.8

for_end:                                          ; preds = %if_end.15
  ret void

if_end.8:                                         ; preds = %entry, %if_end.15
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %if_end.15 ]
  %4 = shl nsw i64 %indvars.iv, 4
  %5 = getelementptr inbounds float, float* %2, i64 %4
  %6 = getelementptr inbounds float, float* %3, i64 %4
  %7 = bitcast float* %5 to <4 x float>*
  %8 = load <4 x float>, <4 x float>* %7, align 64, !tbaa !1725
  %9 = bitcast float* %6 to <4 x float>*
  store <4 x float> %8, <4 x float>* %9, align 64, !tbaa !1728
  %10 = or i64 %4, 4
  %11 = getelementptr inbounds float, float* %2, i64 %10
  %12 = getelementptr inbounds float, float* %3, i64 %10
  %13 = bitcast float* %11 to <4 x float>*
  %14 = load <4 x float>, <4 x float>* %13, align 16, !tbaa !1725
  %15 = bitcast float* %12 to <4 x float>*
  store <4 x float> %14, <4 x float>* %15, align 16, !tbaa !1728
  %16 = or i64 %4, 8
  %17 = getelementptr inbounds float, float* %2, i64 %16
  %18 = load float, float* %17, align 32, !tbaa !1725
  %19 = getelementptr inbounds float, float* %3, i64 %16
  store float %18, float* %19, align 32, !tbaa !1728
  %20 = or i64 %4, 9
  %21 = icmp ult i64 %20, 1001
  br i1 %21, label %if_then.9, label %if_end.9, !prof !5

if_then.9:                                        ; preds = %if_end.8
  %22 = getelementptr inbounds float, float* %2, i64 %20
  %23 = load float, float* %22, align 4, !tbaa !1725
  %24 = getelementptr inbounds float, float* %3, i64 %20
  store float %23, float* %24, align 4, !tbaa !1728
  br label %if_end.9

if_end.9:                                         ; preds = %if_then.9, %if_end.8
  %25 = or i64 %4, 10
  %26 = icmp ult i64 %25, 1001
  br i1 %26, label %if_then.10, label %if_end.10, !prof !5

if_then.10:                                       ; preds = %if_end.9
  %27 = getelementptr inbounds float, float* %2, i64 %25
  %28 = load float, float* %27, align 8, !tbaa !1725
  %29 = getelementptr inbounds float, float* %3, i64 %25
  store float %28, float* %29, align 8, !tbaa !1728
  br label %if_end.10

if_end.10:                                        ; preds = %if_then.10, %if_end.9
  %30 = or i64 %4, 11
  %31 = icmp ult i64 %30, 1001
  br i1 %31, label %if_then.11, label %if_end.11, !prof !5

if_then.11:                                       ; preds = %if_end.10
  %32 = getelementptr inbounds float, float* %2, i64 %30
  %33 = load float, float* %32, align 4, !tbaa !1725
  %34 = getelementptr inbounds float, float* %3, i64 %30
  store float %33, float* %34, align 4, !tbaa !1728
  br label %if_end.11

if_end.11:                                        ; preds = %if_then.11, %if_end.10
  %35 = or i64 %4, 12
  %36 = icmp ult i64 %35, 1001
  br i1 %36, label %if_then.12, label %if_end.12, !prof !5

if_then.12:                                       ; preds = %if_end.11
  %37 = getelementptr inbounds float, float* %2, i64 %35
  %38 = load float, float* %37, align 16, !tbaa !1725
  %39 = getelementptr inbounds float, float* %3, i64 %35
  store float %38, float* %39, align 16, !tbaa !1728
  br label %if_end.12

if_end.12:                                        ; preds = %if_then.12, %if_end.11
  %40 = or i64 %4, 13
  %41 = icmp ult i64 %40, 1001
  br i1 %41, label %if_then.13, label %if_end.13, !prof !5

if_then.13:                                       ; preds = %if_end.12
  %42 = getelementptr inbounds float, float* %2, i64 %40
  %43 = load float, float* %42, align 4, !tbaa !1725
  %44 = getelementptr inbounds float, float* %3, i64 %40
  store float %43, float* %44, align 4, !tbaa !1728
  br label %if_end.13

if_end.13:                                        ; preds = %if_then.13, %if_end.12
  %45 = or i64 %4, 14
  %46 = icmp ult i64 %45, 1001
  br i1 %46, label %if_then.14, label %if_end.14, !prof !5

if_then.14:                                       ; preds = %if_end.13
  %47 = getelementptr inbounds float, float* %2, i64 %45
  %48 = load float, float* %47, align 8, !tbaa !1725
  %49 = getelementptr inbounds float, float* %3, i64 %45
  store float %48, float* %49, align 8, !tbaa !1728
  br label %if_end.14

if_end.14:                                        ; preds = %if_then.14, %if_end.13
  %50 = or i64 %4, 15
  %51 = icmp ult i64 %50, 1001
  br i1 %51, label %if_then.15, label %if_end.15, !prof !5

if_then.15:                                       ; preds = %if_end.14
  %52 = getelementptr inbounds float, float* %2, i64 %50
  %53 = load float, float* %52, align 4, !tbaa !1725
  %54 = getelementptr inbounds float, float* %3, i64 %50
  store float %53, float* %54, align 4, !tbaa !1728
  br label %if_end.15

if_end.15:                                        ; preds = %if_then.15, %if_end.14
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, 63
  br i1 %exitcond.not, label %for_end, label %if_end.8, !prof !51
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_17(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([88 x i8], [88 x i8]* @.str.170, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !1731
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !1745
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !1747
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !1750
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.171, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.172, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.173, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.174, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !1752
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !1766
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 112
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.8, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !1768
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 112
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.9, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !1771
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 32
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.10, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !1773
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 401408, i32 3584, i32 32, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.11, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !1785
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 3
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.51, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !1799
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 3
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.52, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !1801
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 32
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.18, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !1804
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 1
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.54, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !1806
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 96, i32 32, i32 1, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([194 x i8], [194 x i8]* @.str.175, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !1818
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 32
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.149, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !1832
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !1846
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !1860
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 112
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.34, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !1862
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 112
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.35, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !1865
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 32
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.150, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !1867
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 401408, i32 3584, i32 32, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.151, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_17_compute_(i8* %31, i8* %39, i8* %45, i8* %51, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_17_compute_(i8* noalias nocapture readonly align 128 %0, i8* noalias nocapture readonly align 128 %1, i8* noalias nocapture readonly align 128 %2, i8* noalias nocapture align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 1663488, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %for_end38, %if_end, %entry
  ret i32 -1

if_end:                                           ; preds = %entry
  %8 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %9 = tail call i8* %8(i32 1, i32 %4, i64 1605632, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %9, i64 128) ]
  %10 = icmp eq i8* %9, null
  br i1 %10, label %if_then, label %for_begin4.preheader, !prof !5

for_begin4.preheader:                             ; preds = %if_end, %for_end6
  %indvar = phi i64 [ %indvar.next, %for_end6 ], [ 0, %if_end ]
  %11 = phi i32 [ %25, %for_end6 ], [ 0, %if_end ]
  %12 = mul nuw nsw i64 %indvar, 14592
  %13 = mul nuw nsw i64 %indvar, 14336
  %14 = add nsw i64 %13, -14464
  %.off = add nsw i32 %11, -1
  %15 = icmp ult i32 %.off, 112
  br i1 %15, label %for_begin7.preheader.us, label %for_begin7.preheader.preheader

for_begin7.preheader.preheader:                   ; preds = %for_begin4.preheader
  %scevgep101 = getelementptr i8, i8* %6, i64 %12
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(14592) %scevgep101, i8 0, i64 14592, i1 false)
  br label %for_end6

for_begin7.preheader.us:                          ; preds = %for_begin4.preheader, %for_end9.us
  %indvar102 = phi i64 [ %indvar.next103, %for_end9.us ], [ 0, %for_begin4.preheader ]
  %16 = phi i32 [ %21, %for_end9.us ], [ 0, %for_begin4.preheader ]
  %17 = shl nuw nsw i64 %indvar102, 7
  %18 = add nuw nsw i64 %12, %17
  %scevgep106 = getelementptr i8, i8* %6, i64 %18
  %.off58.us = add nsw i32 %16, -1
  %19 = icmp ult i32 %.off58.us, 112
  br i1 %19, label %for_body8.us.us.preheader, label %for_body8.us61.preheader

for_body8.us61.preheader:                         ; preds = %for_begin7.preheader.us
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(128) %scevgep106, i8 0, i64 128, i1 false)
  br label %for_end9.us

for_body8.us.us.preheader:                        ; preds = %for_begin7.preheader.us
  %20 = add nsw i64 %14, %17
  %scevgep107 = getelementptr i8, i8* %0, i64 %20
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(128) %scevgep106, i8* nonnull align 128 dereferenceable(128) %scevgep107, i64 128, i1 false)
  br label %for_end9.us

for_end9.us:                                      ; preds = %for_body8.us61.preheader, %for_body8.us.us.preheader
  %21 = add nuw nsw i32 %16, 1
  %indvar.next103 = add nuw nsw i64 %indvar102, 1
  %exitcond110.not = icmp eq i64 %indvar.next103, 114
  br i1 %exitcond110.not, label %for_end6, label %for_begin7.preheader.us, !prof !51

for_begin12.preheader:                            ; preds = %for_end6
  %22 = bitcast i8* %9 to float*
  %23 = bitcast i8* %6 to float*
  %24 = bitcast i8* %1 to float*
  br label %for_begin15.preheader

for_end6:                                         ; preds = %for_end9.us, %for_begin7.preheader.preheader
  %25 = add nuw nsw i32 %11, 1
  %indvar.next = add nuw nsw i64 %indvar, 1
  %exitcond111.not = icmp eq i64 %indvar.next, 114
  br i1 %exitcond111.not, label %for_begin12.preheader, label %for_begin4.preheader, !prof !51

for_begin15.preheader:                            ; preds = %for_begin12.preheader, %for_end17
  %indvars.iv94 = phi i64 [ 0, %for_begin12.preheader ], [ %28, %for_end17 ]
  %26 = mul nuw nsw i64 %indvars.iv94, 3584
  %27 = mul nuw nsw i64 %indvars.iv94, 3648
  %28 = add nuw nsw i64 %indvars.iv94, 1
  %29 = mul nuw nsw i64 %28, 3648
  %30 = mul i64 %indvars.iv94, 3648
  %31 = add i64 %30, 7296
  br label %for_begin18.preheader

for_begin27.preheader:                            ; preds = %for_end17
  %32 = bitcast i8* %2 to <4 x float>*
  %33 = load <4 x float>, <4 x float>* %32, align 128, !tbaa !1879
  %34 = getelementptr inbounds i8, i8* %2, i64 16
  %35 = bitcast i8* %34 to <4 x float>*
  %36 = load <4 x float>, <4 x float>* %35, align 16, !tbaa !1879
  %37 = getelementptr inbounds i8, i8* %2, i64 32
  %38 = bitcast i8* %37 to <4 x float>*
  %39 = load <4 x float>, <4 x float>* %38, align 32, !tbaa !1879
  %40 = getelementptr inbounds i8, i8* %2, i64 48
  %41 = bitcast i8* %40 to <4 x float>*
  %42 = load <4 x float>, <4 x float>* %41, align 16, !tbaa !1879
  %43 = getelementptr inbounds i8, i8* %2, i64 64
  %44 = bitcast i8* %43 to <4 x float>*
  %45 = load <4 x float>, <4 x float>* %44, align 64, !tbaa !1879
  %46 = getelementptr inbounds i8, i8* %2, i64 80
  %47 = bitcast i8* %46 to <4 x float>*
  %48 = load <4 x float>, <4 x float>* %47, align 16, !tbaa !1879
  %49 = getelementptr inbounds i8, i8* %2, i64 96
  %50 = bitcast i8* %49 to <4 x float>*
  %51 = load <4 x float>, <4 x float>* %50, align 32, !tbaa !1879
  %52 = getelementptr inbounds i8, i8* %2, i64 112
  %53 = bitcast i8* %52 to <4 x float>*
  %54 = load <4 x float>, <4 x float>* %53, align 16, !tbaa !1879
  br label %for_begin30.preheader

for_begin18.preheader:                            ; preds = %for_begin15.preheader, %for_end20
  %indvars.iv91 = phi i64 [ 0, %for_begin15.preheader ], [ %indvars.iv.next92, %for_end20 ]
  %55 = shl nsw i64 %indvars.iv91, 5
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_begin18.preheader
  %index = phi i64 [ 0, %for_begin18.preheader ], [ %index.next, %vector.body ]
  %56 = add nuw nsw i64 %index, %55
  %57 = add nuw nsw i64 %56, %26
  %58 = getelementptr inbounds float, float* %22, i64 %57
  %59 = add nuw nsw i64 %56, %27
  %60 = getelementptr inbounds float, float* %23, i64 %59
  %61 = bitcast float* %60 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %61, align 16, !tbaa !1882
  %62 = getelementptr inbounds float, float* %24, i64 %index
  %63 = bitcast float* %62 to <4 x float>*
  %wide.load112 = load <4 x float>, <4 x float>* %63, align 16, !tbaa !1885
  %64 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load, <4 x float> %wide.load112, <4 x float> zeroinitializer)
  %65 = add nuw nsw i64 %59, 32
  %66 = getelementptr inbounds float, float* %23, i64 %65
  %67 = bitcast float* %66 to <4 x float>*
  %wide.load113 = load <4 x float>, <4 x float>* %67, align 16, !tbaa !1882
  %68 = add nuw nsw i64 %index, 32
  %69 = getelementptr inbounds float, float* %24, i64 %68
  %70 = bitcast float* %69 to <4 x float>*
  %wide.load114 = load <4 x float>, <4 x float>* %70, align 16, !tbaa !1885
  %71 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load113, <4 x float> %wide.load114, <4 x float> %64)
  %72 = add nuw nsw i64 %59, 64
  %73 = getelementptr inbounds float, float* %23, i64 %72
  %74 = bitcast float* %73 to <4 x float>*
  %wide.load115 = load <4 x float>, <4 x float>* %74, align 16, !tbaa !1882
  %75 = add nuw nsw i64 %index, 64
  %76 = getelementptr inbounds float, float* %24, i64 %75
  %77 = bitcast float* %76 to <4 x float>*
  %wide.load116 = load <4 x float>, <4 x float>* %77, align 16, !tbaa !1885
  %78 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load115, <4 x float> %wide.load116, <4 x float> %71)
  %79 = add nuw nsw i64 %56, %29
  %80 = getelementptr inbounds float, float* %23, i64 %79
  %81 = bitcast float* %80 to <4 x float>*
  %wide.load117 = load <4 x float>, <4 x float>* %81, align 16, !tbaa !1882
  %82 = add nuw nsw i64 %index, 96
  %83 = getelementptr inbounds float, float* %24, i64 %82
  %84 = bitcast float* %83 to <4 x float>*
  %wide.load118 = load <4 x float>, <4 x float>* %84, align 16, !tbaa !1885
  %85 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load117, <4 x float> %wide.load118, <4 x float> %78)
  %86 = add nuw nsw i64 %79, 32
  %87 = getelementptr inbounds float, float* %23, i64 %86
  %88 = bitcast float* %87 to <4 x float>*
  %wide.load119 = load <4 x float>, <4 x float>* %88, align 16, !tbaa !1882
  %89 = add nuw nsw i64 %index, 128
  %90 = getelementptr inbounds float, float* %24, i64 %89
  %91 = bitcast float* %90 to <4 x float>*
  %wide.load120 = load <4 x float>, <4 x float>* %91, align 16, !tbaa !1885
  %92 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load119, <4 x float> %wide.load120, <4 x float> %85)
  %93 = add nuw nsw i64 %79, 64
  %94 = getelementptr inbounds float, float* %23, i64 %93
  %95 = bitcast float* %94 to <4 x float>*
  %wide.load121 = load <4 x float>, <4 x float>* %95, align 16, !tbaa !1882
  %96 = add nuw nsw i64 %index, 160
  %97 = getelementptr inbounds float, float* %24, i64 %96
  %98 = bitcast float* %97 to <4 x float>*
  %wide.load122 = load <4 x float>, <4 x float>* %98, align 16, !tbaa !1885
  %99 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load121, <4 x float> %wide.load122, <4 x float> %92)
  %100 = add nuw nsw i64 %56, %31
  %101 = getelementptr inbounds float, float* %23, i64 %100
  %102 = bitcast float* %101 to <4 x float>*
  %wide.load123 = load <4 x float>, <4 x float>* %102, align 16, !tbaa !1882
  %103 = add nuw nsw i64 %index, 192
  %104 = getelementptr inbounds float, float* %24, i64 %103
  %105 = bitcast float* %104 to <4 x float>*
  %wide.load124 = load <4 x float>, <4 x float>* %105, align 16, !tbaa !1885
  %106 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load123, <4 x float> %wide.load124, <4 x float> %99)
  %107 = add nuw nsw i64 %100, 32
  %108 = getelementptr inbounds float, float* %23, i64 %107
  %109 = bitcast float* %108 to <4 x float>*
  %wide.load125 = load <4 x float>, <4 x float>* %109, align 16, !tbaa !1882
  %110 = add nuw nsw i64 %index, 224
  %111 = getelementptr inbounds float, float* %24, i64 %110
  %112 = bitcast float* %111 to <4 x float>*
  %wide.load126 = load <4 x float>, <4 x float>* %112, align 16, !tbaa !1885
  %113 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load125, <4 x float> %wide.load126, <4 x float> %106)
  %114 = add nuw nsw i64 %100, 64
  %115 = getelementptr inbounds float, float* %23, i64 %114
  %116 = bitcast float* %115 to <4 x float>*
  %wide.load127 = load <4 x float>, <4 x float>* %116, align 16, !tbaa !1882
  %117 = add nuw nsw i64 %index, 256
  %118 = getelementptr inbounds float, float* %24, i64 %117
  %119 = bitcast float* %118 to <4 x float>*
  %wide.load128 = load <4 x float>, <4 x float>* %119, align 16, !tbaa !1885
  %120 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load127, <4 x float> %wide.load128, <4 x float> %113)
  %121 = bitcast float* %58 to <4 x float>*
  store <4 x float> %120, <4 x float>* %121, align 16, !tbaa !1888
  %index.next = add i64 %index, 4
  %122 = icmp eq i64 %index.next, 32
  br i1 %122, label %for_end20, label %vector.body, !prof !335, !llvm.loop !1891

for_end17:                                        ; preds = %for_end20
  %exitcond96.not = icmp eq i64 %28, 112
  br i1 %exitcond96.not, label %for_begin27.preheader, label %for_begin15.preheader, !prof !51

for_end20:                                        ; preds = %vector.body
  %indvars.iv.next92 = add nuw nsw i64 %indvars.iv91, 1
  %exitcond93.not = icmp eq i64 %indvars.iv.next92, 112
  br i1 %exitcond93.not, label %for_end17, label %for_begin18.preheader, !prof !51

for_begin30.preheader:                            ; preds = %for_begin27.preheader, %for_end32
  %indvars.iv77 = phi i64 [ 0, %for_begin27.preheader ], [ %indvars.iv.next78, %for_end32 ]
  %123 = mul nuw nsw i64 %indvars.iv77, 3584
  br label %for_begin33.preheader

for_begin36.preheader:                            ; preds = %for_end32
  %124 = bitcast i8* %3 to float*
  br label %for_begin39.preheader

for_begin33.preheader:                            ; preds = %for_begin30.preheader, %for_begin33.preheader
  %indvars.iv74 = phi i64 [ 0, %for_begin30.preheader ], [ %indvars.iv.next75, %for_begin33.preheader ]
  %125 = shl nsw i64 %indvars.iv74, 5
  %126 = add nuw nsw i64 %125, %123
  %127 = getelementptr inbounds float, float* %22, i64 %126
  %128 = bitcast float* %127 to <4 x float>*
  %129 = load <4 x float>, <4 x float>* %128, align 128, !tbaa !1888
  %130 = fadd <4 x float> %33, %129
  %131 = bitcast float* %127 to <4 x float>*
  store <4 x float> %130, <4 x float>* %131, align 128, !tbaa !1888
  %132 = or i64 %126, 4
  %133 = getelementptr inbounds float, float* %22, i64 %132
  %134 = bitcast float* %133 to <4 x float>*
  %135 = load <4 x float>, <4 x float>* %134, align 16, !tbaa !1888
  %136 = fadd <4 x float> %36, %135
  %137 = bitcast float* %133 to <4 x float>*
  store <4 x float> %136, <4 x float>* %137, align 16, !tbaa !1888
  %138 = or i64 %126, 8
  %139 = getelementptr inbounds float, float* %22, i64 %138
  %140 = bitcast float* %139 to <4 x float>*
  %141 = load <4 x float>, <4 x float>* %140, align 32, !tbaa !1888
  %142 = fadd <4 x float> %39, %141
  %143 = bitcast float* %139 to <4 x float>*
  store <4 x float> %142, <4 x float>* %143, align 32, !tbaa !1888
  %144 = or i64 %126, 12
  %145 = getelementptr inbounds float, float* %22, i64 %144
  %146 = bitcast float* %145 to <4 x float>*
  %147 = load <4 x float>, <4 x float>* %146, align 16, !tbaa !1888
  %148 = fadd <4 x float> %42, %147
  %149 = bitcast float* %145 to <4 x float>*
  store <4 x float> %148, <4 x float>* %149, align 16, !tbaa !1888
  %150 = or i64 %126, 16
  %151 = getelementptr inbounds float, float* %22, i64 %150
  %152 = bitcast float* %151 to <4 x float>*
  %153 = load <4 x float>, <4 x float>* %152, align 64, !tbaa !1888
  %154 = fadd <4 x float> %45, %153
  %155 = bitcast float* %151 to <4 x float>*
  store <4 x float> %154, <4 x float>* %155, align 64, !tbaa !1888
  %156 = or i64 %126, 20
  %157 = getelementptr inbounds float, float* %22, i64 %156
  %158 = bitcast float* %157 to <4 x float>*
  %159 = load <4 x float>, <4 x float>* %158, align 16, !tbaa !1888
  %160 = fadd <4 x float> %48, %159
  %161 = bitcast float* %157 to <4 x float>*
  store <4 x float> %160, <4 x float>* %161, align 16, !tbaa !1888
  %162 = or i64 %126, 24
  %163 = getelementptr inbounds float, float* %22, i64 %162
  %164 = bitcast float* %163 to <4 x float>*
  %165 = load <4 x float>, <4 x float>* %164, align 32, !tbaa !1888
  %166 = fadd <4 x float> %51, %165
  %167 = bitcast float* %163 to <4 x float>*
  store <4 x float> %166, <4 x float>* %167, align 32, !tbaa !1888
  %168 = or i64 %126, 28
  %169 = getelementptr inbounds float, float* %22, i64 %168
  %170 = bitcast float* %169 to <4 x float>*
  %171 = load <4 x float>, <4 x float>* %170, align 16, !tbaa !1888
  %172 = fadd <4 x float> %54, %171
  %173 = bitcast float* %169 to <4 x float>*
  store <4 x float> %172, <4 x float>* %173, align 16, !tbaa !1888
  %indvars.iv.next75 = add nuw nsw i64 %indvars.iv74, 1
  %exitcond76.not = icmp eq i64 %indvars.iv.next75, 112
  br i1 %exitcond76.not, label %for_end32, label %for_begin33.preheader, !prof !51

for_end32:                                        ; preds = %for_begin33.preheader
  %indvars.iv.next78 = add nuw nsw i64 %indvars.iv77, 1
  %exitcond79.not = icmp eq i64 %indvars.iv.next78, 112
  br i1 %exitcond79.not, label %for_begin36.preheader, label %for_begin30.preheader, !prof !51

for_begin39.preheader:                            ; preds = %for_begin36.preheader, %for_end41
  %indvars.iv68 = phi i64 [ 0, %for_begin36.preheader ], [ %indvars.iv.next69, %for_end41 ]
  %174 = mul nuw nsw i64 %indvars.iv68, 3584
  br label %for_begin42.preheader

for_end38:                                        ; preds = %for_end41
  %175 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %176 = tail call i32 %175(i32 1, i32 %4, i8* nonnull %9)
  %.not = icmp eq i32 %176, 0
  br i1 %.not, label %if_end46, label %if_then, !prof !51

for_begin42.preheader:                            ; preds = %for_begin39.preheader, %for_begin42.preheader
  %indvars.iv = phi i64 [ 0, %for_begin39.preheader ], [ %indvars.iv.next, %for_begin42.preheader ]
  %177 = shl nsw i64 %indvars.iv, 5
  %178 = add nuw nsw i64 %177, %174
  %179 = getelementptr inbounds float, float* %22, i64 %178
  %180 = getelementptr inbounds float, float* %124, i64 %178
  %181 = bitcast float* %179 to <4 x float>*
  %182 = load <4 x float>, <4 x float>* %181, align 128, !tbaa !1888
  %183 = fcmp olt <4 x float> %182, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %184 = select <4 x i1> %183, <4 x float> %182, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %185 = fcmp ogt <4 x float> %184, zeroinitializer
  %186 = select <4 x i1> %185, <4 x float> %184, <4 x float> zeroinitializer
  %187 = bitcast float* %180 to <4 x float>*
  store <4 x float> %186, <4 x float>* %187, align 128, !tbaa !1892
  %188 = or i64 %178, 4
  %189 = getelementptr inbounds float, float* %22, i64 %188
  %190 = getelementptr inbounds float, float* %124, i64 %188
  %191 = bitcast float* %189 to <4 x float>*
  %192 = load <4 x float>, <4 x float>* %191, align 16, !tbaa !1888
  %193 = fcmp olt <4 x float> %192, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %194 = select <4 x i1> %193, <4 x float> %192, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %195 = fcmp ogt <4 x float> %194, zeroinitializer
  %196 = select <4 x i1> %195, <4 x float> %194, <4 x float> zeroinitializer
  %197 = bitcast float* %190 to <4 x float>*
  store <4 x float> %196, <4 x float>* %197, align 16, !tbaa !1892
  %198 = or i64 %178, 8
  %199 = getelementptr inbounds float, float* %22, i64 %198
  %200 = getelementptr inbounds float, float* %124, i64 %198
  %201 = bitcast float* %199 to <4 x float>*
  %202 = load <4 x float>, <4 x float>* %201, align 32, !tbaa !1888
  %203 = fcmp olt <4 x float> %202, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %204 = select <4 x i1> %203, <4 x float> %202, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %205 = fcmp ogt <4 x float> %204, zeroinitializer
  %206 = select <4 x i1> %205, <4 x float> %204, <4 x float> zeroinitializer
  %207 = bitcast float* %200 to <4 x float>*
  store <4 x float> %206, <4 x float>* %207, align 32, !tbaa !1892
  %208 = or i64 %178, 12
  %209 = getelementptr inbounds float, float* %22, i64 %208
  %210 = getelementptr inbounds float, float* %124, i64 %208
  %211 = bitcast float* %209 to <4 x float>*
  %212 = load <4 x float>, <4 x float>* %211, align 16, !tbaa !1888
  %213 = fcmp olt <4 x float> %212, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %214 = select <4 x i1> %213, <4 x float> %212, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %215 = fcmp ogt <4 x float> %214, zeroinitializer
  %216 = select <4 x i1> %215, <4 x float> %214, <4 x float> zeroinitializer
  %217 = bitcast float* %210 to <4 x float>*
  store <4 x float> %216, <4 x float>* %217, align 16, !tbaa !1892
  %218 = or i64 %178, 16
  %219 = getelementptr inbounds float, float* %22, i64 %218
  %220 = getelementptr inbounds float, float* %124, i64 %218
  %221 = bitcast float* %219 to <4 x float>*
  %222 = load <4 x float>, <4 x float>* %221, align 64, !tbaa !1888
  %223 = fcmp olt <4 x float> %222, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %224 = select <4 x i1> %223, <4 x float> %222, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %225 = fcmp ogt <4 x float> %224, zeroinitializer
  %226 = select <4 x i1> %225, <4 x float> %224, <4 x float> zeroinitializer
  %227 = bitcast float* %220 to <4 x float>*
  store <4 x float> %226, <4 x float>* %227, align 64, !tbaa !1892
  %228 = or i64 %178, 20
  %229 = getelementptr inbounds float, float* %22, i64 %228
  %230 = getelementptr inbounds float, float* %124, i64 %228
  %231 = bitcast float* %229 to <4 x float>*
  %232 = load <4 x float>, <4 x float>* %231, align 16, !tbaa !1888
  %233 = fcmp olt <4 x float> %232, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %234 = select <4 x i1> %233, <4 x float> %232, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %235 = fcmp ogt <4 x float> %234, zeroinitializer
  %236 = select <4 x i1> %235, <4 x float> %234, <4 x float> zeroinitializer
  %237 = bitcast float* %230 to <4 x float>*
  store <4 x float> %236, <4 x float>* %237, align 16, !tbaa !1892
  %238 = or i64 %178, 24
  %239 = getelementptr inbounds float, float* %22, i64 %238
  %240 = getelementptr inbounds float, float* %124, i64 %238
  %241 = bitcast float* %239 to <4 x float>*
  %242 = load <4 x float>, <4 x float>* %241, align 32, !tbaa !1888
  %243 = fcmp olt <4 x float> %242, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %244 = select <4 x i1> %243, <4 x float> %242, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %245 = fcmp ogt <4 x float> %244, zeroinitializer
  %246 = select <4 x i1> %245, <4 x float> %244, <4 x float> zeroinitializer
  %247 = bitcast float* %240 to <4 x float>*
  store <4 x float> %246, <4 x float>* %247, align 32, !tbaa !1892
  %248 = or i64 %178, 28
  %249 = getelementptr inbounds float, float* %22, i64 %248
  %250 = getelementptr inbounds float, float* %124, i64 %248
  %251 = bitcast float* %249 to <4 x float>*
  %252 = load <4 x float>, <4 x float>* %251, align 16, !tbaa !1888
  %253 = fcmp olt <4 x float> %252, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %254 = select <4 x i1> %253, <4 x float> %252, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %255 = fcmp ogt <4 x float> %254, zeroinitializer
  %256 = select <4 x i1> %255, <4 x float> %254, <4 x float> zeroinitializer
  %257 = bitcast float* %250 to <4 x float>*
  store <4 x float> %256, <4 x float>* %257, align 16, !tbaa !1892
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, 112
  br i1 %exitcond.not, label %for_end41, label %for_begin42.preheader, !prof !51

for_end41:                                        ; preds = %for_begin42.preheader
  %indvars.iv.next69 = add nuw nsw i64 %indvars.iv68, 1
  %exitcond70.not = icmp eq i64 %indvars.iv.next69, 112
  br i1 %exitcond70.not, label %for_end38, label %for_begin39.preheader, !prof !51

if_end46:                                         ; preds = %for_end38
  %258 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %259 = tail call i32 %258(i32 1, i32 %4, i8* nonnull %6)
  %.not57 = icmp ne i32 %259, 0
  %spec.select = sext i1 %.not57 to i32
  ret i32 %spec.select
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_4(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.176, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !1895
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !1909
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !1911
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !1914
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.177, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.178, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.179, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.180, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !1916
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !1930
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 14
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.47, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !1932
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 14
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.48, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !1935
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 512
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.49, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !1937
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 100352, i32 7168, i32 512, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.50, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !1949
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 1
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.16, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !1963
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 1
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.17, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !1965
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 512
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.53, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !1968
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 512
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.181, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !1970
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 262144, i32 262144, i32 512, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([204 x i8], [204 x i8]* @.str.182, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !1982
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 512
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.56, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !1996
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !2010
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !2024
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 14
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.57, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !2026
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 14
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.58, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !2029
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 512
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.59, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !2031
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 100352, i32 7168, i32 512, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.60, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_4_compute_(i8* %31, i8* %39, i8* %51, i8* %45, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_4_compute_(i8* noalias align 128 %0, i8* noalias align 128 %1, i8* noalias align 128 %2, i8* noalias align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 401408, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %call_end, %if_end, %entry
  %merge = phi i32 [ -1, %entry ], [ %13, %if_end ], [ %22, %call_end ]
  ret i32 %merge

if_end:                                           ; preds = %entry
  %8 = alloca %14, align 8
  %9 = getelementptr inbounds %14, %14* %8, i64 0, i32 0
  store i8* %6, i8** %9, align 8
  %10 = getelementptr inbounds %14, %14* %8, i64 0, i32 1
  store i8* %0, i8** %10, align 8
  %11 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %12 = bitcast %14* %8 to i8*
  %13 = call i32 %11(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.183, i8* nonnull %12, i32 0)
  %14 = icmp eq i32 %13, 0
  br i1 %14, label %call_end, label %if_then, !prof !5

call_end:                                         ; preds = %if_end
  %15 = alloca %15, align 8
  %16 = getelementptr inbounds %15, %15* %15, i64 0, i32 0
  store i8* %6, i8** %16, align 8
  %17 = getelementptr inbounds %15, %15* %15, i64 0, i32 1
  store i8* %1, i8** %17, align 8
  %18 = getelementptr inbounds %15, %15* %15, i64 0, i32 2
  store i8* %2, i8** %18, align 8
  %19 = getelementptr inbounds %15, %15* %15, i64 0, i32 3
  store i8* %3, i8** %19, align 8
  %20 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %21 = bitcast %15* %15 to i8*
  %22 = call i32 %20(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.184, i8* nonnull %21, i32 0)
  %23 = icmp eq i32 %22, 0
  br i1 %23, label %call_end2, label %if_then, !prof !5

call_end2:                                        ; preds = %call_end
  %24 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %25 = call i32 %24(i32 1, i32 %4, i8* nonnull %6)
  %.not = icmp ne i32 %25, 0
  %spec.select = sext i1 %.not to i32
  ret i32 %spec.select
}

; Function Attrs: nofree norecurse nounwind
define private i32 @__tvm_parallel_lambda.183(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #2 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %9 = load i32, i32* %8, align 4
  %10 = add nsw i32 %9, 13
  %11 = sdiv i32 %10, %9
  %12 = add nsw i32 %0, 1
  %13 = mul nsw i32 %11, %12
  %14 = icmp slt i32 %13, 14
  %15 = select i1 %14, i32 %13, i32 14
  %16 = mul nsw i32 %11, %0
  %17 = icmp slt i32 %16, 14
  %18 = select i1 %17, i32 %16, i32 14
  %19 = icmp slt i32 %18, %15
  br i1 %19, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %20 = sext i32 %18 to i64
  %wide.trip.count = sext i32 %15 to i64
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end6.13
  %indvars.iv13 = phi i64 [ %20, %for_begin1.preheader.preheader ], [ %indvars.iv.next14, %for_end6.13 ]
  %21 = mul nsw i64 %indvars.iv13, 7168
  br label %vector.body139

vector.body139:                                   ; preds = %vector.body139, %for_begin1.preheader
  %index141 = phi i64 [ 0, %for_begin1.preheader ], [ %index.next142.3, %vector.body139 ]
  %22 = add nsw i64 %21, %index141
  %23 = getelementptr inbounds float, float* %7, i64 %22
  %24 = bitcast float* %23 to <4 x float>*
  %wide.load145 = load <4 x float>, <4 x float>* %24, align 4, !tbaa !2043
  %25 = getelementptr inbounds float, float* %23, i64 4
  %26 = bitcast float* %25 to <4 x float>*
  %wide.load146 = load <4 x float>, <4 x float>* %26, align 4, !tbaa !2043
  %27 = getelementptr inbounds float, float* %4, i64 %22
  %28 = bitcast float* %27 to <4 x float>*
  store <4 x float> %wide.load145, <4 x float>* %28, align 4, !tbaa !2046
  %29 = getelementptr inbounds float, float* %27, i64 4
  %30 = bitcast float* %29 to <4 x float>*
  store <4 x float> %wide.load146, <4 x float>* %30, align 4, !tbaa !2046
  %index.next142 = or i64 %index141, 8
  %31 = add nsw i64 %21, %index.next142
  %32 = getelementptr inbounds float, float* %7, i64 %31
  %33 = bitcast float* %32 to <4 x float>*
  %wide.load145.1 = load <4 x float>, <4 x float>* %33, align 4, !tbaa !2043
  %34 = getelementptr inbounds float, float* %32, i64 4
  %35 = bitcast float* %34 to <4 x float>*
  %wide.load146.1 = load <4 x float>, <4 x float>* %35, align 4, !tbaa !2043
  %36 = getelementptr inbounds float, float* %4, i64 %31
  %37 = bitcast float* %36 to <4 x float>*
  store <4 x float> %wide.load145.1, <4 x float>* %37, align 4, !tbaa !2046
  %38 = getelementptr inbounds float, float* %36, i64 4
  %39 = bitcast float* %38 to <4 x float>*
  store <4 x float> %wide.load146.1, <4 x float>* %39, align 4, !tbaa !2046
  %index.next142.1 = or i64 %index141, 16
  %40 = add nsw i64 %21, %index.next142.1
  %41 = getelementptr inbounds float, float* %7, i64 %40
  %42 = bitcast float* %41 to <4 x float>*
  %wide.load145.2 = load <4 x float>, <4 x float>* %42, align 4, !tbaa !2043
  %43 = getelementptr inbounds float, float* %41, i64 4
  %44 = bitcast float* %43 to <4 x float>*
  %wide.load146.2 = load <4 x float>, <4 x float>* %44, align 4, !tbaa !2043
  %45 = getelementptr inbounds float, float* %4, i64 %40
  %46 = bitcast float* %45 to <4 x float>*
  store <4 x float> %wide.load145.2, <4 x float>* %46, align 4, !tbaa !2046
  %47 = getelementptr inbounds float, float* %45, i64 4
  %48 = bitcast float* %47 to <4 x float>*
  store <4 x float> %wide.load146.2, <4 x float>* %48, align 4, !tbaa !2046
  %index.next142.2 = or i64 %index141, 24
  %49 = add nsw i64 %21, %index.next142.2
  %50 = getelementptr inbounds float, float* %7, i64 %49
  %51 = bitcast float* %50 to <4 x float>*
  %wide.load145.3 = load <4 x float>, <4 x float>* %51, align 4, !tbaa !2043
  %52 = getelementptr inbounds float, float* %50, i64 4
  %53 = bitcast float* %52 to <4 x float>*
  %wide.load146.3 = load <4 x float>, <4 x float>* %53, align 4, !tbaa !2043
  %54 = getelementptr inbounds float, float* %4, i64 %49
  %55 = bitcast float* %54 to <4 x float>*
  store <4 x float> %wide.load145.3, <4 x float>* %55, align 4, !tbaa !2046
  %56 = getelementptr inbounds float, float* %54, i64 4
  %57 = bitcast float* %56 to <4 x float>*
  store <4 x float> %wide.load146.3, <4 x float>* %57, align 4, !tbaa !2046
  %index.next142.3 = add nuw nsw i64 %index141, 32
  %58 = icmp eq i64 %index.next142.3, 512
  br i1 %58, label %for_end6, label %vector.body139, !prof !341, !llvm.loop !2049

for_end:                                          ; preds = %for_end6.13, %entry
  ret i32 0

for_end6:                                         ; preds = %vector.body139
  %59 = or i64 %21, 512
  br label %vector.body129

vector.body129:                                   ; preds = %vector.body129, %for_end6
  %index131 = phi i64 [ 0, %for_end6 ], [ %index.next132.3, %vector.body129 ]
  %60 = add nsw i64 %59, %index131
  %61 = getelementptr inbounds float, float* %7, i64 %60
  %62 = bitcast float* %61 to <4 x float>*
  %wide.load135 = load <4 x float>, <4 x float>* %62, align 4, !tbaa !2043
  %63 = getelementptr inbounds float, float* %61, i64 4
  %64 = bitcast float* %63 to <4 x float>*
  %wide.load136 = load <4 x float>, <4 x float>* %64, align 4, !tbaa !2043
  %65 = getelementptr inbounds float, float* %4, i64 %60
  %66 = bitcast float* %65 to <4 x float>*
  store <4 x float> %wide.load135, <4 x float>* %66, align 4, !tbaa !2046
  %67 = getelementptr inbounds float, float* %65, i64 4
  %68 = bitcast float* %67 to <4 x float>*
  store <4 x float> %wide.load136, <4 x float>* %68, align 4, !tbaa !2046
  %index.next132 = or i64 %index131, 8
  %69 = add nsw i64 %59, %index.next132
  %70 = getelementptr inbounds float, float* %7, i64 %69
  %71 = bitcast float* %70 to <4 x float>*
  %wide.load135.1 = load <4 x float>, <4 x float>* %71, align 4, !tbaa !2043
  %72 = getelementptr inbounds float, float* %70, i64 4
  %73 = bitcast float* %72 to <4 x float>*
  %wide.load136.1 = load <4 x float>, <4 x float>* %73, align 4, !tbaa !2043
  %74 = getelementptr inbounds float, float* %4, i64 %69
  %75 = bitcast float* %74 to <4 x float>*
  store <4 x float> %wide.load135.1, <4 x float>* %75, align 4, !tbaa !2046
  %76 = getelementptr inbounds float, float* %74, i64 4
  %77 = bitcast float* %76 to <4 x float>*
  store <4 x float> %wide.load136.1, <4 x float>* %77, align 4, !tbaa !2046
  %index.next132.1 = or i64 %index131, 16
  %78 = add nsw i64 %59, %index.next132.1
  %79 = getelementptr inbounds float, float* %7, i64 %78
  %80 = bitcast float* %79 to <4 x float>*
  %wide.load135.2 = load <4 x float>, <4 x float>* %80, align 4, !tbaa !2043
  %81 = getelementptr inbounds float, float* %79, i64 4
  %82 = bitcast float* %81 to <4 x float>*
  %wide.load136.2 = load <4 x float>, <4 x float>* %82, align 4, !tbaa !2043
  %83 = getelementptr inbounds float, float* %4, i64 %78
  %84 = bitcast float* %83 to <4 x float>*
  store <4 x float> %wide.load135.2, <4 x float>* %84, align 4, !tbaa !2046
  %85 = getelementptr inbounds float, float* %83, i64 4
  %86 = bitcast float* %85 to <4 x float>*
  store <4 x float> %wide.load136.2, <4 x float>* %86, align 4, !tbaa !2046
  %index.next132.2 = or i64 %index131, 24
  %87 = add nsw i64 %59, %index.next132.2
  %88 = getelementptr inbounds float, float* %7, i64 %87
  %89 = bitcast float* %88 to <4 x float>*
  %wide.load135.3 = load <4 x float>, <4 x float>* %89, align 4, !tbaa !2043
  %90 = getelementptr inbounds float, float* %88, i64 4
  %91 = bitcast float* %90 to <4 x float>*
  %wide.load136.3 = load <4 x float>, <4 x float>* %91, align 4, !tbaa !2043
  %92 = getelementptr inbounds float, float* %4, i64 %87
  %93 = bitcast float* %92 to <4 x float>*
  store <4 x float> %wide.load135.3, <4 x float>* %93, align 4, !tbaa !2046
  %94 = getelementptr inbounds float, float* %92, i64 4
  %95 = bitcast float* %94 to <4 x float>*
  store <4 x float> %wide.load136.3, <4 x float>* %95, align 4, !tbaa !2046
  %index.next132.3 = add nuw nsw i64 %index131, 32
  %96 = icmp eq i64 %index.next132.3, 512
  br i1 %96, label %for_end6.1, label %vector.body129, !prof !341, !llvm.loop !2050

for_end6.1:                                       ; preds = %vector.body129
  %97 = add nsw i64 %21, 1024
  br label %vector.body119

vector.body119:                                   ; preds = %vector.body119, %for_end6.1
  %index121 = phi i64 [ 0, %for_end6.1 ], [ %index.next122.3, %vector.body119 ]
  %98 = add nuw nsw i64 %97, %index121
  %99 = getelementptr inbounds float, float* %7, i64 %98
  %100 = bitcast float* %99 to <4 x float>*
  %wide.load125 = load <4 x float>, <4 x float>* %100, align 4, !tbaa !2043
  %101 = getelementptr inbounds float, float* %99, i64 4
  %102 = bitcast float* %101 to <4 x float>*
  %wide.load126 = load <4 x float>, <4 x float>* %102, align 4, !tbaa !2043
  %103 = getelementptr inbounds float, float* %4, i64 %98
  %104 = bitcast float* %103 to <4 x float>*
  store <4 x float> %wide.load125, <4 x float>* %104, align 4, !tbaa !2046
  %105 = getelementptr inbounds float, float* %103, i64 4
  %106 = bitcast float* %105 to <4 x float>*
  store <4 x float> %wide.load126, <4 x float>* %106, align 4, !tbaa !2046
  %index.next122 = or i64 %index121, 8
  %107 = add nsw i64 %97, %index.next122
  %108 = getelementptr inbounds float, float* %7, i64 %107
  %109 = bitcast float* %108 to <4 x float>*
  %wide.load125.1 = load <4 x float>, <4 x float>* %109, align 4, !tbaa !2043
  %110 = getelementptr inbounds float, float* %108, i64 4
  %111 = bitcast float* %110 to <4 x float>*
  %wide.load126.1 = load <4 x float>, <4 x float>* %111, align 4, !tbaa !2043
  %112 = getelementptr inbounds float, float* %4, i64 %107
  %113 = bitcast float* %112 to <4 x float>*
  store <4 x float> %wide.load125.1, <4 x float>* %113, align 4, !tbaa !2046
  %114 = getelementptr inbounds float, float* %112, i64 4
  %115 = bitcast float* %114 to <4 x float>*
  store <4 x float> %wide.load126.1, <4 x float>* %115, align 4, !tbaa !2046
  %index.next122.1 = or i64 %index121, 16
  %116 = add nsw i64 %97, %index.next122.1
  %117 = getelementptr inbounds float, float* %7, i64 %116
  %118 = bitcast float* %117 to <4 x float>*
  %wide.load125.2 = load <4 x float>, <4 x float>* %118, align 4, !tbaa !2043
  %119 = getelementptr inbounds float, float* %117, i64 4
  %120 = bitcast float* %119 to <4 x float>*
  %wide.load126.2 = load <4 x float>, <4 x float>* %120, align 4, !tbaa !2043
  %121 = getelementptr inbounds float, float* %4, i64 %116
  %122 = bitcast float* %121 to <4 x float>*
  store <4 x float> %wide.load125.2, <4 x float>* %122, align 4, !tbaa !2046
  %123 = getelementptr inbounds float, float* %121, i64 4
  %124 = bitcast float* %123 to <4 x float>*
  store <4 x float> %wide.load126.2, <4 x float>* %124, align 4, !tbaa !2046
  %index.next122.2 = or i64 %index121, 24
  %125 = add nsw i64 %97, %index.next122.2
  %126 = getelementptr inbounds float, float* %7, i64 %125
  %127 = bitcast float* %126 to <4 x float>*
  %wide.load125.3 = load <4 x float>, <4 x float>* %127, align 4, !tbaa !2043
  %128 = getelementptr inbounds float, float* %126, i64 4
  %129 = bitcast float* %128 to <4 x float>*
  %wide.load126.3 = load <4 x float>, <4 x float>* %129, align 4, !tbaa !2043
  %130 = getelementptr inbounds float, float* %4, i64 %125
  %131 = bitcast float* %130 to <4 x float>*
  store <4 x float> %wide.load125.3, <4 x float>* %131, align 4, !tbaa !2046
  %132 = getelementptr inbounds float, float* %130, i64 4
  %133 = bitcast float* %132 to <4 x float>*
  store <4 x float> %wide.load126.3, <4 x float>* %133, align 4, !tbaa !2046
  %index.next122.3 = add nuw nsw i64 %index121, 32
  %134 = icmp eq i64 %index.next122.3, 512
  br i1 %134, label %for_end6.2, label %vector.body119, !prof !341, !llvm.loop !2051

for_end6.2:                                       ; preds = %vector.body119
  %135 = add nsw i64 %21, 1536
  br label %vector.body109

vector.body109:                                   ; preds = %vector.body109, %for_end6.2
  %index111 = phi i64 [ 0, %for_end6.2 ], [ %index.next112.3, %vector.body109 ]
  %136 = add nsw i64 %135, %index111
  %137 = getelementptr inbounds float, float* %7, i64 %136
  %138 = bitcast float* %137 to <4 x float>*
  %wide.load115 = load <4 x float>, <4 x float>* %138, align 4, !tbaa !2043
  %139 = getelementptr inbounds float, float* %137, i64 4
  %140 = bitcast float* %139 to <4 x float>*
  %wide.load116 = load <4 x float>, <4 x float>* %140, align 4, !tbaa !2043
  %141 = getelementptr inbounds float, float* %4, i64 %136
  %142 = bitcast float* %141 to <4 x float>*
  store <4 x float> %wide.load115, <4 x float>* %142, align 4, !tbaa !2046
  %143 = getelementptr inbounds float, float* %141, i64 4
  %144 = bitcast float* %143 to <4 x float>*
  store <4 x float> %wide.load116, <4 x float>* %144, align 4, !tbaa !2046
  %index.next112 = or i64 %index111, 8
  %145 = add nsw i64 %135, %index.next112
  %146 = getelementptr inbounds float, float* %7, i64 %145
  %147 = bitcast float* %146 to <4 x float>*
  %wide.load115.1 = load <4 x float>, <4 x float>* %147, align 4, !tbaa !2043
  %148 = getelementptr inbounds float, float* %146, i64 4
  %149 = bitcast float* %148 to <4 x float>*
  %wide.load116.1 = load <4 x float>, <4 x float>* %149, align 4, !tbaa !2043
  %150 = getelementptr inbounds float, float* %4, i64 %145
  %151 = bitcast float* %150 to <4 x float>*
  store <4 x float> %wide.load115.1, <4 x float>* %151, align 4, !tbaa !2046
  %152 = getelementptr inbounds float, float* %150, i64 4
  %153 = bitcast float* %152 to <4 x float>*
  store <4 x float> %wide.load116.1, <4 x float>* %153, align 4, !tbaa !2046
  %index.next112.1 = or i64 %index111, 16
  %154 = add nsw i64 %135, %index.next112.1
  %155 = getelementptr inbounds float, float* %7, i64 %154
  %156 = bitcast float* %155 to <4 x float>*
  %wide.load115.2 = load <4 x float>, <4 x float>* %156, align 4, !tbaa !2043
  %157 = getelementptr inbounds float, float* %155, i64 4
  %158 = bitcast float* %157 to <4 x float>*
  %wide.load116.2 = load <4 x float>, <4 x float>* %158, align 4, !tbaa !2043
  %159 = getelementptr inbounds float, float* %4, i64 %154
  %160 = bitcast float* %159 to <4 x float>*
  store <4 x float> %wide.load115.2, <4 x float>* %160, align 4, !tbaa !2046
  %161 = getelementptr inbounds float, float* %159, i64 4
  %162 = bitcast float* %161 to <4 x float>*
  store <4 x float> %wide.load116.2, <4 x float>* %162, align 4, !tbaa !2046
  %index.next112.2 = or i64 %index111, 24
  %163 = add nsw i64 %135, %index.next112.2
  %164 = getelementptr inbounds float, float* %7, i64 %163
  %165 = bitcast float* %164 to <4 x float>*
  %wide.load115.3 = load <4 x float>, <4 x float>* %165, align 4, !tbaa !2043
  %166 = getelementptr inbounds float, float* %164, i64 4
  %167 = bitcast float* %166 to <4 x float>*
  %wide.load116.3 = load <4 x float>, <4 x float>* %167, align 4, !tbaa !2043
  %168 = getelementptr inbounds float, float* %4, i64 %163
  %169 = bitcast float* %168 to <4 x float>*
  store <4 x float> %wide.load115.3, <4 x float>* %169, align 4, !tbaa !2046
  %170 = getelementptr inbounds float, float* %168, i64 4
  %171 = bitcast float* %170 to <4 x float>*
  store <4 x float> %wide.load116.3, <4 x float>* %171, align 4, !tbaa !2046
  %index.next112.3 = add nuw nsw i64 %index111, 32
  %172 = icmp eq i64 %index.next112.3, 512
  br i1 %172, label %for_end6.3, label %vector.body109, !prof !341, !llvm.loop !2052

for_end6.3:                                       ; preds = %vector.body109
  %173 = add nsw i64 %21, 2048
  br label %vector.body99

vector.body99:                                    ; preds = %vector.body99, %for_end6.3
  %index101 = phi i64 [ 0, %for_end6.3 ], [ %index.next102.3, %vector.body99 ]
  %174 = add nuw nsw i64 %173, %index101
  %175 = getelementptr inbounds float, float* %7, i64 %174
  %176 = bitcast float* %175 to <4 x float>*
  %wide.load105 = load <4 x float>, <4 x float>* %176, align 4, !tbaa !2043
  %177 = getelementptr inbounds float, float* %175, i64 4
  %178 = bitcast float* %177 to <4 x float>*
  %wide.load106 = load <4 x float>, <4 x float>* %178, align 4, !tbaa !2043
  %179 = getelementptr inbounds float, float* %4, i64 %174
  %180 = bitcast float* %179 to <4 x float>*
  store <4 x float> %wide.load105, <4 x float>* %180, align 4, !tbaa !2046
  %181 = getelementptr inbounds float, float* %179, i64 4
  %182 = bitcast float* %181 to <4 x float>*
  store <4 x float> %wide.load106, <4 x float>* %182, align 4, !tbaa !2046
  %index.next102 = or i64 %index101, 8
  %183 = add nsw i64 %173, %index.next102
  %184 = getelementptr inbounds float, float* %7, i64 %183
  %185 = bitcast float* %184 to <4 x float>*
  %wide.load105.1 = load <4 x float>, <4 x float>* %185, align 4, !tbaa !2043
  %186 = getelementptr inbounds float, float* %184, i64 4
  %187 = bitcast float* %186 to <4 x float>*
  %wide.load106.1 = load <4 x float>, <4 x float>* %187, align 4, !tbaa !2043
  %188 = getelementptr inbounds float, float* %4, i64 %183
  %189 = bitcast float* %188 to <4 x float>*
  store <4 x float> %wide.load105.1, <4 x float>* %189, align 4, !tbaa !2046
  %190 = getelementptr inbounds float, float* %188, i64 4
  %191 = bitcast float* %190 to <4 x float>*
  store <4 x float> %wide.load106.1, <4 x float>* %191, align 4, !tbaa !2046
  %index.next102.1 = or i64 %index101, 16
  %192 = add nsw i64 %173, %index.next102.1
  %193 = getelementptr inbounds float, float* %7, i64 %192
  %194 = bitcast float* %193 to <4 x float>*
  %wide.load105.2 = load <4 x float>, <4 x float>* %194, align 4, !tbaa !2043
  %195 = getelementptr inbounds float, float* %193, i64 4
  %196 = bitcast float* %195 to <4 x float>*
  %wide.load106.2 = load <4 x float>, <4 x float>* %196, align 4, !tbaa !2043
  %197 = getelementptr inbounds float, float* %4, i64 %192
  %198 = bitcast float* %197 to <4 x float>*
  store <4 x float> %wide.load105.2, <4 x float>* %198, align 4, !tbaa !2046
  %199 = getelementptr inbounds float, float* %197, i64 4
  %200 = bitcast float* %199 to <4 x float>*
  store <4 x float> %wide.load106.2, <4 x float>* %200, align 4, !tbaa !2046
  %index.next102.2 = or i64 %index101, 24
  %201 = add nsw i64 %173, %index.next102.2
  %202 = getelementptr inbounds float, float* %7, i64 %201
  %203 = bitcast float* %202 to <4 x float>*
  %wide.load105.3 = load <4 x float>, <4 x float>* %203, align 4, !tbaa !2043
  %204 = getelementptr inbounds float, float* %202, i64 4
  %205 = bitcast float* %204 to <4 x float>*
  %wide.load106.3 = load <4 x float>, <4 x float>* %205, align 4, !tbaa !2043
  %206 = getelementptr inbounds float, float* %4, i64 %201
  %207 = bitcast float* %206 to <4 x float>*
  store <4 x float> %wide.load105.3, <4 x float>* %207, align 4, !tbaa !2046
  %208 = getelementptr inbounds float, float* %206, i64 4
  %209 = bitcast float* %208 to <4 x float>*
  store <4 x float> %wide.load106.3, <4 x float>* %209, align 4, !tbaa !2046
  %index.next102.3 = add nuw nsw i64 %index101, 32
  %210 = icmp eq i64 %index.next102.3, 512
  br i1 %210, label %for_end6.4, label %vector.body99, !prof !341, !llvm.loop !2053

for_end6.4:                                       ; preds = %vector.body99
  %211 = add nsw i64 %21, 2560
  br label %vector.body89

vector.body89:                                    ; preds = %vector.body89, %for_end6.4
  %index91 = phi i64 [ 0, %for_end6.4 ], [ %index.next92.3, %vector.body89 ]
  %212 = add nsw i64 %211, %index91
  %213 = getelementptr inbounds float, float* %7, i64 %212
  %214 = bitcast float* %213 to <4 x float>*
  %wide.load95 = load <4 x float>, <4 x float>* %214, align 4, !tbaa !2043
  %215 = getelementptr inbounds float, float* %213, i64 4
  %216 = bitcast float* %215 to <4 x float>*
  %wide.load96 = load <4 x float>, <4 x float>* %216, align 4, !tbaa !2043
  %217 = getelementptr inbounds float, float* %4, i64 %212
  %218 = bitcast float* %217 to <4 x float>*
  store <4 x float> %wide.load95, <4 x float>* %218, align 4, !tbaa !2046
  %219 = getelementptr inbounds float, float* %217, i64 4
  %220 = bitcast float* %219 to <4 x float>*
  store <4 x float> %wide.load96, <4 x float>* %220, align 4, !tbaa !2046
  %index.next92 = or i64 %index91, 8
  %221 = add nsw i64 %211, %index.next92
  %222 = getelementptr inbounds float, float* %7, i64 %221
  %223 = bitcast float* %222 to <4 x float>*
  %wide.load95.1 = load <4 x float>, <4 x float>* %223, align 4, !tbaa !2043
  %224 = getelementptr inbounds float, float* %222, i64 4
  %225 = bitcast float* %224 to <4 x float>*
  %wide.load96.1 = load <4 x float>, <4 x float>* %225, align 4, !tbaa !2043
  %226 = getelementptr inbounds float, float* %4, i64 %221
  %227 = bitcast float* %226 to <4 x float>*
  store <4 x float> %wide.load95.1, <4 x float>* %227, align 4, !tbaa !2046
  %228 = getelementptr inbounds float, float* %226, i64 4
  %229 = bitcast float* %228 to <4 x float>*
  store <4 x float> %wide.load96.1, <4 x float>* %229, align 4, !tbaa !2046
  %index.next92.1 = or i64 %index91, 16
  %230 = add nsw i64 %211, %index.next92.1
  %231 = getelementptr inbounds float, float* %7, i64 %230
  %232 = bitcast float* %231 to <4 x float>*
  %wide.load95.2 = load <4 x float>, <4 x float>* %232, align 4, !tbaa !2043
  %233 = getelementptr inbounds float, float* %231, i64 4
  %234 = bitcast float* %233 to <4 x float>*
  %wide.load96.2 = load <4 x float>, <4 x float>* %234, align 4, !tbaa !2043
  %235 = getelementptr inbounds float, float* %4, i64 %230
  %236 = bitcast float* %235 to <4 x float>*
  store <4 x float> %wide.load95.2, <4 x float>* %236, align 4, !tbaa !2046
  %237 = getelementptr inbounds float, float* %235, i64 4
  %238 = bitcast float* %237 to <4 x float>*
  store <4 x float> %wide.load96.2, <4 x float>* %238, align 4, !tbaa !2046
  %index.next92.2 = or i64 %index91, 24
  %239 = add nsw i64 %211, %index.next92.2
  %240 = getelementptr inbounds float, float* %7, i64 %239
  %241 = bitcast float* %240 to <4 x float>*
  %wide.load95.3 = load <4 x float>, <4 x float>* %241, align 4, !tbaa !2043
  %242 = getelementptr inbounds float, float* %240, i64 4
  %243 = bitcast float* %242 to <4 x float>*
  %wide.load96.3 = load <4 x float>, <4 x float>* %243, align 4, !tbaa !2043
  %244 = getelementptr inbounds float, float* %4, i64 %239
  %245 = bitcast float* %244 to <4 x float>*
  store <4 x float> %wide.load95.3, <4 x float>* %245, align 4, !tbaa !2046
  %246 = getelementptr inbounds float, float* %244, i64 4
  %247 = bitcast float* %246 to <4 x float>*
  store <4 x float> %wide.load96.3, <4 x float>* %247, align 4, !tbaa !2046
  %index.next92.3 = add nuw nsw i64 %index91, 32
  %248 = icmp eq i64 %index.next92.3, 512
  br i1 %248, label %for_end6.5, label %vector.body89, !prof !341, !llvm.loop !2054

for_end6.5:                                       ; preds = %vector.body89
  %249 = add nsw i64 %21, 3072
  br label %vector.body79

vector.body79:                                    ; preds = %vector.body79, %for_end6.5
  %index81 = phi i64 [ 0, %for_end6.5 ], [ %index.next82.3, %vector.body79 ]
  %250 = add nuw nsw i64 %249, %index81
  %251 = getelementptr inbounds float, float* %7, i64 %250
  %252 = bitcast float* %251 to <4 x float>*
  %wide.load85 = load <4 x float>, <4 x float>* %252, align 4, !tbaa !2043
  %253 = getelementptr inbounds float, float* %251, i64 4
  %254 = bitcast float* %253 to <4 x float>*
  %wide.load86 = load <4 x float>, <4 x float>* %254, align 4, !tbaa !2043
  %255 = getelementptr inbounds float, float* %4, i64 %250
  %256 = bitcast float* %255 to <4 x float>*
  store <4 x float> %wide.load85, <4 x float>* %256, align 4, !tbaa !2046
  %257 = getelementptr inbounds float, float* %255, i64 4
  %258 = bitcast float* %257 to <4 x float>*
  store <4 x float> %wide.load86, <4 x float>* %258, align 4, !tbaa !2046
  %index.next82 = or i64 %index81, 8
  %259 = add nsw i64 %249, %index.next82
  %260 = getelementptr inbounds float, float* %7, i64 %259
  %261 = bitcast float* %260 to <4 x float>*
  %wide.load85.1 = load <4 x float>, <4 x float>* %261, align 4, !tbaa !2043
  %262 = getelementptr inbounds float, float* %260, i64 4
  %263 = bitcast float* %262 to <4 x float>*
  %wide.load86.1 = load <4 x float>, <4 x float>* %263, align 4, !tbaa !2043
  %264 = getelementptr inbounds float, float* %4, i64 %259
  %265 = bitcast float* %264 to <4 x float>*
  store <4 x float> %wide.load85.1, <4 x float>* %265, align 4, !tbaa !2046
  %266 = getelementptr inbounds float, float* %264, i64 4
  %267 = bitcast float* %266 to <4 x float>*
  store <4 x float> %wide.load86.1, <4 x float>* %267, align 4, !tbaa !2046
  %index.next82.1 = or i64 %index81, 16
  %268 = add nsw i64 %249, %index.next82.1
  %269 = getelementptr inbounds float, float* %7, i64 %268
  %270 = bitcast float* %269 to <4 x float>*
  %wide.load85.2 = load <4 x float>, <4 x float>* %270, align 4, !tbaa !2043
  %271 = getelementptr inbounds float, float* %269, i64 4
  %272 = bitcast float* %271 to <4 x float>*
  %wide.load86.2 = load <4 x float>, <4 x float>* %272, align 4, !tbaa !2043
  %273 = getelementptr inbounds float, float* %4, i64 %268
  %274 = bitcast float* %273 to <4 x float>*
  store <4 x float> %wide.load85.2, <4 x float>* %274, align 4, !tbaa !2046
  %275 = getelementptr inbounds float, float* %273, i64 4
  %276 = bitcast float* %275 to <4 x float>*
  store <4 x float> %wide.load86.2, <4 x float>* %276, align 4, !tbaa !2046
  %index.next82.2 = or i64 %index81, 24
  %277 = add nsw i64 %249, %index.next82.2
  %278 = getelementptr inbounds float, float* %7, i64 %277
  %279 = bitcast float* %278 to <4 x float>*
  %wide.load85.3 = load <4 x float>, <4 x float>* %279, align 4, !tbaa !2043
  %280 = getelementptr inbounds float, float* %278, i64 4
  %281 = bitcast float* %280 to <4 x float>*
  %wide.load86.3 = load <4 x float>, <4 x float>* %281, align 4, !tbaa !2043
  %282 = getelementptr inbounds float, float* %4, i64 %277
  %283 = bitcast float* %282 to <4 x float>*
  store <4 x float> %wide.load85.3, <4 x float>* %283, align 4, !tbaa !2046
  %284 = getelementptr inbounds float, float* %282, i64 4
  %285 = bitcast float* %284 to <4 x float>*
  store <4 x float> %wide.load86.3, <4 x float>* %285, align 4, !tbaa !2046
  %index.next82.3 = add nuw nsw i64 %index81, 32
  %286 = icmp eq i64 %index.next82.3, 512
  br i1 %286, label %for_end6.6, label %vector.body79, !prof !341, !llvm.loop !2055

for_end6.6:                                       ; preds = %vector.body79
  %287 = add nsw i64 %21, 3584
  br label %vector.body69

vector.body69:                                    ; preds = %vector.body69, %for_end6.6
  %index71 = phi i64 [ 0, %for_end6.6 ], [ %index.next72.3, %vector.body69 ]
  %288 = add nsw i64 %287, %index71
  %289 = getelementptr inbounds float, float* %7, i64 %288
  %290 = bitcast float* %289 to <4 x float>*
  %wide.load75 = load <4 x float>, <4 x float>* %290, align 4, !tbaa !2043
  %291 = getelementptr inbounds float, float* %289, i64 4
  %292 = bitcast float* %291 to <4 x float>*
  %wide.load76 = load <4 x float>, <4 x float>* %292, align 4, !tbaa !2043
  %293 = getelementptr inbounds float, float* %4, i64 %288
  %294 = bitcast float* %293 to <4 x float>*
  store <4 x float> %wide.load75, <4 x float>* %294, align 4, !tbaa !2046
  %295 = getelementptr inbounds float, float* %293, i64 4
  %296 = bitcast float* %295 to <4 x float>*
  store <4 x float> %wide.load76, <4 x float>* %296, align 4, !tbaa !2046
  %index.next72 = or i64 %index71, 8
  %297 = add nsw i64 %287, %index.next72
  %298 = getelementptr inbounds float, float* %7, i64 %297
  %299 = bitcast float* %298 to <4 x float>*
  %wide.load75.1 = load <4 x float>, <4 x float>* %299, align 4, !tbaa !2043
  %300 = getelementptr inbounds float, float* %298, i64 4
  %301 = bitcast float* %300 to <4 x float>*
  %wide.load76.1 = load <4 x float>, <4 x float>* %301, align 4, !tbaa !2043
  %302 = getelementptr inbounds float, float* %4, i64 %297
  %303 = bitcast float* %302 to <4 x float>*
  store <4 x float> %wide.load75.1, <4 x float>* %303, align 4, !tbaa !2046
  %304 = getelementptr inbounds float, float* %302, i64 4
  %305 = bitcast float* %304 to <4 x float>*
  store <4 x float> %wide.load76.1, <4 x float>* %305, align 4, !tbaa !2046
  %index.next72.1 = or i64 %index71, 16
  %306 = add nsw i64 %287, %index.next72.1
  %307 = getelementptr inbounds float, float* %7, i64 %306
  %308 = bitcast float* %307 to <4 x float>*
  %wide.load75.2 = load <4 x float>, <4 x float>* %308, align 4, !tbaa !2043
  %309 = getelementptr inbounds float, float* %307, i64 4
  %310 = bitcast float* %309 to <4 x float>*
  %wide.load76.2 = load <4 x float>, <4 x float>* %310, align 4, !tbaa !2043
  %311 = getelementptr inbounds float, float* %4, i64 %306
  %312 = bitcast float* %311 to <4 x float>*
  store <4 x float> %wide.load75.2, <4 x float>* %312, align 4, !tbaa !2046
  %313 = getelementptr inbounds float, float* %311, i64 4
  %314 = bitcast float* %313 to <4 x float>*
  store <4 x float> %wide.load76.2, <4 x float>* %314, align 4, !tbaa !2046
  %index.next72.2 = or i64 %index71, 24
  %315 = add nsw i64 %287, %index.next72.2
  %316 = getelementptr inbounds float, float* %7, i64 %315
  %317 = bitcast float* %316 to <4 x float>*
  %wide.load75.3 = load <4 x float>, <4 x float>* %317, align 4, !tbaa !2043
  %318 = getelementptr inbounds float, float* %316, i64 4
  %319 = bitcast float* %318 to <4 x float>*
  %wide.load76.3 = load <4 x float>, <4 x float>* %319, align 4, !tbaa !2043
  %320 = getelementptr inbounds float, float* %4, i64 %315
  %321 = bitcast float* %320 to <4 x float>*
  store <4 x float> %wide.load75.3, <4 x float>* %321, align 4, !tbaa !2046
  %322 = getelementptr inbounds float, float* %320, i64 4
  %323 = bitcast float* %322 to <4 x float>*
  store <4 x float> %wide.load76.3, <4 x float>* %323, align 4, !tbaa !2046
  %index.next72.3 = add nuw nsw i64 %index71, 32
  %324 = icmp eq i64 %index.next72.3, 512
  br i1 %324, label %for_end6.7, label %vector.body69, !prof !341, !llvm.loop !2056

for_end6.7:                                       ; preds = %vector.body69
  %325 = add nsw i64 %21, 4096
  br label %vector.body59

vector.body59:                                    ; preds = %vector.body59, %for_end6.7
  %index61 = phi i64 [ 0, %for_end6.7 ], [ %index.next62.3, %vector.body59 ]
  %326 = add nuw nsw i64 %325, %index61
  %327 = getelementptr inbounds float, float* %7, i64 %326
  %328 = bitcast float* %327 to <4 x float>*
  %wide.load65 = load <4 x float>, <4 x float>* %328, align 4, !tbaa !2043
  %329 = getelementptr inbounds float, float* %327, i64 4
  %330 = bitcast float* %329 to <4 x float>*
  %wide.load66 = load <4 x float>, <4 x float>* %330, align 4, !tbaa !2043
  %331 = getelementptr inbounds float, float* %4, i64 %326
  %332 = bitcast float* %331 to <4 x float>*
  store <4 x float> %wide.load65, <4 x float>* %332, align 4, !tbaa !2046
  %333 = getelementptr inbounds float, float* %331, i64 4
  %334 = bitcast float* %333 to <4 x float>*
  store <4 x float> %wide.load66, <4 x float>* %334, align 4, !tbaa !2046
  %index.next62 = or i64 %index61, 8
  %335 = add nsw i64 %325, %index.next62
  %336 = getelementptr inbounds float, float* %7, i64 %335
  %337 = bitcast float* %336 to <4 x float>*
  %wide.load65.1 = load <4 x float>, <4 x float>* %337, align 4, !tbaa !2043
  %338 = getelementptr inbounds float, float* %336, i64 4
  %339 = bitcast float* %338 to <4 x float>*
  %wide.load66.1 = load <4 x float>, <4 x float>* %339, align 4, !tbaa !2043
  %340 = getelementptr inbounds float, float* %4, i64 %335
  %341 = bitcast float* %340 to <4 x float>*
  store <4 x float> %wide.load65.1, <4 x float>* %341, align 4, !tbaa !2046
  %342 = getelementptr inbounds float, float* %340, i64 4
  %343 = bitcast float* %342 to <4 x float>*
  store <4 x float> %wide.load66.1, <4 x float>* %343, align 4, !tbaa !2046
  %index.next62.1 = or i64 %index61, 16
  %344 = add nsw i64 %325, %index.next62.1
  %345 = getelementptr inbounds float, float* %7, i64 %344
  %346 = bitcast float* %345 to <4 x float>*
  %wide.load65.2 = load <4 x float>, <4 x float>* %346, align 4, !tbaa !2043
  %347 = getelementptr inbounds float, float* %345, i64 4
  %348 = bitcast float* %347 to <4 x float>*
  %wide.load66.2 = load <4 x float>, <4 x float>* %348, align 4, !tbaa !2043
  %349 = getelementptr inbounds float, float* %4, i64 %344
  %350 = bitcast float* %349 to <4 x float>*
  store <4 x float> %wide.load65.2, <4 x float>* %350, align 4, !tbaa !2046
  %351 = getelementptr inbounds float, float* %349, i64 4
  %352 = bitcast float* %351 to <4 x float>*
  store <4 x float> %wide.load66.2, <4 x float>* %352, align 4, !tbaa !2046
  %index.next62.2 = or i64 %index61, 24
  %353 = add nsw i64 %325, %index.next62.2
  %354 = getelementptr inbounds float, float* %7, i64 %353
  %355 = bitcast float* %354 to <4 x float>*
  %wide.load65.3 = load <4 x float>, <4 x float>* %355, align 4, !tbaa !2043
  %356 = getelementptr inbounds float, float* %354, i64 4
  %357 = bitcast float* %356 to <4 x float>*
  %wide.load66.3 = load <4 x float>, <4 x float>* %357, align 4, !tbaa !2043
  %358 = getelementptr inbounds float, float* %4, i64 %353
  %359 = bitcast float* %358 to <4 x float>*
  store <4 x float> %wide.load65.3, <4 x float>* %359, align 4, !tbaa !2046
  %360 = getelementptr inbounds float, float* %358, i64 4
  %361 = bitcast float* %360 to <4 x float>*
  store <4 x float> %wide.load66.3, <4 x float>* %361, align 4, !tbaa !2046
  %index.next62.3 = add nuw nsw i64 %index61, 32
  %362 = icmp eq i64 %index.next62.3, 512
  br i1 %362, label %for_end6.8, label %vector.body59, !prof !341, !llvm.loop !2057

for_end6.8:                                       ; preds = %vector.body59
  %363 = add nsw i64 %21, 4608
  br label %vector.body49

vector.body49:                                    ; preds = %vector.body49, %for_end6.8
  %index51 = phi i64 [ 0, %for_end6.8 ], [ %index.next52.3, %vector.body49 ]
  %364 = add nsw i64 %363, %index51
  %365 = getelementptr inbounds float, float* %7, i64 %364
  %366 = bitcast float* %365 to <4 x float>*
  %wide.load55 = load <4 x float>, <4 x float>* %366, align 4, !tbaa !2043
  %367 = getelementptr inbounds float, float* %365, i64 4
  %368 = bitcast float* %367 to <4 x float>*
  %wide.load56 = load <4 x float>, <4 x float>* %368, align 4, !tbaa !2043
  %369 = getelementptr inbounds float, float* %4, i64 %364
  %370 = bitcast float* %369 to <4 x float>*
  store <4 x float> %wide.load55, <4 x float>* %370, align 4, !tbaa !2046
  %371 = getelementptr inbounds float, float* %369, i64 4
  %372 = bitcast float* %371 to <4 x float>*
  store <4 x float> %wide.load56, <4 x float>* %372, align 4, !tbaa !2046
  %index.next52 = or i64 %index51, 8
  %373 = add nsw i64 %363, %index.next52
  %374 = getelementptr inbounds float, float* %7, i64 %373
  %375 = bitcast float* %374 to <4 x float>*
  %wide.load55.1 = load <4 x float>, <4 x float>* %375, align 4, !tbaa !2043
  %376 = getelementptr inbounds float, float* %374, i64 4
  %377 = bitcast float* %376 to <4 x float>*
  %wide.load56.1 = load <4 x float>, <4 x float>* %377, align 4, !tbaa !2043
  %378 = getelementptr inbounds float, float* %4, i64 %373
  %379 = bitcast float* %378 to <4 x float>*
  store <4 x float> %wide.load55.1, <4 x float>* %379, align 4, !tbaa !2046
  %380 = getelementptr inbounds float, float* %378, i64 4
  %381 = bitcast float* %380 to <4 x float>*
  store <4 x float> %wide.load56.1, <4 x float>* %381, align 4, !tbaa !2046
  %index.next52.1 = or i64 %index51, 16
  %382 = add nsw i64 %363, %index.next52.1
  %383 = getelementptr inbounds float, float* %7, i64 %382
  %384 = bitcast float* %383 to <4 x float>*
  %wide.load55.2 = load <4 x float>, <4 x float>* %384, align 4, !tbaa !2043
  %385 = getelementptr inbounds float, float* %383, i64 4
  %386 = bitcast float* %385 to <4 x float>*
  %wide.load56.2 = load <4 x float>, <4 x float>* %386, align 4, !tbaa !2043
  %387 = getelementptr inbounds float, float* %4, i64 %382
  %388 = bitcast float* %387 to <4 x float>*
  store <4 x float> %wide.load55.2, <4 x float>* %388, align 4, !tbaa !2046
  %389 = getelementptr inbounds float, float* %387, i64 4
  %390 = bitcast float* %389 to <4 x float>*
  store <4 x float> %wide.load56.2, <4 x float>* %390, align 4, !tbaa !2046
  %index.next52.2 = or i64 %index51, 24
  %391 = add nsw i64 %363, %index.next52.2
  %392 = getelementptr inbounds float, float* %7, i64 %391
  %393 = bitcast float* %392 to <4 x float>*
  %wide.load55.3 = load <4 x float>, <4 x float>* %393, align 4, !tbaa !2043
  %394 = getelementptr inbounds float, float* %392, i64 4
  %395 = bitcast float* %394 to <4 x float>*
  %wide.load56.3 = load <4 x float>, <4 x float>* %395, align 4, !tbaa !2043
  %396 = getelementptr inbounds float, float* %4, i64 %391
  %397 = bitcast float* %396 to <4 x float>*
  store <4 x float> %wide.load55.3, <4 x float>* %397, align 4, !tbaa !2046
  %398 = getelementptr inbounds float, float* %396, i64 4
  %399 = bitcast float* %398 to <4 x float>*
  store <4 x float> %wide.load56.3, <4 x float>* %399, align 4, !tbaa !2046
  %index.next52.3 = add nuw nsw i64 %index51, 32
  %400 = icmp eq i64 %index.next52.3, 512
  br i1 %400, label %for_end6.9, label %vector.body49, !prof !341, !llvm.loop !2058

for_end6.9:                                       ; preds = %vector.body49
  %401 = add nsw i64 %21, 5120
  br label %vector.body39

vector.body39:                                    ; preds = %vector.body39, %for_end6.9
  %index41 = phi i64 [ 0, %for_end6.9 ], [ %index.next42.3, %vector.body39 ]
  %402 = add nuw nsw i64 %401, %index41
  %403 = getelementptr inbounds float, float* %7, i64 %402
  %404 = bitcast float* %403 to <4 x float>*
  %wide.load45 = load <4 x float>, <4 x float>* %404, align 4, !tbaa !2043
  %405 = getelementptr inbounds float, float* %403, i64 4
  %406 = bitcast float* %405 to <4 x float>*
  %wide.load46 = load <4 x float>, <4 x float>* %406, align 4, !tbaa !2043
  %407 = getelementptr inbounds float, float* %4, i64 %402
  %408 = bitcast float* %407 to <4 x float>*
  store <4 x float> %wide.load45, <4 x float>* %408, align 4, !tbaa !2046
  %409 = getelementptr inbounds float, float* %407, i64 4
  %410 = bitcast float* %409 to <4 x float>*
  store <4 x float> %wide.load46, <4 x float>* %410, align 4, !tbaa !2046
  %index.next42 = or i64 %index41, 8
  %411 = add nsw i64 %401, %index.next42
  %412 = getelementptr inbounds float, float* %7, i64 %411
  %413 = bitcast float* %412 to <4 x float>*
  %wide.load45.1 = load <4 x float>, <4 x float>* %413, align 4, !tbaa !2043
  %414 = getelementptr inbounds float, float* %412, i64 4
  %415 = bitcast float* %414 to <4 x float>*
  %wide.load46.1 = load <4 x float>, <4 x float>* %415, align 4, !tbaa !2043
  %416 = getelementptr inbounds float, float* %4, i64 %411
  %417 = bitcast float* %416 to <4 x float>*
  store <4 x float> %wide.load45.1, <4 x float>* %417, align 4, !tbaa !2046
  %418 = getelementptr inbounds float, float* %416, i64 4
  %419 = bitcast float* %418 to <4 x float>*
  store <4 x float> %wide.load46.1, <4 x float>* %419, align 4, !tbaa !2046
  %index.next42.1 = or i64 %index41, 16
  %420 = add nsw i64 %401, %index.next42.1
  %421 = getelementptr inbounds float, float* %7, i64 %420
  %422 = bitcast float* %421 to <4 x float>*
  %wide.load45.2 = load <4 x float>, <4 x float>* %422, align 4, !tbaa !2043
  %423 = getelementptr inbounds float, float* %421, i64 4
  %424 = bitcast float* %423 to <4 x float>*
  %wide.load46.2 = load <4 x float>, <4 x float>* %424, align 4, !tbaa !2043
  %425 = getelementptr inbounds float, float* %4, i64 %420
  %426 = bitcast float* %425 to <4 x float>*
  store <4 x float> %wide.load45.2, <4 x float>* %426, align 4, !tbaa !2046
  %427 = getelementptr inbounds float, float* %425, i64 4
  %428 = bitcast float* %427 to <4 x float>*
  store <4 x float> %wide.load46.2, <4 x float>* %428, align 4, !tbaa !2046
  %index.next42.2 = or i64 %index41, 24
  %429 = add nsw i64 %401, %index.next42.2
  %430 = getelementptr inbounds float, float* %7, i64 %429
  %431 = bitcast float* %430 to <4 x float>*
  %wide.load45.3 = load <4 x float>, <4 x float>* %431, align 4, !tbaa !2043
  %432 = getelementptr inbounds float, float* %430, i64 4
  %433 = bitcast float* %432 to <4 x float>*
  %wide.load46.3 = load <4 x float>, <4 x float>* %433, align 4, !tbaa !2043
  %434 = getelementptr inbounds float, float* %4, i64 %429
  %435 = bitcast float* %434 to <4 x float>*
  store <4 x float> %wide.load45.3, <4 x float>* %435, align 4, !tbaa !2046
  %436 = getelementptr inbounds float, float* %434, i64 4
  %437 = bitcast float* %436 to <4 x float>*
  store <4 x float> %wide.load46.3, <4 x float>* %437, align 4, !tbaa !2046
  %index.next42.3 = add nuw nsw i64 %index41, 32
  %438 = icmp eq i64 %index.next42.3, 512
  br i1 %438, label %for_end6.10, label %vector.body39, !prof !341, !llvm.loop !2059

for_end6.10:                                      ; preds = %vector.body39
  %439 = add nsw i64 %21, 5632
  br label %vector.body29

vector.body29:                                    ; preds = %vector.body29, %for_end6.10
  %index31 = phi i64 [ 0, %for_end6.10 ], [ %index.next32.3, %vector.body29 ]
  %440 = add nsw i64 %439, %index31
  %441 = getelementptr inbounds float, float* %7, i64 %440
  %442 = bitcast float* %441 to <4 x float>*
  %wide.load35 = load <4 x float>, <4 x float>* %442, align 4, !tbaa !2043
  %443 = getelementptr inbounds float, float* %441, i64 4
  %444 = bitcast float* %443 to <4 x float>*
  %wide.load36 = load <4 x float>, <4 x float>* %444, align 4, !tbaa !2043
  %445 = getelementptr inbounds float, float* %4, i64 %440
  %446 = bitcast float* %445 to <4 x float>*
  store <4 x float> %wide.load35, <4 x float>* %446, align 4, !tbaa !2046
  %447 = getelementptr inbounds float, float* %445, i64 4
  %448 = bitcast float* %447 to <4 x float>*
  store <4 x float> %wide.load36, <4 x float>* %448, align 4, !tbaa !2046
  %index.next32 = or i64 %index31, 8
  %449 = add nsw i64 %439, %index.next32
  %450 = getelementptr inbounds float, float* %7, i64 %449
  %451 = bitcast float* %450 to <4 x float>*
  %wide.load35.1 = load <4 x float>, <4 x float>* %451, align 4, !tbaa !2043
  %452 = getelementptr inbounds float, float* %450, i64 4
  %453 = bitcast float* %452 to <4 x float>*
  %wide.load36.1 = load <4 x float>, <4 x float>* %453, align 4, !tbaa !2043
  %454 = getelementptr inbounds float, float* %4, i64 %449
  %455 = bitcast float* %454 to <4 x float>*
  store <4 x float> %wide.load35.1, <4 x float>* %455, align 4, !tbaa !2046
  %456 = getelementptr inbounds float, float* %454, i64 4
  %457 = bitcast float* %456 to <4 x float>*
  store <4 x float> %wide.load36.1, <4 x float>* %457, align 4, !tbaa !2046
  %index.next32.1 = or i64 %index31, 16
  %458 = add nsw i64 %439, %index.next32.1
  %459 = getelementptr inbounds float, float* %7, i64 %458
  %460 = bitcast float* %459 to <4 x float>*
  %wide.load35.2 = load <4 x float>, <4 x float>* %460, align 4, !tbaa !2043
  %461 = getelementptr inbounds float, float* %459, i64 4
  %462 = bitcast float* %461 to <4 x float>*
  %wide.load36.2 = load <4 x float>, <4 x float>* %462, align 4, !tbaa !2043
  %463 = getelementptr inbounds float, float* %4, i64 %458
  %464 = bitcast float* %463 to <4 x float>*
  store <4 x float> %wide.load35.2, <4 x float>* %464, align 4, !tbaa !2046
  %465 = getelementptr inbounds float, float* %463, i64 4
  %466 = bitcast float* %465 to <4 x float>*
  store <4 x float> %wide.load36.2, <4 x float>* %466, align 4, !tbaa !2046
  %index.next32.2 = or i64 %index31, 24
  %467 = add nsw i64 %439, %index.next32.2
  %468 = getelementptr inbounds float, float* %7, i64 %467
  %469 = bitcast float* %468 to <4 x float>*
  %wide.load35.3 = load <4 x float>, <4 x float>* %469, align 4, !tbaa !2043
  %470 = getelementptr inbounds float, float* %468, i64 4
  %471 = bitcast float* %470 to <4 x float>*
  %wide.load36.3 = load <4 x float>, <4 x float>* %471, align 4, !tbaa !2043
  %472 = getelementptr inbounds float, float* %4, i64 %467
  %473 = bitcast float* %472 to <4 x float>*
  store <4 x float> %wide.load35.3, <4 x float>* %473, align 4, !tbaa !2046
  %474 = getelementptr inbounds float, float* %472, i64 4
  %475 = bitcast float* %474 to <4 x float>*
  store <4 x float> %wide.load36.3, <4 x float>* %475, align 4, !tbaa !2046
  %index.next32.3 = add nuw nsw i64 %index31, 32
  %476 = icmp eq i64 %index.next32.3, 512
  br i1 %476, label %for_end6.11, label %vector.body29, !prof !341, !llvm.loop !2060

for_end6.11:                                      ; preds = %vector.body29
  %477 = add nsw i64 %21, 6144
  br label %vector.body19

vector.body19:                                    ; preds = %vector.body19, %for_end6.11
  %index21 = phi i64 [ 0, %for_end6.11 ], [ %index.next22.3, %vector.body19 ]
  %478 = add nuw nsw i64 %477, %index21
  %479 = getelementptr inbounds float, float* %7, i64 %478
  %480 = bitcast float* %479 to <4 x float>*
  %wide.load25 = load <4 x float>, <4 x float>* %480, align 4, !tbaa !2043
  %481 = getelementptr inbounds float, float* %479, i64 4
  %482 = bitcast float* %481 to <4 x float>*
  %wide.load26 = load <4 x float>, <4 x float>* %482, align 4, !tbaa !2043
  %483 = getelementptr inbounds float, float* %4, i64 %478
  %484 = bitcast float* %483 to <4 x float>*
  store <4 x float> %wide.load25, <4 x float>* %484, align 4, !tbaa !2046
  %485 = getelementptr inbounds float, float* %483, i64 4
  %486 = bitcast float* %485 to <4 x float>*
  store <4 x float> %wide.load26, <4 x float>* %486, align 4, !tbaa !2046
  %index.next22 = or i64 %index21, 8
  %487 = add nsw i64 %477, %index.next22
  %488 = getelementptr inbounds float, float* %7, i64 %487
  %489 = bitcast float* %488 to <4 x float>*
  %wide.load25.1 = load <4 x float>, <4 x float>* %489, align 4, !tbaa !2043
  %490 = getelementptr inbounds float, float* %488, i64 4
  %491 = bitcast float* %490 to <4 x float>*
  %wide.load26.1 = load <4 x float>, <4 x float>* %491, align 4, !tbaa !2043
  %492 = getelementptr inbounds float, float* %4, i64 %487
  %493 = bitcast float* %492 to <4 x float>*
  store <4 x float> %wide.load25.1, <4 x float>* %493, align 4, !tbaa !2046
  %494 = getelementptr inbounds float, float* %492, i64 4
  %495 = bitcast float* %494 to <4 x float>*
  store <4 x float> %wide.load26.1, <4 x float>* %495, align 4, !tbaa !2046
  %index.next22.1 = or i64 %index21, 16
  %496 = add nsw i64 %477, %index.next22.1
  %497 = getelementptr inbounds float, float* %7, i64 %496
  %498 = bitcast float* %497 to <4 x float>*
  %wide.load25.2 = load <4 x float>, <4 x float>* %498, align 4, !tbaa !2043
  %499 = getelementptr inbounds float, float* %497, i64 4
  %500 = bitcast float* %499 to <4 x float>*
  %wide.load26.2 = load <4 x float>, <4 x float>* %500, align 4, !tbaa !2043
  %501 = getelementptr inbounds float, float* %4, i64 %496
  %502 = bitcast float* %501 to <4 x float>*
  store <4 x float> %wide.load25.2, <4 x float>* %502, align 4, !tbaa !2046
  %503 = getelementptr inbounds float, float* %501, i64 4
  %504 = bitcast float* %503 to <4 x float>*
  store <4 x float> %wide.load26.2, <4 x float>* %504, align 4, !tbaa !2046
  %index.next22.2 = or i64 %index21, 24
  %505 = add nsw i64 %477, %index.next22.2
  %506 = getelementptr inbounds float, float* %7, i64 %505
  %507 = bitcast float* %506 to <4 x float>*
  %wide.load25.3 = load <4 x float>, <4 x float>* %507, align 4, !tbaa !2043
  %508 = getelementptr inbounds float, float* %506, i64 4
  %509 = bitcast float* %508 to <4 x float>*
  %wide.load26.3 = load <4 x float>, <4 x float>* %509, align 4, !tbaa !2043
  %510 = getelementptr inbounds float, float* %4, i64 %505
  %511 = bitcast float* %510 to <4 x float>*
  store <4 x float> %wide.load25.3, <4 x float>* %511, align 4, !tbaa !2046
  %512 = getelementptr inbounds float, float* %510, i64 4
  %513 = bitcast float* %512 to <4 x float>*
  store <4 x float> %wide.load26.3, <4 x float>* %513, align 4, !tbaa !2046
  %index.next22.3 = add nuw nsw i64 %index21, 32
  %514 = icmp eq i64 %index.next22.3, 512
  br i1 %514, label %for_end6.12, label %vector.body19, !prof !341, !llvm.loop !2061

for_end6.12:                                      ; preds = %vector.body19
  %515 = add nsw i64 %21, 6656
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_end6.12
  %index = phi i64 [ 0, %for_end6.12 ], [ %index.next.3, %vector.body ]
  %516 = add nsw i64 %515, %index
  %517 = getelementptr inbounds float, float* %7, i64 %516
  %518 = bitcast float* %517 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %518, align 4, !tbaa !2043
  %519 = getelementptr inbounds float, float* %517, i64 4
  %520 = bitcast float* %519 to <4 x float>*
  %wide.load16 = load <4 x float>, <4 x float>* %520, align 4, !tbaa !2043
  %521 = getelementptr inbounds float, float* %4, i64 %516
  %522 = bitcast float* %521 to <4 x float>*
  store <4 x float> %wide.load, <4 x float>* %522, align 4, !tbaa !2046
  %523 = getelementptr inbounds float, float* %521, i64 4
  %524 = bitcast float* %523 to <4 x float>*
  store <4 x float> %wide.load16, <4 x float>* %524, align 4, !tbaa !2046
  %index.next = or i64 %index, 8
  %525 = add nsw i64 %515, %index.next
  %526 = getelementptr inbounds float, float* %7, i64 %525
  %527 = bitcast float* %526 to <4 x float>*
  %wide.load.1 = load <4 x float>, <4 x float>* %527, align 4, !tbaa !2043
  %528 = getelementptr inbounds float, float* %526, i64 4
  %529 = bitcast float* %528 to <4 x float>*
  %wide.load16.1 = load <4 x float>, <4 x float>* %529, align 4, !tbaa !2043
  %530 = getelementptr inbounds float, float* %4, i64 %525
  %531 = bitcast float* %530 to <4 x float>*
  store <4 x float> %wide.load.1, <4 x float>* %531, align 4, !tbaa !2046
  %532 = getelementptr inbounds float, float* %530, i64 4
  %533 = bitcast float* %532 to <4 x float>*
  store <4 x float> %wide.load16.1, <4 x float>* %533, align 4, !tbaa !2046
  %index.next.1 = or i64 %index, 16
  %534 = add nsw i64 %515, %index.next.1
  %535 = getelementptr inbounds float, float* %7, i64 %534
  %536 = bitcast float* %535 to <4 x float>*
  %wide.load.2 = load <4 x float>, <4 x float>* %536, align 4, !tbaa !2043
  %537 = getelementptr inbounds float, float* %535, i64 4
  %538 = bitcast float* %537 to <4 x float>*
  %wide.load16.2 = load <4 x float>, <4 x float>* %538, align 4, !tbaa !2043
  %539 = getelementptr inbounds float, float* %4, i64 %534
  %540 = bitcast float* %539 to <4 x float>*
  store <4 x float> %wide.load.2, <4 x float>* %540, align 4, !tbaa !2046
  %541 = getelementptr inbounds float, float* %539, i64 4
  %542 = bitcast float* %541 to <4 x float>*
  store <4 x float> %wide.load16.2, <4 x float>* %542, align 4, !tbaa !2046
  %index.next.2 = or i64 %index, 24
  %543 = add nsw i64 %515, %index.next.2
  %544 = getelementptr inbounds float, float* %7, i64 %543
  %545 = bitcast float* %544 to <4 x float>*
  %wide.load.3 = load <4 x float>, <4 x float>* %545, align 4, !tbaa !2043
  %546 = getelementptr inbounds float, float* %544, i64 4
  %547 = bitcast float* %546 to <4 x float>*
  %wide.load16.3 = load <4 x float>, <4 x float>* %547, align 4, !tbaa !2043
  %548 = getelementptr inbounds float, float* %4, i64 %543
  %549 = bitcast float* %548 to <4 x float>*
  store <4 x float> %wide.load.3, <4 x float>* %549, align 4, !tbaa !2046
  %550 = getelementptr inbounds float, float* %548, i64 4
  %551 = bitcast float* %550 to <4 x float>*
  store <4 x float> %wide.load16.3, <4 x float>* %551, align 4, !tbaa !2046
  %index.next.3 = add nuw nsw i64 %index, 32
  %552 = icmp eq i64 %index.next.3, 512
  br i1 %552, label %for_end6.13, label %vector.body, !prof !341, !llvm.loop !2062

for_end6.13:                                      ; preds = %vector.body
  %indvars.iv.next14 = add nsw i64 %indvars.iv13, 1
  %exitcond15.not = icmp eq i64 %indvars.iv.next14, %wide.trip.count
  br i1 %exitcond15.not, label %for_end, label %for_begin1.preheader, !prof !51
}

; Function Attrs: nofree nounwind
define private i32 @__tvm_parallel_lambda.184(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #3 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds i8, i8* %2, i64 16
  %9 = bitcast i8* %8 to float**
  %10 = load float*, float** %9, align 8
  %11 = getelementptr inbounds i8, i8* %2, i64 24
  %12 = bitcast i8* %11 to float**
  %13 = load float*, float** %12, align 8
  %14 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %15 = load i32, i32* %14, align 4
  %16 = add nsw i32 %15, 195
  %17 = sdiv i32 %16, %15
  %18 = add nsw i32 %0, 1
  %19 = mul nsw i32 %17, %18
  %20 = icmp slt i32 %19, 196
  %21 = select i1 %20, i32 %19, i32 196
  %22 = mul nsw i32 %17, %0
  %23 = icmp slt i32 %22, 196
  %24 = select i1 %23, i32 %22, i32 196
  %25 = icmp slt i32 %24, %21
  br i1 %25, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %26 = sext i32 %24 to i64
  %wide.trip.count = sext i32 %21 to i64
  %27 = bitcast float* %13 to <64 x float>*
  %28 = load <64 x float>, <64 x float>* %27, align 128, !tbaa !2063
  %29 = getelementptr inbounds float, float* %13, i64 64
  %30 = bitcast float* %29 to <64 x float>*
  %31 = load <64 x float>, <64 x float>* %30, align 128, !tbaa !2063
  %32 = getelementptr inbounds float, float* %13, i64 128
  %33 = bitcast float* %32 to <64 x float>*
  %34 = load <64 x float>, <64 x float>* %33, align 128, !tbaa !2063
  %35 = getelementptr inbounds float, float* %13, i64 192
  %36 = bitcast float* %35 to <64 x float>*
  %37 = load <64 x float>, <64 x float>* %36, align 128, !tbaa !2063
  %38 = getelementptr inbounds float, float* %13, i64 256
  %39 = bitcast float* %38 to <64 x float>*
  %40 = load <64 x float>, <64 x float>* %39, align 128, !tbaa !2063
  %41 = getelementptr inbounds float, float* %13, i64 320
  %42 = bitcast float* %41 to <64 x float>*
  %43 = load <64 x float>, <64 x float>* %42, align 128, !tbaa !2063
  %44 = getelementptr inbounds float, float* %13, i64 384
  %45 = bitcast float* %44 to <64 x float>*
  %46 = load <64 x float>, <64 x float>* %45, align 128, !tbaa !2063
  %47 = getelementptr inbounds float, float* %13, i64 448
  %48 = bitcast float* %47 to <64 x float>*
  %49 = load <64 x float>, <64 x float>* %48, align 128, !tbaa !2063
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end6.7
  %indvars.iv14 = phi i64 [ %26, %for_begin1.preheader.preheader ], [ %indvars.iv.next15, %for_end6.7 ]
  %50 = phi i32 [ %24, %for_begin1.preheader.preheader ], [ %290, %for_end6.7 ]
  %51 = shl nsw i32 %50, 9
  %52 = sext i32 %51 to i64
  br label %for_body5

for_end:                                          ; preds = %for_end6.7, %entry
  ret i32 0

for_body5:                                        ; preds = %for_body5, %for_begin1.preheader
  %indvars.iv = phi i64 [ 0, %for_begin1.preheader ], [ %indvars.iv.next.140, %for_body5 ]
  %.010 = phi <64 x float> [ zeroinitializer, %for_begin1.preheader ], [ %72, %for_body5 ]
  %53 = add nsw i64 %indvars.iv, %52
  %54 = getelementptr inbounds float, float* %4, i64 %53
  %55 = load float, float* %54, align 4, !tbaa !2046
  %56 = insertelement <64 x float> undef, float %55, i32 0
  %57 = shufflevector <64 x float> %56, <64 x float> undef, <64 x i32> zeroinitializer
  %58 = shl nuw nsw i64 %indvars.iv, 9
  %59 = getelementptr inbounds float, float* %7, i64 %58
  %60 = bitcast float* %59 to <64 x float>*
  %61 = load <64 x float>, <64 x float>* %60, align 128, !tbaa !2066
  %62 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %57, <64 x float> %61, <64 x float> %.010)
  %indvars.iv.next = or i64 %indvars.iv, 1
  %63 = add nsw i64 %indvars.iv.next, %52
  %64 = getelementptr inbounds float, float* %4, i64 %63
  %65 = load float, float* %64, align 4, !tbaa !2046
  %66 = insertelement <64 x float> undef, float %65, i32 0
  %67 = shufflevector <64 x float> %66, <64 x float> undef, <64 x i32> zeroinitializer
  %68 = shl nuw nsw i64 %indvars.iv.next, 9
  %69 = getelementptr inbounds float, float* %7, i64 %68
  %70 = bitcast float* %69 to <64 x float>*
  %71 = load <64 x float>, <64 x float>* %70, align 128, !tbaa !2066
  %72 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %67, <64 x float> %71, <64 x float> %62)
  %indvars.iv.next.140 = add nuw nsw i64 %indvars.iv, 2
  %exitcond.not.1 = icmp eq i64 %indvars.iv.next.140, 512
  br i1 %exitcond.not.1, label %for_end6, label %for_body5, !prof !51

for_end6:                                         ; preds = %for_body5
  %73 = fadd <64 x float> %72, %28
  %74 = fcmp olt <64 x float> %73, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %75 = select <64 x i1> %74, <64 x float> %73, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %76 = fcmp ogt <64 x float> %75, zeroinitializer
  %77 = select <64 x i1> %76, <64 x float> %75, <64 x float> zeroinitializer
  %78 = getelementptr inbounds float, float* %10, i64 %52
  %79 = bitcast float* %78 to <64 x float>*
  store <64 x float> %77, <64 x float>* %79, align 128, !tbaa !2069
  br label %for_body5.1

for_body5.1:                                      ; preds = %for_body5.1, %for_end6
  %indvars.iv.1 = phi i64 [ 0, %for_end6 ], [ %indvars.iv.next.1.1, %for_body5.1 ]
  %.010.1 = phi <64 x float> [ zeroinitializer, %for_end6 ], [ %101, %for_body5.1 ]
  %80 = add nsw i64 %indvars.iv.1, %52
  %81 = getelementptr inbounds float, float* %4, i64 %80
  %82 = load float, float* %81, align 4, !tbaa !2046
  %83 = insertelement <64 x float> undef, float %82, i32 0
  %84 = shufflevector <64 x float> %83, <64 x float> undef, <64 x i32> zeroinitializer
  %85 = shl nuw nsw i64 %indvars.iv.1, 9
  %86 = or i64 %85, 64
  %87 = getelementptr inbounds float, float* %7, i64 %86
  %88 = bitcast float* %87 to <64 x float>*
  %89 = load <64 x float>, <64 x float>* %88, align 128, !tbaa !2066
  %90 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %84, <64 x float> %89, <64 x float> %.010.1)
  %indvars.iv.next.1 = or i64 %indvars.iv.1, 1
  %91 = add nsw i64 %indvars.iv.next.1, %52
  %92 = getelementptr inbounds float, float* %4, i64 %91
  %93 = load float, float* %92, align 4, !tbaa !2046
  %94 = insertelement <64 x float> undef, float %93, i32 0
  %95 = shufflevector <64 x float> %94, <64 x float> undef, <64 x i32> zeroinitializer
  %96 = shl nuw nsw i64 %indvars.iv.next.1, 9
  %97 = or i64 %96, 64
  %98 = getelementptr inbounds float, float* %7, i64 %97
  %99 = bitcast float* %98 to <64 x float>*
  %100 = load <64 x float>, <64 x float>* %99, align 128, !tbaa !2066
  %101 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %95, <64 x float> %100, <64 x float> %90)
  %indvars.iv.next.1.1 = add nuw nsw i64 %indvars.iv.1, 2
  %exitcond.1.not.1 = icmp eq i64 %indvars.iv.next.1.1, 512
  br i1 %exitcond.1.not.1, label %for_end6.1, label %for_body5.1, !prof !51

for_end6.1:                                       ; preds = %for_body5.1
  %102 = or i64 %52, 64
  %103 = fadd <64 x float> %101, %31
  %104 = fcmp olt <64 x float> %103, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %105 = select <64 x i1> %104, <64 x float> %103, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %106 = fcmp ogt <64 x float> %105, zeroinitializer
  %107 = select <64 x i1> %106, <64 x float> %105, <64 x float> zeroinitializer
  %108 = getelementptr inbounds float, float* %10, i64 %102
  %109 = bitcast float* %108 to <64 x float>*
  store <64 x float> %107, <64 x float>* %109, align 128, !tbaa !2069
  br label %for_body5.2

for_body5.2:                                      ; preds = %for_body5.2, %for_end6.1
  %indvars.iv.2 = phi i64 [ 0, %for_end6.1 ], [ %indvars.iv.next.2.1, %for_body5.2 ]
  %.010.2 = phi <64 x float> [ zeroinitializer, %for_end6.1 ], [ %131, %for_body5.2 ]
  %110 = add nsw i64 %indvars.iv.2, %52
  %111 = getelementptr inbounds float, float* %4, i64 %110
  %112 = load float, float* %111, align 4, !tbaa !2046
  %113 = insertelement <64 x float> undef, float %112, i32 0
  %114 = shufflevector <64 x float> %113, <64 x float> undef, <64 x i32> zeroinitializer
  %115 = shl nuw nsw i64 %indvars.iv.2, 9
  %116 = or i64 %115, 128
  %117 = getelementptr inbounds float, float* %7, i64 %116
  %118 = bitcast float* %117 to <64 x float>*
  %119 = load <64 x float>, <64 x float>* %118, align 128, !tbaa !2066
  %120 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %114, <64 x float> %119, <64 x float> %.010.2)
  %indvars.iv.next.2 = or i64 %indvars.iv.2, 1
  %121 = add nsw i64 %indvars.iv.next.2, %52
  %122 = getelementptr inbounds float, float* %4, i64 %121
  %123 = load float, float* %122, align 4, !tbaa !2046
  %124 = insertelement <64 x float> undef, float %123, i32 0
  %125 = shufflevector <64 x float> %124, <64 x float> undef, <64 x i32> zeroinitializer
  %126 = shl nuw nsw i64 %indvars.iv.next.2, 9
  %127 = or i64 %126, 128
  %128 = getelementptr inbounds float, float* %7, i64 %127
  %129 = bitcast float* %128 to <64 x float>*
  %130 = load <64 x float>, <64 x float>* %129, align 128, !tbaa !2066
  %131 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %125, <64 x float> %130, <64 x float> %120)
  %indvars.iv.next.2.1 = add nuw nsw i64 %indvars.iv.2, 2
  %exitcond.2.not.1 = icmp eq i64 %indvars.iv.next.2.1, 512
  br i1 %exitcond.2.not.1, label %for_end6.2, label %for_body5.2, !prof !51

for_end6.2:                                       ; preds = %for_body5.2
  %132 = or i64 %52, 128
  %133 = fadd <64 x float> %131, %34
  %134 = fcmp olt <64 x float> %133, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %135 = select <64 x i1> %134, <64 x float> %133, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %136 = fcmp ogt <64 x float> %135, zeroinitializer
  %137 = select <64 x i1> %136, <64 x float> %135, <64 x float> zeroinitializer
  %138 = getelementptr inbounds float, float* %10, i64 %132
  %139 = bitcast float* %138 to <64 x float>*
  store <64 x float> %137, <64 x float>* %139, align 128, !tbaa !2069
  br label %for_body5.3

for_body5.3:                                      ; preds = %for_body5.3, %for_end6.2
  %indvars.iv.3 = phi i64 [ 0, %for_end6.2 ], [ %indvars.iv.next.3.1, %for_body5.3 ]
  %.010.3 = phi <64 x float> [ zeroinitializer, %for_end6.2 ], [ %161, %for_body5.3 ]
  %140 = add nsw i64 %indvars.iv.3, %52
  %141 = getelementptr inbounds float, float* %4, i64 %140
  %142 = load float, float* %141, align 4, !tbaa !2046
  %143 = insertelement <64 x float> undef, float %142, i32 0
  %144 = shufflevector <64 x float> %143, <64 x float> undef, <64 x i32> zeroinitializer
  %145 = shl nuw nsw i64 %indvars.iv.3, 9
  %146 = or i64 %145, 192
  %147 = getelementptr inbounds float, float* %7, i64 %146
  %148 = bitcast float* %147 to <64 x float>*
  %149 = load <64 x float>, <64 x float>* %148, align 128, !tbaa !2066
  %150 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %144, <64 x float> %149, <64 x float> %.010.3)
  %indvars.iv.next.3 = or i64 %indvars.iv.3, 1
  %151 = add nsw i64 %indvars.iv.next.3, %52
  %152 = getelementptr inbounds float, float* %4, i64 %151
  %153 = load float, float* %152, align 4, !tbaa !2046
  %154 = insertelement <64 x float> undef, float %153, i32 0
  %155 = shufflevector <64 x float> %154, <64 x float> undef, <64 x i32> zeroinitializer
  %156 = shl nuw nsw i64 %indvars.iv.next.3, 9
  %157 = or i64 %156, 192
  %158 = getelementptr inbounds float, float* %7, i64 %157
  %159 = bitcast float* %158 to <64 x float>*
  %160 = load <64 x float>, <64 x float>* %159, align 128, !tbaa !2066
  %161 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %155, <64 x float> %160, <64 x float> %150)
  %indvars.iv.next.3.1 = add nuw nsw i64 %indvars.iv.3, 2
  %exitcond.3.not.1 = icmp eq i64 %indvars.iv.next.3.1, 512
  br i1 %exitcond.3.not.1, label %for_end6.3, label %for_body5.3, !prof !51

for_end6.3:                                       ; preds = %for_body5.3
  %162 = or i64 %52, 192
  %163 = fadd <64 x float> %161, %37
  %164 = fcmp olt <64 x float> %163, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %165 = select <64 x i1> %164, <64 x float> %163, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %166 = fcmp ogt <64 x float> %165, zeroinitializer
  %167 = select <64 x i1> %166, <64 x float> %165, <64 x float> zeroinitializer
  %168 = getelementptr inbounds float, float* %10, i64 %162
  %169 = bitcast float* %168 to <64 x float>*
  store <64 x float> %167, <64 x float>* %169, align 128, !tbaa !2069
  br label %for_body5.4

for_body5.4:                                      ; preds = %for_body5.4, %for_end6.3
  %indvars.iv.4 = phi i64 [ 0, %for_end6.3 ], [ %indvars.iv.next.4.1, %for_body5.4 ]
  %.010.4 = phi <64 x float> [ zeroinitializer, %for_end6.3 ], [ %191, %for_body5.4 ]
  %170 = add nsw i64 %indvars.iv.4, %52
  %171 = getelementptr inbounds float, float* %4, i64 %170
  %172 = load float, float* %171, align 4, !tbaa !2046
  %173 = insertelement <64 x float> undef, float %172, i32 0
  %174 = shufflevector <64 x float> %173, <64 x float> undef, <64 x i32> zeroinitializer
  %175 = shl nuw nsw i64 %indvars.iv.4, 9
  %176 = or i64 %175, 256
  %177 = getelementptr inbounds float, float* %7, i64 %176
  %178 = bitcast float* %177 to <64 x float>*
  %179 = load <64 x float>, <64 x float>* %178, align 128, !tbaa !2066
  %180 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %174, <64 x float> %179, <64 x float> %.010.4)
  %indvars.iv.next.4 = or i64 %indvars.iv.4, 1
  %181 = add nsw i64 %indvars.iv.next.4, %52
  %182 = getelementptr inbounds float, float* %4, i64 %181
  %183 = load float, float* %182, align 4, !tbaa !2046
  %184 = insertelement <64 x float> undef, float %183, i32 0
  %185 = shufflevector <64 x float> %184, <64 x float> undef, <64 x i32> zeroinitializer
  %186 = shl nuw nsw i64 %indvars.iv.next.4, 9
  %187 = or i64 %186, 256
  %188 = getelementptr inbounds float, float* %7, i64 %187
  %189 = bitcast float* %188 to <64 x float>*
  %190 = load <64 x float>, <64 x float>* %189, align 128, !tbaa !2066
  %191 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %185, <64 x float> %190, <64 x float> %180)
  %indvars.iv.next.4.1 = add nuw nsw i64 %indvars.iv.4, 2
  %exitcond.4.not.1 = icmp eq i64 %indvars.iv.next.4.1, 512
  br i1 %exitcond.4.not.1, label %for_end6.4, label %for_body5.4, !prof !51

for_end6.4:                                       ; preds = %for_body5.4
  %192 = or i64 %52, 256
  %193 = fadd <64 x float> %191, %40
  %194 = fcmp olt <64 x float> %193, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %195 = select <64 x i1> %194, <64 x float> %193, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %196 = fcmp ogt <64 x float> %195, zeroinitializer
  %197 = select <64 x i1> %196, <64 x float> %195, <64 x float> zeroinitializer
  %198 = getelementptr inbounds float, float* %10, i64 %192
  %199 = bitcast float* %198 to <64 x float>*
  store <64 x float> %197, <64 x float>* %199, align 128, !tbaa !2069
  br label %for_body5.5

for_body5.5:                                      ; preds = %for_body5.5, %for_end6.4
  %indvars.iv.5 = phi i64 [ 0, %for_end6.4 ], [ %indvars.iv.next.5.1, %for_body5.5 ]
  %.010.5 = phi <64 x float> [ zeroinitializer, %for_end6.4 ], [ %221, %for_body5.5 ]
  %200 = add nsw i64 %indvars.iv.5, %52
  %201 = getelementptr inbounds float, float* %4, i64 %200
  %202 = load float, float* %201, align 4, !tbaa !2046
  %203 = insertelement <64 x float> undef, float %202, i32 0
  %204 = shufflevector <64 x float> %203, <64 x float> undef, <64 x i32> zeroinitializer
  %205 = shl nuw nsw i64 %indvars.iv.5, 9
  %206 = or i64 %205, 320
  %207 = getelementptr inbounds float, float* %7, i64 %206
  %208 = bitcast float* %207 to <64 x float>*
  %209 = load <64 x float>, <64 x float>* %208, align 128, !tbaa !2066
  %210 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %204, <64 x float> %209, <64 x float> %.010.5)
  %indvars.iv.next.5 = or i64 %indvars.iv.5, 1
  %211 = add nsw i64 %indvars.iv.next.5, %52
  %212 = getelementptr inbounds float, float* %4, i64 %211
  %213 = load float, float* %212, align 4, !tbaa !2046
  %214 = insertelement <64 x float> undef, float %213, i32 0
  %215 = shufflevector <64 x float> %214, <64 x float> undef, <64 x i32> zeroinitializer
  %216 = shl nuw nsw i64 %indvars.iv.next.5, 9
  %217 = or i64 %216, 320
  %218 = getelementptr inbounds float, float* %7, i64 %217
  %219 = bitcast float* %218 to <64 x float>*
  %220 = load <64 x float>, <64 x float>* %219, align 128, !tbaa !2066
  %221 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %215, <64 x float> %220, <64 x float> %210)
  %indvars.iv.next.5.1 = add nuw nsw i64 %indvars.iv.5, 2
  %exitcond.5.not.1 = icmp eq i64 %indvars.iv.next.5.1, 512
  br i1 %exitcond.5.not.1, label %for_end6.5, label %for_body5.5, !prof !51

for_end6.5:                                       ; preds = %for_body5.5
  %222 = or i64 %52, 320
  %223 = fadd <64 x float> %221, %43
  %224 = fcmp olt <64 x float> %223, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %225 = select <64 x i1> %224, <64 x float> %223, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %226 = fcmp ogt <64 x float> %225, zeroinitializer
  %227 = select <64 x i1> %226, <64 x float> %225, <64 x float> zeroinitializer
  %228 = getelementptr inbounds float, float* %10, i64 %222
  %229 = bitcast float* %228 to <64 x float>*
  store <64 x float> %227, <64 x float>* %229, align 128, !tbaa !2069
  br label %for_body5.6

for_body5.6:                                      ; preds = %for_body5.6, %for_end6.5
  %indvars.iv.6 = phi i64 [ 0, %for_end6.5 ], [ %indvars.iv.next.6.1, %for_body5.6 ]
  %.010.6 = phi <64 x float> [ zeroinitializer, %for_end6.5 ], [ %251, %for_body5.6 ]
  %230 = add nsw i64 %indvars.iv.6, %52
  %231 = getelementptr inbounds float, float* %4, i64 %230
  %232 = load float, float* %231, align 4, !tbaa !2046
  %233 = insertelement <64 x float> undef, float %232, i32 0
  %234 = shufflevector <64 x float> %233, <64 x float> undef, <64 x i32> zeroinitializer
  %235 = shl nuw nsw i64 %indvars.iv.6, 9
  %236 = or i64 %235, 384
  %237 = getelementptr inbounds float, float* %7, i64 %236
  %238 = bitcast float* %237 to <64 x float>*
  %239 = load <64 x float>, <64 x float>* %238, align 128, !tbaa !2066
  %240 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %234, <64 x float> %239, <64 x float> %.010.6)
  %indvars.iv.next.6 = or i64 %indvars.iv.6, 1
  %241 = add nsw i64 %indvars.iv.next.6, %52
  %242 = getelementptr inbounds float, float* %4, i64 %241
  %243 = load float, float* %242, align 4, !tbaa !2046
  %244 = insertelement <64 x float> undef, float %243, i32 0
  %245 = shufflevector <64 x float> %244, <64 x float> undef, <64 x i32> zeroinitializer
  %246 = shl nuw nsw i64 %indvars.iv.next.6, 9
  %247 = or i64 %246, 384
  %248 = getelementptr inbounds float, float* %7, i64 %247
  %249 = bitcast float* %248 to <64 x float>*
  %250 = load <64 x float>, <64 x float>* %249, align 128, !tbaa !2066
  %251 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %245, <64 x float> %250, <64 x float> %240)
  %indvars.iv.next.6.1 = add nuw nsw i64 %indvars.iv.6, 2
  %exitcond.6.not.1 = icmp eq i64 %indvars.iv.next.6.1, 512
  br i1 %exitcond.6.not.1, label %for_end6.6, label %for_body5.6, !prof !51

for_end6.6:                                       ; preds = %for_body5.6
  %252 = or i64 %52, 384
  %253 = fadd <64 x float> %251, %46
  %254 = fcmp olt <64 x float> %253, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %255 = select <64 x i1> %254, <64 x float> %253, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %256 = fcmp ogt <64 x float> %255, zeroinitializer
  %257 = select <64 x i1> %256, <64 x float> %255, <64 x float> zeroinitializer
  %258 = getelementptr inbounds float, float* %10, i64 %252
  %259 = bitcast float* %258 to <64 x float>*
  store <64 x float> %257, <64 x float>* %259, align 128, !tbaa !2069
  br label %for_body5.7

for_body5.7:                                      ; preds = %for_body5.7, %for_end6.6
  %indvars.iv.7 = phi i64 [ 0, %for_end6.6 ], [ %indvars.iv.next.7.1, %for_body5.7 ]
  %.010.7 = phi <64 x float> [ zeroinitializer, %for_end6.6 ], [ %281, %for_body5.7 ]
  %260 = add nsw i64 %indvars.iv.7, %52
  %261 = getelementptr inbounds float, float* %4, i64 %260
  %262 = load float, float* %261, align 4, !tbaa !2046
  %263 = insertelement <64 x float> undef, float %262, i32 0
  %264 = shufflevector <64 x float> %263, <64 x float> undef, <64 x i32> zeroinitializer
  %265 = shl nuw nsw i64 %indvars.iv.7, 9
  %266 = or i64 %265, 448
  %267 = getelementptr inbounds float, float* %7, i64 %266
  %268 = bitcast float* %267 to <64 x float>*
  %269 = load <64 x float>, <64 x float>* %268, align 128, !tbaa !2066
  %270 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %264, <64 x float> %269, <64 x float> %.010.7)
  %indvars.iv.next.7 = or i64 %indvars.iv.7, 1
  %271 = add nsw i64 %indvars.iv.next.7, %52
  %272 = getelementptr inbounds float, float* %4, i64 %271
  %273 = load float, float* %272, align 4, !tbaa !2046
  %274 = insertelement <64 x float> undef, float %273, i32 0
  %275 = shufflevector <64 x float> %274, <64 x float> undef, <64 x i32> zeroinitializer
  %276 = shl nuw nsw i64 %indvars.iv.next.7, 9
  %277 = or i64 %276, 448
  %278 = getelementptr inbounds float, float* %7, i64 %277
  %279 = bitcast float* %278 to <64 x float>*
  %280 = load <64 x float>, <64 x float>* %279, align 128, !tbaa !2066
  %281 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %275, <64 x float> %280, <64 x float> %270)
  %indvars.iv.next.7.1 = add nuw nsw i64 %indvars.iv.7, 2
  %exitcond.7.not.1 = icmp eq i64 %indvars.iv.next.7.1, 512
  br i1 %exitcond.7.not.1, label %for_end6.7, label %for_body5.7, !prof !51

for_end6.7:                                       ; preds = %for_body5.7
  %282 = or i64 %52, 448
  %283 = fadd <64 x float> %281, %49
  %284 = fcmp olt <64 x float> %283, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %285 = select <64 x i1> %284, <64 x float> %283, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %286 = fcmp ogt <64 x float> %285, zeroinitializer
  %287 = select <64 x i1> %286, <64 x float> %285, <64 x float> zeroinitializer
  %288 = getelementptr inbounds float, float* %10, i64 %282
  %289 = bitcast float* %288 to <64 x float>*
  store <64 x float> %287, <64 x float>* %289, align 128, !tbaa !2069
  %indvars.iv.next15 = add nsw i64 %indvars.iv14, 1
  %290 = add nsw i32 %50, 1
  %exitcond16.not = icmp eq i64 %indvars.iv.next15, %wide.trip.count
  br i1 %exitcond16.not, label %for_end, label %for_begin1.preheader, !prof !51
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_1(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.185, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !2072
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !2086
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !2088
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !2091
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.186, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.187, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.188, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.189, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !2093
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !2107
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 7
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.123, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !2109
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 7
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.124, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !2112
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 1024
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.125, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !2114
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 50176, i32 7168, i32 1024, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.126, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !2126
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 3
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.51, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !2140
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 3
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.52, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !2142
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 1024
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.127, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !2145
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 1
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.54, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !2147
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 3072, i32 1024, i32 1, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.190, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !2159
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 1024
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.130, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !2173
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !2187
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !2201
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 7
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.131, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !2203
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 7
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.132, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !2206
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 1024
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.133, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !2208
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 50176, i32 7168, i32 1024, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.134, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_1_compute_(i8* %31, i8* %39, i8* %45, i8* %51, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_1_compute_(i8* noalias nocapture readonly align 128 %0, i8* noalias nocapture readonly align 128 %1, i8* noalias nocapture readonly align 128 %2, i8* noalias nocapture align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 331776, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %for_end38, %if_end, %entry
  ret i32 -1

if_end:                                           ; preds = %entry
  %8 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %9 = tail call i8* %8(i32 1, i32 %4, i64 200704, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %9, i64 128) ]
  %10 = icmp eq i8* %9, null
  br i1 %10, label %if_then, label %for_begin7.preheader.preheader, !prof !5

for_begin7.preheader.preheader:                   ; preds = %if_end
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(36864) %6, i8 0, i64 36864, i1 false)
  %scevgep104.1 = getelementptr i8, i8* %6, i64 36864
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep104.1, i8 0, i64 4096, i1 false)
  %scevgep109.1.1 = getelementptr i8, i8* %6, i64 40960
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.1.1, i8* nonnull align 128 dereferenceable(4096) %0, i64 4096, i1 false)
  %scevgep110.2.1 = getelementptr i8, i8* %0, i64 4096
  %scevgep109.2.1 = getelementptr i8, i8* %6, i64 45056
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.2.1, i8* nonnull align 128 dereferenceable(4096) %scevgep110.2.1, i64 4096, i1 false)
  %scevgep110.3.1 = getelementptr i8, i8* %0, i64 8192
  %scevgep109.3.1 = getelementptr i8, i8* %6, i64 49152
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.3.1, i8* nonnull align 128 dereferenceable(4096) %scevgep110.3.1, i64 4096, i1 false)
  %scevgep110.4.1 = getelementptr i8, i8* %0, i64 12288
  %scevgep109.4.1 = getelementptr i8, i8* %6, i64 53248
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.4.1, i8* nonnull align 128 dereferenceable(4096) %scevgep110.4.1, i64 4096, i1 false)
  %scevgep110.5.1 = getelementptr i8, i8* %0, i64 16384
  %scevgep109.5.1 = getelementptr i8, i8* %6, i64 57344
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.5.1, i8* nonnull align 128 dereferenceable(4096) %scevgep110.5.1, i64 4096, i1 false)
  %scevgep110.6.1 = getelementptr i8, i8* %0, i64 20480
  %scevgep109.6.1 = getelementptr i8, i8* %6, i64 61440
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.6.1, i8* nonnull align 128 dereferenceable(4096) %scevgep110.6.1, i64 4096, i1 false)
  %scevgep110.7.1 = getelementptr i8, i8* %0, i64 24576
  %scevgep109.7.1 = getelementptr i8, i8* %6, i64 65536
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.7.1, i8* nonnull align 128 dereferenceable(4096) %scevgep110.7.1, i64 4096, i1 false)
  %scevgep109.8.1 = getelementptr i8, i8* %6, i64 69632
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.8.1, i8 0, i64 4096, i1 false)
  %scevgep104.2 = getelementptr i8, i8* %6, i64 73728
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep104.2, i8 0, i64 4096, i1 false)
  %scevgep110.1.2 = getelementptr i8, i8* %0, i64 28672
  %scevgep109.1.2 = getelementptr i8, i8* %6, i64 77824
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.1.2, i8* nonnull align 128 dereferenceable(4096) %scevgep110.1.2, i64 4096, i1 false)
  %scevgep110.2.2 = getelementptr i8, i8* %0, i64 32768
  %scevgep109.2.2 = getelementptr i8, i8* %6, i64 81920
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.2.2, i8* nonnull align 128 dereferenceable(4096) %scevgep110.2.2, i64 4096, i1 false)
  %scevgep110.3.2 = getelementptr i8, i8* %0, i64 36864
  %scevgep109.3.2 = getelementptr i8, i8* %6, i64 86016
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.3.2, i8* nonnull align 128 dereferenceable(4096) %scevgep110.3.2, i64 4096, i1 false)
  %scevgep110.4.2 = getelementptr i8, i8* %0, i64 40960
  %scevgep109.4.2 = getelementptr i8, i8* %6, i64 90112
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.4.2, i8* nonnull align 128 dereferenceable(4096) %scevgep110.4.2, i64 4096, i1 false)
  %scevgep110.5.2 = getelementptr i8, i8* %0, i64 45056
  %scevgep109.5.2 = getelementptr i8, i8* %6, i64 94208
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.5.2, i8* nonnull align 128 dereferenceable(4096) %scevgep110.5.2, i64 4096, i1 false)
  %scevgep110.6.2 = getelementptr i8, i8* %0, i64 49152
  %scevgep109.6.2 = getelementptr i8, i8* %6, i64 98304
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.6.2, i8* nonnull align 128 dereferenceable(4096) %scevgep110.6.2, i64 4096, i1 false)
  %scevgep110.7.2 = getelementptr i8, i8* %0, i64 53248
  %scevgep109.7.2 = getelementptr i8, i8* %6, i64 102400
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.7.2, i8* nonnull align 128 dereferenceable(4096) %scevgep110.7.2, i64 4096, i1 false)
  %scevgep109.8.2 = getelementptr i8, i8* %6, i64 106496
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.8.2, i8 0, i64 4096, i1 false)
  %scevgep104.3 = getelementptr i8, i8* %6, i64 110592
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep104.3, i8 0, i64 4096, i1 false)
  %scevgep110.1.3 = getelementptr i8, i8* %0, i64 57344
  %scevgep109.1.3 = getelementptr i8, i8* %6, i64 114688
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.1.3, i8* nonnull align 128 dereferenceable(4096) %scevgep110.1.3, i64 4096, i1 false)
  %scevgep110.2.3 = getelementptr i8, i8* %0, i64 61440
  %scevgep109.2.3 = getelementptr i8, i8* %6, i64 118784
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.2.3, i8* nonnull align 128 dereferenceable(4096) %scevgep110.2.3, i64 4096, i1 false)
  %scevgep110.3.3 = getelementptr i8, i8* %0, i64 65536
  %scevgep109.3.3 = getelementptr i8, i8* %6, i64 122880
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.3.3, i8* nonnull align 128 dereferenceable(4096) %scevgep110.3.3, i64 4096, i1 false)
  %scevgep110.4.3 = getelementptr i8, i8* %0, i64 69632
  %scevgep109.4.3 = getelementptr i8, i8* %6, i64 126976
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.4.3, i8* nonnull align 128 dereferenceable(4096) %scevgep110.4.3, i64 4096, i1 false)
  %scevgep110.5.3 = getelementptr i8, i8* %0, i64 73728
  %scevgep109.5.3 = getelementptr i8, i8* %6, i64 131072
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.5.3, i8* nonnull align 128 dereferenceable(4096) %scevgep110.5.3, i64 4096, i1 false)
  %scevgep110.6.3 = getelementptr i8, i8* %0, i64 77824
  %scevgep109.6.3 = getelementptr i8, i8* %6, i64 135168
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.6.3, i8* nonnull align 128 dereferenceable(4096) %scevgep110.6.3, i64 4096, i1 false)
  %scevgep110.7.3 = getelementptr i8, i8* %0, i64 81920
  %scevgep109.7.3 = getelementptr i8, i8* %6, i64 139264
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.7.3, i8* nonnull align 128 dereferenceable(4096) %scevgep110.7.3, i64 4096, i1 false)
  %scevgep109.8.3 = getelementptr i8, i8* %6, i64 143360
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.8.3, i8 0, i64 4096, i1 false)
  %scevgep104.4 = getelementptr i8, i8* %6, i64 147456
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep104.4, i8 0, i64 4096, i1 false)
  %scevgep110.1.4 = getelementptr i8, i8* %0, i64 86016
  %scevgep109.1.4 = getelementptr i8, i8* %6, i64 151552
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.1.4, i8* nonnull align 128 dereferenceable(4096) %scevgep110.1.4, i64 4096, i1 false)
  %scevgep110.2.4 = getelementptr i8, i8* %0, i64 90112
  %scevgep109.2.4 = getelementptr i8, i8* %6, i64 155648
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.2.4, i8* nonnull align 128 dereferenceable(4096) %scevgep110.2.4, i64 4096, i1 false)
  %scevgep110.3.4 = getelementptr i8, i8* %0, i64 94208
  %scevgep109.3.4 = getelementptr i8, i8* %6, i64 159744
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.3.4, i8* nonnull align 128 dereferenceable(4096) %scevgep110.3.4, i64 4096, i1 false)
  %scevgep110.4.4 = getelementptr i8, i8* %0, i64 98304
  %scevgep109.4.4 = getelementptr i8, i8* %6, i64 163840
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.4.4, i8* nonnull align 128 dereferenceable(4096) %scevgep110.4.4, i64 4096, i1 false)
  %scevgep110.5.4 = getelementptr i8, i8* %0, i64 102400
  %scevgep109.5.4 = getelementptr i8, i8* %6, i64 167936
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.5.4, i8* nonnull align 128 dereferenceable(4096) %scevgep110.5.4, i64 4096, i1 false)
  %scevgep110.6.4 = getelementptr i8, i8* %0, i64 106496
  %scevgep109.6.4 = getelementptr i8, i8* %6, i64 172032
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.6.4, i8* nonnull align 128 dereferenceable(4096) %scevgep110.6.4, i64 4096, i1 false)
  %scevgep110.7.4 = getelementptr i8, i8* %0, i64 110592
  %scevgep109.7.4 = getelementptr i8, i8* %6, i64 176128
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.7.4, i8* nonnull align 128 dereferenceable(4096) %scevgep110.7.4, i64 4096, i1 false)
  %scevgep109.8.4 = getelementptr i8, i8* %6, i64 180224
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.8.4, i8 0, i64 4096, i1 false)
  %scevgep104.5 = getelementptr i8, i8* %6, i64 184320
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep104.5, i8 0, i64 4096, i1 false)
  %scevgep110.1.5 = getelementptr i8, i8* %0, i64 114688
  %scevgep109.1.5 = getelementptr i8, i8* %6, i64 188416
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.1.5, i8* nonnull align 128 dereferenceable(4096) %scevgep110.1.5, i64 4096, i1 false)
  %scevgep110.2.5 = getelementptr i8, i8* %0, i64 118784
  %scevgep109.2.5 = getelementptr i8, i8* %6, i64 192512
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.2.5, i8* nonnull align 128 dereferenceable(4096) %scevgep110.2.5, i64 4096, i1 false)
  %scevgep110.3.5 = getelementptr i8, i8* %0, i64 122880
  %scevgep109.3.5 = getelementptr i8, i8* %6, i64 196608
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.3.5, i8* nonnull align 128 dereferenceable(4096) %scevgep110.3.5, i64 4096, i1 false)
  %scevgep110.4.5 = getelementptr i8, i8* %0, i64 126976
  %scevgep109.4.5 = getelementptr i8, i8* %6, i64 200704
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.4.5, i8* nonnull align 128 dereferenceable(4096) %scevgep110.4.5, i64 4096, i1 false)
  %scevgep110.5.5 = getelementptr i8, i8* %0, i64 131072
  %scevgep109.5.5 = getelementptr i8, i8* %6, i64 204800
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.5.5, i8* nonnull align 128 dereferenceable(4096) %scevgep110.5.5, i64 4096, i1 false)
  %scevgep110.6.5 = getelementptr i8, i8* %0, i64 135168
  %scevgep109.6.5 = getelementptr i8, i8* %6, i64 208896
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.6.5, i8* nonnull align 128 dereferenceable(4096) %scevgep110.6.5, i64 4096, i1 false)
  %scevgep110.7.5 = getelementptr i8, i8* %0, i64 139264
  %scevgep109.7.5 = getelementptr i8, i8* %6, i64 212992
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.7.5, i8* nonnull align 128 dereferenceable(4096) %scevgep110.7.5, i64 4096, i1 false)
  %scevgep109.8.5 = getelementptr i8, i8* %6, i64 217088
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.8.5, i8 0, i64 4096, i1 false)
  %scevgep104.6 = getelementptr i8, i8* %6, i64 221184
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep104.6, i8 0, i64 4096, i1 false)
  %scevgep110.1.6 = getelementptr i8, i8* %0, i64 143360
  %scevgep109.1.6 = getelementptr i8, i8* %6, i64 225280
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.1.6, i8* nonnull align 128 dereferenceable(4096) %scevgep110.1.6, i64 4096, i1 false)
  %scevgep110.2.6 = getelementptr i8, i8* %0, i64 147456
  %scevgep109.2.6 = getelementptr i8, i8* %6, i64 229376
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.2.6, i8* nonnull align 128 dereferenceable(4096) %scevgep110.2.6, i64 4096, i1 false)
  %scevgep110.3.6 = getelementptr i8, i8* %0, i64 151552
  %scevgep109.3.6 = getelementptr i8, i8* %6, i64 233472
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.3.6, i8* nonnull align 128 dereferenceable(4096) %scevgep110.3.6, i64 4096, i1 false)
  %scevgep110.4.6 = getelementptr i8, i8* %0, i64 155648
  %scevgep109.4.6 = getelementptr i8, i8* %6, i64 237568
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.4.6, i8* nonnull align 128 dereferenceable(4096) %scevgep110.4.6, i64 4096, i1 false)
  %scevgep110.5.6 = getelementptr i8, i8* %0, i64 159744
  %scevgep109.5.6 = getelementptr i8, i8* %6, i64 241664
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.5.6, i8* nonnull align 128 dereferenceable(4096) %scevgep110.5.6, i64 4096, i1 false)
  %scevgep110.6.6 = getelementptr i8, i8* %0, i64 163840
  %scevgep109.6.6 = getelementptr i8, i8* %6, i64 245760
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.6.6, i8* nonnull align 128 dereferenceable(4096) %scevgep110.6.6, i64 4096, i1 false)
  %scevgep110.7.6 = getelementptr i8, i8* %0, i64 167936
  %scevgep109.7.6 = getelementptr i8, i8* %6, i64 249856
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.7.6, i8* nonnull align 128 dereferenceable(4096) %scevgep110.7.6, i64 4096, i1 false)
  %scevgep109.8.6 = getelementptr i8, i8* %6, i64 253952
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.8.6, i8 0, i64 4096, i1 false)
  %scevgep104.7 = getelementptr i8, i8* %6, i64 258048
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep104.7, i8 0, i64 4096, i1 false)
  %scevgep110.1.7 = getelementptr i8, i8* %0, i64 172032
  %scevgep109.1.7 = getelementptr i8, i8* %6, i64 262144
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.1.7, i8* nonnull align 128 dereferenceable(4096) %scevgep110.1.7, i64 4096, i1 false)
  %scevgep110.2.7 = getelementptr i8, i8* %0, i64 176128
  %scevgep109.2.7 = getelementptr i8, i8* %6, i64 266240
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.2.7, i8* nonnull align 128 dereferenceable(4096) %scevgep110.2.7, i64 4096, i1 false)
  %scevgep110.3.7 = getelementptr i8, i8* %0, i64 180224
  %scevgep109.3.7 = getelementptr i8, i8* %6, i64 270336
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.3.7, i8* nonnull align 128 dereferenceable(4096) %scevgep110.3.7, i64 4096, i1 false)
  %scevgep110.4.7 = getelementptr i8, i8* %0, i64 184320
  %scevgep109.4.7 = getelementptr i8, i8* %6, i64 274432
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.4.7, i8* nonnull align 128 dereferenceable(4096) %scevgep110.4.7, i64 4096, i1 false)
  %scevgep110.5.7 = getelementptr i8, i8* %0, i64 188416
  %scevgep109.5.7 = getelementptr i8, i8* %6, i64 278528
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.5.7, i8* nonnull align 128 dereferenceable(4096) %scevgep110.5.7, i64 4096, i1 false)
  %scevgep110.6.7 = getelementptr i8, i8* %0, i64 192512
  %scevgep109.6.7 = getelementptr i8, i8* %6, i64 282624
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.6.7, i8* nonnull align 128 dereferenceable(4096) %scevgep110.6.7, i64 4096, i1 false)
  %scevgep110.7.7 = getelementptr i8, i8* %0, i64 196608
  %scevgep109.7.7 = getelementptr i8, i8* %6, i64 286720
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.7.7, i8* nonnull align 128 dereferenceable(4096) %scevgep110.7.7, i64 4096, i1 false)
  %scevgep109.8.7 = getelementptr i8, i8* %6, i64 290816
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %scevgep109.8.7, i8 0, i64 4096, i1 false)
  %scevgep104.8 = getelementptr i8, i8* %6, i64 294912
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(36864) %scevgep104.8, i8 0, i64 36864, i1 false)
  %11 = bitcast i8* %9 to float*
  %12 = bitcast i8* %6 to float*
  %13 = bitcast i8* %1 to float*
  br label %for_begin15.preheader

for_begin15.preheader:                            ; preds = %for_begin7.preheader.preheader, %for_end17
  %indvars.iv97 = phi i64 [ 0, %for_begin7.preheader.preheader ], [ %16, %for_end17 ]
  %14 = mul nuw nsw i64 %indvars.iv97, 7168
  %15 = mul nuw nsw i64 %indvars.iv97, 9216
  %16 = add nuw nsw i64 %indvars.iv97, 1
  %17 = mul nuw nsw i64 %16, 9216
  %18 = mul i64 %indvars.iv97, 9216
  %19 = add i64 %18, 18432
  br label %for_begin18.preheader

for_begin27.preheader:                            ; preds = %for_end17
  %20 = bitcast i8* %2 to float*
  br label %for_begin30.preheader

for_begin18.preheader:                            ; preds = %for_begin15.preheader, %for_end20
  %indvars.iv94 = phi i64 [ 0, %for_begin15.preheader ], [ %indvars.iv.next95, %for_end20 ]
  %21 = shl nsw i64 %indvars.iv94, 10
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_begin18.preheader
  %index = phi i64 [ 0, %for_begin18.preheader ], [ %index.next, %vector.body ]
  %22 = add nuw nsw i64 %index, %21
  %23 = add nuw nsw i64 %22, %14
  %24 = getelementptr inbounds float, float* %11, i64 %23
  %25 = add nuw nsw i64 %22, %15
  %26 = getelementptr inbounds float, float* %12, i64 %25
  %27 = bitcast float* %26 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %27, align 16, !tbaa !2220
  %28 = getelementptr inbounds float, float* %13, i64 %index
  %29 = bitcast float* %28 to <4 x float>*
  %wide.load115 = load <4 x float>, <4 x float>* %29, align 16, !tbaa !2223
  %30 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load, <4 x float> %wide.load115, <4 x float> zeroinitializer)
  %31 = add nuw nsw i64 %25, 1024
  %32 = getelementptr inbounds float, float* %12, i64 %31
  %33 = bitcast float* %32 to <4 x float>*
  %wide.load116 = load <4 x float>, <4 x float>* %33, align 16, !tbaa !2220
  %34 = add nuw nsw i64 %index, 1024
  %35 = getelementptr inbounds float, float* %13, i64 %34
  %36 = bitcast float* %35 to <4 x float>*
  %wide.load117 = load <4 x float>, <4 x float>* %36, align 16, !tbaa !2223
  %37 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load116, <4 x float> %wide.load117, <4 x float> %30)
  %38 = add nuw nsw i64 %25, 2048
  %39 = getelementptr inbounds float, float* %12, i64 %38
  %40 = bitcast float* %39 to <4 x float>*
  %wide.load118 = load <4 x float>, <4 x float>* %40, align 16, !tbaa !2220
  %41 = add nuw nsw i64 %index, 2048
  %42 = getelementptr inbounds float, float* %13, i64 %41
  %43 = bitcast float* %42 to <4 x float>*
  %wide.load119 = load <4 x float>, <4 x float>* %43, align 16, !tbaa !2223
  %44 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load118, <4 x float> %wide.load119, <4 x float> %37)
  %45 = add nuw nsw i64 %22, %17
  %46 = getelementptr inbounds float, float* %12, i64 %45
  %47 = bitcast float* %46 to <4 x float>*
  %wide.load120 = load <4 x float>, <4 x float>* %47, align 16, !tbaa !2220
  %48 = add nuw nsw i64 %index, 3072
  %49 = getelementptr inbounds float, float* %13, i64 %48
  %50 = bitcast float* %49 to <4 x float>*
  %wide.load121 = load <4 x float>, <4 x float>* %50, align 16, !tbaa !2223
  %51 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load120, <4 x float> %wide.load121, <4 x float> %44)
  %52 = add nuw nsw i64 %45, 1024
  %53 = getelementptr inbounds float, float* %12, i64 %52
  %54 = bitcast float* %53 to <4 x float>*
  %wide.load122 = load <4 x float>, <4 x float>* %54, align 16, !tbaa !2220
  %55 = add nuw nsw i64 %index, 4096
  %56 = getelementptr inbounds float, float* %13, i64 %55
  %57 = bitcast float* %56 to <4 x float>*
  %wide.load123 = load <4 x float>, <4 x float>* %57, align 16, !tbaa !2223
  %58 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load122, <4 x float> %wide.load123, <4 x float> %51)
  %59 = add nuw nsw i64 %45, 2048
  %60 = getelementptr inbounds float, float* %12, i64 %59
  %61 = bitcast float* %60 to <4 x float>*
  %wide.load124 = load <4 x float>, <4 x float>* %61, align 16, !tbaa !2220
  %62 = add nuw nsw i64 %index, 5120
  %63 = getelementptr inbounds float, float* %13, i64 %62
  %64 = bitcast float* %63 to <4 x float>*
  %wide.load125 = load <4 x float>, <4 x float>* %64, align 16, !tbaa !2223
  %65 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load124, <4 x float> %wide.load125, <4 x float> %58)
  %66 = add nuw nsw i64 %22, %19
  %67 = getelementptr inbounds float, float* %12, i64 %66
  %68 = bitcast float* %67 to <4 x float>*
  %wide.load126 = load <4 x float>, <4 x float>* %68, align 16, !tbaa !2220
  %69 = add nuw nsw i64 %index, 6144
  %70 = getelementptr inbounds float, float* %13, i64 %69
  %71 = bitcast float* %70 to <4 x float>*
  %wide.load127 = load <4 x float>, <4 x float>* %71, align 16, !tbaa !2223
  %72 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load126, <4 x float> %wide.load127, <4 x float> %65)
  %73 = add nuw nsw i64 %66, 1024
  %74 = getelementptr inbounds float, float* %12, i64 %73
  %75 = bitcast float* %74 to <4 x float>*
  %wide.load128 = load <4 x float>, <4 x float>* %75, align 16, !tbaa !2220
  %76 = add nuw nsw i64 %index, 7168
  %77 = getelementptr inbounds float, float* %13, i64 %76
  %78 = bitcast float* %77 to <4 x float>*
  %wide.load129 = load <4 x float>, <4 x float>* %78, align 16, !tbaa !2223
  %79 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load128, <4 x float> %wide.load129, <4 x float> %72)
  %80 = add nuw nsw i64 %66, 2048
  %81 = getelementptr inbounds float, float* %12, i64 %80
  %82 = bitcast float* %81 to <4 x float>*
  %wide.load130 = load <4 x float>, <4 x float>* %82, align 16, !tbaa !2220
  %83 = add nuw nsw i64 %index, 8192
  %84 = getelementptr inbounds float, float* %13, i64 %83
  %85 = bitcast float* %84 to <4 x float>*
  %wide.load131 = load <4 x float>, <4 x float>* %85, align 16, !tbaa !2223
  %86 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load130, <4 x float> %wide.load131, <4 x float> %79)
  %87 = bitcast float* %24 to <4 x float>*
  store <4 x float> %86, <4 x float>* %87, align 16, !tbaa !2226
  %index.next = add i64 %index, 4
  %88 = icmp eq i64 %index.next, 1024
  br i1 %88, label %for_end20, label %vector.body, !prof !335, !llvm.loop !2229

for_end17:                                        ; preds = %for_end20
  %exitcond99.not = icmp eq i64 %16, 7
  br i1 %exitcond99.not, label %for_begin27.preheader, label %for_begin15.preheader, !prof !51

for_end20:                                        ; preds = %vector.body
  %indvars.iv.next95 = add nuw nsw i64 %indvars.iv94, 1
  %exitcond96.not = icmp eq i64 %indvars.iv.next95, 7
  br i1 %exitcond96.not, label %for_end17, label %for_begin18.preheader, !prof !51

for_begin30.preheader:                            ; preds = %for_begin27.preheader, %for_end35.6
  %indvars.iv80 = phi i64 [ 0, %for_begin27.preheader ], [ %indvars.iv.next81, %for_end35.6 ]
  %89 = mul nuw nsw i64 %indvars.iv80, 7168
  br label %vector.body206

vector.body206:                                   ; preds = %vector.body206, %for_begin30.preheader
  %index208 = phi i64 [ 0, %for_begin30.preheader ], [ %index.next209.1, %vector.body206 ]
  %90 = add nuw nsw i64 %89, %index208
  %91 = getelementptr inbounds float, float* %20, i64 %index208
  %92 = bitcast float* %91 to <4 x float>*
  %wide.load212 = load <4 x float>, <4 x float>* %92, align 64, !tbaa !2230
  %93 = getelementptr inbounds float, float* %91, i64 4
  %94 = bitcast float* %93 to <4 x float>*
  %wide.load213 = load <4 x float>, <4 x float>* %94, align 16, !tbaa !2230
  %95 = getelementptr inbounds float, float* %11, i64 %90
  %96 = bitcast float* %95 to <4 x float>*
  %wide.load214 = load <4 x float>, <4 x float>* %96, align 64, !tbaa !2226
  %97 = getelementptr inbounds float, float* %95, i64 4
  %98 = bitcast float* %97 to <4 x float>*
  %wide.load215 = load <4 x float>, <4 x float>* %98, align 16, !tbaa !2226
  %99 = fadd <4 x float> %wide.load212, %wide.load214
  %100 = fadd <4 x float> %wide.load213, %wide.load215
  %101 = bitcast float* %95 to <4 x float>*
  store <4 x float> %99, <4 x float>* %101, align 64, !tbaa !2226
  %102 = bitcast float* %97 to <4 x float>*
  store <4 x float> %100, <4 x float>* %102, align 16, !tbaa !2226
  %index.next209 = or i64 %index208, 8
  %103 = add nuw nsw i64 %89, %index.next209
  %104 = getelementptr inbounds float, float* %20, i64 %index.next209
  %105 = bitcast float* %104 to <4 x float>*
  %wide.load212.1 = load <4 x float>, <4 x float>* %105, align 32, !tbaa !2230
  %106 = getelementptr inbounds float, float* %104, i64 4
  %107 = bitcast float* %106 to <4 x float>*
  %wide.load213.1 = load <4 x float>, <4 x float>* %107, align 16, !tbaa !2230
  %108 = getelementptr inbounds float, float* %11, i64 %103
  %109 = bitcast float* %108 to <4 x float>*
  %wide.load214.1 = load <4 x float>, <4 x float>* %109, align 32, !tbaa !2226
  %110 = getelementptr inbounds float, float* %108, i64 4
  %111 = bitcast float* %110 to <4 x float>*
  %wide.load215.1 = load <4 x float>, <4 x float>* %111, align 16, !tbaa !2226
  %112 = fadd <4 x float> %wide.load212.1, %wide.load214.1
  %113 = fadd <4 x float> %wide.load213.1, %wide.load215.1
  %114 = bitcast float* %108 to <4 x float>*
  store <4 x float> %112, <4 x float>* %114, align 32, !tbaa !2226
  %115 = bitcast float* %110 to <4 x float>*
  store <4 x float> %113, <4 x float>* %115, align 16, !tbaa !2226
  %index.next209.1 = add nuw nsw i64 %index208, 16
  %116 = icmp eq i64 %index.next209.1, 1024
  br i1 %116, label %for_end35, label %vector.body206, !prof !341, !llvm.loop !2233

for_begin36.preheader:                            ; preds = %for_end35.6
  %117 = bitcast i8* %3 to float*
  br label %for_begin39.preheader

for_end35:                                        ; preds = %vector.body206
  %118 = add nuw nsw i64 %89, 1024
  br label %vector.body194

vector.body194:                                   ; preds = %vector.body194, %for_end35
  %index196 = phi i64 [ 0, %for_end35 ], [ %index.next197.1, %vector.body194 ]
  %119 = add nuw nsw i64 %118, %index196
  %120 = getelementptr inbounds float, float* %20, i64 %index196
  %121 = bitcast float* %120 to <4 x float>*
  %wide.load200 = load <4 x float>, <4 x float>* %121, align 64, !tbaa !2230
  %122 = getelementptr inbounds float, float* %120, i64 4
  %123 = bitcast float* %122 to <4 x float>*
  %wide.load201 = load <4 x float>, <4 x float>* %123, align 16, !tbaa !2230
  %124 = getelementptr inbounds float, float* %11, i64 %119
  %125 = bitcast float* %124 to <4 x float>*
  %wide.load202 = load <4 x float>, <4 x float>* %125, align 64, !tbaa !2226
  %126 = getelementptr inbounds float, float* %124, i64 4
  %127 = bitcast float* %126 to <4 x float>*
  %wide.load203 = load <4 x float>, <4 x float>* %127, align 16, !tbaa !2226
  %128 = fadd <4 x float> %wide.load200, %wide.load202
  %129 = fadd <4 x float> %wide.load201, %wide.load203
  %130 = bitcast float* %124 to <4 x float>*
  store <4 x float> %128, <4 x float>* %130, align 64, !tbaa !2226
  %131 = bitcast float* %126 to <4 x float>*
  store <4 x float> %129, <4 x float>* %131, align 16, !tbaa !2226
  %index.next197 = or i64 %index196, 8
  %132 = add nuw nsw i64 %118, %index.next197
  %133 = getelementptr inbounds float, float* %20, i64 %index.next197
  %134 = bitcast float* %133 to <4 x float>*
  %wide.load200.1 = load <4 x float>, <4 x float>* %134, align 32, !tbaa !2230
  %135 = getelementptr inbounds float, float* %133, i64 4
  %136 = bitcast float* %135 to <4 x float>*
  %wide.load201.1 = load <4 x float>, <4 x float>* %136, align 16, !tbaa !2230
  %137 = getelementptr inbounds float, float* %11, i64 %132
  %138 = bitcast float* %137 to <4 x float>*
  %wide.load202.1 = load <4 x float>, <4 x float>* %138, align 32, !tbaa !2226
  %139 = getelementptr inbounds float, float* %137, i64 4
  %140 = bitcast float* %139 to <4 x float>*
  %wide.load203.1 = load <4 x float>, <4 x float>* %140, align 16, !tbaa !2226
  %141 = fadd <4 x float> %wide.load200.1, %wide.load202.1
  %142 = fadd <4 x float> %wide.load201.1, %wide.load203.1
  %143 = bitcast float* %137 to <4 x float>*
  store <4 x float> %141, <4 x float>* %143, align 32, !tbaa !2226
  %144 = bitcast float* %139 to <4 x float>*
  store <4 x float> %142, <4 x float>* %144, align 16, !tbaa !2226
  %index.next197.1 = add nuw nsw i64 %index196, 16
  %145 = icmp eq i64 %index.next197.1, 1024
  br i1 %145, label %for_end35.1, label %vector.body194, !prof !341, !llvm.loop !2234

for_begin39.preheader:                            ; preds = %for_begin36.preheader, %for_end44.6
  %indvars.iv71 = phi i64 [ 0, %for_begin36.preheader ], [ %indvars.iv.next72, %for_end44.6 ]
  %146 = mul nuw nsw i64 %indvars.iv71, 7168
  br label %vector.body278

vector.body278:                                   ; preds = %vector.body278, %for_begin39.preheader
  %index280 = phi i64 [ 0, %for_begin39.preheader ], [ %index.next281, %vector.body278 ]
  %147 = add nuw nsw i64 %146, %index280
  %148 = getelementptr inbounds float, float* %11, i64 %147
  %149 = bitcast float* %148 to <4 x float>*
  %wide.load284 = load <4 x float>, <4 x float>* %149, align 32, !tbaa !2226
  %150 = getelementptr inbounds float, float* %148, i64 4
  %151 = bitcast float* %150 to <4 x float>*
  %wide.load285 = load <4 x float>, <4 x float>* %151, align 16, !tbaa !2226
  %152 = fcmp olt <4 x float> %wide.load284, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %153 = fcmp olt <4 x float> %wide.load285, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %154 = select <4 x i1> %152, <4 x float> %wide.load284, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %155 = select <4 x i1> %153, <4 x float> %wide.load285, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %156 = fcmp ogt <4 x float> %154, zeroinitializer
  %157 = fcmp ogt <4 x float> %155, zeroinitializer
  %158 = select <4 x i1> %156, <4 x float> %154, <4 x float> zeroinitializer
  %159 = select <4 x i1> %157, <4 x float> %155, <4 x float> zeroinitializer
  %160 = getelementptr inbounds float, float* %117, i64 %147
  %161 = bitcast float* %160 to <4 x float>*
  store <4 x float> %158, <4 x float>* %161, align 32, !tbaa !2235
  %162 = getelementptr inbounds float, float* %160, i64 4
  %163 = bitcast float* %162 to <4 x float>*
  store <4 x float> %159, <4 x float>* %163, align 16, !tbaa !2235
  %index.next281 = add i64 %index280, 8
  %164 = icmp eq i64 %index.next281, 1024
  br i1 %164, label %for_end44, label %vector.body278, !prof !341, !llvm.loop !2238

for_end38:                                        ; preds = %for_end44.6
  %165 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %166 = tail call i32 %165(i32 1, i32 %4, i8* nonnull %9)
  %.not = icmp eq i32 %166, 0
  br i1 %.not, label %if_end46, label %if_then, !prof !51

for_end44:                                        ; preds = %vector.body278
  %167 = add nuw nsw i64 %146, 1024
  br label %vector.body268

vector.body268:                                   ; preds = %vector.body268, %for_end44
  %index270 = phi i64 [ 0, %for_end44 ], [ %index.next271, %vector.body268 ]
  %168 = add nuw nsw i64 %167, %index270
  %169 = getelementptr inbounds float, float* %11, i64 %168
  %170 = bitcast float* %169 to <4 x float>*
  %wide.load274 = load <4 x float>, <4 x float>* %170, align 32, !tbaa !2226
  %171 = getelementptr inbounds float, float* %169, i64 4
  %172 = bitcast float* %171 to <4 x float>*
  %wide.load275 = load <4 x float>, <4 x float>* %172, align 16, !tbaa !2226
  %173 = fcmp olt <4 x float> %wide.load274, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %174 = fcmp olt <4 x float> %wide.load275, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %175 = select <4 x i1> %173, <4 x float> %wide.load274, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %176 = select <4 x i1> %174, <4 x float> %wide.load275, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %177 = fcmp ogt <4 x float> %175, zeroinitializer
  %178 = fcmp ogt <4 x float> %176, zeroinitializer
  %179 = select <4 x i1> %177, <4 x float> %175, <4 x float> zeroinitializer
  %180 = select <4 x i1> %178, <4 x float> %176, <4 x float> zeroinitializer
  %181 = getelementptr inbounds float, float* %117, i64 %168
  %182 = bitcast float* %181 to <4 x float>*
  store <4 x float> %179, <4 x float>* %182, align 32, !tbaa !2235
  %183 = getelementptr inbounds float, float* %181, i64 4
  %184 = bitcast float* %183 to <4 x float>*
  store <4 x float> %180, <4 x float>* %184, align 16, !tbaa !2235
  %index.next271 = add i64 %index270, 8
  %185 = icmp eq i64 %index.next271, 1024
  br i1 %185, label %for_end44.1, label %vector.body268, !prof !341, !llvm.loop !2239

if_end46:                                         ; preds = %for_end38
  %186 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %187 = tail call i32 %186(i32 1, i32 %4, i8* nonnull %6)
  %.not57 = icmp ne i32 %187, 0
  %spec.select = sext i1 %.not57 to i32
  ret i32 %spec.select

for_end44.1:                                      ; preds = %vector.body268
  %188 = add nuw nsw i64 %146, 2048
  br label %vector.body258

vector.body258:                                   ; preds = %vector.body258, %for_end44.1
  %index260 = phi i64 [ 0, %for_end44.1 ], [ %index.next261, %vector.body258 ]
  %189 = add nuw nsw i64 %188, %index260
  %190 = getelementptr inbounds float, float* %11, i64 %189
  %191 = bitcast float* %190 to <4 x float>*
  %wide.load264 = load <4 x float>, <4 x float>* %191, align 32, !tbaa !2226
  %192 = getelementptr inbounds float, float* %190, i64 4
  %193 = bitcast float* %192 to <4 x float>*
  %wide.load265 = load <4 x float>, <4 x float>* %193, align 16, !tbaa !2226
  %194 = fcmp olt <4 x float> %wide.load264, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %195 = fcmp olt <4 x float> %wide.load265, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %196 = select <4 x i1> %194, <4 x float> %wide.load264, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %197 = select <4 x i1> %195, <4 x float> %wide.load265, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %198 = fcmp ogt <4 x float> %196, zeroinitializer
  %199 = fcmp ogt <4 x float> %197, zeroinitializer
  %200 = select <4 x i1> %198, <4 x float> %196, <4 x float> zeroinitializer
  %201 = select <4 x i1> %199, <4 x float> %197, <4 x float> zeroinitializer
  %202 = getelementptr inbounds float, float* %117, i64 %189
  %203 = bitcast float* %202 to <4 x float>*
  store <4 x float> %200, <4 x float>* %203, align 32, !tbaa !2235
  %204 = getelementptr inbounds float, float* %202, i64 4
  %205 = bitcast float* %204 to <4 x float>*
  store <4 x float> %201, <4 x float>* %205, align 16, !tbaa !2235
  %index.next261 = add i64 %index260, 8
  %206 = icmp eq i64 %index.next261, 1024
  br i1 %206, label %for_end44.2, label %vector.body258, !prof !341, !llvm.loop !2240

for_end44.2:                                      ; preds = %vector.body258
  %207 = add nuw nsw i64 %146, 3072
  br label %vector.body248

vector.body248:                                   ; preds = %vector.body248, %for_end44.2
  %index250 = phi i64 [ 0, %for_end44.2 ], [ %index.next251, %vector.body248 ]
  %208 = add nuw nsw i64 %207, %index250
  %209 = getelementptr inbounds float, float* %11, i64 %208
  %210 = bitcast float* %209 to <4 x float>*
  %wide.load254 = load <4 x float>, <4 x float>* %210, align 32, !tbaa !2226
  %211 = getelementptr inbounds float, float* %209, i64 4
  %212 = bitcast float* %211 to <4 x float>*
  %wide.load255 = load <4 x float>, <4 x float>* %212, align 16, !tbaa !2226
  %213 = fcmp olt <4 x float> %wide.load254, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %214 = fcmp olt <4 x float> %wide.load255, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %215 = select <4 x i1> %213, <4 x float> %wide.load254, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %216 = select <4 x i1> %214, <4 x float> %wide.load255, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %217 = fcmp ogt <4 x float> %215, zeroinitializer
  %218 = fcmp ogt <4 x float> %216, zeroinitializer
  %219 = select <4 x i1> %217, <4 x float> %215, <4 x float> zeroinitializer
  %220 = select <4 x i1> %218, <4 x float> %216, <4 x float> zeroinitializer
  %221 = getelementptr inbounds float, float* %117, i64 %208
  %222 = bitcast float* %221 to <4 x float>*
  store <4 x float> %219, <4 x float>* %222, align 32, !tbaa !2235
  %223 = getelementptr inbounds float, float* %221, i64 4
  %224 = bitcast float* %223 to <4 x float>*
  store <4 x float> %220, <4 x float>* %224, align 16, !tbaa !2235
  %index.next251 = add i64 %index250, 8
  %225 = icmp eq i64 %index.next251, 1024
  br i1 %225, label %for_end44.3, label %vector.body248, !prof !341, !llvm.loop !2241

for_end44.3:                                      ; preds = %vector.body248
  %226 = add nuw nsw i64 %146, 4096
  br label %vector.body238

vector.body238:                                   ; preds = %vector.body238, %for_end44.3
  %index240 = phi i64 [ 0, %for_end44.3 ], [ %index.next241, %vector.body238 ]
  %227 = add nuw nsw i64 %226, %index240
  %228 = getelementptr inbounds float, float* %11, i64 %227
  %229 = bitcast float* %228 to <4 x float>*
  %wide.load244 = load <4 x float>, <4 x float>* %229, align 32, !tbaa !2226
  %230 = getelementptr inbounds float, float* %228, i64 4
  %231 = bitcast float* %230 to <4 x float>*
  %wide.load245 = load <4 x float>, <4 x float>* %231, align 16, !tbaa !2226
  %232 = fcmp olt <4 x float> %wide.load244, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %233 = fcmp olt <4 x float> %wide.load245, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %234 = select <4 x i1> %232, <4 x float> %wide.load244, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %235 = select <4 x i1> %233, <4 x float> %wide.load245, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %236 = fcmp ogt <4 x float> %234, zeroinitializer
  %237 = fcmp ogt <4 x float> %235, zeroinitializer
  %238 = select <4 x i1> %236, <4 x float> %234, <4 x float> zeroinitializer
  %239 = select <4 x i1> %237, <4 x float> %235, <4 x float> zeroinitializer
  %240 = getelementptr inbounds float, float* %117, i64 %227
  %241 = bitcast float* %240 to <4 x float>*
  store <4 x float> %238, <4 x float>* %241, align 32, !tbaa !2235
  %242 = getelementptr inbounds float, float* %240, i64 4
  %243 = bitcast float* %242 to <4 x float>*
  store <4 x float> %239, <4 x float>* %243, align 16, !tbaa !2235
  %index.next241 = add i64 %index240, 8
  %244 = icmp eq i64 %index.next241, 1024
  br i1 %244, label %for_end44.4, label %vector.body238, !prof !341, !llvm.loop !2242

for_end44.4:                                      ; preds = %vector.body238
  %245 = add nuw nsw i64 %146, 5120
  br label %vector.body228

vector.body228:                                   ; preds = %vector.body228, %for_end44.4
  %index230 = phi i64 [ 0, %for_end44.4 ], [ %index.next231, %vector.body228 ]
  %246 = add nuw nsw i64 %245, %index230
  %247 = getelementptr inbounds float, float* %11, i64 %246
  %248 = bitcast float* %247 to <4 x float>*
  %wide.load234 = load <4 x float>, <4 x float>* %248, align 32, !tbaa !2226
  %249 = getelementptr inbounds float, float* %247, i64 4
  %250 = bitcast float* %249 to <4 x float>*
  %wide.load235 = load <4 x float>, <4 x float>* %250, align 16, !tbaa !2226
  %251 = fcmp olt <4 x float> %wide.load234, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %252 = fcmp olt <4 x float> %wide.load235, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %253 = select <4 x i1> %251, <4 x float> %wide.load234, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %254 = select <4 x i1> %252, <4 x float> %wide.load235, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %255 = fcmp ogt <4 x float> %253, zeroinitializer
  %256 = fcmp ogt <4 x float> %254, zeroinitializer
  %257 = select <4 x i1> %255, <4 x float> %253, <4 x float> zeroinitializer
  %258 = select <4 x i1> %256, <4 x float> %254, <4 x float> zeroinitializer
  %259 = getelementptr inbounds float, float* %117, i64 %246
  %260 = bitcast float* %259 to <4 x float>*
  store <4 x float> %257, <4 x float>* %260, align 32, !tbaa !2235
  %261 = getelementptr inbounds float, float* %259, i64 4
  %262 = bitcast float* %261 to <4 x float>*
  store <4 x float> %258, <4 x float>* %262, align 16, !tbaa !2235
  %index.next231 = add i64 %index230, 8
  %263 = icmp eq i64 %index.next231, 1024
  br i1 %263, label %for_end44.5, label %vector.body228, !prof !341, !llvm.loop !2243

for_end44.5:                                      ; preds = %vector.body228
  %264 = add nuw nsw i64 %146, 6144
  br label %vector.body218

vector.body218:                                   ; preds = %vector.body218, %for_end44.5
  %index220 = phi i64 [ 0, %for_end44.5 ], [ %index.next221, %vector.body218 ]
  %265 = add nuw nsw i64 %264, %index220
  %266 = getelementptr inbounds float, float* %11, i64 %265
  %267 = bitcast float* %266 to <4 x float>*
  %wide.load224 = load <4 x float>, <4 x float>* %267, align 32, !tbaa !2226
  %268 = getelementptr inbounds float, float* %266, i64 4
  %269 = bitcast float* %268 to <4 x float>*
  %wide.load225 = load <4 x float>, <4 x float>* %269, align 16, !tbaa !2226
  %270 = fcmp olt <4 x float> %wide.load224, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %271 = fcmp olt <4 x float> %wide.load225, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %272 = select <4 x i1> %270, <4 x float> %wide.load224, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %273 = select <4 x i1> %271, <4 x float> %wide.load225, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %274 = fcmp ogt <4 x float> %272, zeroinitializer
  %275 = fcmp ogt <4 x float> %273, zeroinitializer
  %276 = select <4 x i1> %274, <4 x float> %272, <4 x float> zeroinitializer
  %277 = select <4 x i1> %275, <4 x float> %273, <4 x float> zeroinitializer
  %278 = getelementptr inbounds float, float* %117, i64 %265
  %279 = bitcast float* %278 to <4 x float>*
  store <4 x float> %276, <4 x float>* %279, align 32, !tbaa !2235
  %280 = getelementptr inbounds float, float* %278, i64 4
  %281 = bitcast float* %280 to <4 x float>*
  store <4 x float> %277, <4 x float>* %281, align 16, !tbaa !2235
  %index.next221 = add i64 %index220, 8
  %282 = icmp eq i64 %index.next221, 1024
  br i1 %282, label %for_end44.6, label %vector.body218, !prof !341, !llvm.loop !2244

for_end44.6:                                      ; preds = %vector.body218
  %indvars.iv.next72 = add nuw nsw i64 %indvars.iv71, 1
  %exitcond73.not = icmp eq i64 %indvars.iv.next72, 7
  br i1 %exitcond73.not, label %for_end38, label %for_begin39.preheader, !prof !51

for_end35.1:                                      ; preds = %vector.body194
  %283 = add nuw nsw i64 %89, 2048
  br label %vector.body182

vector.body182:                                   ; preds = %vector.body182, %for_end35.1
  %index184 = phi i64 [ 0, %for_end35.1 ], [ %index.next185.1, %vector.body182 ]
  %284 = add nuw nsw i64 %283, %index184
  %285 = getelementptr inbounds float, float* %20, i64 %index184
  %286 = bitcast float* %285 to <4 x float>*
  %wide.load188 = load <4 x float>, <4 x float>* %286, align 64, !tbaa !2230
  %287 = getelementptr inbounds float, float* %285, i64 4
  %288 = bitcast float* %287 to <4 x float>*
  %wide.load189 = load <4 x float>, <4 x float>* %288, align 16, !tbaa !2230
  %289 = getelementptr inbounds float, float* %11, i64 %284
  %290 = bitcast float* %289 to <4 x float>*
  %wide.load190 = load <4 x float>, <4 x float>* %290, align 64, !tbaa !2226
  %291 = getelementptr inbounds float, float* %289, i64 4
  %292 = bitcast float* %291 to <4 x float>*
  %wide.load191 = load <4 x float>, <4 x float>* %292, align 16, !tbaa !2226
  %293 = fadd <4 x float> %wide.load188, %wide.load190
  %294 = fadd <4 x float> %wide.load189, %wide.load191
  %295 = bitcast float* %289 to <4 x float>*
  store <4 x float> %293, <4 x float>* %295, align 64, !tbaa !2226
  %296 = bitcast float* %291 to <4 x float>*
  store <4 x float> %294, <4 x float>* %296, align 16, !tbaa !2226
  %index.next185 = or i64 %index184, 8
  %297 = add nuw nsw i64 %283, %index.next185
  %298 = getelementptr inbounds float, float* %20, i64 %index.next185
  %299 = bitcast float* %298 to <4 x float>*
  %wide.load188.1 = load <4 x float>, <4 x float>* %299, align 32, !tbaa !2230
  %300 = getelementptr inbounds float, float* %298, i64 4
  %301 = bitcast float* %300 to <4 x float>*
  %wide.load189.1 = load <4 x float>, <4 x float>* %301, align 16, !tbaa !2230
  %302 = getelementptr inbounds float, float* %11, i64 %297
  %303 = bitcast float* %302 to <4 x float>*
  %wide.load190.1 = load <4 x float>, <4 x float>* %303, align 32, !tbaa !2226
  %304 = getelementptr inbounds float, float* %302, i64 4
  %305 = bitcast float* %304 to <4 x float>*
  %wide.load191.1 = load <4 x float>, <4 x float>* %305, align 16, !tbaa !2226
  %306 = fadd <4 x float> %wide.load188.1, %wide.load190.1
  %307 = fadd <4 x float> %wide.load189.1, %wide.load191.1
  %308 = bitcast float* %302 to <4 x float>*
  store <4 x float> %306, <4 x float>* %308, align 32, !tbaa !2226
  %309 = bitcast float* %304 to <4 x float>*
  store <4 x float> %307, <4 x float>* %309, align 16, !tbaa !2226
  %index.next185.1 = add nuw nsw i64 %index184, 16
  %310 = icmp eq i64 %index.next185.1, 1024
  br i1 %310, label %for_end35.2, label %vector.body182, !prof !341, !llvm.loop !2245

for_end35.2:                                      ; preds = %vector.body182
  %311 = add nuw nsw i64 %89, 3072
  br label %vector.body170

vector.body170:                                   ; preds = %vector.body170, %for_end35.2
  %index172 = phi i64 [ 0, %for_end35.2 ], [ %index.next173.1, %vector.body170 ]
  %312 = add nuw nsw i64 %311, %index172
  %313 = getelementptr inbounds float, float* %20, i64 %index172
  %314 = bitcast float* %313 to <4 x float>*
  %wide.load176 = load <4 x float>, <4 x float>* %314, align 64, !tbaa !2230
  %315 = getelementptr inbounds float, float* %313, i64 4
  %316 = bitcast float* %315 to <4 x float>*
  %wide.load177 = load <4 x float>, <4 x float>* %316, align 16, !tbaa !2230
  %317 = getelementptr inbounds float, float* %11, i64 %312
  %318 = bitcast float* %317 to <4 x float>*
  %wide.load178 = load <4 x float>, <4 x float>* %318, align 64, !tbaa !2226
  %319 = getelementptr inbounds float, float* %317, i64 4
  %320 = bitcast float* %319 to <4 x float>*
  %wide.load179 = load <4 x float>, <4 x float>* %320, align 16, !tbaa !2226
  %321 = fadd <4 x float> %wide.load176, %wide.load178
  %322 = fadd <4 x float> %wide.load177, %wide.load179
  %323 = bitcast float* %317 to <4 x float>*
  store <4 x float> %321, <4 x float>* %323, align 64, !tbaa !2226
  %324 = bitcast float* %319 to <4 x float>*
  store <4 x float> %322, <4 x float>* %324, align 16, !tbaa !2226
  %index.next173 = or i64 %index172, 8
  %325 = add nuw nsw i64 %311, %index.next173
  %326 = getelementptr inbounds float, float* %20, i64 %index.next173
  %327 = bitcast float* %326 to <4 x float>*
  %wide.load176.1 = load <4 x float>, <4 x float>* %327, align 32, !tbaa !2230
  %328 = getelementptr inbounds float, float* %326, i64 4
  %329 = bitcast float* %328 to <4 x float>*
  %wide.load177.1 = load <4 x float>, <4 x float>* %329, align 16, !tbaa !2230
  %330 = getelementptr inbounds float, float* %11, i64 %325
  %331 = bitcast float* %330 to <4 x float>*
  %wide.load178.1 = load <4 x float>, <4 x float>* %331, align 32, !tbaa !2226
  %332 = getelementptr inbounds float, float* %330, i64 4
  %333 = bitcast float* %332 to <4 x float>*
  %wide.load179.1 = load <4 x float>, <4 x float>* %333, align 16, !tbaa !2226
  %334 = fadd <4 x float> %wide.load176.1, %wide.load178.1
  %335 = fadd <4 x float> %wide.load177.1, %wide.load179.1
  %336 = bitcast float* %330 to <4 x float>*
  store <4 x float> %334, <4 x float>* %336, align 32, !tbaa !2226
  %337 = bitcast float* %332 to <4 x float>*
  store <4 x float> %335, <4 x float>* %337, align 16, !tbaa !2226
  %index.next173.1 = add nuw nsw i64 %index172, 16
  %338 = icmp eq i64 %index.next173.1, 1024
  br i1 %338, label %for_end35.3, label %vector.body170, !prof !341, !llvm.loop !2246

for_end35.3:                                      ; preds = %vector.body170
  %339 = add nuw nsw i64 %89, 4096
  br label %vector.body158

vector.body158:                                   ; preds = %vector.body158, %for_end35.3
  %index160 = phi i64 [ 0, %for_end35.3 ], [ %index.next161.1, %vector.body158 ]
  %340 = add nuw nsw i64 %339, %index160
  %341 = getelementptr inbounds float, float* %20, i64 %index160
  %342 = bitcast float* %341 to <4 x float>*
  %wide.load164 = load <4 x float>, <4 x float>* %342, align 64, !tbaa !2230
  %343 = getelementptr inbounds float, float* %341, i64 4
  %344 = bitcast float* %343 to <4 x float>*
  %wide.load165 = load <4 x float>, <4 x float>* %344, align 16, !tbaa !2230
  %345 = getelementptr inbounds float, float* %11, i64 %340
  %346 = bitcast float* %345 to <4 x float>*
  %wide.load166 = load <4 x float>, <4 x float>* %346, align 64, !tbaa !2226
  %347 = getelementptr inbounds float, float* %345, i64 4
  %348 = bitcast float* %347 to <4 x float>*
  %wide.load167 = load <4 x float>, <4 x float>* %348, align 16, !tbaa !2226
  %349 = fadd <4 x float> %wide.load164, %wide.load166
  %350 = fadd <4 x float> %wide.load165, %wide.load167
  %351 = bitcast float* %345 to <4 x float>*
  store <4 x float> %349, <4 x float>* %351, align 64, !tbaa !2226
  %352 = bitcast float* %347 to <4 x float>*
  store <4 x float> %350, <4 x float>* %352, align 16, !tbaa !2226
  %index.next161 = or i64 %index160, 8
  %353 = add nuw nsw i64 %339, %index.next161
  %354 = getelementptr inbounds float, float* %20, i64 %index.next161
  %355 = bitcast float* %354 to <4 x float>*
  %wide.load164.1 = load <4 x float>, <4 x float>* %355, align 32, !tbaa !2230
  %356 = getelementptr inbounds float, float* %354, i64 4
  %357 = bitcast float* %356 to <4 x float>*
  %wide.load165.1 = load <4 x float>, <4 x float>* %357, align 16, !tbaa !2230
  %358 = getelementptr inbounds float, float* %11, i64 %353
  %359 = bitcast float* %358 to <4 x float>*
  %wide.load166.1 = load <4 x float>, <4 x float>* %359, align 32, !tbaa !2226
  %360 = getelementptr inbounds float, float* %358, i64 4
  %361 = bitcast float* %360 to <4 x float>*
  %wide.load167.1 = load <4 x float>, <4 x float>* %361, align 16, !tbaa !2226
  %362 = fadd <4 x float> %wide.load164.1, %wide.load166.1
  %363 = fadd <4 x float> %wide.load165.1, %wide.load167.1
  %364 = bitcast float* %358 to <4 x float>*
  store <4 x float> %362, <4 x float>* %364, align 32, !tbaa !2226
  %365 = bitcast float* %360 to <4 x float>*
  store <4 x float> %363, <4 x float>* %365, align 16, !tbaa !2226
  %index.next161.1 = add nuw nsw i64 %index160, 16
  %366 = icmp eq i64 %index.next161.1, 1024
  br i1 %366, label %for_end35.4, label %vector.body158, !prof !341, !llvm.loop !2247

for_end35.4:                                      ; preds = %vector.body158
  %367 = add nuw nsw i64 %89, 5120
  br label %vector.body146

vector.body146:                                   ; preds = %vector.body146, %for_end35.4
  %index148 = phi i64 [ 0, %for_end35.4 ], [ %index.next149.1, %vector.body146 ]
  %368 = add nuw nsw i64 %367, %index148
  %369 = getelementptr inbounds float, float* %20, i64 %index148
  %370 = bitcast float* %369 to <4 x float>*
  %wide.load152 = load <4 x float>, <4 x float>* %370, align 64, !tbaa !2230
  %371 = getelementptr inbounds float, float* %369, i64 4
  %372 = bitcast float* %371 to <4 x float>*
  %wide.load153 = load <4 x float>, <4 x float>* %372, align 16, !tbaa !2230
  %373 = getelementptr inbounds float, float* %11, i64 %368
  %374 = bitcast float* %373 to <4 x float>*
  %wide.load154 = load <4 x float>, <4 x float>* %374, align 64, !tbaa !2226
  %375 = getelementptr inbounds float, float* %373, i64 4
  %376 = bitcast float* %375 to <4 x float>*
  %wide.load155 = load <4 x float>, <4 x float>* %376, align 16, !tbaa !2226
  %377 = fadd <4 x float> %wide.load152, %wide.load154
  %378 = fadd <4 x float> %wide.load153, %wide.load155
  %379 = bitcast float* %373 to <4 x float>*
  store <4 x float> %377, <4 x float>* %379, align 64, !tbaa !2226
  %380 = bitcast float* %375 to <4 x float>*
  store <4 x float> %378, <4 x float>* %380, align 16, !tbaa !2226
  %index.next149 = or i64 %index148, 8
  %381 = add nuw nsw i64 %367, %index.next149
  %382 = getelementptr inbounds float, float* %20, i64 %index.next149
  %383 = bitcast float* %382 to <4 x float>*
  %wide.load152.1 = load <4 x float>, <4 x float>* %383, align 32, !tbaa !2230
  %384 = getelementptr inbounds float, float* %382, i64 4
  %385 = bitcast float* %384 to <4 x float>*
  %wide.load153.1 = load <4 x float>, <4 x float>* %385, align 16, !tbaa !2230
  %386 = getelementptr inbounds float, float* %11, i64 %381
  %387 = bitcast float* %386 to <4 x float>*
  %wide.load154.1 = load <4 x float>, <4 x float>* %387, align 32, !tbaa !2226
  %388 = getelementptr inbounds float, float* %386, i64 4
  %389 = bitcast float* %388 to <4 x float>*
  %wide.load155.1 = load <4 x float>, <4 x float>* %389, align 16, !tbaa !2226
  %390 = fadd <4 x float> %wide.load152.1, %wide.load154.1
  %391 = fadd <4 x float> %wide.load153.1, %wide.load155.1
  %392 = bitcast float* %386 to <4 x float>*
  store <4 x float> %390, <4 x float>* %392, align 32, !tbaa !2226
  %393 = bitcast float* %388 to <4 x float>*
  store <4 x float> %391, <4 x float>* %393, align 16, !tbaa !2226
  %index.next149.1 = add nuw nsw i64 %index148, 16
  %394 = icmp eq i64 %index.next149.1, 1024
  br i1 %394, label %for_end35.5, label %vector.body146, !prof !341, !llvm.loop !2248

for_end35.5:                                      ; preds = %vector.body146
  %395 = add nuw nsw i64 %89, 6144
  br label %vector.body134

vector.body134:                                   ; preds = %vector.body134, %for_end35.5
  %index136 = phi i64 [ 0, %for_end35.5 ], [ %index.next137.1, %vector.body134 ]
  %396 = add nuw nsw i64 %395, %index136
  %397 = getelementptr inbounds float, float* %20, i64 %index136
  %398 = bitcast float* %397 to <4 x float>*
  %wide.load140 = load <4 x float>, <4 x float>* %398, align 64, !tbaa !2230
  %399 = getelementptr inbounds float, float* %397, i64 4
  %400 = bitcast float* %399 to <4 x float>*
  %wide.load141 = load <4 x float>, <4 x float>* %400, align 16, !tbaa !2230
  %401 = getelementptr inbounds float, float* %11, i64 %396
  %402 = bitcast float* %401 to <4 x float>*
  %wide.load142 = load <4 x float>, <4 x float>* %402, align 64, !tbaa !2226
  %403 = getelementptr inbounds float, float* %401, i64 4
  %404 = bitcast float* %403 to <4 x float>*
  %wide.load143 = load <4 x float>, <4 x float>* %404, align 16, !tbaa !2226
  %405 = fadd <4 x float> %wide.load140, %wide.load142
  %406 = fadd <4 x float> %wide.load141, %wide.load143
  %407 = bitcast float* %401 to <4 x float>*
  store <4 x float> %405, <4 x float>* %407, align 64, !tbaa !2226
  %408 = bitcast float* %403 to <4 x float>*
  store <4 x float> %406, <4 x float>* %408, align 16, !tbaa !2226
  %index.next137 = or i64 %index136, 8
  %409 = add nuw nsw i64 %395, %index.next137
  %410 = getelementptr inbounds float, float* %20, i64 %index.next137
  %411 = bitcast float* %410 to <4 x float>*
  %wide.load140.1 = load <4 x float>, <4 x float>* %411, align 32, !tbaa !2230
  %412 = getelementptr inbounds float, float* %410, i64 4
  %413 = bitcast float* %412 to <4 x float>*
  %wide.load141.1 = load <4 x float>, <4 x float>* %413, align 16, !tbaa !2230
  %414 = getelementptr inbounds float, float* %11, i64 %409
  %415 = bitcast float* %414 to <4 x float>*
  %wide.load142.1 = load <4 x float>, <4 x float>* %415, align 32, !tbaa !2226
  %416 = getelementptr inbounds float, float* %414, i64 4
  %417 = bitcast float* %416 to <4 x float>*
  %wide.load143.1 = load <4 x float>, <4 x float>* %417, align 16, !tbaa !2226
  %418 = fadd <4 x float> %wide.load140.1, %wide.load142.1
  %419 = fadd <4 x float> %wide.load141.1, %wide.load143.1
  %420 = bitcast float* %414 to <4 x float>*
  store <4 x float> %418, <4 x float>* %420, align 32, !tbaa !2226
  %421 = bitcast float* %416 to <4 x float>*
  store <4 x float> %419, <4 x float>* %421, align 16, !tbaa !2226
  %index.next137.1 = add nuw nsw i64 %index136, 16
  %422 = icmp eq i64 %index.next137.1, 1024
  br i1 %422, label %for_end35.6, label %vector.body134, !prof !341, !llvm.loop !2249

for_end35.6:                                      ; preds = %vector.body134
  %indvars.iv.next81 = add nuw nsw i64 %indvars.iv80, 1
  %exitcond82.not = icmp eq i64 %indvars.iv.next81, 7
  br i1 %exitcond82.not, label %for_begin36.preheader, label %for_begin30.preheader, !prof !51
}

define dllexport i32 @fused_nn_avg_pool2d(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 2
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([72 x i8], [72 x i8]* @.str.191, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !2250
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !2264
  %18 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %19 = load i8*, i8** %18, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %19, i64 128) ]
  %20 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %21 = load i64*, i64** %20, align 8
  %22 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %23 = load i64*, i64** %22, align 8
  %24 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %25 = load i32, i32* %24, align 4
  %26 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %27 = load i8*, i8** %26, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %27, i64 128) ]
  %28 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %29 = load i64*, i64** %28, align 8
  %30 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %31 = load i64*, i64** %30, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %32 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %32(i8* getelementptr inbounds ([147 x i8], [147 x i8]* @.str.192, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %33 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %33(i8* getelementptr inbounds ([147 x i8], [147 x i8]* @.str.193, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %35 = load i32, i32* %34, align 4
  %36 = icmp eq i32 %35, 4
  br i1 %36, label %assert_end8, label %assert_fail5, !prof !5

assert_fail5:                                     ; preds = %assert_end4
  %37 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %37(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end4
  %38 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %39 = load i16, i16* %38, align 2
  %40 = icmp eq i16 %39, 1
  %41 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %42 = load i8, i8* %41, align 1
  %43 = icmp eq i8 %42, 32
  %44 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %45 = load i8, i8* %44, align 1
  %46 = icmp eq i8 %45, 2
  %47 = and i1 %43, %46
  %48 = and i1 %40, %47
  br i1 %48, label %assert_end10, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %49 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %49(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end10:                                     ; preds = %assert_end8
  %50 = load i64, i64* %21, align 8, !tbaa !2266
  %51 = trunc i64 %50 to i32
  %52 = icmp eq i32 %51, 1
  br i1 %52, label %assert_end12, label %assert_fail11, !prof !5

assert_fail11:                                    ; preds = %assert_end10
  %53 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %53(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end10
  %54 = getelementptr inbounds i64, i64* %21, i64 1
  %55 = load i64, i64* %54, align 8, !tbaa !2280
  %56 = trunc i64 %55 to i32
  %57 = icmp eq i32 %56, 7
  br i1 %57, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.123, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %59 = getelementptr inbounds i64, i64* %21, i64 2
  %60 = load i64, i64* %59, align 8, !tbaa !2282
  %61 = trunc i64 %60 to i32
  %62 = icmp eq i32 %61, 7
  br i1 %62, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.124, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %64 = getelementptr inbounds i64, i64* %21, i64 3
  %65 = load i64, i64* %64, align 8, !tbaa !2285
  %66 = trunc i64 %65 to i32
  %67 = icmp eq i32 %66, 1024
  br i1 %67, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %68 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %68(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.125, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %.not = icmp eq i64* %23, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end18
  %69 = bitcast i64* %23 to <4 x i64>*
  %70 = load <4 x i64>, <4 x i64>* %69, align 8, !tbaa !2287
  %71 = trunc <4 x i64> %70 to <4 x i32>
  %72 = icmp eq <4 x i32> %71, <i32 50176, i32 7168, i32 1024, i32 1>
  %73 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %72)
  br i1 %73, label %if_end, label %assert_fail19, !prof !5

if_end:                                           ; preds = %if_then, %assert_end18
  %74 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %75 = load i64, i64* %74, align 8
  %76 = icmp eq i64 %75, 0
  br i1 %76, label %assert_end22, label %assert_fail21, !prof !5

assert_fail19:                                    ; preds = %if_then
  %77 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %77(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.126, i64 0, i64 0))
  ret i32 -1

assert_fail21:                                    ; preds = %if_end
  %78 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %78(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %if_end
  %79 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %80 = load i32, i32* %79, align 4
  %81 = icmp eq i32 %80, 1
  br i1 %81, label %assert_end24, label %assert_fail23, !prof !5

assert_fail23:                                    ; preds = %assert_end22
  %82 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %82(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end24:                                     ; preds = %assert_end22
  %83 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %84 = load i32, i32* %83, align 4
  %85 = icmp eq i32 %84, 4
  br i1 %85, label %assert_end28, label %assert_fail25, !prof !5

assert_fail25:                                    ; preds = %assert_end24
  %86 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %86(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end24
  %87 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %88 = load i16, i16* %87, align 2
  %89 = icmp eq i16 %88, 1
  %90 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %91 = load i8, i8* %90, align 1
  %92 = icmp eq i8 %91, 32
  %93 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %94 = load i8, i8* %93, align 1
  %95 = icmp eq i8 %94, 2
  %96 = and i1 %92, %95
  %97 = and i1 %89, %96
  br i1 %97, label %assert_end30, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %98 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %98(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end30:                                     ; preds = %assert_end28
  %99 = load i64, i64* %29, align 8, !tbaa !2299
  %100 = trunc i64 %99 to i32
  %101 = icmp eq i32 %100, 1
  br i1 %101, label %assert_end32, label %assert_fail31, !prof !5

assert_fail31:                                    ; preds = %assert_end30
  %102 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %102(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.16, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end30
  %103 = getelementptr inbounds i64, i64* %29, i64 1
  %104 = load i64, i64* %103, align 8, !tbaa !2313
  %105 = trunc i64 %104 to i32
  %106 = icmp eq i32 %105, 1
  br i1 %106, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %107 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %107(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.17, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %108 = getelementptr inbounds i64, i64* %29, i64 2
  %109 = load i64, i64* %108, align 8, !tbaa !2315
  %110 = trunc i64 %109 to i32
  %111 = icmp eq i32 %110, 1
  br i1 %111, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.194, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %113 = getelementptr inbounds i64, i64* %29, i64 3
  %114 = load i64, i64* %113, align 8, !tbaa !2318
  %115 = trunc i64 %114 to i32
  %116 = icmp eq i32 %115, 1024
  br i1 %116, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %117 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %117(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.128, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %.not49 = icmp eq i64* %31, null
  br i1 %.not49, label %if_end40, label %if_then39, !prof !51

if_then39:                                        ; preds = %assert_end38
  %118 = bitcast i64* %31 to <4 x i64>*
  %119 = load <4 x i64>, <4 x i64>* %118, align 8, !tbaa !2320
  %120 = trunc <4 x i64> %119 to <4 x i32>
  %121 = icmp eq <4 x i32> %120, <i32 1024, i32 1024, i32 1024, i32 1>
  %122 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %121)
  br i1 %122, label %if_end40, label %assert_fail41, !prof !5

if_end40:                                         ; preds = %if_then39, %assert_end38
  %123 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %124 = load i64, i64* %123, align 8
  %125 = icmp eq i64 %124, 0
  br i1 %125, label %assert_end44, label %assert_fail43, !prof !5

assert_fail41:                                    ; preds = %if_then39
  %126 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %126(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.195, i64 0, i64 0))
  ret i32 -1

assert_fail43:                                    ; preds = %if_end40
  %127 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %127(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end44:                                     ; preds = %if_end40
  %128 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %129 = load i32, i32* %128, align 4
  %130 = icmp eq i32 %129, 1
  br i1 %130, label %assert_end46, label %assert_fail45, !prof !5

assert_fail45:                                    ; preds = %assert_end44
  %131 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %131(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end46:                                     ; preds = %assert_end44
  %132 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %133 = load i32, i32* %132, align 4
  %134 = icmp eq i32 %25, %133
  br i1 %134, label %assert_end48, label %assert_fail47, !prof !5

assert_fail47:                                    ; preds = %assert_end46
  %135 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %135(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %assert_end46
  %136 = tail call fastcc i32 @fused_nn_avg_pool2d_compute_(i8* %19, i8* %27, i32 %25)
  ret i32 %136
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_avg_pool2d_compute_(i8* noalias nocapture readonly align 128 %0, i8* noalias nocapture align 128 %1, i32 %2) unnamed_addr #1 {
entry:
  %3 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %4 = tail call i8* %3(i32 1, i32 %2, i64 4096, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %4, i64 128) ]
  %5 = icmp eq i8* %4, null
  br i1 %5, label %if_then, label %for_begin.preheader, !prof !5

for_begin.preheader:                              ; preds = %entry
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %4, i8 0, i64 4096, i1 false)
  %6 = bitcast i8* %0 to float*
  %7 = bitcast i8* %4 to float*
  %8 = bitcast i8* %4 to <64 x float>*
  %9 = getelementptr inbounds i8, i8* %4, i64 256
  %10 = bitcast i8* %9 to <64 x float>*
  %11 = getelementptr inbounds i8, i8* %4, i64 512
  %12 = bitcast i8* %11 to <64 x float>*
  %13 = getelementptr inbounds i8, i8* %4, i64 768
  %14 = bitcast i8* %13 to <64 x float>*
  %15 = getelementptr inbounds i8, i8* %4, i64 1024
  %16 = bitcast i8* %15 to <64 x float>*
  %17 = getelementptr inbounds i8, i8* %4, i64 1280
  %18 = bitcast i8* %17 to <64 x float>*
  %19 = getelementptr inbounds i8, i8* %4, i64 1536
  %20 = bitcast i8* %19 to <64 x float>*
  %21 = getelementptr inbounds i8, i8* %4, i64 1792
  %22 = bitcast i8* %21 to <64 x float>*
  %23 = getelementptr inbounds i8, i8* %4, i64 2048
  %24 = bitcast i8* %23 to <64 x float>*
  %25 = getelementptr inbounds i8, i8* %4, i64 2304
  %26 = bitcast i8* %25 to <64 x float>*
  %27 = getelementptr inbounds i8, i8* %4, i64 2560
  %28 = bitcast i8* %27 to <64 x float>*
  %29 = getelementptr inbounds i8, i8* %4, i64 2816
  %30 = bitcast i8* %29 to <64 x float>*
  %31 = getelementptr inbounds i8, i8* %4, i64 3072
  %32 = bitcast i8* %31 to <64 x float>*
  %33 = getelementptr inbounds i8, i8* %4, i64 3328
  %34 = bitcast i8* %33 to <64 x float>*
  %35 = getelementptr inbounds i8, i8* %4, i64 3584
  %36 = bitcast i8* %35 to <64 x float>*
  %37 = getelementptr inbounds i8, i8* %4, i64 3840
  %38 = bitcast i8* %37 to <64 x float>*
  %.promoted = load <64 x float>, <64 x float>* %30, align 128, !tbaa !2332
  %.promoted38 = load <64 x float>, <64 x float>* %32, align 128, !tbaa !2332
  %.promoted40 = load <64 x float>, <64 x float>* %34, align 128, !tbaa !2332
  %.promoted42 = load <64 x float>, <64 x float>* %36, align 128, !tbaa !2332
  %.promoted44 = load <64 x float>, <64 x float>* %38, align 128, !tbaa !2332
  br label %for_begin4.preheader

if_then:                                          ; preds = %entry
  ret i32 -1

for_begin4.preheader:                             ; preds = %for_begin4.preheader, %for_begin.preheader
  %39 = phi <64 x float> [ %.promoted44, %for_begin.preheader ], [ %134, %for_begin4.preheader ]
  %40 = phi <64 x float> [ %.promoted42, %for_begin.preheader ], [ %129, %for_begin4.preheader ]
  %41 = phi <64 x float> [ %.promoted40, %for_begin.preheader ], [ %124, %for_begin4.preheader ]
  %42 = phi <64 x float> [ %.promoted38, %for_begin.preheader ], [ %119, %for_begin4.preheader ]
  %43 = phi <64 x float> [ %.promoted, %for_begin.preheader ], [ %114, %for_begin4.preheader ]
  %44 = phi <64 x float> [ zeroinitializer, %for_begin.preheader ], [ %109, %for_begin4.preheader ]
  %45 = phi <64 x float> [ zeroinitializer, %for_begin.preheader ], [ %104, %for_begin4.preheader ]
  %46 = phi <64 x float> [ zeroinitializer, %for_begin.preheader ], [ %99, %for_begin4.preheader ]
  %47 = phi <64 x float> [ zeroinitializer, %for_begin.preheader ], [ %94, %for_begin4.preheader ]
  %48 = phi <64 x float> [ zeroinitializer, %for_begin.preheader ], [ %89, %for_begin4.preheader ]
  %49 = phi <64 x float> [ zeroinitializer, %for_begin.preheader ], [ %84, %for_begin4.preheader ]
  %50 = phi <64 x float> [ zeroinitializer, %for_begin.preheader ], [ %79, %for_begin4.preheader ]
  %51 = phi <64 x float> [ zeroinitializer, %for_begin.preheader ], [ %74, %for_begin4.preheader ]
  %52 = phi <64 x float> [ zeroinitializer, %for_begin.preheader ], [ %69, %for_begin4.preheader ]
  %53 = phi <64 x float> [ zeroinitializer, %for_begin.preheader ], [ %64, %for_begin4.preheader ]
  %54 = phi <64 x float> [ zeroinitializer, %for_begin.preheader ], [ %59, %for_begin4.preheader ]
  %indvars.iv18 = phi i64 [ 0, %for_begin.preheader ], [ %indvars.iv.next19, %for_begin4.preheader ]
  %55 = shl nsw i64 %indvars.iv18, 10
  %56 = getelementptr inbounds float, float* %6, i64 %55
  %57 = bitcast float* %56 to <64 x float>*
  %58 = load <64 x float>, <64 x float>* %57, align 128, !tbaa !2335
  %59 = fadd <64 x float> %58, %54
  %60 = or i64 %55, 64
  %61 = getelementptr inbounds float, float* %6, i64 %60
  %62 = bitcast float* %61 to <64 x float>*
  %63 = load <64 x float>, <64 x float>* %62, align 128, !tbaa !2335
  %64 = fadd <64 x float> %63, %53
  %65 = or i64 %55, 128
  %66 = getelementptr inbounds float, float* %6, i64 %65
  %67 = bitcast float* %66 to <64 x float>*
  %68 = load <64 x float>, <64 x float>* %67, align 128, !tbaa !2335
  %69 = fadd <64 x float> %68, %52
  %70 = or i64 %55, 192
  %71 = getelementptr inbounds float, float* %6, i64 %70
  %72 = bitcast float* %71 to <64 x float>*
  %73 = load <64 x float>, <64 x float>* %72, align 128, !tbaa !2335
  %74 = fadd <64 x float> %73, %51
  %75 = or i64 %55, 256
  %76 = getelementptr inbounds float, float* %6, i64 %75
  %77 = bitcast float* %76 to <64 x float>*
  %78 = load <64 x float>, <64 x float>* %77, align 128, !tbaa !2335
  %79 = fadd <64 x float> %78, %50
  %80 = or i64 %55, 320
  %81 = getelementptr inbounds float, float* %6, i64 %80
  %82 = bitcast float* %81 to <64 x float>*
  %83 = load <64 x float>, <64 x float>* %82, align 128, !tbaa !2335
  %84 = fadd <64 x float> %83, %49
  %85 = or i64 %55, 384
  %86 = getelementptr inbounds float, float* %6, i64 %85
  %87 = bitcast float* %86 to <64 x float>*
  %88 = load <64 x float>, <64 x float>* %87, align 128, !tbaa !2335
  %89 = fadd <64 x float> %88, %48
  %90 = or i64 %55, 448
  %91 = getelementptr inbounds float, float* %6, i64 %90
  %92 = bitcast float* %91 to <64 x float>*
  %93 = load <64 x float>, <64 x float>* %92, align 128, !tbaa !2335
  %94 = fadd <64 x float> %93, %47
  %95 = or i64 %55, 512
  %96 = getelementptr inbounds float, float* %6, i64 %95
  %97 = bitcast float* %96 to <64 x float>*
  %98 = load <64 x float>, <64 x float>* %97, align 128, !tbaa !2335
  %99 = fadd <64 x float> %98, %46
  %100 = or i64 %55, 576
  %101 = getelementptr inbounds float, float* %6, i64 %100
  %102 = bitcast float* %101 to <64 x float>*
  %103 = load <64 x float>, <64 x float>* %102, align 128, !tbaa !2335
  %104 = fadd <64 x float> %103, %45
  %105 = or i64 %55, 640
  %106 = getelementptr inbounds float, float* %6, i64 %105
  %107 = bitcast float* %106 to <64 x float>*
  %108 = load <64 x float>, <64 x float>* %107, align 128, !tbaa !2335
  %109 = fadd <64 x float> %108, %44
  %110 = or i64 %55, 704
  %111 = getelementptr inbounds float, float* %6, i64 %110
  %112 = bitcast float* %111 to <64 x float>*
  %113 = load <64 x float>, <64 x float>* %112, align 128, !tbaa !2335
  %114 = fadd <64 x float> %113, %43
  %115 = or i64 %55, 768
  %116 = getelementptr inbounds float, float* %6, i64 %115
  %117 = bitcast float* %116 to <64 x float>*
  %118 = load <64 x float>, <64 x float>* %117, align 128, !tbaa !2335
  %119 = fadd <64 x float> %118, %42
  %120 = or i64 %55, 832
  %121 = getelementptr inbounds float, float* %6, i64 %120
  %122 = bitcast float* %121 to <64 x float>*
  %123 = load <64 x float>, <64 x float>* %122, align 128, !tbaa !2335
  %124 = fadd <64 x float> %123, %41
  %125 = or i64 %55, 896
  %126 = getelementptr inbounds float, float* %6, i64 %125
  %127 = bitcast float* %126 to <64 x float>*
  %128 = load <64 x float>, <64 x float>* %127, align 128, !tbaa !2335
  %129 = fadd <64 x float> %128, %40
  %130 = or i64 %55, 960
  %131 = getelementptr inbounds float, float* %6, i64 %130
  %132 = bitcast float* %131 to <64 x float>*
  %133 = load <64 x float>, <64 x float>* %132, align 128, !tbaa !2335
  %134 = fadd <64 x float> %133, %39
  %indvars.iv.next19 = add nuw nsw i64 %indvars.iv18, 1
  %exitcond20.not = icmp eq i64 %indvars.iv.next19, 49
  br i1 %exitcond20.not, label %for_begin7.preheader, label %for_begin4.preheader, !prof !51

for_begin7.preheader:                             ; preds = %for_begin4.preheader
  store <64 x float> %59, <64 x float>* %8, align 128, !tbaa !2332
  store <64 x float> %64, <64 x float>* %10, align 128, !tbaa !2332
  store <64 x float> %69, <64 x float>* %12, align 128, !tbaa !2332
  store <64 x float> %74, <64 x float>* %14, align 128, !tbaa !2332
  store <64 x float> %79, <64 x float>* %16, align 128, !tbaa !2332
  store <64 x float> %84, <64 x float>* %18, align 128, !tbaa !2332
  store <64 x float> %89, <64 x float>* %20, align 128, !tbaa !2332
  store <64 x float> %94, <64 x float>* %22, align 128, !tbaa !2332
  store <64 x float> %99, <64 x float>* %24, align 128, !tbaa !2332
  store <64 x float> %104, <64 x float>* %26, align 128, !tbaa !2332
  store <64 x float> %109, <64 x float>* %28, align 128, !tbaa !2332
  store <64 x float> %114, <64 x float>* %30, align 128, !tbaa !2332
  store <64 x float> %119, <64 x float>* %32, align 128, !tbaa !2332
  store <64 x float> %124, <64 x float>* %34, align 128, !tbaa !2332
  store <64 x float> %129, <64 x float>* %36, align 128, !tbaa !2332
  store <64 x float> %134, <64 x float>* %38, align 128, !tbaa !2332
  %135 = bitcast i8* %1 to float*
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_begin7.preheader
  %index = phi i64 [ 0, %for_begin7.preheader ], [ %index.next.1, %vector.body ]
  %136 = getelementptr inbounds float, float* %7, i64 %index
  %137 = bitcast float* %136 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %137, align 64, !tbaa !2332
  %138 = getelementptr inbounds float, float* %136, i64 4
  %139 = bitcast float* %138 to <4 x float>*
  %wide.load61 = load <4 x float>, <4 x float>* %139, align 16, !tbaa !2332
  %140 = fmul <4 x float> %wide.load, <float 0x3F94E5E0A0000000, float 0x3F94E5E0A0000000, float 0x3F94E5E0A0000000, float 0x3F94E5E0A0000000>
  %141 = fmul <4 x float> %wide.load61, <float 0x3F94E5E0A0000000, float 0x3F94E5E0A0000000, float 0x3F94E5E0A0000000, float 0x3F94E5E0A0000000>
  %142 = getelementptr inbounds float, float* %135, i64 %index
  %143 = bitcast float* %142 to <4 x float>*
  store <4 x float> %140, <4 x float>* %143, align 64, !tbaa !2338
  %144 = getelementptr inbounds float, float* %142, i64 4
  %145 = bitcast float* %144 to <4 x float>*
  store <4 x float> %141, <4 x float>* %145, align 16, !tbaa !2338
  %index.next = or i64 %index, 8
  %146 = getelementptr inbounds float, float* %7, i64 %index.next
  %147 = bitcast float* %146 to <4 x float>*
  %wide.load.1 = load <4 x float>, <4 x float>* %147, align 32, !tbaa !2332
  %148 = getelementptr inbounds float, float* %146, i64 4
  %149 = bitcast float* %148 to <4 x float>*
  %wide.load61.1 = load <4 x float>, <4 x float>* %149, align 16, !tbaa !2332
  %150 = fmul <4 x float> %wide.load.1, <float 0x3F94E5E0A0000000, float 0x3F94E5E0A0000000, float 0x3F94E5E0A0000000, float 0x3F94E5E0A0000000>
  %151 = fmul <4 x float> %wide.load61.1, <float 0x3F94E5E0A0000000, float 0x3F94E5E0A0000000, float 0x3F94E5E0A0000000, float 0x3F94E5E0A0000000>
  %152 = getelementptr inbounds float, float* %135, i64 %index.next
  %153 = bitcast float* %152 to <4 x float>*
  store <4 x float> %150, <4 x float>* %153, align 32, !tbaa !2338
  %154 = getelementptr inbounds float, float* %152, i64 4
  %155 = bitcast float* %154 to <4 x float>*
  store <4 x float> %151, <4 x float>* %155, align 16, !tbaa !2338
  %index.next.1 = add nuw nsw i64 %index, 16
  %156 = icmp eq i64 %index.next.1, 1024
  br i1 %156, label %for_end9, label %vector.body, !prof !341, !llvm.loop !2341

for_end9:                                         ; preds = %vector.body
  %157 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %158 = tail call i32 %157(i32 1, i32 %2, i8* nonnull %4)
  %.not = icmp ne i32 %158, 0
  %spec.select = sext i1 %.not to i32
  ret i32 %spec.select
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_14(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([88 x i8], [88 x i8]* @.str.196, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !2342
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !2356
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !2358
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !2361
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.197, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.198, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.199, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.200, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !2363
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !2377
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 56
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.66, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !2379
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 56
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.67, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !2382
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 64
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.201, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !2384
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 200704, i32 3584, i32 64, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.202, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !2396
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 1
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.16, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !2410
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 1
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.17, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !2412
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 64
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.203, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !2415
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 128
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.159, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !2417
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 8192, i32 8192, i32 128, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([200 x i8], [200 x i8]* @.str.204, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !2429
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 128
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.72, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !2443
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !2457
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !2471
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 56
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.82, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !2473
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 56
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.83, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !2476
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 128
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.75, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !2478
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 401408, i32 7168, i32 128, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.84, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_14_compute_(i8* %31, i8* %39, i8* %51, i8* %45, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_14_compute_(i8* noalias align 128 %0, i8* noalias align 128 %1, i8* noalias align 128 %2, i8* noalias align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 802816, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %call_end, %if_end, %entry
  %merge = phi i32 [ -1, %entry ], [ %13, %if_end ], [ %22, %call_end ]
  ret i32 %merge

if_end:                                           ; preds = %entry
  %8 = alloca %16, align 8
  %9 = getelementptr inbounds %16, %16* %8, i64 0, i32 0
  store i8* %6, i8** %9, align 8
  %10 = getelementptr inbounds %16, %16* %8, i64 0, i32 1
  store i8* %0, i8** %10, align 8
  %11 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %12 = bitcast %16* %8 to i8*
  %13 = call i32 %11(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.205, i8* nonnull %12, i32 0)
  %14 = icmp eq i32 %13, 0
  br i1 %14, label %call_end, label %if_then, !prof !5

call_end:                                         ; preds = %if_end
  %15 = alloca %17, align 8
  %16 = getelementptr inbounds %17, %17* %15, i64 0, i32 0
  store i8* %6, i8** %16, align 8
  %17 = getelementptr inbounds %17, %17* %15, i64 0, i32 1
  store i8* %1, i8** %17, align 8
  %18 = getelementptr inbounds %17, %17* %15, i64 0, i32 2
  store i8* %2, i8** %18, align 8
  %19 = getelementptr inbounds %17, %17* %15, i64 0, i32 3
  store i8* %3, i8** %19, align 8
  %20 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %21 = bitcast %17* %15 to i8*
  %22 = call i32 %20(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.206, i8* nonnull %21, i32 0)
  %23 = icmp eq i32 %22, 0
  br i1 %23, label %call_end2, label %if_then, !prof !5

call_end2:                                        ; preds = %call_end
  %24 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %25 = call i32 %24(i32 1, i32 %4, i8* nonnull %6)
  %.not = icmp ne i32 %25, 0
  %spec.select = sext i1 %.not to i32
  ret i32 %spec.select
}

; Function Attrs: nofree norecurse nounwind
define private i32 @__tvm_parallel_lambda.205(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #2 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %9 = load i32, i32* %8, align 4
  %10 = add nsw i32 %9, 55
  %11 = sdiv i32 %10, %9
  %12 = add nsw i32 %0, 1
  %13 = mul nsw i32 %11, %12
  %14 = icmp slt i32 %13, 56
  %15 = select i1 %14, i32 %13, i32 56
  %16 = mul nsw i32 %11, %0
  %17 = icmp slt i32 %16, 56
  %18 = select i1 %17, i32 %16, i32 56
  %19 = icmp slt i32 %18, %15
  br i1 %19, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %20 = sext i32 %18 to i64
  %wide.trip.count = sext i32 %15 to i64
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end3
  %indvars.iv13 = phi i64 [ %20, %for_begin1.preheader.preheader ], [ %indvars.iv.next14, %for_end3 ]
  %21 = mul nsw i64 %indvars.iv13, 3584
  br label %for_begin4.preheader

for_end:                                          ; preds = %for_end3, %entry
  ret i32 0

for_begin4.preheader:                             ; preds = %for_begin1.preheader, %for_begin4.preheader
  %indvars.iv10 = phi i64 [ 0, %for_begin1.preheader ], [ %indvars.iv.next11, %for_begin4.preheader ]
  %22 = shl nsw i64 %indvars.iv10, 6
  %23 = add nsw i64 %22, %21
  %24 = getelementptr inbounds float, float* %7, i64 %23
  %25 = bitcast float* %24 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %25, align 4, !tbaa !2490
  %26 = getelementptr inbounds float, float* %4, i64 %23
  %27 = bitcast float* %26 to <4 x float>*
  store <4 x float> %wide.load, <4 x float>* %27, align 4, !tbaa !2493
  %28 = or i64 %23, 4
  %29 = getelementptr inbounds float, float* %7, i64 %28
  %30 = bitcast float* %29 to <4 x float>*
  %wide.load.1 = load <4 x float>, <4 x float>* %30, align 4, !tbaa !2490
  %31 = getelementptr inbounds float, float* %4, i64 %28
  %32 = bitcast float* %31 to <4 x float>*
  store <4 x float> %wide.load.1, <4 x float>* %32, align 4, !tbaa !2493
  %33 = or i64 %23, 8
  %34 = getelementptr inbounds float, float* %7, i64 %33
  %35 = bitcast float* %34 to <4 x float>*
  %wide.load.2 = load <4 x float>, <4 x float>* %35, align 4, !tbaa !2490
  %36 = getelementptr inbounds float, float* %4, i64 %33
  %37 = bitcast float* %36 to <4 x float>*
  store <4 x float> %wide.load.2, <4 x float>* %37, align 4, !tbaa !2493
  %38 = or i64 %23, 12
  %39 = getelementptr inbounds float, float* %7, i64 %38
  %40 = bitcast float* %39 to <4 x float>*
  %wide.load.3 = load <4 x float>, <4 x float>* %40, align 4, !tbaa !2490
  %41 = getelementptr inbounds float, float* %4, i64 %38
  %42 = bitcast float* %41 to <4 x float>*
  store <4 x float> %wide.load.3, <4 x float>* %42, align 4, !tbaa !2493
  %43 = or i64 %23, 16
  %44 = getelementptr inbounds float, float* %7, i64 %43
  %45 = bitcast float* %44 to <4 x float>*
  %wide.load.4 = load <4 x float>, <4 x float>* %45, align 4, !tbaa !2490
  %46 = getelementptr inbounds float, float* %4, i64 %43
  %47 = bitcast float* %46 to <4 x float>*
  store <4 x float> %wide.load.4, <4 x float>* %47, align 4, !tbaa !2493
  %48 = or i64 %23, 20
  %49 = getelementptr inbounds float, float* %7, i64 %48
  %50 = bitcast float* %49 to <4 x float>*
  %wide.load.5 = load <4 x float>, <4 x float>* %50, align 4, !tbaa !2490
  %51 = getelementptr inbounds float, float* %4, i64 %48
  %52 = bitcast float* %51 to <4 x float>*
  store <4 x float> %wide.load.5, <4 x float>* %52, align 4, !tbaa !2493
  %53 = or i64 %23, 24
  %54 = getelementptr inbounds float, float* %7, i64 %53
  %55 = bitcast float* %54 to <4 x float>*
  %wide.load.6 = load <4 x float>, <4 x float>* %55, align 4, !tbaa !2490
  %56 = getelementptr inbounds float, float* %4, i64 %53
  %57 = bitcast float* %56 to <4 x float>*
  store <4 x float> %wide.load.6, <4 x float>* %57, align 4, !tbaa !2493
  %58 = or i64 %23, 28
  %59 = getelementptr inbounds float, float* %7, i64 %58
  %60 = bitcast float* %59 to <4 x float>*
  %wide.load.7 = load <4 x float>, <4 x float>* %60, align 4, !tbaa !2490
  %61 = getelementptr inbounds float, float* %4, i64 %58
  %62 = bitcast float* %61 to <4 x float>*
  store <4 x float> %wide.load.7, <4 x float>* %62, align 4, !tbaa !2493
  %63 = or i64 %23, 32
  %64 = getelementptr inbounds float, float* %7, i64 %63
  %65 = bitcast float* %64 to <4 x float>*
  %wide.load.8 = load <4 x float>, <4 x float>* %65, align 4, !tbaa !2490
  %66 = getelementptr inbounds float, float* %4, i64 %63
  %67 = bitcast float* %66 to <4 x float>*
  store <4 x float> %wide.load.8, <4 x float>* %67, align 4, !tbaa !2493
  %68 = or i64 %23, 36
  %69 = getelementptr inbounds float, float* %7, i64 %68
  %70 = bitcast float* %69 to <4 x float>*
  %wide.load.9 = load <4 x float>, <4 x float>* %70, align 4, !tbaa !2490
  %71 = getelementptr inbounds float, float* %4, i64 %68
  %72 = bitcast float* %71 to <4 x float>*
  store <4 x float> %wide.load.9, <4 x float>* %72, align 4, !tbaa !2493
  %73 = or i64 %23, 40
  %74 = getelementptr inbounds float, float* %7, i64 %73
  %75 = bitcast float* %74 to <4 x float>*
  %wide.load.10 = load <4 x float>, <4 x float>* %75, align 4, !tbaa !2490
  %76 = getelementptr inbounds float, float* %4, i64 %73
  %77 = bitcast float* %76 to <4 x float>*
  store <4 x float> %wide.load.10, <4 x float>* %77, align 4, !tbaa !2493
  %78 = or i64 %23, 44
  %79 = getelementptr inbounds float, float* %7, i64 %78
  %80 = bitcast float* %79 to <4 x float>*
  %wide.load.11 = load <4 x float>, <4 x float>* %80, align 4, !tbaa !2490
  %81 = getelementptr inbounds float, float* %4, i64 %78
  %82 = bitcast float* %81 to <4 x float>*
  store <4 x float> %wide.load.11, <4 x float>* %82, align 4, !tbaa !2493
  %83 = or i64 %23, 48
  %84 = getelementptr inbounds float, float* %7, i64 %83
  %85 = bitcast float* %84 to <4 x float>*
  %wide.load.12 = load <4 x float>, <4 x float>* %85, align 4, !tbaa !2490
  %86 = getelementptr inbounds float, float* %4, i64 %83
  %87 = bitcast float* %86 to <4 x float>*
  store <4 x float> %wide.load.12, <4 x float>* %87, align 4, !tbaa !2493
  %88 = or i64 %23, 52
  %89 = getelementptr inbounds float, float* %7, i64 %88
  %90 = bitcast float* %89 to <4 x float>*
  %wide.load.13 = load <4 x float>, <4 x float>* %90, align 4, !tbaa !2490
  %91 = getelementptr inbounds float, float* %4, i64 %88
  %92 = bitcast float* %91 to <4 x float>*
  store <4 x float> %wide.load.13, <4 x float>* %92, align 4, !tbaa !2493
  %93 = or i64 %23, 56
  %94 = getelementptr inbounds float, float* %7, i64 %93
  %95 = bitcast float* %94 to <4 x float>*
  %wide.load.14 = load <4 x float>, <4 x float>* %95, align 4, !tbaa !2490
  %96 = getelementptr inbounds float, float* %4, i64 %93
  %97 = bitcast float* %96 to <4 x float>*
  store <4 x float> %wide.load.14, <4 x float>* %97, align 4, !tbaa !2493
  %98 = or i64 %23, 60
  %99 = getelementptr inbounds float, float* %7, i64 %98
  %100 = bitcast float* %99 to <4 x float>*
  %wide.load.15 = load <4 x float>, <4 x float>* %100, align 4, !tbaa !2490
  %101 = getelementptr inbounds float, float* %4, i64 %98
  %102 = bitcast float* %101 to <4 x float>*
  store <4 x float> %wide.load.15, <4 x float>* %102, align 4, !tbaa !2493
  %indvars.iv.next11 = add nuw nsw i64 %indvars.iv10, 1
  %exitcond12.not = icmp eq i64 %indvars.iv.next11, 56
  br i1 %exitcond12.not, label %for_end3, label %for_begin4.preheader, !prof !51

for_end3:                                         ; preds = %for_begin4.preheader
  %indvars.iv.next14 = add nsw i64 %indvars.iv13, 1
  %exitcond15.not = icmp eq i64 %indvars.iv.next14, %wide.trip.count
  br i1 %exitcond15.not, label %for_end, label %for_begin1.preheader, !prof !51
}

; Function Attrs: nofree nounwind
define private i32 @__tvm_parallel_lambda.206(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #3 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds i8, i8* %2, i64 16
  %9 = bitcast i8* %8 to float**
  %10 = load float*, float** %9, align 8
  %11 = getelementptr inbounds i8, i8* %2, i64 24
  %12 = bitcast i8* %11 to float**
  %13 = load float*, float** %12, align 8
  %14 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %15 = load i32, i32* %14, align 4
  %16 = add nsw i32 %15, 3135
  %17 = sdiv i32 %16, %15
  %18 = add nsw i32 %0, 1
  %19 = mul nsw i32 %17, %18
  %20 = icmp slt i32 %19, 3136
  %21 = select i1 %20, i32 %19, i32 3136
  %22 = mul nsw i32 %17, %0
  %23 = icmp slt i32 %22, 3136
  %24 = select i1 %23, i32 %22, i32 3136
  %25 = icmp slt i32 %24, %21
  br i1 %25, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %26 = sext i32 %24 to i64
  %wide.trip.count = sext i32 %21 to i64
  %27 = bitcast float* %13 to <64 x float>*
  %28 = load <64 x float>, <64 x float>* %27, align 128, !tbaa !2496
  %29 = getelementptr inbounds float, float* %13, i64 64
  %30 = bitcast float* %29 to <64 x float>*
  %31 = load <64 x float>, <64 x float>* %30, align 128, !tbaa !2496
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end6.1
  %indvars.iv13 = phi i64 [ %26, %for_begin1.preheader.preheader ], [ %indvars.iv.next14, %for_end6.1 ]
  %32 = phi i32 [ %24, %for_begin1.preheader.preheader ], [ %95, %for_end6.1 ]
  %33 = shl nsw i32 %32, 6
  %34 = trunc i64 %indvars.iv13 to i32
  %35 = shl nsw i32 %34, 7
  %36 = sext i32 %33 to i64
  %37 = sext i32 %35 to i64
  br label %for_body5

for_end:                                          ; preds = %for_end6.1, %entry
  ret i32 0

for_body5:                                        ; preds = %for_body5, %for_begin1.preheader
  %indvars.iv = phi i64 [ 0, %for_begin1.preheader ], [ %indvars.iv.next.121, %for_body5 ]
  %.010 = phi <64 x float> [ zeroinitializer, %for_begin1.preheader ], [ %57, %for_body5 ]
  %38 = add nsw i64 %indvars.iv, %36
  %39 = getelementptr inbounds float, float* %4, i64 %38
  %40 = load float, float* %39, align 4, !tbaa !2493
  %41 = insertelement <64 x float> undef, float %40, i32 0
  %42 = shufflevector <64 x float> %41, <64 x float> undef, <64 x i32> zeroinitializer
  %43 = shl nuw nsw i64 %indvars.iv, 7
  %44 = getelementptr inbounds float, float* %7, i64 %43
  %45 = bitcast float* %44 to <64 x float>*
  %46 = load <64 x float>, <64 x float>* %45, align 128, !tbaa !2499
  %47 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %42, <64 x float> %46, <64 x float> %.010)
  %indvars.iv.next = or i64 %indvars.iv, 1
  %48 = add nsw i64 %indvars.iv.next, %36
  %49 = getelementptr inbounds float, float* %4, i64 %48
  %50 = load float, float* %49, align 4, !tbaa !2493
  %51 = insertelement <64 x float> undef, float %50, i32 0
  %52 = shufflevector <64 x float> %51, <64 x float> undef, <64 x i32> zeroinitializer
  %53 = shl nuw nsw i64 %indvars.iv.next, 7
  %54 = getelementptr inbounds float, float* %7, i64 %53
  %55 = bitcast float* %54 to <64 x float>*
  %56 = load <64 x float>, <64 x float>* %55, align 128, !tbaa !2499
  %57 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %52, <64 x float> %56, <64 x float> %47)
  %indvars.iv.next.121 = add nuw nsw i64 %indvars.iv, 2
  %exitcond.not.1 = icmp eq i64 %indvars.iv.next.121, 64
  br i1 %exitcond.not.1, label %for_end6, label %for_body5, !prof !51

for_end6:                                         ; preds = %for_body5
  %58 = fadd <64 x float> %57, %28
  %59 = fcmp olt <64 x float> %58, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %60 = select <64 x i1> %59, <64 x float> %58, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %61 = fcmp ogt <64 x float> %60, zeroinitializer
  %62 = select <64 x i1> %61, <64 x float> %60, <64 x float> zeroinitializer
  %63 = getelementptr inbounds float, float* %10, i64 %37
  %64 = bitcast float* %63 to <64 x float>*
  store <64 x float> %62, <64 x float>* %64, align 128, !tbaa !2502
  br label %for_body5.1

for_body5.1:                                      ; preds = %for_body5.1, %for_end6
  %indvars.iv.1 = phi i64 [ 0, %for_end6 ], [ %indvars.iv.next.1.1, %for_body5.1 ]
  %.010.1 = phi <64 x float> [ zeroinitializer, %for_end6 ], [ %86, %for_body5.1 ]
  %65 = add nsw i64 %indvars.iv.1, %36
  %66 = getelementptr inbounds float, float* %4, i64 %65
  %67 = load float, float* %66, align 4, !tbaa !2493
  %68 = insertelement <64 x float> undef, float %67, i32 0
  %69 = shufflevector <64 x float> %68, <64 x float> undef, <64 x i32> zeroinitializer
  %70 = shl nuw nsw i64 %indvars.iv.1, 7
  %71 = or i64 %70, 64
  %72 = getelementptr inbounds float, float* %7, i64 %71
  %73 = bitcast float* %72 to <64 x float>*
  %74 = load <64 x float>, <64 x float>* %73, align 128, !tbaa !2499
  %75 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %69, <64 x float> %74, <64 x float> %.010.1)
  %indvars.iv.next.1 = or i64 %indvars.iv.1, 1
  %76 = add nsw i64 %indvars.iv.next.1, %36
  %77 = getelementptr inbounds float, float* %4, i64 %76
  %78 = load float, float* %77, align 4, !tbaa !2493
  %79 = insertelement <64 x float> undef, float %78, i32 0
  %80 = shufflevector <64 x float> %79, <64 x float> undef, <64 x i32> zeroinitializer
  %81 = shl nuw nsw i64 %indvars.iv.next.1, 7
  %82 = or i64 %81, 64
  %83 = getelementptr inbounds float, float* %7, i64 %82
  %84 = bitcast float* %83 to <64 x float>*
  %85 = load <64 x float>, <64 x float>* %84, align 128, !tbaa !2499
  %86 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %80, <64 x float> %85, <64 x float> %75)
  %indvars.iv.next.1.1 = add nuw nsw i64 %indvars.iv.1, 2
  %exitcond.1.not.1 = icmp eq i64 %indvars.iv.next.1.1, 64
  br i1 %exitcond.1.not.1, label %for_end6.1, label %for_body5.1, !prof !51

for_end6.1:                                       ; preds = %for_body5.1
  %87 = or i64 %37, 64
  %88 = fadd <64 x float> %86, %31
  %89 = fcmp olt <64 x float> %88, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %90 = select <64 x i1> %89, <64 x float> %88, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %91 = fcmp ogt <64 x float> %90, zeroinitializer
  %92 = select <64 x i1> %91, <64 x float> %90, <64 x float> zeroinitializer
  %93 = getelementptr inbounds float, float* %10, i64 %87
  %94 = bitcast float* %93 to <64 x float>*
  store <64 x float> %92, <64 x float>* %94, align 128, !tbaa !2502
  %indvars.iv.next14 = add nsw i64 %indvars.iv13, 1
  %95 = add nsw i32 %32, 1
  %exitcond15.not = icmp eq i64 %indvars.iv.next14, %wide.trip.count
  br i1 %exitcond15.not, label %for_end, label %for_begin1.preheader, !prof !51
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_3(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.207, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !2505
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !2519
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !2521
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !2524
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.208, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.209, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.210, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.211, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !2526
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !2540
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 14
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.47, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !2542
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 14
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.48, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !2545
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 512
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.49, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !2547
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 100352, i32 7168, i32 512, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.50, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !2559
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 3
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.51, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !2573
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 3
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.52, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !2575
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 512
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.53, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !2578
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 1
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.54, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !2580
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 1536, i32 512, i32 1, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([197 x i8], [197 x i8]* @.str.55, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !2592
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 512
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.56, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !2606
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !2620
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !2634
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 7
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.131, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !2636
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 7
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.132, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !2639
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 512
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.59, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !2641
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 25088, i32 3584, i32 512, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.212, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_3_compute_(i8* %31, i8* %39, i8* %45, i8* %51, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_3_compute_(i8* noalias nocapture readonly align 128 %0, i8* noalias nocapture readonly align 128 %1, i8* noalias nocapture readonly align 128 %2, i8* noalias nocapture align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 460800, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %for_end38, %if_end, %entry
  ret i32 -1

if_end:                                           ; preds = %entry
  %8 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %9 = tail call i8* %8(i32 1, i32 %4, i64 100352, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %9, i64 128) ]
  %10 = icmp eq i8* %9, null
  br i1 %10, label %if_then, label %for_begin4.preheader, !prof !5

for_begin4.preheader:                             ; preds = %if_end, %for_end6
  %indvar = phi i64 [ %indvar.next, %for_end6 ], [ 0, %if_end ]
  %11 = phi i32 [ %18, %for_end6 ], [ 0, %if_end ]
  %12 = mul nuw nsw i64 %indvar, 30720
  %13 = mul nuw nsw i64 %indvar, 28672
  %scevgep103 = getelementptr i8, i8* %6, i64 %12
  %14 = icmp ult i32 %11, 14
  br i1 %14, label %for_end9.us.14, label %for_begin7.preheader.preheader

for_begin7.preheader.preheader:                   ; preds = %for_begin4.preheader
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(30720) %scevgep103, i8 0, i64 30720, i1 false)
  br label %for_end6

for_begin12.preheader:                            ; preds = %for_end6
  %15 = bitcast i8* %9 to float*
  %16 = bitcast i8* %6 to float*
  %17 = bitcast i8* %1 to float*
  br label %for_begin15.preheader

for_end6:                                         ; preds = %for_begin7.preheader.preheader, %for_end9.us.14
  %18 = add nuw nsw i32 %11, 1
  %indvar.next = add nuw nsw i64 %indvar, 1
  %exitcond113.not = icmp eq i64 %indvar.next, 15
  br i1 %exitcond113.not, label %for_begin12.preheader, label %for_begin4.preheader, !prof !51

for_begin15.preheader:                            ; preds = %for_begin12.preheader, %for_end17
  %indvars.iv96 = phi i64 [ 0, %for_begin12.preheader ], [ %indvars.iv.next97, %for_end17 ]
  %19 = mul nuw nsw i64 %indvars.iv96, 3584
  %20 = mul nuw nsw i64 %indvars.iv96, 15360
  br label %for_begin18.preheader

for_begin27.preheader:                            ; preds = %for_end17
  %21 = bitcast i8* %2 to float*
  br label %for_begin30.preheader

for_begin18.preheader:                            ; preds = %for_begin15.preheader, %for_end20
  %indvars.iv93 = phi i64 [ 0, %for_begin15.preheader ], [ %indvars.iv.next94, %for_end20 ]
  %22 = shl nsw i64 %indvars.iv93, 9
  %23 = add nuw nsw i64 %22, %19
  %24 = shl nsw i64 %indvars.iv93, 10
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_begin18.preheader
  %index = phi i64 [ 0, %for_begin18.preheader ], [ %index.next, %vector.body ]
  %25 = add nuw nsw i64 %23, %index
  %26 = getelementptr inbounds float, float* %15, i64 %25
  %27 = add nuw nsw i64 %index, %20
  %28 = add nuw nsw i64 %27, %24
  %29 = getelementptr inbounds float, float* %16, i64 %28
  %30 = bitcast float* %29 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %30, align 16, !tbaa !2653
  %31 = getelementptr inbounds float, float* %17, i64 %index
  %32 = bitcast float* %31 to <4 x float>*
  %wide.load114 = load <4 x float>, <4 x float>* %32, align 16, !tbaa !2656
  %33 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load, <4 x float> %wide.load114, <4 x float> zeroinitializer)
  %34 = add nuw nsw i64 %index, 512
  %35 = add nuw nsw i64 %34, %20
  %36 = add nuw nsw i64 %35, %24
  %37 = getelementptr inbounds float, float* %16, i64 %36
  %38 = bitcast float* %37 to <4 x float>*
  %wide.load115 = load <4 x float>, <4 x float>* %38, align 16, !tbaa !2653
  %39 = getelementptr inbounds float, float* %17, i64 %34
  %40 = bitcast float* %39 to <4 x float>*
  %wide.load116 = load <4 x float>, <4 x float>* %40, align 16, !tbaa !2656
  %41 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load115, <4 x float> %wide.load116, <4 x float> %33)
  %42 = add nuw nsw i64 %index, 1024
  %43 = add nuw nsw i64 %42, %20
  %44 = add nuw nsw i64 %43, %24
  %45 = getelementptr inbounds float, float* %16, i64 %44
  %46 = bitcast float* %45 to <4 x float>*
  %wide.load117 = load <4 x float>, <4 x float>* %46, align 16, !tbaa !2653
  %47 = getelementptr inbounds float, float* %17, i64 %42
  %48 = bitcast float* %47 to <4 x float>*
  %wide.load118 = load <4 x float>, <4 x float>* %48, align 16, !tbaa !2656
  %49 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load117, <4 x float> %wide.load118, <4 x float> %41)
  %50 = add nuw nsw i64 %28, 7680
  %51 = getelementptr inbounds float, float* %16, i64 %50
  %52 = bitcast float* %51 to <4 x float>*
  %wide.load119 = load <4 x float>, <4 x float>* %52, align 16, !tbaa !2653
  %53 = add nuw nsw i64 %index, 1536
  %54 = getelementptr inbounds float, float* %17, i64 %53
  %55 = bitcast float* %54 to <4 x float>*
  %wide.load120 = load <4 x float>, <4 x float>* %55, align 16, !tbaa !2656
  %56 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load119, <4 x float> %wide.load120, <4 x float> %49)
  %57 = add nuw nsw i64 %36, 7680
  %58 = getelementptr inbounds float, float* %16, i64 %57
  %59 = bitcast float* %58 to <4 x float>*
  %wide.load121 = load <4 x float>, <4 x float>* %59, align 16, !tbaa !2653
  %60 = add nuw nsw i64 %index, 2048
  %61 = getelementptr inbounds float, float* %17, i64 %60
  %62 = bitcast float* %61 to <4 x float>*
  %wide.load122 = load <4 x float>, <4 x float>* %62, align 16, !tbaa !2656
  %63 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load121, <4 x float> %wide.load122, <4 x float> %56)
  %64 = add nuw nsw i64 %44, 7680
  %65 = getelementptr inbounds float, float* %16, i64 %64
  %66 = bitcast float* %65 to <4 x float>*
  %wide.load123 = load <4 x float>, <4 x float>* %66, align 16, !tbaa !2653
  %67 = add nuw nsw i64 %index, 2560
  %68 = getelementptr inbounds float, float* %17, i64 %67
  %69 = bitcast float* %68 to <4 x float>*
  %wide.load124 = load <4 x float>, <4 x float>* %69, align 16, !tbaa !2656
  %70 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load123, <4 x float> %wide.load124, <4 x float> %63)
  %71 = add nuw nsw i64 %28, 15360
  %72 = getelementptr inbounds float, float* %16, i64 %71
  %73 = bitcast float* %72 to <4 x float>*
  %wide.load125 = load <4 x float>, <4 x float>* %73, align 16, !tbaa !2653
  %74 = add nuw nsw i64 %index, 3072
  %75 = getelementptr inbounds float, float* %17, i64 %74
  %76 = bitcast float* %75 to <4 x float>*
  %wide.load126 = load <4 x float>, <4 x float>* %76, align 16, !tbaa !2656
  %77 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load125, <4 x float> %wide.load126, <4 x float> %70)
  %78 = add nuw nsw i64 %36, 15360
  %79 = getelementptr inbounds float, float* %16, i64 %78
  %80 = bitcast float* %79 to <4 x float>*
  %wide.load127 = load <4 x float>, <4 x float>* %80, align 16, !tbaa !2653
  %81 = add nuw nsw i64 %index, 3584
  %82 = getelementptr inbounds float, float* %17, i64 %81
  %83 = bitcast float* %82 to <4 x float>*
  %wide.load128 = load <4 x float>, <4 x float>* %83, align 16, !tbaa !2656
  %84 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load127, <4 x float> %wide.load128, <4 x float> %77)
  %85 = add nuw nsw i64 %44, 15360
  %86 = getelementptr inbounds float, float* %16, i64 %85
  %87 = bitcast float* %86 to <4 x float>*
  %wide.load129 = load <4 x float>, <4 x float>* %87, align 16, !tbaa !2653
  %88 = add nuw nsw i64 %index, 4096
  %89 = getelementptr inbounds float, float* %17, i64 %88
  %90 = bitcast float* %89 to <4 x float>*
  %wide.load130 = load <4 x float>, <4 x float>* %90, align 16, !tbaa !2656
  %91 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load129, <4 x float> %wide.load130, <4 x float> %84)
  %92 = bitcast float* %26 to <4 x float>*
  store <4 x float> %91, <4 x float>* %92, align 16, !tbaa !2659
  %index.next = add i64 %index, 4
  %93 = icmp eq i64 %index.next, 512
  br i1 %93, label %for_end20, label %vector.body, !prof !335, !llvm.loop !2662

for_end17:                                        ; preds = %for_end20
  %indvars.iv.next97 = add nuw nsw i64 %indvars.iv96, 1
  %exitcond98.not = icmp eq i64 %indvars.iv.next97, 7
  br i1 %exitcond98.not, label %for_begin27.preheader, label %for_begin15.preheader, !prof !51

for_end20:                                        ; preds = %vector.body
  %indvars.iv.next94 = add nuw nsw i64 %indvars.iv93, 1
  %exitcond95.not = icmp eq i64 %indvars.iv.next94, 7
  br i1 %exitcond95.not, label %for_end17, label %for_begin18.preheader, !prof !51

for_begin30.preheader:                            ; preds = %for_begin27.preheader, %for_end35.6
  %indvars.iv79 = phi i64 [ 0, %for_begin27.preheader ], [ %indvars.iv.next80, %for_end35.6 ]
  %94 = mul nuw nsw i64 %indvars.iv79, 3584
  br label %vector.body205

vector.body205:                                   ; preds = %vector.body205, %for_begin30.preheader
  %index207 = phi i64 [ 0, %for_begin30.preheader ], [ %index.next208.1, %vector.body205 ]
  %95 = add nuw nsw i64 %94, %index207
  %96 = getelementptr inbounds float, float* %21, i64 %index207
  %97 = bitcast float* %96 to <4 x float>*
  %wide.load211 = load <4 x float>, <4 x float>* %97, align 64, !tbaa !2663
  %98 = getelementptr inbounds float, float* %96, i64 4
  %99 = bitcast float* %98 to <4 x float>*
  %wide.load212 = load <4 x float>, <4 x float>* %99, align 16, !tbaa !2663
  %100 = getelementptr inbounds float, float* %15, i64 %95
  %101 = bitcast float* %100 to <4 x float>*
  %wide.load213 = load <4 x float>, <4 x float>* %101, align 64, !tbaa !2659
  %102 = getelementptr inbounds float, float* %100, i64 4
  %103 = bitcast float* %102 to <4 x float>*
  %wide.load214 = load <4 x float>, <4 x float>* %103, align 16, !tbaa !2659
  %104 = fadd <4 x float> %wide.load211, %wide.load213
  %105 = fadd <4 x float> %wide.load212, %wide.load214
  %106 = bitcast float* %100 to <4 x float>*
  store <4 x float> %104, <4 x float>* %106, align 64, !tbaa !2659
  %107 = bitcast float* %102 to <4 x float>*
  store <4 x float> %105, <4 x float>* %107, align 16, !tbaa !2659
  %index.next208 = or i64 %index207, 8
  %108 = add nuw nsw i64 %94, %index.next208
  %109 = getelementptr inbounds float, float* %21, i64 %index.next208
  %110 = bitcast float* %109 to <4 x float>*
  %wide.load211.1 = load <4 x float>, <4 x float>* %110, align 32, !tbaa !2663
  %111 = getelementptr inbounds float, float* %109, i64 4
  %112 = bitcast float* %111 to <4 x float>*
  %wide.load212.1 = load <4 x float>, <4 x float>* %112, align 16, !tbaa !2663
  %113 = getelementptr inbounds float, float* %15, i64 %108
  %114 = bitcast float* %113 to <4 x float>*
  %wide.load213.1 = load <4 x float>, <4 x float>* %114, align 32, !tbaa !2659
  %115 = getelementptr inbounds float, float* %113, i64 4
  %116 = bitcast float* %115 to <4 x float>*
  %wide.load214.1 = load <4 x float>, <4 x float>* %116, align 16, !tbaa !2659
  %117 = fadd <4 x float> %wide.load211.1, %wide.load213.1
  %118 = fadd <4 x float> %wide.load212.1, %wide.load214.1
  %119 = bitcast float* %113 to <4 x float>*
  store <4 x float> %117, <4 x float>* %119, align 32, !tbaa !2659
  %120 = bitcast float* %115 to <4 x float>*
  store <4 x float> %118, <4 x float>* %120, align 16, !tbaa !2659
  %index.next208.1 = add nuw nsw i64 %index207, 16
  %121 = icmp eq i64 %index.next208.1, 512
  br i1 %121, label %for_end35, label %vector.body205, !prof !341, !llvm.loop !2666

for_begin36.preheader:                            ; preds = %for_end35.6
  %122 = bitcast i8* %3 to float*
  br label %for_begin39.preheader

for_end35:                                        ; preds = %vector.body205
  %123 = add nuw nsw i64 %94, 512
  br label %vector.body193

vector.body193:                                   ; preds = %vector.body193, %for_end35
  %index195 = phi i64 [ 0, %for_end35 ], [ %index.next196.1, %vector.body193 ]
  %124 = add nuw nsw i64 %123, %index195
  %125 = getelementptr inbounds float, float* %21, i64 %index195
  %126 = bitcast float* %125 to <4 x float>*
  %wide.load199 = load <4 x float>, <4 x float>* %126, align 64, !tbaa !2663
  %127 = getelementptr inbounds float, float* %125, i64 4
  %128 = bitcast float* %127 to <4 x float>*
  %wide.load200 = load <4 x float>, <4 x float>* %128, align 16, !tbaa !2663
  %129 = getelementptr inbounds float, float* %15, i64 %124
  %130 = bitcast float* %129 to <4 x float>*
  %wide.load201 = load <4 x float>, <4 x float>* %130, align 64, !tbaa !2659
  %131 = getelementptr inbounds float, float* %129, i64 4
  %132 = bitcast float* %131 to <4 x float>*
  %wide.load202 = load <4 x float>, <4 x float>* %132, align 16, !tbaa !2659
  %133 = fadd <4 x float> %wide.load199, %wide.load201
  %134 = fadd <4 x float> %wide.load200, %wide.load202
  %135 = bitcast float* %129 to <4 x float>*
  store <4 x float> %133, <4 x float>* %135, align 64, !tbaa !2659
  %136 = bitcast float* %131 to <4 x float>*
  store <4 x float> %134, <4 x float>* %136, align 16, !tbaa !2659
  %index.next196 = or i64 %index195, 8
  %137 = add nuw nsw i64 %123, %index.next196
  %138 = getelementptr inbounds float, float* %21, i64 %index.next196
  %139 = bitcast float* %138 to <4 x float>*
  %wide.load199.1 = load <4 x float>, <4 x float>* %139, align 32, !tbaa !2663
  %140 = getelementptr inbounds float, float* %138, i64 4
  %141 = bitcast float* %140 to <4 x float>*
  %wide.load200.1 = load <4 x float>, <4 x float>* %141, align 16, !tbaa !2663
  %142 = getelementptr inbounds float, float* %15, i64 %137
  %143 = bitcast float* %142 to <4 x float>*
  %wide.load201.1 = load <4 x float>, <4 x float>* %143, align 32, !tbaa !2659
  %144 = getelementptr inbounds float, float* %142, i64 4
  %145 = bitcast float* %144 to <4 x float>*
  %wide.load202.1 = load <4 x float>, <4 x float>* %145, align 16, !tbaa !2659
  %146 = fadd <4 x float> %wide.load199.1, %wide.load201.1
  %147 = fadd <4 x float> %wide.load200.1, %wide.load202.1
  %148 = bitcast float* %142 to <4 x float>*
  store <4 x float> %146, <4 x float>* %148, align 32, !tbaa !2659
  %149 = bitcast float* %144 to <4 x float>*
  store <4 x float> %147, <4 x float>* %149, align 16, !tbaa !2659
  %index.next196.1 = add nuw nsw i64 %index195, 16
  %150 = icmp eq i64 %index.next196.1, 512
  br i1 %150, label %for_end35.1, label %vector.body193, !prof !341, !llvm.loop !2667

for_begin39.preheader:                            ; preds = %for_begin36.preheader, %for_end44.6
  %indvars.iv70 = phi i64 [ 0, %for_begin36.preheader ], [ %indvars.iv.next71, %for_end44.6 ]
  %151 = mul nuw nsw i64 %indvars.iv70, 3584
  br label %vector.body277

vector.body277:                                   ; preds = %vector.body277, %for_begin39.preheader
  %index279 = phi i64 [ 0, %for_begin39.preheader ], [ %index.next280, %vector.body277 ]
  %152 = add nuw nsw i64 %151, %index279
  %153 = getelementptr inbounds float, float* %15, i64 %152
  %154 = bitcast float* %153 to <4 x float>*
  %wide.load283 = load <4 x float>, <4 x float>* %154, align 32, !tbaa !2659
  %155 = getelementptr inbounds float, float* %153, i64 4
  %156 = bitcast float* %155 to <4 x float>*
  %wide.load284 = load <4 x float>, <4 x float>* %156, align 16, !tbaa !2659
  %157 = fcmp olt <4 x float> %wide.load283, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %158 = fcmp olt <4 x float> %wide.load284, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %159 = select <4 x i1> %157, <4 x float> %wide.load283, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %160 = select <4 x i1> %158, <4 x float> %wide.load284, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %161 = fcmp ogt <4 x float> %159, zeroinitializer
  %162 = fcmp ogt <4 x float> %160, zeroinitializer
  %163 = select <4 x i1> %161, <4 x float> %159, <4 x float> zeroinitializer
  %164 = select <4 x i1> %162, <4 x float> %160, <4 x float> zeroinitializer
  %165 = getelementptr inbounds float, float* %122, i64 %152
  %166 = bitcast float* %165 to <4 x float>*
  store <4 x float> %163, <4 x float>* %166, align 32, !tbaa !2668
  %167 = getelementptr inbounds float, float* %165, i64 4
  %168 = bitcast float* %167 to <4 x float>*
  store <4 x float> %164, <4 x float>* %168, align 16, !tbaa !2668
  %index.next280 = add i64 %index279, 8
  %169 = icmp eq i64 %index.next280, 512
  br i1 %169, label %for_end44, label %vector.body277, !prof !341, !llvm.loop !2671

for_end38:                                        ; preds = %for_end44.6
  %170 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %171 = tail call i32 %170(i32 1, i32 %4, i8* nonnull %9)
  %.not = icmp eq i32 %171, 0
  br i1 %.not, label %if_end46, label %if_then, !prof !51

for_end44:                                        ; preds = %vector.body277
  %172 = add nuw nsw i64 %151, 512
  br label %vector.body267

vector.body267:                                   ; preds = %vector.body267, %for_end44
  %index269 = phi i64 [ 0, %for_end44 ], [ %index.next270, %vector.body267 ]
  %173 = add nuw nsw i64 %172, %index269
  %174 = getelementptr inbounds float, float* %15, i64 %173
  %175 = bitcast float* %174 to <4 x float>*
  %wide.load273 = load <4 x float>, <4 x float>* %175, align 32, !tbaa !2659
  %176 = getelementptr inbounds float, float* %174, i64 4
  %177 = bitcast float* %176 to <4 x float>*
  %wide.load274 = load <4 x float>, <4 x float>* %177, align 16, !tbaa !2659
  %178 = fcmp olt <4 x float> %wide.load273, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %179 = fcmp olt <4 x float> %wide.load274, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %180 = select <4 x i1> %178, <4 x float> %wide.load273, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %181 = select <4 x i1> %179, <4 x float> %wide.load274, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %182 = fcmp ogt <4 x float> %180, zeroinitializer
  %183 = fcmp ogt <4 x float> %181, zeroinitializer
  %184 = select <4 x i1> %182, <4 x float> %180, <4 x float> zeroinitializer
  %185 = select <4 x i1> %183, <4 x float> %181, <4 x float> zeroinitializer
  %186 = getelementptr inbounds float, float* %122, i64 %173
  %187 = bitcast float* %186 to <4 x float>*
  store <4 x float> %184, <4 x float>* %187, align 32, !tbaa !2668
  %188 = getelementptr inbounds float, float* %186, i64 4
  %189 = bitcast float* %188 to <4 x float>*
  store <4 x float> %185, <4 x float>* %189, align 16, !tbaa !2668
  %index.next270 = add i64 %index269, 8
  %190 = icmp eq i64 %index.next270, 512
  br i1 %190, label %for_end44.1, label %vector.body267, !prof !341, !llvm.loop !2672

if_end46:                                         ; preds = %for_end38
  %191 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %192 = tail call i32 %191(i32 1, i32 %4, i8* nonnull %6)
  %.not57 = icmp ne i32 %192, 0
  %spec.select = sext i1 %.not57 to i32
  ret i32 %spec.select

for_end44.1:                                      ; preds = %vector.body267
  %193 = add nuw nsw i64 %151, 1024
  br label %vector.body257

vector.body257:                                   ; preds = %vector.body257, %for_end44.1
  %index259 = phi i64 [ 0, %for_end44.1 ], [ %index.next260, %vector.body257 ]
  %194 = add nuw nsw i64 %193, %index259
  %195 = getelementptr inbounds float, float* %15, i64 %194
  %196 = bitcast float* %195 to <4 x float>*
  %wide.load263 = load <4 x float>, <4 x float>* %196, align 32, !tbaa !2659
  %197 = getelementptr inbounds float, float* %195, i64 4
  %198 = bitcast float* %197 to <4 x float>*
  %wide.load264 = load <4 x float>, <4 x float>* %198, align 16, !tbaa !2659
  %199 = fcmp olt <4 x float> %wide.load263, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %200 = fcmp olt <4 x float> %wide.load264, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %201 = select <4 x i1> %199, <4 x float> %wide.load263, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %202 = select <4 x i1> %200, <4 x float> %wide.load264, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %203 = fcmp ogt <4 x float> %201, zeroinitializer
  %204 = fcmp ogt <4 x float> %202, zeroinitializer
  %205 = select <4 x i1> %203, <4 x float> %201, <4 x float> zeroinitializer
  %206 = select <4 x i1> %204, <4 x float> %202, <4 x float> zeroinitializer
  %207 = getelementptr inbounds float, float* %122, i64 %194
  %208 = bitcast float* %207 to <4 x float>*
  store <4 x float> %205, <4 x float>* %208, align 32, !tbaa !2668
  %209 = getelementptr inbounds float, float* %207, i64 4
  %210 = bitcast float* %209 to <4 x float>*
  store <4 x float> %206, <4 x float>* %210, align 16, !tbaa !2668
  %index.next260 = add i64 %index259, 8
  %211 = icmp eq i64 %index.next260, 512
  br i1 %211, label %for_end44.2, label %vector.body257, !prof !341, !llvm.loop !2673

for_end44.2:                                      ; preds = %vector.body257
  %212 = add nuw nsw i64 %151, 1536
  br label %vector.body247

vector.body247:                                   ; preds = %vector.body247, %for_end44.2
  %index249 = phi i64 [ 0, %for_end44.2 ], [ %index.next250, %vector.body247 ]
  %213 = add nuw nsw i64 %212, %index249
  %214 = getelementptr inbounds float, float* %15, i64 %213
  %215 = bitcast float* %214 to <4 x float>*
  %wide.load253 = load <4 x float>, <4 x float>* %215, align 32, !tbaa !2659
  %216 = getelementptr inbounds float, float* %214, i64 4
  %217 = bitcast float* %216 to <4 x float>*
  %wide.load254 = load <4 x float>, <4 x float>* %217, align 16, !tbaa !2659
  %218 = fcmp olt <4 x float> %wide.load253, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %219 = fcmp olt <4 x float> %wide.load254, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %220 = select <4 x i1> %218, <4 x float> %wide.load253, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %221 = select <4 x i1> %219, <4 x float> %wide.load254, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %222 = fcmp ogt <4 x float> %220, zeroinitializer
  %223 = fcmp ogt <4 x float> %221, zeroinitializer
  %224 = select <4 x i1> %222, <4 x float> %220, <4 x float> zeroinitializer
  %225 = select <4 x i1> %223, <4 x float> %221, <4 x float> zeroinitializer
  %226 = getelementptr inbounds float, float* %122, i64 %213
  %227 = bitcast float* %226 to <4 x float>*
  store <4 x float> %224, <4 x float>* %227, align 32, !tbaa !2668
  %228 = getelementptr inbounds float, float* %226, i64 4
  %229 = bitcast float* %228 to <4 x float>*
  store <4 x float> %225, <4 x float>* %229, align 16, !tbaa !2668
  %index.next250 = add i64 %index249, 8
  %230 = icmp eq i64 %index.next250, 512
  br i1 %230, label %for_end44.3, label %vector.body247, !prof !341, !llvm.loop !2674

for_end44.3:                                      ; preds = %vector.body247
  %231 = add nuw nsw i64 %151, 2048
  br label %vector.body237

vector.body237:                                   ; preds = %vector.body237, %for_end44.3
  %index239 = phi i64 [ 0, %for_end44.3 ], [ %index.next240, %vector.body237 ]
  %232 = add nuw nsw i64 %231, %index239
  %233 = getelementptr inbounds float, float* %15, i64 %232
  %234 = bitcast float* %233 to <4 x float>*
  %wide.load243 = load <4 x float>, <4 x float>* %234, align 32, !tbaa !2659
  %235 = getelementptr inbounds float, float* %233, i64 4
  %236 = bitcast float* %235 to <4 x float>*
  %wide.load244 = load <4 x float>, <4 x float>* %236, align 16, !tbaa !2659
  %237 = fcmp olt <4 x float> %wide.load243, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %238 = fcmp olt <4 x float> %wide.load244, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %239 = select <4 x i1> %237, <4 x float> %wide.load243, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %240 = select <4 x i1> %238, <4 x float> %wide.load244, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %241 = fcmp ogt <4 x float> %239, zeroinitializer
  %242 = fcmp ogt <4 x float> %240, zeroinitializer
  %243 = select <4 x i1> %241, <4 x float> %239, <4 x float> zeroinitializer
  %244 = select <4 x i1> %242, <4 x float> %240, <4 x float> zeroinitializer
  %245 = getelementptr inbounds float, float* %122, i64 %232
  %246 = bitcast float* %245 to <4 x float>*
  store <4 x float> %243, <4 x float>* %246, align 32, !tbaa !2668
  %247 = getelementptr inbounds float, float* %245, i64 4
  %248 = bitcast float* %247 to <4 x float>*
  store <4 x float> %244, <4 x float>* %248, align 16, !tbaa !2668
  %index.next240 = add i64 %index239, 8
  %249 = icmp eq i64 %index.next240, 512
  br i1 %249, label %for_end44.4, label %vector.body237, !prof !341, !llvm.loop !2675

for_end44.4:                                      ; preds = %vector.body237
  %250 = add nuw nsw i64 %151, 2560
  br label %vector.body227

vector.body227:                                   ; preds = %vector.body227, %for_end44.4
  %index229 = phi i64 [ 0, %for_end44.4 ], [ %index.next230, %vector.body227 ]
  %251 = add nuw nsw i64 %250, %index229
  %252 = getelementptr inbounds float, float* %15, i64 %251
  %253 = bitcast float* %252 to <4 x float>*
  %wide.load233 = load <4 x float>, <4 x float>* %253, align 32, !tbaa !2659
  %254 = getelementptr inbounds float, float* %252, i64 4
  %255 = bitcast float* %254 to <4 x float>*
  %wide.load234 = load <4 x float>, <4 x float>* %255, align 16, !tbaa !2659
  %256 = fcmp olt <4 x float> %wide.load233, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %257 = fcmp olt <4 x float> %wide.load234, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %258 = select <4 x i1> %256, <4 x float> %wide.load233, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %259 = select <4 x i1> %257, <4 x float> %wide.load234, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %260 = fcmp ogt <4 x float> %258, zeroinitializer
  %261 = fcmp ogt <4 x float> %259, zeroinitializer
  %262 = select <4 x i1> %260, <4 x float> %258, <4 x float> zeroinitializer
  %263 = select <4 x i1> %261, <4 x float> %259, <4 x float> zeroinitializer
  %264 = getelementptr inbounds float, float* %122, i64 %251
  %265 = bitcast float* %264 to <4 x float>*
  store <4 x float> %262, <4 x float>* %265, align 32, !tbaa !2668
  %266 = getelementptr inbounds float, float* %264, i64 4
  %267 = bitcast float* %266 to <4 x float>*
  store <4 x float> %263, <4 x float>* %267, align 16, !tbaa !2668
  %index.next230 = add i64 %index229, 8
  %268 = icmp eq i64 %index.next230, 512
  br i1 %268, label %for_end44.5, label %vector.body227, !prof !341, !llvm.loop !2676

for_end44.5:                                      ; preds = %vector.body227
  %269 = add nuw nsw i64 %151, 3072
  br label %vector.body217

vector.body217:                                   ; preds = %vector.body217, %for_end44.5
  %index219 = phi i64 [ 0, %for_end44.5 ], [ %index.next220, %vector.body217 ]
  %270 = add nuw nsw i64 %269, %index219
  %271 = getelementptr inbounds float, float* %15, i64 %270
  %272 = bitcast float* %271 to <4 x float>*
  %wide.load223 = load <4 x float>, <4 x float>* %272, align 32, !tbaa !2659
  %273 = getelementptr inbounds float, float* %271, i64 4
  %274 = bitcast float* %273 to <4 x float>*
  %wide.load224 = load <4 x float>, <4 x float>* %274, align 16, !tbaa !2659
  %275 = fcmp olt <4 x float> %wide.load223, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %276 = fcmp olt <4 x float> %wide.load224, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %277 = select <4 x i1> %275, <4 x float> %wide.load223, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %278 = select <4 x i1> %276, <4 x float> %wide.load224, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %279 = fcmp ogt <4 x float> %277, zeroinitializer
  %280 = fcmp ogt <4 x float> %278, zeroinitializer
  %281 = select <4 x i1> %279, <4 x float> %277, <4 x float> zeroinitializer
  %282 = select <4 x i1> %280, <4 x float> %278, <4 x float> zeroinitializer
  %283 = getelementptr inbounds float, float* %122, i64 %270
  %284 = bitcast float* %283 to <4 x float>*
  store <4 x float> %281, <4 x float>* %284, align 32, !tbaa !2668
  %285 = getelementptr inbounds float, float* %283, i64 4
  %286 = bitcast float* %285 to <4 x float>*
  store <4 x float> %282, <4 x float>* %286, align 16, !tbaa !2668
  %index.next220 = add i64 %index219, 8
  %287 = icmp eq i64 %index.next220, 512
  br i1 %287, label %for_end44.6, label %vector.body217, !prof !341, !llvm.loop !2677

for_end44.6:                                      ; preds = %vector.body217
  %indvars.iv.next71 = add nuw nsw i64 %indvars.iv70, 1
  %exitcond72.not = icmp eq i64 %indvars.iv.next71, 7
  br i1 %exitcond72.not, label %for_end38, label %for_begin39.preheader, !prof !51

for_end35.1:                                      ; preds = %vector.body193
  %288 = add nuw nsw i64 %94, 1024
  br label %vector.body181

vector.body181:                                   ; preds = %vector.body181, %for_end35.1
  %index183 = phi i64 [ 0, %for_end35.1 ], [ %index.next184.1, %vector.body181 ]
  %289 = add nuw nsw i64 %288, %index183
  %290 = getelementptr inbounds float, float* %21, i64 %index183
  %291 = bitcast float* %290 to <4 x float>*
  %wide.load187 = load <4 x float>, <4 x float>* %291, align 64, !tbaa !2663
  %292 = getelementptr inbounds float, float* %290, i64 4
  %293 = bitcast float* %292 to <4 x float>*
  %wide.load188 = load <4 x float>, <4 x float>* %293, align 16, !tbaa !2663
  %294 = getelementptr inbounds float, float* %15, i64 %289
  %295 = bitcast float* %294 to <4 x float>*
  %wide.load189 = load <4 x float>, <4 x float>* %295, align 64, !tbaa !2659
  %296 = getelementptr inbounds float, float* %294, i64 4
  %297 = bitcast float* %296 to <4 x float>*
  %wide.load190 = load <4 x float>, <4 x float>* %297, align 16, !tbaa !2659
  %298 = fadd <4 x float> %wide.load187, %wide.load189
  %299 = fadd <4 x float> %wide.load188, %wide.load190
  %300 = bitcast float* %294 to <4 x float>*
  store <4 x float> %298, <4 x float>* %300, align 64, !tbaa !2659
  %301 = bitcast float* %296 to <4 x float>*
  store <4 x float> %299, <4 x float>* %301, align 16, !tbaa !2659
  %index.next184 = or i64 %index183, 8
  %302 = add nuw nsw i64 %288, %index.next184
  %303 = getelementptr inbounds float, float* %21, i64 %index.next184
  %304 = bitcast float* %303 to <4 x float>*
  %wide.load187.1 = load <4 x float>, <4 x float>* %304, align 32, !tbaa !2663
  %305 = getelementptr inbounds float, float* %303, i64 4
  %306 = bitcast float* %305 to <4 x float>*
  %wide.load188.1 = load <4 x float>, <4 x float>* %306, align 16, !tbaa !2663
  %307 = getelementptr inbounds float, float* %15, i64 %302
  %308 = bitcast float* %307 to <4 x float>*
  %wide.load189.1 = load <4 x float>, <4 x float>* %308, align 32, !tbaa !2659
  %309 = getelementptr inbounds float, float* %307, i64 4
  %310 = bitcast float* %309 to <4 x float>*
  %wide.load190.1 = load <4 x float>, <4 x float>* %310, align 16, !tbaa !2659
  %311 = fadd <4 x float> %wide.load187.1, %wide.load189.1
  %312 = fadd <4 x float> %wide.load188.1, %wide.load190.1
  %313 = bitcast float* %307 to <4 x float>*
  store <4 x float> %311, <4 x float>* %313, align 32, !tbaa !2659
  %314 = bitcast float* %309 to <4 x float>*
  store <4 x float> %312, <4 x float>* %314, align 16, !tbaa !2659
  %index.next184.1 = add nuw nsw i64 %index183, 16
  %315 = icmp eq i64 %index.next184.1, 512
  br i1 %315, label %for_end35.2, label %vector.body181, !prof !341, !llvm.loop !2678

for_end35.2:                                      ; preds = %vector.body181
  %316 = add nuw nsw i64 %94, 1536
  br label %vector.body169

vector.body169:                                   ; preds = %vector.body169, %for_end35.2
  %index171 = phi i64 [ 0, %for_end35.2 ], [ %index.next172.1, %vector.body169 ]
  %317 = add nuw nsw i64 %316, %index171
  %318 = getelementptr inbounds float, float* %21, i64 %index171
  %319 = bitcast float* %318 to <4 x float>*
  %wide.load175 = load <4 x float>, <4 x float>* %319, align 64, !tbaa !2663
  %320 = getelementptr inbounds float, float* %318, i64 4
  %321 = bitcast float* %320 to <4 x float>*
  %wide.load176 = load <4 x float>, <4 x float>* %321, align 16, !tbaa !2663
  %322 = getelementptr inbounds float, float* %15, i64 %317
  %323 = bitcast float* %322 to <4 x float>*
  %wide.load177 = load <4 x float>, <4 x float>* %323, align 64, !tbaa !2659
  %324 = getelementptr inbounds float, float* %322, i64 4
  %325 = bitcast float* %324 to <4 x float>*
  %wide.load178 = load <4 x float>, <4 x float>* %325, align 16, !tbaa !2659
  %326 = fadd <4 x float> %wide.load175, %wide.load177
  %327 = fadd <4 x float> %wide.load176, %wide.load178
  %328 = bitcast float* %322 to <4 x float>*
  store <4 x float> %326, <4 x float>* %328, align 64, !tbaa !2659
  %329 = bitcast float* %324 to <4 x float>*
  store <4 x float> %327, <4 x float>* %329, align 16, !tbaa !2659
  %index.next172 = or i64 %index171, 8
  %330 = add nuw nsw i64 %316, %index.next172
  %331 = getelementptr inbounds float, float* %21, i64 %index.next172
  %332 = bitcast float* %331 to <4 x float>*
  %wide.load175.1 = load <4 x float>, <4 x float>* %332, align 32, !tbaa !2663
  %333 = getelementptr inbounds float, float* %331, i64 4
  %334 = bitcast float* %333 to <4 x float>*
  %wide.load176.1 = load <4 x float>, <4 x float>* %334, align 16, !tbaa !2663
  %335 = getelementptr inbounds float, float* %15, i64 %330
  %336 = bitcast float* %335 to <4 x float>*
  %wide.load177.1 = load <4 x float>, <4 x float>* %336, align 32, !tbaa !2659
  %337 = getelementptr inbounds float, float* %335, i64 4
  %338 = bitcast float* %337 to <4 x float>*
  %wide.load178.1 = load <4 x float>, <4 x float>* %338, align 16, !tbaa !2659
  %339 = fadd <4 x float> %wide.load175.1, %wide.load177.1
  %340 = fadd <4 x float> %wide.load176.1, %wide.load178.1
  %341 = bitcast float* %335 to <4 x float>*
  store <4 x float> %339, <4 x float>* %341, align 32, !tbaa !2659
  %342 = bitcast float* %337 to <4 x float>*
  store <4 x float> %340, <4 x float>* %342, align 16, !tbaa !2659
  %index.next172.1 = add nuw nsw i64 %index171, 16
  %343 = icmp eq i64 %index.next172.1, 512
  br i1 %343, label %for_end35.3, label %vector.body169, !prof !341, !llvm.loop !2679

for_end35.3:                                      ; preds = %vector.body169
  %344 = add nuw nsw i64 %94, 2048
  br label %vector.body157

vector.body157:                                   ; preds = %vector.body157, %for_end35.3
  %index159 = phi i64 [ 0, %for_end35.3 ], [ %index.next160.1, %vector.body157 ]
  %345 = add nuw nsw i64 %344, %index159
  %346 = getelementptr inbounds float, float* %21, i64 %index159
  %347 = bitcast float* %346 to <4 x float>*
  %wide.load163 = load <4 x float>, <4 x float>* %347, align 64, !tbaa !2663
  %348 = getelementptr inbounds float, float* %346, i64 4
  %349 = bitcast float* %348 to <4 x float>*
  %wide.load164 = load <4 x float>, <4 x float>* %349, align 16, !tbaa !2663
  %350 = getelementptr inbounds float, float* %15, i64 %345
  %351 = bitcast float* %350 to <4 x float>*
  %wide.load165 = load <4 x float>, <4 x float>* %351, align 64, !tbaa !2659
  %352 = getelementptr inbounds float, float* %350, i64 4
  %353 = bitcast float* %352 to <4 x float>*
  %wide.load166 = load <4 x float>, <4 x float>* %353, align 16, !tbaa !2659
  %354 = fadd <4 x float> %wide.load163, %wide.load165
  %355 = fadd <4 x float> %wide.load164, %wide.load166
  %356 = bitcast float* %350 to <4 x float>*
  store <4 x float> %354, <4 x float>* %356, align 64, !tbaa !2659
  %357 = bitcast float* %352 to <4 x float>*
  store <4 x float> %355, <4 x float>* %357, align 16, !tbaa !2659
  %index.next160 = or i64 %index159, 8
  %358 = add nuw nsw i64 %344, %index.next160
  %359 = getelementptr inbounds float, float* %21, i64 %index.next160
  %360 = bitcast float* %359 to <4 x float>*
  %wide.load163.1 = load <4 x float>, <4 x float>* %360, align 32, !tbaa !2663
  %361 = getelementptr inbounds float, float* %359, i64 4
  %362 = bitcast float* %361 to <4 x float>*
  %wide.load164.1 = load <4 x float>, <4 x float>* %362, align 16, !tbaa !2663
  %363 = getelementptr inbounds float, float* %15, i64 %358
  %364 = bitcast float* %363 to <4 x float>*
  %wide.load165.1 = load <4 x float>, <4 x float>* %364, align 32, !tbaa !2659
  %365 = getelementptr inbounds float, float* %363, i64 4
  %366 = bitcast float* %365 to <4 x float>*
  %wide.load166.1 = load <4 x float>, <4 x float>* %366, align 16, !tbaa !2659
  %367 = fadd <4 x float> %wide.load163.1, %wide.load165.1
  %368 = fadd <4 x float> %wide.load164.1, %wide.load166.1
  %369 = bitcast float* %363 to <4 x float>*
  store <4 x float> %367, <4 x float>* %369, align 32, !tbaa !2659
  %370 = bitcast float* %365 to <4 x float>*
  store <4 x float> %368, <4 x float>* %370, align 16, !tbaa !2659
  %index.next160.1 = add nuw nsw i64 %index159, 16
  %371 = icmp eq i64 %index.next160.1, 512
  br i1 %371, label %for_end35.4, label %vector.body157, !prof !341, !llvm.loop !2680

for_end35.4:                                      ; preds = %vector.body157
  %372 = add nuw nsw i64 %94, 2560
  br label %vector.body145

vector.body145:                                   ; preds = %vector.body145, %for_end35.4
  %index147 = phi i64 [ 0, %for_end35.4 ], [ %index.next148.1, %vector.body145 ]
  %373 = add nuw nsw i64 %372, %index147
  %374 = getelementptr inbounds float, float* %21, i64 %index147
  %375 = bitcast float* %374 to <4 x float>*
  %wide.load151 = load <4 x float>, <4 x float>* %375, align 64, !tbaa !2663
  %376 = getelementptr inbounds float, float* %374, i64 4
  %377 = bitcast float* %376 to <4 x float>*
  %wide.load152 = load <4 x float>, <4 x float>* %377, align 16, !tbaa !2663
  %378 = getelementptr inbounds float, float* %15, i64 %373
  %379 = bitcast float* %378 to <4 x float>*
  %wide.load153 = load <4 x float>, <4 x float>* %379, align 64, !tbaa !2659
  %380 = getelementptr inbounds float, float* %378, i64 4
  %381 = bitcast float* %380 to <4 x float>*
  %wide.load154 = load <4 x float>, <4 x float>* %381, align 16, !tbaa !2659
  %382 = fadd <4 x float> %wide.load151, %wide.load153
  %383 = fadd <4 x float> %wide.load152, %wide.load154
  %384 = bitcast float* %378 to <4 x float>*
  store <4 x float> %382, <4 x float>* %384, align 64, !tbaa !2659
  %385 = bitcast float* %380 to <4 x float>*
  store <4 x float> %383, <4 x float>* %385, align 16, !tbaa !2659
  %index.next148 = or i64 %index147, 8
  %386 = add nuw nsw i64 %372, %index.next148
  %387 = getelementptr inbounds float, float* %21, i64 %index.next148
  %388 = bitcast float* %387 to <4 x float>*
  %wide.load151.1 = load <4 x float>, <4 x float>* %388, align 32, !tbaa !2663
  %389 = getelementptr inbounds float, float* %387, i64 4
  %390 = bitcast float* %389 to <4 x float>*
  %wide.load152.1 = load <4 x float>, <4 x float>* %390, align 16, !tbaa !2663
  %391 = getelementptr inbounds float, float* %15, i64 %386
  %392 = bitcast float* %391 to <4 x float>*
  %wide.load153.1 = load <4 x float>, <4 x float>* %392, align 32, !tbaa !2659
  %393 = getelementptr inbounds float, float* %391, i64 4
  %394 = bitcast float* %393 to <4 x float>*
  %wide.load154.1 = load <4 x float>, <4 x float>* %394, align 16, !tbaa !2659
  %395 = fadd <4 x float> %wide.load151.1, %wide.load153.1
  %396 = fadd <4 x float> %wide.load152.1, %wide.load154.1
  %397 = bitcast float* %391 to <4 x float>*
  store <4 x float> %395, <4 x float>* %397, align 32, !tbaa !2659
  %398 = bitcast float* %393 to <4 x float>*
  store <4 x float> %396, <4 x float>* %398, align 16, !tbaa !2659
  %index.next148.1 = add nuw nsw i64 %index147, 16
  %399 = icmp eq i64 %index.next148.1, 512
  br i1 %399, label %for_end35.5, label %vector.body145, !prof !341, !llvm.loop !2681

for_end35.5:                                      ; preds = %vector.body145
  %400 = add nuw nsw i64 %94, 3072
  br label %vector.body133

vector.body133:                                   ; preds = %vector.body133, %for_end35.5
  %index135 = phi i64 [ 0, %for_end35.5 ], [ %index.next136.1, %vector.body133 ]
  %401 = add nuw nsw i64 %400, %index135
  %402 = getelementptr inbounds float, float* %21, i64 %index135
  %403 = bitcast float* %402 to <4 x float>*
  %wide.load139 = load <4 x float>, <4 x float>* %403, align 64, !tbaa !2663
  %404 = getelementptr inbounds float, float* %402, i64 4
  %405 = bitcast float* %404 to <4 x float>*
  %wide.load140 = load <4 x float>, <4 x float>* %405, align 16, !tbaa !2663
  %406 = getelementptr inbounds float, float* %15, i64 %401
  %407 = bitcast float* %406 to <4 x float>*
  %wide.load141 = load <4 x float>, <4 x float>* %407, align 64, !tbaa !2659
  %408 = getelementptr inbounds float, float* %406, i64 4
  %409 = bitcast float* %408 to <4 x float>*
  %wide.load142 = load <4 x float>, <4 x float>* %409, align 16, !tbaa !2659
  %410 = fadd <4 x float> %wide.load139, %wide.load141
  %411 = fadd <4 x float> %wide.load140, %wide.load142
  %412 = bitcast float* %406 to <4 x float>*
  store <4 x float> %410, <4 x float>* %412, align 64, !tbaa !2659
  %413 = bitcast float* %408 to <4 x float>*
  store <4 x float> %411, <4 x float>* %413, align 16, !tbaa !2659
  %index.next136 = or i64 %index135, 8
  %414 = add nuw nsw i64 %400, %index.next136
  %415 = getelementptr inbounds float, float* %21, i64 %index.next136
  %416 = bitcast float* %415 to <4 x float>*
  %wide.load139.1 = load <4 x float>, <4 x float>* %416, align 32, !tbaa !2663
  %417 = getelementptr inbounds float, float* %415, i64 4
  %418 = bitcast float* %417 to <4 x float>*
  %wide.load140.1 = load <4 x float>, <4 x float>* %418, align 16, !tbaa !2663
  %419 = getelementptr inbounds float, float* %15, i64 %414
  %420 = bitcast float* %419 to <4 x float>*
  %wide.load141.1 = load <4 x float>, <4 x float>* %420, align 32, !tbaa !2659
  %421 = getelementptr inbounds float, float* %419, i64 4
  %422 = bitcast float* %421 to <4 x float>*
  %wide.load142.1 = load <4 x float>, <4 x float>* %422, align 16, !tbaa !2659
  %423 = fadd <4 x float> %wide.load139.1, %wide.load141.1
  %424 = fadd <4 x float> %wide.load140.1, %wide.load142.1
  %425 = bitcast float* %419 to <4 x float>*
  store <4 x float> %423, <4 x float>* %425, align 32, !tbaa !2659
  %426 = bitcast float* %421 to <4 x float>*
  store <4 x float> %424, <4 x float>* %426, align 16, !tbaa !2659
  %index.next136.1 = add nuw nsw i64 %index135, 16
  %427 = icmp eq i64 %index.next136.1, 512
  br i1 %427, label %for_end35.6, label %vector.body133, !prof !341, !llvm.loop !2682

for_end35.6:                                      ; preds = %vector.body133
  %indvars.iv.next80 = add nuw nsw i64 %indvars.iv79, 1
  %exitcond81.not = icmp eq i64 %indvars.iv.next80, 7
  br i1 %exitcond81.not, label %for_begin36.preheader, label %for_begin30.preheader, !prof !51

for_end9.us.14:                                   ; preds = %for_begin4.preheader
  %scevgep109 = getelementptr i8, i8* %0, i64 %13
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep103, i8* nonnull align 128 dereferenceable(2048) %scevgep109, i64 2048, i1 false)
  %428 = or i64 %13, 2048
  %scevgep109.1 = getelementptr i8, i8* %0, i64 %428
  %429 = add nuw nsw i64 %12, 2048
  %scevgep108.1 = getelementptr i8, i8* %6, i64 %429
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.1, i8* nonnull align 128 dereferenceable(2048) %scevgep109.1, i64 2048, i1 false)
  %430 = add nuw nsw i64 %13, 4096
  %scevgep109.2 = getelementptr i8, i8* %0, i64 %430
  %431 = add nuw nsw i64 %12, 4096
  %scevgep108.2 = getelementptr i8, i8* %6, i64 %431
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.2, i8* nonnull align 128 dereferenceable(2048) %scevgep109.2, i64 2048, i1 false)
  %432 = add nuw nsw i64 %13, 6144
  %scevgep109.3 = getelementptr i8, i8* %0, i64 %432
  %433 = add nuw nsw i64 %12, 6144
  %scevgep108.3 = getelementptr i8, i8* %6, i64 %433
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.3, i8* nonnull align 128 dereferenceable(2048) %scevgep109.3, i64 2048, i1 false)
  %434 = add nuw nsw i64 %13, 8192
  %scevgep109.4 = getelementptr i8, i8* %0, i64 %434
  %435 = add nuw nsw i64 %12, 8192
  %scevgep108.4 = getelementptr i8, i8* %6, i64 %435
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.4, i8* nonnull align 128 dereferenceable(2048) %scevgep109.4, i64 2048, i1 false)
  %436 = add nuw nsw i64 %13, 10240
  %scevgep109.5 = getelementptr i8, i8* %0, i64 %436
  %437 = add nuw nsw i64 %12, 10240
  %scevgep108.5 = getelementptr i8, i8* %6, i64 %437
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.5, i8* nonnull align 128 dereferenceable(2048) %scevgep109.5, i64 2048, i1 false)
  %438 = add nuw nsw i64 %13, 12288
  %scevgep109.6 = getelementptr i8, i8* %0, i64 %438
  %439 = add nuw nsw i64 %12, 12288
  %scevgep108.6 = getelementptr i8, i8* %6, i64 %439
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.6, i8* nonnull align 128 dereferenceable(2048) %scevgep109.6, i64 2048, i1 false)
  %440 = add nuw nsw i64 %13, 14336
  %scevgep109.7 = getelementptr i8, i8* %0, i64 %440
  %441 = add nuw nsw i64 %12, 14336
  %scevgep108.7 = getelementptr i8, i8* %6, i64 %441
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.7, i8* nonnull align 128 dereferenceable(2048) %scevgep109.7, i64 2048, i1 false)
  %442 = add nuw nsw i64 %13, 16384
  %scevgep109.8 = getelementptr i8, i8* %0, i64 %442
  %443 = add nuw nsw i64 %12, 16384
  %scevgep108.8 = getelementptr i8, i8* %6, i64 %443
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.8, i8* nonnull align 128 dereferenceable(2048) %scevgep109.8, i64 2048, i1 false)
  %444 = add nuw nsw i64 %13, 18432
  %scevgep109.9 = getelementptr i8, i8* %0, i64 %444
  %445 = add nuw nsw i64 %12, 18432
  %scevgep108.9 = getelementptr i8, i8* %6, i64 %445
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.9, i8* nonnull align 128 dereferenceable(2048) %scevgep109.9, i64 2048, i1 false)
  %446 = add nuw nsw i64 %13, 20480
  %scevgep109.10 = getelementptr i8, i8* %0, i64 %446
  %447 = add nuw nsw i64 %12, 20480
  %scevgep108.10 = getelementptr i8, i8* %6, i64 %447
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.10, i8* nonnull align 128 dereferenceable(2048) %scevgep109.10, i64 2048, i1 false)
  %448 = add nuw nsw i64 %13, 22528
  %scevgep109.11 = getelementptr i8, i8* %0, i64 %448
  %449 = add nuw nsw i64 %12, 22528
  %scevgep108.11 = getelementptr i8, i8* %6, i64 %449
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.11, i8* nonnull align 128 dereferenceable(2048) %scevgep109.11, i64 2048, i1 false)
  %450 = add nuw nsw i64 %13, 24576
  %scevgep109.12 = getelementptr i8, i8* %0, i64 %450
  %451 = add nuw nsw i64 %12, 24576
  %scevgep108.12 = getelementptr i8, i8* %6, i64 %451
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.12, i8* nonnull align 128 dereferenceable(2048) %scevgep109.12, i64 2048, i1 false)
  %452 = add nuw nsw i64 %13, 26624
  %scevgep109.13 = getelementptr i8, i8* %0, i64 %452
  %453 = add nuw nsw i64 %12, 26624
  %scevgep108.13 = getelementptr i8, i8* %6, i64 %453
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.13, i8* nonnull align 128 dereferenceable(2048) %scevgep109.13, i64 2048, i1 false)
  %454 = add nuw nsw i64 %12, 28672
  %scevgep108.14 = getelementptr i8, i8* %6, i64 %454
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(2048) %scevgep108.14, i8 0, i64 2048, i1 false)
  br label %for_end6
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_10(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([88 x i8], [88 x i8]* @.str.213, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !2683
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !2697
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !2699
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !2702
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.214, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.215, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.216, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.217, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !2704
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !2718
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 28
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.90, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !2720
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 28
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.91, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !2723
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 128
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.68, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !2725
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 100352, i32 3584, i32 128, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.218, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !2737
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 1
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.16, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !2751
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 1
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.17, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !2753
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 128
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.70, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !2756
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 256
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.95, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !2758
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 32768, i32 32768, i32 256, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.219, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !2770
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 256
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.97, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !2784
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !2798
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !2812
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 28
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.73, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !2814
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 28
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.74, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !2817
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 256
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.98, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !2819
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 200704, i32 7168, i32 256, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.99, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_10_compute_(i8* %31, i8* %39, i8* %51, i8* %45, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_10_compute_(i8* noalias align 128 %0, i8* noalias align 128 %1, i8* noalias align 128 %2, i8* noalias align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 401408, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %call_end, %if_end, %entry
  %merge = phi i32 [ -1, %entry ], [ %13, %if_end ], [ %22, %call_end ]
  ret i32 %merge

if_end:                                           ; preds = %entry
  %8 = alloca %18, align 8
  %9 = getelementptr inbounds %18, %18* %8, i64 0, i32 0
  store i8* %6, i8** %9, align 8
  %10 = getelementptr inbounds %18, %18* %8, i64 0, i32 1
  store i8* %0, i8** %10, align 8
  %11 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %12 = bitcast %18* %8 to i8*
  %13 = call i32 %11(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.220, i8* nonnull %12, i32 0)
  %14 = icmp eq i32 %13, 0
  br i1 %14, label %call_end, label %if_then, !prof !5

call_end:                                         ; preds = %if_end
  %15 = alloca %19, align 8
  %16 = getelementptr inbounds %19, %19* %15, i64 0, i32 0
  store i8* %6, i8** %16, align 8
  %17 = getelementptr inbounds %19, %19* %15, i64 0, i32 1
  store i8* %1, i8** %17, align 8
  %18 = getelementptr inbounds %19, %19* %15, i64 0, i32 2
  store i8* %2, i8** %18, align 8
  %19 = getelementptr inbounds %19, %19* %15, i64 0, i32 3
  store i8* %3, i8** %19, align 8
  %20 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %21 = bitcast %19* %15 to i8*
  %22 = call i32 %20(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.221, i8* nonnull %21, i32 0)
  %23 = icmp eq i32 %22, 0
  br i1 %23, label %call_end2, label %if_then, !prof !5

call_end2:                                        ; preds = %call_end
  %24 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %25 = call i32 %24(i32 1, i32 %4, i8* nonnull %6)
  %.not = icmp ne i32 %25, 0
  %spec.select = sext i1 %.not to i32
  ret i32 %spec.select
}

; Function Attrs: nofree norecurse nounwind
define private i32 @__tvm_parallel_lambda.220(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #2 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %9 = load i32, i32* %8, align 4
  %10 = add nsw i32 %9, 27
  %11 = sdiv i32 %10, %9
  %12 = add nsw i32 %0, 1
  %13 = mul nsw i32 %11, %12
  %14 = icmp slt i32 %13, 28
  %15 = select i1 %14, i32 %13, i32 28
  %16 = mul nsw i32 %11, %0
  %17 = icmp slt i32 %16, 28
  %18 = select i1 %17, i32 %16, i32 28
  %19 = icmp slt i32 %18, %15
  br i1 %19, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %20 = sext i32 %18 to i64
  %wide.trip.count = sext i32 %15 to i64
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end3
  %indvars.iv13 = phi i64 [ %20, %for_begin1.preheader.preheader ], [ %indvars.iv.next14, %for_end3 ]
  %21 = mul nsw i64 %indvars.iv13, 3584
  br label %for_begin4.preheader

for_end:                                          ; preds = %for_end3, %entry
  ret i32 0

for_begin4.preheader:                             ; preds = %for_begin1.preheader, %for_begin4.preheader
  %indvars.iv10 = phi i64 [ 0, %for_begin1.preheader ], [ %indvars.iv.next11, %for_begin4.preheader ]
  %22 = shl nsw i64 %indvars.iv10, 7
  %23 = add nsw i64 %22, %21
  %24 = getelementptr inbounds float, float* %7, i64 %23
  %25 = bitcast float* %24 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %25, align 4, !tbaa !2831
  %26 = getelementptr inbounds float, float* %24, i64 4
  %27 = bitcast float* %26 to <4 x float>*
  %wide.load16 = load <4 x float>, <4 x float>* %27, align 4, !tbaa !2831
  %28 = getelementptr inbounds float, float* %4, i64 %23
  %29 = bitcast float* %28 to <4 x float>*
  store <4 x float> %wide.load, <4 x float>* %29, align 4, !tbaa !2834
  %30 = getelementptr inbounds float, float* %28, i64 4
  %31 = bitcast float* %30 to <4 x float>*
  store <4 x float> %wide.load16, <4 x float>* %31, align 4, !tbaa !2834
  %32 = or i64 %23, 8
  %33 = getelementptr inbounds float, float* %7, i64 %32
  %34 = bitcast float* %33 to <4 x float>*
  %wide.load.1 = load <4 x float>, <4 x float>* %34, align 4, !tbaa !2831
  %35 = getelementptr inbounds float, float* %33, i64 4
  %36 = bitcast float* %35 to <4 x float>*
  %wide.load16.1 = load <4 x float>, <4 x float>* %36, align 4, !tbaa !2831
  %37 = getelementptr inbounds float, float* %4, i64 %32
  %38 = bitcast float* %37 to <4 x float>*
  store <4 x float> %wide.load.1, <4 x float>* %38, align 4, !tbaa !2834
  %39 = getelementptr inbounds float, float* %37, i64 4
  %40 = bitcast float* %39 to <4 x float>*
  store <4 x float> %wide.load16.1, <4 x float>* %40, align 4, !tbaa !2834
  %41 = or i64 %23, 16
  %42 = getelementptr inbounds float, float* %7, i64 %41
  %43 = bitcast float* %42 to <4 x float>*
  %wide.load.2 = load <4 x float>, <4 x float>* %43, align 4, !tbaa !2831
  %44 = getelementptr inbounds float, float* %42, i64 4
  %45 = bitcast float* %44 to <4 x float>*
  %wide.load16.2 = load <4 x float>, <4 x float>* %45, align 4, !tbaa !2831
  %46 = getelementptr inbounds float, float* %4, i64 %41
  %47 = bitcast float* %46 to <4 x float>*
  store <4 x float> %wide.load.2, <4 x float>* %47, align 4, !tbaa !2834
  %48 = getelementptr inbounds float, float* %46, i64 4
  %49 = bitcast float* %48 to <4 x float>*
  store <4 x float> %wide.load16.2, <4 x float>* %49, align 4, !tbaa !2834
  %50 = or i64 %23, 24
  %51 = getelementptr inbounds float, float* %7, i64 %50
  %52 = bitcast float* %51 to <4 x float>*
  %wide.load.3 = load <4 x float>, <4 x float>* %52, align 4, !tbaa !2831
  %53 = getelementptr inbounds float, float* %51, i64 4
  %54 = bitcast float* %53 to <4 x float>*
  %wide.load16.3 = load <4 x float>, <4 x float>* %54, align 4, !tbaa !2831
  %55 = getelementptr inbounds float, float* %4, i64 %50
  %56 = bitcast float* %55 to <4 x float>*
  store <4 x float> %wide.load.3, <4 x float>* %56, align 4, !tbaa !2834
  %57 = getelementptr inbounds float, float* %55, i64 4
  %58 = bitcast float* %57 to <4 x float>*
  store <4 x float> %wide.load16.3, <4 x float>* %58, align 4, !tbaa !2834
  %59 = or i64 %23, 32
  %60 = getelementptr inbounds float, float* %7, i64 %59
  %61 = bitcast float* %60 to <4 x float>*
  %wide.load.4 = load <4 x float>, <4 x float>* %61, align 4, !tbaa !2831
  %62 = getelementptr inbounds float, float* %60, i64 4
  %63 = bitcast float* %62 to <4 x float>*
  %wide.load16.4 = load <4 x float>, <4 x float>* %63, align 4, !tbaa !2831
  %64 = getelementptr inbounds float, float* %4, i64 %59
  %65 = bitcast float* %64 to <4 x float>*
  store <4 x float> %wide.load.4, <4 x float>* %65, align 4, !tbaa !2834
  %66 = getelementptr inbounds float, float* %64, i64 4
  %67 = bitcast float* %66 to <4 x float>*
  store <4 x float> %wide.load16.4, <4 x float>* %67, align 4, !tbaa !2834
  %68 = or i64 %23, 40
  %69 = getelementptr inbounds float, float* %7, i64 %68
  %70 = bitcast float* %69 to <4 x float>*
  %wide.load.5 = load <4 x float>, <4 x float>* %70, align 4, !tbaa !2831
  %71 = getelementptr inbounds float, float* %69, i64 4
  %72 = bitcast float* %71 to <4 x float>*
  %wide.load16.5 = load <4 x float>, <4 x float>* %72, align 4, !tbaa !2831
  %73 = getelementptr inbounds float, float* %4, i64 %68
  %74 = bitcast float* %73 to <4 x float>*
  store <4 x float> %wide.load.5, <4 x float>* %74, align 4, !tbaa !2834
  %75 = getelementptr inbounds float, float* %73, i64 4
  %76 = bitcast float* %75 to <4 x float>*
  store <4 x float> %wide.load16.5, <4 x float>* %76, align 4, !tbaa !2834
  %77 = or i64 %23, 48
  %78 = getelementptr inbounds float, float* %7, i64 %77
  %79 = bitcast float* %78 to <4 x float>*
  %wide.load.6 = load <4 x float>, <4 x float>* %79, align 4, !tbaa !2831
  %80 = getelementptr inbounds float, float* %78, i64 4
  %81 = bitcast float* %80 to <4 x float>*
  %wide.load16.6 = load <4 x float>, <4 x float>* %81, align 4, !tbaa !2831
  %82 = getelementptr inbounds float, float* %4, i64 %77
  %83 = bitcast float* %82 to <4 x float>*
  store <4 x float> %wide.load.6, <4 x float>* %83, align 4, !tbaa !2834
  %84 = getelementptr inbounds float, float* %82, i64 4
  %85 = bitcast float* %84 to <4 x float>*
  store <4 x float> %wide.load16.6, <4 x float>* %85, align 4, !tbaa !2834
  %86 = or i64 %23, 56
  %87 = getelementptr inbounds float, float* %7, i64 %86
  %88 = bitcast float* %87 to <4 x float>*
  %wide.load.7 = load <4 x float>, <4 x float>* %88, align 4, !tbaa !2831
  %89 = getelementptr inbounds float, float* %87, i64 4
  %90 = bitcast float* %89 to <4 x float>*
  %wide.load16.7 = load <4 x float>, <4 x float>* %90, align 4, !tbaa !2831
  %91 = getelementptr inbounds float, float* %4, i64 %86
  %92 = bitcast float* %91 to <4 x float>*
  store <4 x float> %wide.load.7, <4 x float>* %92, align 4, !tbaa !2834
  %93 = getelementptr inbounds float, float* %91, i64 4
  %94 = bitcast float* %93 to <4 x float>*
  store <4 x float> %wide.load16.7, <4 x float>* %94, align 4, !tbaa !2834
  %95 = or i64 %23, 64
  %96 = getelementptr inbounds float, float* %7, i64 %95
  %97 = bitcast float* %96 to <4 x float>*
  %wide.load.8 = load <4 x float>, <4 x float>* %97, align 4, !tbaa !2831
  %98 = getelementptr inbounds float, float* %96, i64 4
  %99 = bitcast float* %98 to <4 x float>*
  %wide.load16.8 = load <4 x float>, <4 x float>* %99, align 4, !tbaa !2831
  %100 = getelementptr inbounds float, float* %4, i64 %95
  %101 = bitcast float* %100 to <4 x float>*
  store <4 x float> %wide.load.8, <4 x float>* %101, align 4, !tbaa !2834
  %102 = getelementptr inbounds float, float* %100, i64 4
  %103 = bitcast float* %102 to <4 x float>*
  store <4 x float> %wide.load16.8, <4 x float>* %103, align 4, !tbaa !2834
  %104 = or i64 %23, 72
  %105 = getelementptr inbounds float, float* %7, i64 %104
  %106 = bitcast float* %105 to <4 x float>*
  %wide.load.9 = load <4 x float>, <4 x float>* %106, align 4, !tbaa !2831
  %107 = getelementptr inbounds float, float* %105, i64 4
  %108 = bitcast float* %107 to <4 x float>*
  %wide.load16.9 = load <4 x float>, <4 x float>* %108, align 4, !tbaa !2831
  %109 = getelementptr inbounds float, float* %4, i64 %104
  %110 = bitcast float* %109 to <4 x float>*
  store <4 x float> %wide.load.9, <4 x float>* %110, align 4, !tbaa !2834
  %111 = getelementptr inbounds float, float* %109, i64 4
  %112 = bitcast float* %111 to <4 x float>*
  store <4 x float> %wide.load16.9, <4 x float>* %112, align 4, !tbaa !2834
  %113 = or i64 %23, 80
  %114 = getelementptr inbounds float, float* %7, i64 %113
  %115 = bitcast float* %114 to <4 x float>*
  %wide.load.10 = load <4 x float>, <4 x float>* %115, align 4, !tbaa !2831
  %116 = getelementptr inbounds float, float* %114, i64 4
  %117 = bitcast float* %116 to <4 x float>*
  %wide.load16.10 = load <4 x float>, <4 x float>* %117, align 4, !tbaa !2831
  %118 = getelementptr inbounds float, float* %4, i64 %113
  %119 = bitcast float* %118 to <4 x float>*
  store <4 x float> %wide.load.10, <4 x float>* %119, align 4, !tbaa !2834
  %120 = getelementptr inbounds float, float* %118, i64 4
  %121 = bitcast float* %120 to <4 x float>*
  store <4 x float> %wide.load16.10, <4 x float>* %121, align 4, !tbaa !2834
  %122 = or i64 %23, 88
  %123 = getelementptr inbounds float, float* %7, i64 %122
  %124 = bitcast float* %123 to <4 x float>*
  %wide.load.11 = load <4 x float>, <4 x float>* %124, align 4, !tbaa !2831
  %125 = getelementptr inbounds float, float* %123, i64 4
  %126 = bitcast float* %125 to <4 x float>*
  %wide.load16.11 = load <4 x float>, <4 x float>* %126, align 4, !tbaa !2831
  %127 = getelementptr inbounds float, float* %4, i64 %122
  %128 = bitcast float* %127 to <4 x float>*
  store <4 x float> %wide.load.11, <4 x float>* %128, align 4, !tbaa !2834
  %129 = getelementptr inbounds float, float* %127, i64 4
  %130 = bitcast float* %129 to <4 x float>*
  store <4 x float> %wide.load16.11, <4 x float>* %130, align 4, !tbaa !2834
  %131 = or i64 %23, 96
  %132 = getelementptr inbounds float, float* %7, i64 %131
  %133 = bitcast float* %132 to <4 x float>*
  %wide.load.12 = load <4 x float>, <4 x float>* %133, align 4, !tbaa !2831
  %134 = getelementptr inbounds float, float* %132, i64 4
  %135 = bitcast float* %134 to <4 x float>*
  %wide.load16.12 = load <4 x float>, <4 x float>* %135, align 4, !tbaa !2831
  %136 = getelementptr inbounds float, float* %4, i64 %131
  %137 = bitcast float* %136 to <4 x float>*
  store <4 x float> %wide.load.12, <4 x float>* %137, align 4, !tbaa !2834
  %138 = getelementptr inbounds float, float* %136, i64 4
  %139 = bitcast float* %138 to <4 x float>*
  store <4 x float> %wide.load16.12, <4 x float>* %139, align 4, !tbaa !2834
  %140 = or i64 %23, 104
  %141 = getelementptr inbounds float, float* %7, i64 %140
  %142 = bitcast float* %141 to <4 x float>*
  %wide.load.13 = load <4 x float>, <4 x float>* %142, align 4, !tbaa !2831
  %143 = getelementptr inbounds float, float* %141, i64 4
  %144 = bitcast float* %143 to <4 x float>*
  %wide.load16.13 = load <4 x float>, <4 x float>* %144, align 4, !tbaa !2831
  %145 = getelementptr inbounds float, float* %4, i64 %140
  %146 = bitcast float* %145 to <4 x float>*
  store <4 x float> %wide.load.13, <4 x float>* %146, align 4, !tbaa !2834
  %147 = getelementptr inbounds float, float* %145, i64 4
  %148 = bitcast float* %147 to <4 x float>*
  store <4 x float> %wide.load16.13, <4 x float>* %148, align 4, !tbaa !2834
  %149 = or i64 %23, 112
  %150 = getelementptr inbounds float, float* %7, i64 %149
  %151 = bitcast float* %150 to <4 x float>*
  %wide.load.14 = load <4 x float>, <4 x float>* %151, align 4, !tbaa !2831
  %152 = getelementptr inbounds float, float* %150, i64 4
  %153 = bitcast float* %152 to <4 x float>*
  %wide.load16.14 = load <4 x float>, <4 x float>* %153, align 4, !tbaa !2831
  %154 = getelementptr inbounds float, float* %4, i64 %149
  %155 = bitcast float* %154 to <4 x float>*
  store <4 x float> %wide.load.14, <4 x float>* %155, align 4, !tbaa !2834
  %156 = getelementptr inbounds float, float* %154, i64 4
  %157 = bitcast float* %156 to <4 x float>*
  store <4 x float> %wide.load16.14, <4 x float>* %157, align 4, !tbaa !2834
  %158 = or i64 %23, 120
  %159 = getelementptr inbounds float, float* %7, i64 %158
  %160 = bitcast float* %159 to <4 x float>*
  %wide.load.15 = load <4 x float>, <4 x float>* %160, align 4, !tbaa !2831
  %161 = getelementptr inbounds float, float* %159, i64 4
  %162 = bitcast float* %161 to <4 x float>*
  %wide.load16.15 = load <4 x float>, <4 x float>* %162, align 4, !tbaa !2831
  %163 = getelementptr inbounds float, float* %4, i64 %158
  %164 = bitcast float* %163 to <4 x float>*
  store <4 x float> %wide.load.15, <4 x float>* %164, align 4, !tbaa !2834
  %165 = getelementptr inbounds float, float* %163, i64 4
  %166 = bitcast float* %165 to <4 x float>*
  store <4 x float> %wide.load16.15, <4 x float>* %166, align 4, !tbaa !2834
  %indvars.iv.next11 = add nuw nsw i64 %indvars.iv10, 1
  %exitcond12.not = icmp eq i64 %indvars.iv.next11, 28
  br i1 %exitcond12.not, label %for_end3, label %for_begin4.preheader, !prof !51

for_end3:                                         ; preds = %for_begin4.preheader
  %indvars.iv.next14 = add nsw i64 %indvars.iv13, 1
  %exitcond15.not = icmp eq i64 %indvars.iv.next14, %wide.trip.count
  br i1 %exitcond15.not, label %for_end, label %for_begin1.preheader, !prof !51
}

; Function Attrs: nofree nounwind
define private i32 @__tvm_parallel_lambda.221(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #3 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds i8, i8* %2, i64 16
  %9 = bitcast i8* %8 to float**
  %10 = load float*, float** %9, align 8
  %11 = getelementptr inbounds i8, i8* %2, i64 24
  %12 = bitcast i8* %11 to float**
  %13 = load float*, float** %12, align 8
  %14 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %15 = load i32, i32* %14, align 4
  %16 = add nsw i32 %15, 783
  %17 = sdiv i32 %16, %15
  %18 = add nsw i32 %0, 1
  %19 = mul nsw i32 %17, %18
  %20 = icmp slt i32 %19, 784
  %21 = select i1 %20, i32 %19, i32 784
  %22 = mul nsw i32 %17, %0
  %23 = icmp slt i32 %22, 784
  %24 = select i1 %23, i32 %22, i32 784
  %25 = icmp slt i32 %24, %21
  br i1 %25, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %26 = sext i32 %24 to i64
  %wide.trip.count = sext i32 %21 to i64
  %27 = bitcast float* %13 to <64 x float>*
  %28 = load <64 x float>, <64 x float>* %27, align 128, !tbaa !2837
  %29 = getelementptr inbounds float, float* %13, i64 64
  %30 = bitcast float* %29 to <64 x float>*
  %31 = load <64 x float>, <64 x float>* %30, align 128, !tbaa !2837
  %32 = getelementptr inbounds float, float* %13, i64 128
  %33 = bitcast float* %32 to <64 x float>*
  %34 = load <64 x float>, <64 x float>* %33, align 128, !tbaa !2837
  %35 = getelementptr inbounds float, float* %13, i64 192
  %36 = bitcast float* %35 to <64 x float>*
  %37 = load <64 x float>, <64 x float>* %36, align 128, !tbaa !2837
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end6.3
  %indvars.iv14 = phi i64 [ %26, %for_begin1.preheader.preheader ], [ %indvars.iv.next15, %for_end6.3 ]
  %38 = phi i32 [ %24, %for_begin1.preheader.preheader ], [ %161, %for_end6.3 ]
  %39 = shl nsw i32 %38, 7
  %40 = trunc i64 %indvars.iv14 to i32
  %41 = shl nsw i32 %40, 8
  %42 = sext i32 %39 to i64
  %43 = sext i32 %41 to i64
  br label %for_body5

for_end:                                          ; preds = %for_end6.3, %entry
  ret i32 0

for_body5:                                        ; preds = %for_body5, %for_begin1.preheader
  %indvars.iv = phi i64 [ 0, %for_begin1.preheader ], [ %indvars.iv.next.128, %for_body5 ]
  %.010 = phi <64 x float> [ zeroinitializer, %for_begin1.preheader ], [ %63, %for_body5 ]
  %44 = add nsw i64 %indvars.iv, %42
  %45 = getelementptr inbounds float, float* %4, i64 %44
  %46 = load float, float* %45, align 4, !tbaa !2834
  %47 = insertelement <64 x float> undef, float %46, i32 0
  %48 = shufflevector <64 x float> %47, <64 x float> undef, <64 x i32> zeroinitializer
  %49 = shl nuw nsw i64 %indvars.iv, 8
  %50 = getelementptr inbounds float, float* %7, i64 %49
  %51 = bitcast float* %50 to <64 x float>*
  %52 = load <64 x float>, <64 x float>* %51, align 128, !tbaa !2840
  %53 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %48, <64 x float> %52, <64 x float> %.010)
  %indvars.iv.next = or i64 %indvars.iv, 1
  %54 = add nsw i64 %indvars.iv.next, %42
  %55 = getelementptr inbounds float, float* %4, i64 %54
  %56 = load float, float* %55, align 4, !tbaa !2834
  %57 = insertelement <64 x float> undef, float %56, i32 0
  %58 = shufflevector <64 x float> %57, <64 x float> undef, <64 x i32> zeroinitializer
  %59 = shl nuw nsw i64 %indvars.iv.next, 8
  %60 = getelementptr inbounds float, float* %7, i64 %59
  %61 = bitcast float* %60 to <64 x float>*
  %62 = load <64 x float>, <64 x float>* %61, align 128, !tbaa !2840
  %63 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %58, <64 x float> %62, <64 x float> %53)
  %indvars.iv.next.128 = add nuw nsw i64 %indvars.iv, 2
  %exitcond.not.1 = icmp eq i64 %indvars.iv.next.128, 128
  br i1 %exitcond.not.1, label %for_end6, label %for_body5, !prof !51

for_end6:                                         ; preds = %for_body5
  %64 = fadd <64 x float> %63, %28
  %65 = fcmp olt <64 x float> %64, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %66 = select <64 x i1> %65, <64 x float> %64, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %67 = fcmp ogt <64 x float> %66, zeroinitializer
  %68 = select <64 x i1> %67, <64 x float> %66, <64 x float> zeroinitializer
  %69 = getelementptr inbounds float, float* %10, i64 %43
  %70 = bitcast float* %69 to <64 x float>*
  store <64 x float> %68, <64 x float>* %70, align 128, !tbaa !2843
  br label %for_body5.1

for_body5.1:                                      ; preds = %for_body5.1, %for_end6
  %indvars.iv.1 = phi i64 [ 0, %for_end6 ], [ %indvars.iv.next.1.1, %for_body5.1 ]
  %.010.1 = phi <64 x float> [ zeroinitializer, %for_end6 ], [ %92, %for_body5.1 ]
  %71 = add nsw i64 %indvars.iv.1, %42
  %72 = getelementptr inbounds float, float* %4, i64 %71
  %73 = load float, float* %72, align 4, !tbaa !2834
  %74 = insertelement <64 x float> undef, float %73, i32 0
  %75 = shufflevector <64 x float> %74, <64 x float> undef, <64 x i32> zeroinitializer
  %76 = shl nuw nsw i64 %indvars.iv.1, 8
  %77 = or i64 %76, 64
  %78 = getelementptr inbounds float, float* %7, i64 %77
  %79 = bitcast float* %78 to <64 x float>*
  %80 = load <64 x float>, <64 x float>* %79, align 128, !tbaa !2840
  %81 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %75, <64 x float> %80, <64 x float> %.010.1)
  %indvars.iv.next.1 = or i64 %indvars.iv.1, 1
  %82 = add nsw i64 %indvars.iv.next.1, %42
  %83 = getelementptr inbounds float, float* %4, i64 %82
  %84 = load float, float* %83, align 4, !tbaa !2834
  %85 = insertelement <64 x float> undef, float %84, i32 0
  %86 = shufflevector <64 x float> %85, <64 x float> undef, <64 x i32> zeroinitializer
  %87 = shl nuw nsw i64 %indvars.iv.next.1, 8
  %88 = or i64 %87, 64
  %89 = getelementptr inbounds float, float* %7, i64 %88
  %90 = bitcast float* %89 to <64 x float>*
  %91 = load <64 x float>, <64 x float>* %90, align 128, !tbaa !2840
  %92 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %86, <64 x float> %91, <64 x float> %81)
  %indvars.iv.next.1.1 = add nuw nsw i64 %indvars.iv.1, 2
  %exitcond.1.not.1 = icmp eq i64 %indvars.iv.next.1.1, 128
  br i1 %exitcond.1.not.1, label %for_end6.1, label %for_body5.1, !prof !51

for_end6.1:                                       ; preds = %for_body5.1
  %93 = or i64 %43, 64
  %94 = fadd <64 x float> %92, %31
  %95 = fcmp olt <64 x float> %94, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %96 = select <64 x i1> %95, <64 x float> %94, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %97 = fcmp ogt <64 x float> %96, zeroinitializer
  %98 = select <64 x i1> %97, <64 x float> %96, <64 x float> zeroinitializer
  %99 = getelementptr inbounds float, float* %10, i64 %93
  %100 = bitcast float* %99 to <64 x float>*
  store <64 x float> %98, <64 x float>* %100, align 128, !tbaa !2843
  br label %for_body5.2

for_body5.2:                                      ; preds = %for_body5.2, %for_end6.1
  %indvars.iv.2 = phi i64 [ 0, %for_end6.1 ], [ %indvars.iv.next.2.1, %for_body5.2 ]
  %.010.2 = phi <64 x float> [ zeroinitializer, %for_end6.1 ], [ %122, %for_body5.2 ]
  %101 = add nsw i64 %indvars.iv.2, %42
  %102 = getelementptr inbounds float, float* %4, i64 %101
  %103 = load float, float* %102, align 4, !tbaa !2834
  %104 = insertelement <64 x float> undef, float %103, i32 0
  %105 = shufflevector <64 x float> %104, <64 x float> undef, <64 x i32> zeroinitializer
  %106 = shl nuw nsw i64 %indvars.iv.2, 8
  %107 = or i64 %106, 128
  %108 = getelementptr inbounds float, float* %7, i64 %107
  %109 = bitcast float* %108 to <64 x float>*
  %110 = load <64 x float>, <64 x float>* %109, align 128, !tbaa !2840
  %111 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %105, <64 x float> %110, <64 x float> %.010.2)
  %indvars.iv.next.2 = or i64 %indvars.iv.2, 1
  %112 = add nsw i64 %indvars.iv.next.2, %42
  %113 = getelementptr inbounds float, float* %4, i64 %112
  %114 = load float, float* %113, align 4, !tbaa !2834
  %115 = insertelement <64 x float> undef, float %114, i32 0
  %116 = shufflevector <64 x float> %115, <64 x float> undef, <64 x i32> zeroinitializer
  %117 = shl nuw nsw i64 %indvars.iv.next.2, 8
  %118 = or i64 %117, 128
  %119 = getelementptr inbounds float, float* %7, i64 %118
  %120 = bitcast float* %119 to <64 x float>*
  %121 = load <64 x float>, <64 x float>* %120, align 128, !tbaa !2840
  %122 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %116, <64 x float> %121, <64 x float> %111)
  %indvars.iv.next.2.1 = add nuw nsw i64 %indvars.iv.2, 2
  %exitcond.2.not.1 = icmp eq i64 %indvars.iv.next.2.1, 128
  br i1 %exitcond.2.not.1, label %for_end6.2, label %for_body5.2, !prof !51

for_end6.2:                                       ; preds = %for_body5.2
  %123 = or i64 %43, 128
  %124 = fadd <64 x float> %122, %34
  %125 = fcmp olt <64 x float> %124, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %126 = select <64 x i1> %125, <64 x float> %124, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %127 = fcmp ogt <64 x float> %126, zeroinitializer
  %128 = select <64 x i1> %127, <64 x float> %126, <64 x float> zeroinitializer
  %129 = getelementptr inbounds float, float* %10, i64 %123
  %130 = bitcast float* %129 to <64 x float>*
  store <64 x float> %128, <64 x float>* %130, align 128, !tbaa !2843
  br label %for_body5.3

for_body5.3:                                      ; preds = %for_body5.3, %for_end6.2
  %indvars.iv.3 = phi i64 [ 0, %for_end6.2 ], [ %indvars.iv.next.3.1, %for_body5.3 ]
  %.010.3 = phi <64 x float> [ zeroinitializer, %for_end6.2 ], [ %152, %for_body5.3 ]
  %131 = add nsw i64 %indvars.iv.3, %42
  %132 = getelementptr inbounds float, float* %4, i64 %131
  %133 = load float, float* %132, align 4, !tbaa !2834
  %134 = insertelement <64 x float> undef, float %133, i32 0
  %135 = shufflevector <64 x float> %134, <64 x float> undef, <64 x i32> zeroinitializer
  %136 = shl nuw nsw i64 %indvars.iv.3, 8
  %137 = or i64 %136, 192
  %138 = getelementptr inbounds float, float* %7, i64 %137
  %139 = bitcast float* %138 to <64 x float>*
  %140 = load <64 x float>, <64 x float>* %139, align 128, !tbaa !2840
  %141 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %135, <64 x float> %140, <64 x float> %.010.3)
  %indvars.iv.next.3 = or i64 %indvars.iv.3, 1
  %142 = add nsw i64 %indvars.iv.next.3, %42
  %143 = getelementptr inbounds float, float* %4, i64 %142
  %144 = load float, float* %143, align 4, !tbaa !2834
  %145 = insertelement <64 x float> undef, float %144, i32 0
  %146 = shufflevector <64 x float> %145, <64 x float> undef, <64 x i32> zeroinitializer
  %147 = shl nuw nsw i64 %indvars.iv.next.3, 8
  %148 = or i64 %147, 192
  %149 = getelementptr inbounds float, float* %7, i64 %148
  %150 = bitcast float* %149 to <64 x float>*
  %151 = load <64 x float>, <64 x float>* %150, align 128, !tbaa !2840
  %152 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %146, <64 x float> %151, <64 x float> %141)
  %indvars.iv.next.3.1 = add nuw nsw i64 %indvars.iv.3, 2
  %exitcond.3.not.1 = icmp eq i64 %indvars.iv.next.3.1, 128
  br i1 %exitcond.3.not.1, label %for_end6.3, label %for_body5.3, !prof !51

for_end6.3:                                       ; preds = %for_body5.3
  %153 = or i64 %43, 192
  %154 = fadd <64 x float> %152, %37
  %155 = fcmp olt <64 x float> %154, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %156 = select <64 x i1> %155, <64 x float> %154, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %157 = fcmp ogt <64 x float> %156, zeroinitializer
  %158 = select <64 x i1> %157, <64 x float> %156, <64 x float> zeroinitializer
  %159 = getelementptr inbounds float, float* %10, i64 %153
  %160 = bitcast float* %159 to <64 x float>*
  store <64 x float> %158, <64 x float>* %160, align 128, !tbaa !2843
  %indvars.iv.next15 = add nsw i64 %indvars.iv14, 1
  %161 = add nsw i32 %38, 1
  %exitcond16.not = icmp eq i64 %indvars.iv.next15, %wide.trip.count
  br i1 %exitcond16.not, label %for_end, label %for_begin1.preheader, !prof !51
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_2(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.222, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !2846
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !2860
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !2862
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !2865
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.223, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.224, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.225, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.226, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !2867
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !2881
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 7
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.123, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !2883
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 7
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.124, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !2886
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 512
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.49, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !2888
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 25088, i32 3584, i32 512, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.227, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !2900
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 1
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.16, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !2914
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 1
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.17, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !2916
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 512
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.53, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !2919
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 1024
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.128, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !2921
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 524288, i32 524288, i32 1024, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([205 x i8], [205 x i8]* @.str.228, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !2933
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 1024
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.130, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !2947
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !2961
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !2975
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 7
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.131, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !2977
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 7
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.132, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !2980
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 1024
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.133, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !2982
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 50176, i32 7168, i32 1024, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.134, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_2_compute_(i8* %31, i8* %39, i8* %51, i8* %45, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_2_compute_(i8* noalias align 128 %0, i8* noalias align 128 %1, i8* noalias align 128 %2, i8* noalias align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 100352, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %call_end, %if_end, %entry
  %merge = phi i32 [ -1, %entry ], [ %13, %if_end ], [ %22, %call_end ]
  ret i32 %merge

if_end:                                           ; preds = %entry
  %8 = alloca %20, align 8
  %9 = getelementptr inbounds %20, %20* %8, i64 0, i32 0
  store i8* %6, i8** %9, align 8
  %10 = getelementptr inbounds %20, %20* %8, i64 0, i32 1
  store i8* %0, i8** %10, align 8
  %11 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %12 = bitcast %20* %8 to i8*
  %13 = call i32 %11(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.229, i8* nonnull %12, i32 0)
  %14 = icmp eq i32 %13, 0
  br i1 %14, label %call_end, label %if_then, !prof !5

call_end:                                         ; preds = %if_end
  %15 = alloca %21, align 8
  %16 = getelementptr inbounds %21, %21* %15, i64 0, i32 0
  store i8* %6, i8** %16, align 8
  %17 = getelementptr inbounds %21, %21* %15, i64 0, i32 1
  store i8* %1, i8** %17, align 8
  %18 = getelementptr inbounds %21, %21* %15, i64 0, i32 2
  store i8* %2, i8** %18, align 8
  %19 = getelementptr inbounds %21, %21* %15, i64 0, i32 3
  store i8* %3, i8** %19, align 8
  %20 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %21 = bitcast %21* %15 to i8*
  %22 = call i32 %20(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.230, i8* nonnull %21, i32 0)
  %23 = icmp eq i32 %22, 0
  br i1 %23, label %call_end2, label %if_then, !prof !5

call_end2:                                        ; preds = %call_end
  %24 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %25 = call i32 %24(i32 1, i32 %4, i8* nonnull %6)
  %.not = icmp ne i32 %25, 0
  %spec.select = sext i1 %.not to i32
  ret i32 %spec.select
}

; Function Attrs: nofree norecurse nounwind
define private i32 @__tvm_parallel_lambda.229(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #2 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %9 = load i32, i32* %8, align 4
  %10 = add nsw i32 %9, 6
  %11 = sdiv i32 %10, %9
  %12 = add nsw i32 %0, 1
  %13 = mul nsw i32 %11, %12
  %14 = icmp slt i32 %13, 7
  %15 = select i1 %14, i32 %13, i32 7
  %16 = mul nsw i32 %11, %0
  %17 = icmp slt i32 %16, 7
  %18 = select i1 %17, i32 %16, i32 7
  %19 = icmp slt i32 %18, %15
  br i1 %19, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %20 = sext i32 %18 to i64
  %wide.trip.count = sext i32 %15 to i64
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end6.6
  %indvars.iv13 = phi i64 [ %20, %for_begin1.preheader.preheader ], [ %indvars.iv.next14, %for_end6.6 ]
  %21 = mul nsw i64 %indvars.iv13, 3584
  br label %vector.body69

vector.body69:                                    ; preds = %vector.body69, %for_begin1.preheader
  %index71 = phi i64 [ 0, %for_begin1.preheader ], [ %index.next72.3, %vector.body69 ]
  %22 = add nsw i64 %21, %index71
  %23 = getelementptr inbounds float, float* %7, i64 %22
  %24 = bitcast float* %23 to <4 x float>*
  %wide.load75 = load <4 x float>, <4 x float>* %24, align 4, !tbaa !2994
  %25 = getelementptr inbounds float, float* %23, i64 4
  %26 = bitcast float* %25 to <4 x float>*
  %wide.load76 = load <4 x float>, <4 x float>* %26, align 4, !tbaa !2994
  %27 = getelementptr inbounds float, float* %4, i64 %22
  %28 = bitcast float* %27 to <4 x float>*
  store <4 x float> %wide.load75, <4 x float>* %28, align 4, !tbaa !2997
  %29 = getelementptr inbounds float, float* %27, i64 4
  %30 = bitcast float* %29 to <4 x float>*
  store <4 x float> %wide.load76, <4 x float>* %30, align 4, !tbaa !2997
  %index.next72 = or i64 %index71, 8
  %31 = add nsw i64 %21, %index.next72
  %32 = getelementptr inbounds float, float* %7, i64 %31
  %33 = bitcast float* %32 to <4 x float>*
  %wide.load75.1 = load <4 x float>, <4 x float>* %33, align 4, !tbaa !2994
  %34 = getelementptr inbounds float, float* %32, i64 4
  %35 = bitcast float* %34 to <4 x float>*
  %wide.load76.1 = load <4 x float>, <4 x float>* %35, align 4, !tbaa !2994
  %36 = getelementptr inbounds float, float* %4, i64 %31
  %37 = bitcast float* %36 to <4 x float>*
  store <4 x float> %wide.load75.1, <4 x float>* %37, align 4, !tbaa !2997
  %38 = getelementptr inbounds float, float* %36, i64 4
  %39 = bitcast float* %38 to <4 x float>*
  store <4 x float> %wide.load76.1, <4 x float>* %39, align 4, !tbaa !2997
  %index.next72.1 = or i64 %index71, 16
  %40 = add nsw i64 %21, %index.next72.1
  %41 = getelementptr inbounds float, float* %7, i64 %40
  %42 = bitcast float* %41 to <4 x float>*
  %wide.load75.2 = load <4 x float>, <4 x float>* %42, align 4, !tbaa !2994
  %43 = getelementptr inbounds float, float* %41, i64 4
  %44 = bitcast float* %43 to <4 x float>*
  %wide.load76.2 = load <4 x float>, <4 x float>* %44, align 4, !tbaa !2994
  %45 = getelementptr inbounds float, float* %4, i64 %40
  %46 = bitcast float* %45 to <4 x float>*
  store <4 x float> %wide.load75.2, <4 x float>* %46, align 4, !tbaa !2997
  %47 = getelementptr inbounds float, float* %45, i64 4
  %48 = bitcast float* %47 to <4 x float>*
  store <4 x float> %wide.load76.2, <4 x float>* %48, align 4, !tbaa !2997
  %index.next72.2 = or i64 %index71, 24
  %49 = add nsw i64 %21, %index.next72.2
  %50 = getelementptr inbounds float, float* %7, i64 %49
  %51 = bitcast float* %50 to <4 x float>*
  %wide.load75.3 = load <4 x float>, <4 x float>* %51, align 4, !tbaa !2994
  %52 = getelementptr inbounds float, float* %50, i64 4
  %53 = bitcast float* %52 to <4 x float>*
  %wide.load76.3 = load <4 x float>, <4 x float>* %53, align 4, !tbaa !2994
  %54 = getelementptr inbounds float, float* %4, i64 %49
  %55 = bitcast float* %54 to <4 x float>*
  store <4 x float> %wide.load75.3, <4 x float>* %55, align 4, !tbaa !2997
  %56 = getelementptr inbounds float, float* %54, i64 4
  %57 = bitcast float* %56 to <4 x float>*
  store <4 x float> %wide.load76.3, <4 x float>* %57, align 4, !tbaa !2997
  %index.next72.3 = add nuw nsw i64 %index71, 32
  %58 = icmp eq i64 %index.next72.3, 512
  br i1 %58, label %for_end6, label %vector.body69, !prof !341, !llvm.loop !3000

for_end:                                          ; preds = %for_end6.6, %entry
  ret i32 0

for_end6:                                         ; preds = %vector.body69
  %59 = add nsw i64 %21, 512
  br label %vector.body59

vector.body59:                                    ; preds = %vector.body59, %for_end6
  %index61 = phi i64 [ 0, %for_end6 ], [ %index.next62.3, %vector.body59 ]
  %60 = add nsw i64 %59, %index61
  %61 = getelementptr inbounds float, float* %7, i64 %60
  %62 = bitcast float* %61 to <4 x float>*
  %wide.load65 = load <4 x float>, <4 x float>* %62, align 4, !tbaa !2994
  %63 = getelementptr inbounds float, float* %61, i64 4
  %64 = bitcast float* %63 to <4 x float>*
  %wide.load66 = load <4 x float>, <4 x float>* %64, align 4, !tbaa !2994
  %65 = getelementptr inbounds float, float* %4, i64 %60
  %66 = bitcast float* %65 to <4 x float>*
  store <4 x float> %wide.load65, <4 x float>* %66, align 4, !tbaa !2997
  %67 = getelementptr inbounds float, float* %65, i64 4
  %68 = bitcast float* %67 to <4 x float>*
  store <4 x float> %wide.load66, <4 x float>* %68, align 4, !tbaa !2997
  %index.next62 = or i64 %index61, 8
  %69 = add nsw i64 %59, %index.next62
  %70 = getelementptr inbounds float, float* %7, i64 %69
  %71 = bitcast float* %70 to <4 x float>*
  %wide.load65.1 = load <4 x float>, <4 x float>* %71, align 4, !tbaa !2994
  %72 = getelementptr inbounds float, float* %70, i64 4
  %73 = bitcast float* %72 to <4 x float>*
  %wide.load66.1 = load <4 x float>, <4 x float>* %73, align 4, !tbaa !2994
  %74 = getelementptr inbounds float, float* %4, i64 %69
  %75 = bitcast float* %74 to <4 x float>*
  store <4 x float> %wide.load65.1, <4 x float>* %75, align 4, !tbaa !2997
  %76 = getelementptr inbounds float, float* %74, i64 4
  %77 = bitcast float* %76 to <4 x float>*
  store <4 x float> %wide.load66.1, <4 x float>* %77, align 4, !tbaa !2997
  %index.next62.1 = or i64 %index61, 16
  %78 = add nsw i64 %59, %index.next62.1
  %79 = getelementptr inbounds float, float* %7, i64 %78
  %80 = bitcast float* %79 to <4 x float>*
  %wide.load65.2 = load <4 x float>, <4 x float>* %80, align 4, !tbaa !2994
  %81 = getelementptr inbounds float, float* %79, i64 4
  %82 = bitcast float* %81 to <4 x float>*
  %wide.load66.2 = load <4 x float>, <4 x float>* %82, align 4, !tbaa !2994
  %83 = getelementptr inbounds float, float* %4, i64 %78
  %84 = bitcast float* %83 to <4 x float>*
  store <4 x float> %wide.load65.2, <4 x float>* %84, align 4, !tbaa !2997
  %85 = getelementptr inbounds float, float* %83, i64 4
  %86 = bitcast float* %85 to <4 x float>*
  store <4 x float> %wide.load66.2, <4 x float>* %86, align 4, !tbaa !2997
  %index.next62.2 = or i64 %index61, 24
  %87 = add nsw i64 %59, %index.next62.2
  %88 = getelementptr inbounds float, float* %7, i64 %87
  %89 = bitcast float* %88 to <4 x float>*
  %wide.load65.3 = load <4 x float>, <4 x float>* %89, align 4, !tbaa !2994
  %90 = getelementptr inbounds float, float* %88, i64 4
  %91 = bitcast float* %90 to <4 x float>*
  %wide.load66.3 = load <4 x float>, <4 x float>* %91, align 4, !tbaa !2994
  %92 = getelementptr inbounds float, float* %4, i64 %87
  %93 = bitcast float* %92 to <4 x float>*
  store <4 x float> %wide.load65.3, <4 x float>* %93, align 4, !tbaa !2997
  %94 = getelementptr inbounds float, float* %92, i64 4
  %95 = bitcast float* %94 to <4 x float>*
  store <4 x float> %wide.load66.3, <4 x float>* %95, align 4, !tbaa !2997
  %index.next62.3 = add nuw nsw i64 %index61, 32
  %96 = icmp eq i64 %index.next62.3, 512
  br i1 %96, label %for_end6.1, label %vector.body59, !prof !341, !llvm.loop !3001

for_end6.1:                                       ; preds = %vector.body59
  %97 = add nsw i64 %21, 1024
  br label %vector.body49

vector.body49:                                    ; preds = %vector.body49, %for_end6.1
  %index51 = phi i64 [ 0, %for_end6.1 ], [ %index.next52.3, %vector.body49 ]
  %98 = add nsw i64 %97, %index51
  %99 = getelementptr inbounds float, float* %7, i64 %98
  %100 = bitcast float* %99 to <4 x float>*
  %wide.load55 = load <4 x float>, <4 x float>* %100, align 4, !tbaa !2994
  %101 = getelementptr inbounds float, float* %99, i64 4
  %102 = bitcast float* %101 to <4 x float>*
  %wide.load56 = load <4 x float>, <4 x float>* %102, align 4, !tbaa !2994
  %103 = getelementptr inbounds float, float* %4, i64 %98
  %104 = bitcast float* %103 to <4 x float>*
  store <4 x float> %wide.load55, <4 x float>* %104, align 4, !tbaa !2997
  %105 = getelementptr inbounds float, float* %103, i64 4
  %106 = bitcast float* %105 to <4 x float>*
  store <4 x float> %wide.load56, <4 x float>* %106, align 4, !tbaa !2997
  %index.next52 = or i64 %index51, 8
  %107 = add nsw i64 %97, %index.next52
  %108 = getelementptr inbounds float, float* %7, i64 %107
  %109 = bitcast float* %108 to <4 x float>*
  %wide.load55.1 = load <4 x float>, <4 x float>* %109, align 4, !tbaa !2994
  %110 = getelementptr inbounds float, float* %108, i64 4
  %111 = bitcast float* %110 to <4 x float>*
  %wide.load56.1 = load <4 x float>, <4 x float>* %111, align 4, !tbaa !2994
  %112 = getelementptr inbounds float, float* %4, i64 %107
  %113 = bitcast float* %112 to <4 x float>*
  store <4 x float> %wide.load55.1, <4 x float>* %113, align 4, !tbaa !2997
  %114 = getelementptr inbounds float, float* %112, i64 4
  %115 = bitcast float* %114 to <4 x float>*
  store <4 x float> %wide.load56.1, <4 x float>* %115, align 4, !tbaa !2997
  %index.next52.1 = or i64 %index51, 16
  %116 = add nsw i64 %97, %index.next52.1
  %117 = getelementptr inbounds float, float* %7, i64 %116
  %118 = bitcast float* %117 to <4 x float>*
  %wide.load55.2 = load <4 x float>, <4 x float>* %118, align 4, !tbaa !2994
  %119 = getelementptr inbounds float, float* %117, i64 4
  %120 = bitcast float* %119 to <4 x float>*
  %wide.load56.2 = load <4 x float>, <4 x float>* %120, align 4, !tbaa !2994
  %121 = getelementptr inbounds float, float* %4, i64 %116
  %122 = bitcast float* %121 to <4 x float>*
  store <4 x float> %wide.load55.2, <4 x float>* %122, align 4, !tbaa !2997
  %123 = getelementptr inbounds float, float* %121, i64 4
  %124 = bitcast float* %123 to <4 x float>*
  store <4 x float> %wide.load56.2, <4 x float>* %124, align 4, !tbaa !2997
  %index.next52.2 = or i64 %index51, 24
  %125 = add nsw i64 %97, %index.next52.2
  %126 = getelementptr inbounds float, float* %7, i64 %125
  %127 = bitcast float* %126 to <4 x float>*
  %wide.load55.3 = load <4 x float>, <4 x float>* %127, align 4, !tbaa !2994
  %128 = getelementptr inbounds float, float* %126, i64 4
  %129 = bitcast float* %128 to <4 x float>*
  %wide.load56.3 = load <4 x float>, <4 x float>* %129, align 4, !tbaa !2994
  %130 = getelementptr inbounds float, float* %4, i64 %125
  %131 = bitcast float* %130 to <4 x float>*
  store <4 x float> %wide.load55.3, <4 x float>* %131, align 4, !tbaa !2997
  %132 = getelementptr inbounds float, float* %130, i64 4
  %133 = bitcast float* %132 to <4 x float>*
  store <4 x float> %wide.load56.3, <4 x float>* %133, align 4, !tbaa !2997
  %index.next52.3 = add nuw nsw i64 %index51, 32
  %134 = icmp eq i64 %index.next52.3, 512
  br i1 %134, label %for_end6.2, label %vector.body49, !prof !341, !llvm.loop !3002

for_end6.2:                                       ; preds = %vector.body49
  %135 = add nsw i64 %21, 1536
  br label %vector.body39

vector.body39:                                    ; preds = %vector.body39, %for_end6.2
  %index41 = phi i64 [ 0, %for_end6.2 ], [ %index.next42.3, %vector.body39 ]
  %136 = add nsw i64 %135, %index41
  %137 = getelementptr inbounds float, float* %7, i64 %136
  %138 = bitcast float* %137 to <4 x float>*
  %wide.load45 = load <4 x float>, <4 x float>* %138, align 4, !tbaa !2994
  %139 = getelementptr inbounds float, float* %137, i64 4
  %140 = bitcast float* %139 to <4 x float>*
  %wide.load46 = load <4 x float>, <4 x float>* %140, align 4, !tbaa !2994
  %141 = getelementptr inbounds float, float* %4, i64 %136
  %142 = bitcast float* %141 to <4 x float>*
  store <4 x float> %wide.load45, <4 x float>* %142, align 4, !tbaa !2997
  %143 = getelementptr inbounds float, float* %141, i64 4
  %144 = bitcast float* %143 to <4 x float>*
  store <4 x float> %wide.load46, <4 x float>* %144, align 4, !tbaa !2997
  %index.next42 = or i64 %index41, 8
  %145 = add nsw i64 %135, %index.next42
  %146 = getelementptr inbounds float, float* %7, i64 %145
  %147 = bitcast float* %146 to <4 x float>*
  %wide.load45.1 = load <4 x float>, <4 x float>* %147, align 4, !tbaa !2994
  %148 = getelementptr inbounds float, float* %146, i64 4
  %149 = bitcast float* %148 to <4 x float>*
  %wide.load46.1 = load <4 x float>, <4 x float>* %149, align 4, !tbaa !2994
  %150 = getelementptr inbounds float, float* %4, i64 %145
  %151 = bitcast float* %150 to <4 x float>*
  store <4 x float> %wide.load45.1, <4 x float>* %151, align 4, !tbaa !2997
  %152 = getelementptr inbounds float, float* %150, i64 4
  %153 = bitcast float* %152 to <4 x float>*
  store <4 x float> %wide.load46.1, <4 x float>* %153, align 4, !tbaa !2997
  %index.next42.1 = or i64 %index41, 16
  %154 = add nsw i64 %135, %index.next42.1
  %155 = getelementptr inbounds float, float* %7, i64 %154
  %156 = bitcast float* %155 to <4 x float>*
  %wide.load45.2 = load <4 x float>, <4 x float>* %156, align 4, !tbaa !2994
  %157 = getelementptr inbounds float, float* %155, i64 4
  %158 = bitcast float* %157 to <4 x float>*
  %wide.load46.2 = load <4 x float>, <4 x float>* %158, align 4, !tbaa !2994
  %159 = getelementptr inbounds float, float* %4, i64 %154
  %160 = bitcast float* %159 to <4 x float>*
  store <4 x float> %wide.load45.2, <4 x float>* %160, align 4, !tbaa !2997
  %161 = getelementptr inbounds float, float* %159, i64 4
  %162 = bitcast float* %161 to <4 x float>*
  store <4 x float> %wide.load46.2, <4 x float>* %162, align 4, !tbaa !2997
  %index.next42.2 = or i64 %index41, 24
  %163 = add nsw i64 %135, %index.next42.2
  %164 = getelementptr inbounds float, float* %7, i64 %163
  %165 = bitcast float* %164 to <4 x float>*
  %wide.load45.3 = load <4 x float>, <4 x float>* %165, align 4, !tbaa !2994
  %166 = getelementptr inbounds float, float* %164, i64 4
  %167 = bitcast float* %166 to <4 x float>*
  %wide.load46.3 = load <4 x float>, <4 x float>* %167, align 4, !tbaa !2994
  %168 = getelementptr inbounds float, float* %4, i64 %163
  %169 = bitcast float* %168 to <4 x float>*
  store <4 x float> %wide.load45.3, <4 x float>* %169, align 4, !tbaa !2997
  %170 = getelementptr inbounds float, float* %168, i64 4
  %171 = bitcast float* %170 to <4 x float>*
  store <4 x float> %wide.load46.3, <4 x float>* %171, align 4, !tbaa !2997
  %index.next42.3 = add nuw nsw i64 %index41, 32
  %172 = icmp eq i64 %index.next42.3, 512
  br i1 %172, label %for_end6.3, label %vector.body39, !prof !341, !llvm.loop !3003

for_end6.3:                                       ; preds = %vector.body39
  %173 = add nsw i64 %21, 2048
  br label %vector.body29

vector.body29:                                    ; preds = %vector.body29, %for_end6.3
  %index31 = phi i64 [ 0, %for_end6.3 ], [ %index.next32.3, %vector.body29 ]
  %174 = add nsw i64 %173, %index31
  %175 = getelementptr inbounds float, float* %7, i64 %174
  %176 = bitcast float* %175 to <4 x float>*
  %wide.load35 = load <4 x float>, <4 x float>* %176, align 4, !tbaa !2994
  %177 = getelementptr inbounds float, float* %175, i64 4
  %178 = bitcast float* %177 to <4 x float>*
  %wide.load36 = load <4 x float>, <4 x float>* %178, align 4, !tbaa !2994
  %179 = getelementptr inbounds float, float* %4, i64 %174
  %180 = bitcast float* %179 to <4 x float>*
  store <4 x float> %wide.load35, <4 x float>* %180, align 4, !tbaa !2997
  %181 = getelementptr inbounds float, float* %179, i64 4
  %182 = bitcast float* %181 to <4 x float>*
  store <4 x float> %wide.load36, <4 x float>* %182, align 4, !tbaa !2997
  %index.next32 = or i64 %index31, 8
  %183 = add nsw i64 %173, %index.next32
  %184 = getelementptr inbounds float, float* %7, i64 %183
  %185 = bitcast float* %184 to <4 x float>*
  %wide.load35.1 = load <4 x float>, <4 x float>* %185, align 4, !tbaa !2994
  %186 = getelementptr inbounds float, float* %184, i64 4
  %187 = bitcast float* %186 to <4 x float>*
  %wide.load36.1 = load <4 x float>, <4 x float>* %187, align 4, !tbaa !2994
  %188 = getelementptr inbounds float, float* %4, i64 %183
  %189 = bitcast float* %188 to <4 x float>*
  store <4 x float> %wide.load35.1, <4 x float>* %189, align 4, !tbaa !2997
  %190 = getelementptr inbounds float, float* %188, i64 4
  %191 = bitcast float* %190 to <4 x float>*
  store <4 x float> %wide.load36.1, <4 x float>* %191, align 4, !tbaa !2997
  %index.next32.1 = or i64 %index31, 16
  %192 = add nsw i64 %173, %index.next32.1
  %193 = getelementptr inbounds float, float* %7, i64 %192
  %194 = bitcast float* %193 to <4 x float>*
  %wide.load35.2 = load <4 x float>, <4 x float>* %194, align 4, !tbaa !2994
  %195 = getelementptr inbounds float, float* %193, i64 4
  %196 = bitcast float* %195 to <4 x float>*
  %wide.load36.2 = load <4 x float>, <4 x float>* %196, align 4, !tbaa !2994
  %197 = getelementptr inbounds float, float* %4, i64 %192
  %198 = bitcast float* %197 to <4 x float>*
  store <4 x float> %wide.load35.2, <4 x float>* %198, align 4, !tbaa !2997
  %199 = getelementptr inbounds float, float* %197, i64 4
  %200 = bitcast float* %199 to <4 x float>*
  store <4 x float> %wide.load36.2, <4 x float>* %200, align 4, !tbaa !2997
  %index.next32.2 = or i64 %index31, 24
  %201 = add nsw i64 %173, %index.next32.2
  %202 = getelementptr inbounds float, float* %7, i64 %201
  %203 = bitcast float* %202 to <4 x float>*
  %wide.load35.3 = load <4 x float>, <4 x float>* %203, align 4, !tbaa !2994
  %204 = getelementptr inbounds float, float* %202, i64 4
  %205 = bitcast float* %204 to <4 x float>*
  %wide.load36.3 = load <4 x float>, <4 x float>* %205, align 4, !tbaa !2994
  %206 = getelementptr inbounds float, float* %4, i64 %201
  %207 = bitcast float* %206 to <4 x float>*
  store <4 x float> %wide.load35.3, <4 x float>* %207, align 4, !tbaa !2997
  %208 = getelementptr inbounds float, float* %206, i64 4
  %209 = bitcast float* %208 to <4 x float>*
  store <4 x float> %wide.load36.3, <4 x float>* %209, align 4, !tbaa !2997
  %index.next32.3 = add nuw nsw i64 %index31, 32
  %210 = icmp eq i64 %index.next32.3, 512
  br i1 %210, label %for_end6.4, label %vector.body29, !prof !341, !llvm.loop !3004

for_end6.4:                                       ; preds = %vector.body29
  %211 = add nsw i64 %21, 2560
  br label %vector.body19

vector.body19:                                    ; preds = %vector.body19, %for_end6.4
  %index21 = phi i64 [ 0, %for_end6.4 ], [ %index.next22.3, %vector.body19 ]
  %212 = add nsw i64 %211, %index21
  %213 = getelementptr inbounds float, float* %7, i64 %212
  %214 = bitcast float* %213 to <4 x float>*
  %wide.load25 = load <4 x float>, <4 x float>* %214, align 4, !tbaa !2994
  %215 = getelementptr inbounds float, float* %213, i64 4
  %216 = bitcast float* %215 to <4 x float>*
  %wide.load26 = load <4 x float>, <4 x float>* %216, align 4, !tbaa !2994
  %217 = getelementptr inbounds float, float* %4, i64 %212
  %218 = bitcast float* %217 to <4 x float>*
  store <4 x float> %wide.load25, <4 x float>* %218, align 4, !tbaa !2997
  %219 = getelementptr inbounds float, float* %217, i64 4
  %220 = bitcast float* %219 to <4 x float>*
  store <4 x float> %wide.load26, <4 x float>* %220, align 4, !tbaa !2997
  %index.next22 = or i64 %index21, 8
  %221 = add nsw i64 %211, %index.next22
  %222 = getelementptr inbounds float, float* %7, i64 %221
  %223 = bitcast float* %222 to <4 x float>*
  %wide.load25.1 = load <4 x float>, <4 x float>* %223, align 4, !tbaa !2994
  %224 = getelementptr inbounds float, float* %222, i64 4
  %225 = bitcast float* %224 to <4 x float>*
  %wide.load26.1 = load <4 x float>, <4 x float>* %225, align 4, !tbaa !2994
  %226 = getelementptr inbounds float, float* %4, i64 %221
  %227 = bitcast float* %226 to <4 x float>*
  store <4 x float> %wide.load25.1, <4 x float>* %227, align 4, !tbaa !2997
  %228 = getelementptr inbounds float, float* %226, i64 4
  %229 = bitcast float* %228 to <4 x float>*
  store <4 x float> %wide.load26.1, <4 x float>* %229, align 4, !tbaa !2997
  %index.next22.1 = or i64 %index21, 16
  %230 = add nsw i64 %211, %index.next22.1
  %231 = getelementptr inbounds float, float* %7, i64 %230
  %232 = bitcast float* %231 to <4 x float>*
  %wide.load25.2 = load <4 x float>, <4 x float>* %232, align 4, !tbaa !2994
  %233 = getelementptr inbounds float, float* %231, i64 4
  %234 = bitcast float* %233 to <4 x float>*
  %wide.load26.2 = load <4 x float>, <4 x float>* %234, align 4, !tbaa !2994
  %235 = getelementptr inbounds float, float* %4, i64 %230
  %236 = bitcast float* %235 to <4 x float>*
  store <4 x float> %wide.load25.2, <4 x float>* %236, align 4, !tbaa !2997
  %237 = getelementptr inbounds float, float* %235, i64 4
  %238 = bitcast float* %237 to <4 x float>*
  store <4 x float> %wide.load26.2, <4 x float>* %238, align 4, !tbaa !2997
  %index.next22.2 = or i64 %index21, 24
  %239 = add nsw i64 %211, %index.next22.2
  %240 = getelementptr inbounds float, float* %7, i64 %239
  %241 = bitcast float* %240 to <4 x float>*
  %wide.load25.3 = load <4 x float>, <4 x float>* %241, align 4, !tbaa !2994
  %242 = getelementptr inbounds float, float* %240, i64 4
  %243 = bitcast float* %242 to <4 x float>*
  %wide.load26.3 = load <4 x float>, <4 x float>* %243, align 4, !tbaa !2994
  %244 = getelementptr inbounds float, float* %4, i64 %239
  %245 = bitcast float* %244 to <4 x float>*
  store <4 x float> %wide.load25.3, <4 x float>* %245, align 4, !tbaa !2997
  %246 = getelementptr inbounds float, float* %244, i64 4
  %247 = bitcast float* %246 to <4 x float>*
  store <4 x float> %wide.load26.3, <4 x float>* %247, align 4, !tbaa !2997
  %index.next22.3 = add nuw nsw i64 %index21, 32
  %248 = icmp eq i64 %index.next22.3, 512
  br i1 %248, label %for_end6.5, label %vector.body19, !prof !341, !llvm.loop !3005

for_end6.5:                                       ; preds = %vector.body19
  %249 = add nsw i64 %21, 3072
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_end6.5
  %index = phi i64 [ 0, %for_end6.5 ], [ %index.next.3, %vector.body ]
  %250 = add nsw i64 %249, %index
  %251 = getelementptr inbounds float, float* %7, i64 %250
  %252 = bitcast float* %251 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %252, align 4, !tbaa !2994
  %253 = getelementptr inbounds float, float* %251, i64 4
  %254 = bitcast float* %253 to <4 x float>*
  %wide.load16 = load <4 x float>, <4 x float>* %254, align 4, !tbaa !2994
  %255 = getelementptr inbounds float, float* %4, i64 %250
  %256 = bitcast float* %255 to <4 x float>*
  store <4 x float> %wide.load, <4 x float>* %256, align 4, !tbaa !2997
  %257 = getelementptr inbounds float, float* %255, i64 4
  %258 = bitcast float* %257 to <4 x float>*
  store <4 x float> %wide.load16, <4 x float>* %258, align 4, !tbaa !2997
  %index.next = or i64 %index, 8
  %259 = add nsw i64 %249, %index.next
  %260 = getelementptr inbounds float, float* %7, i64 %259
  %261 = bitcast float* %260 to <4 x float>*
  %wide.load.1 = load <4 x float>, <4 x float>* %261, align 4, !tbaa !2994
  %262 = getelementptr inbounds float, float* %260, i64 4
  %263 = bitcast float* %262 to <4 x float>*
  %wide.load16.1 = load <4 x float>, <4 x float>* %263, align 4, !tbaa !2994
  %264 = getelementptr inbounds float, float* %4, i64 %259
  %265 = bitcast float* %264 to <4 x float>*
  store <4 x float> %wide.load.1, <4 x float>* %265, align 4, !tbaa !2997
  %266 = getelementptr inbounds float, float* %264, i64 4
  %267 = bitcast float* %266 to <4 x float>*
  store <4 x float> %wide.load16.1, <4 x float>* %267, align 4, !tbaa !2997
  %index.next.1 = or i64 %index, 16
  %268 = add nsw i64 %249, %index.next.1
  %269 = getelementptr inbounds float, float* %7, i64 %268
  %270 = bitcast float* %269 to <4 x float>*
  %wide.load.2 = load <4 x float>, <4 x float>* %270, align 4, !tbaa !2994
  %271 = getelementptr inbounds float, float* %269, i64 4
  %272 = bitcast float* %271 to <4 x float>*
  %wide.load16.2 = load <4 x float>, <4 x float>* %272, align 4, !tbaa !2994
  %273 = getelementptr inbounds float, float* %4, i64 %268
  %274 = bitcast float* %273 to <4 x float>*
  store <4 x float> %wide.load.2, <4 x float>* %274, align 4, !tbaa !2997
  %275 = getelementptr inbounds float, float* %273, i64 4
  %276 = bitcast float* %275 to <4 x float>*
  store <4 x float> %wide.load16.2, <4 x float>* %276, align 4, !tbaa !2997
  %index.next.2 = or i64 %index, 24
  %277 = add nsw i64 %249, %index.next.2
  %278 = getelementptr inbounds float, float* %7, i64 %277
  %279 = bitcast float* %278 to <4 x float>*
  %wide.load.3 = load <4 x float>, <4 x float>* %279, align 4, !tbaa !2994
  %280 = getelementptr inbounds float, float* %278, i64 4
  %281 = bitcast float* %280 to <4 x float>*
  %wide.load16.3 = load <4 x float>, <4 x float>* %281, align 4, !tbaa !2994
  %282 = getelementptr inbounds float, float* %4, i64 %277
  %283 = bitcast float* %282 to <4 x float>*
  store <4 x float> %wide.load.3, <4 x float>* %283, align 4, !tbaa !2997
  %284 = getelementptr inbounds float, float* %282, i64 4
  %285 = bitcast float* %284 to <4 x float>*
  store <4 x float> %wide.load16.3, <4 x float>* %285, align 4, !tbaa !2997
  %index.next.3 = add nuw nsw i64 %index, 32
  %286 = icmp eq i64 %index.next.3, 512
  br i1 %286, label %for_end6.6, label %vector.body, !prof !341, !llvm.loop !3006

for_end6.6:                                       ; preds = %vector.body
  %indvars.iv.next14 = add nsw i64 %indvars.iv13, 1
  %exitcond15.not = icmp eq i64 %indvars.iv.next14, %wide.trip.count
  br i1 %exitcond15.not, label %for_end, label %for_begin1.preheader, !prof !51
}

; Function Attrs: nofree nounwind
define private i32 @__tvm_parallel_lambda.230(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #3 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds i8, i8* %2, i64 16
  %9 = bitcast i8* %8 to float**
  %10 = load float*, float** %9, align 8
  %11 = getelementptr inbounds i8, i8* %2, i64 24
  %12 = bitcast i8* %11 to float**
  %13 = load float*, float** %12, align 8
  %14 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %15 = load i32, i32* %14, align 4
  %16 = add nsw i32 %15, 48
  %17 = sdiv i32 %16, %15
  %18 = add nsw i32 %0, 1
  %19 = mul nsw i32 %17, %18
  %20 = icmp slt i32 %19, 49
  %21 = select i1 %20, i32 %19, i32 49
  %22 = mul nsw i32 %17, %0
  %23 = icmp slt i32 %22, 49
  %24 = select i1 %23, i32 %22, i32 49
  %25 = icmp slt i32 %24, %21
  br i1 %25, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %26 = sext i32 %24 to i64
  %wide.trip.count = sext i32 %21 to i64
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end3
  %indvars.iv14 = phi i64 [ %26, %for_begin1.preheader.preheader ], [ %indvars.iv.next15, %for_end3 ]
  %27 = phi i32 [ %24, %for_begin1.preheader.preheader ], [ %34, %for_end3 ]
  %28 = shl nsw i32 %27, 9
  %29 = trunc i64 %indvars.iv14 to i32
  %30 = shl nsw i32 %29, 10
  %31 = sext i32 %28 to i64
  %32 = sext i32 %30 to i64
  br label %for_begin4.preheader

for_end:                                          ; preds = %for_end3, %entry
  ret i32 0

for_begin4.preheader:                             ; preds = %for_begin1.preheader, %for_end6
  %indvars.iv11 = phi i64 [ 0, %for_begin1.preheader ], [ %indvars.iv.next12, %for_end6 ]
  %33 = shl nsw i64 %indvars.iv11, 6
  br label %for_body5

for_end3:                                         ; preds = %for_end6
  %indvars.iv.next15 = add nsw i64 %indvars.iv14, 1
  %34 = add nsw i32 %27, 1
  %exitcond16.not = icmp eq i64 %indvars.iv.next15, %wide.trip.count
  br i1 %exitcond16.not, label %for_end, label %for_begin1.preheader, !prof !51

for_body5:                                        ; preds = %for_body5, %for_begin4.preheader
  %indvars.iv = phi i64 [ 0, %for_begin4.preheader ], [ %indvars.iv.next.1, %for_body5 ]
  %.010 = phi <64 x float> [ zeroinitializer, %for_begin4.preheader ], [ %56, %for_body5 ]
  %35 = add nsw i64 %indvars.iv, %31
  %36 = getelementptr inbounds float, float* %4, i64 %35
  %37 = load float, float* %36, align 4, !tbaa !2997
  %38 = insertelement <64 x float> undef, float %37, i32 0
  %39 = shufflevector <64 x float> %38, <64 x float> undef, <64 x i32> zeroinitializer
  %40 = shl nuw nsw i64 %indvars.iv, 10
  %41 = add nuw nsw i64 %40, %33
  %42 = getelementptr inbounds float, float* %7, i64 %41
  %43 = bitcast float* %42 to <64 x float>*
  %44 = load <64 x float>, <64 x float>* %43, align 128, !tbaa !3007
  %45 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %39, <64 x float> %44, <64 x float> %.010)
  %indvars.iv.next = or i64 %indvars.iv, 1
  %46 = add nsw i64 %indvars.iv.next, %31
  %47 = getelementptr inbounds float, float* %4, i64 %46
  %48 = load float, float* %47, align 4, !tbaa !2997
  %49 = insertelement <64 x float> undef, float %48, i32 0
  %50 = shufflevector <64 x float> %49, <64 x float> undef, <64 x i32> zeroinitializer
  %51 = shl nuw nsw i64 %indvars.iv.next, 10
  %52 = add nuw nsw i64 %51, %33
  %53 = getelementptr inbounds float, float* %7, i64 %52
  %54 = bitcast float* %53 to <64 x float>*
  %55 = load <64 x float>, <64 x float>* %54, align 128, !tbaa !3007
  %56 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %50, <64 x float> %55, <64 x float> %45)
  %indvars.iv.next.1 = add nuw nsw i64 %indvars.iv, 2
  %exitcond.not.1 = icmp eq i64 %indvars.iv.next.1, 512
  br i1 %exitcond.not.1, label %for_end6, label %for_body5, !prof !51

for_end6:                                         ; preds = %for_body5
  %57 = add nuw nsw i64 %33, %32
  %58 = getelementptr inbounds float, float* %13, i64 %33
  %59 = bitcast float* %58 to <64 x float>*
  %60 = load <64 x float>, <64 x float>* %59, align 128, !tbaa !3010
  %61 = fadd <64 x float> %56, %60
  %62 = fcmp olt <64 x float> %61, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %63 = select <64 x i1> %62, <64 x float> %61, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %64 = fcmp ogt <64 x float> %63, zeroinitializer
  %65 = select <64 x i1> %64, <64 x float> %63, <64 x float> zeroinitializer
  %66 = getelementptr inbounds float, float* %10, i64 %57
  %67 = bitcast float* %66 to <64 x float>*
  store <64 x float> %65, <64 x float>* %67, align 128, !tbaa !3013
  %indvars.iv.next12 = add nuw nsw i64 %indvars.iv11, 1
  %exitcond13.not = icmp eq i64 %indvars.iv.next12, 16
  br i1 %exitcond13.not, label %for_end3, label %for_begin4.preheader, !prof !51
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_6(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.231, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !3016
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !3030
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !3032
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !3035
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.232, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.233, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.234, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.235, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !3037
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !3051
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 14
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.47, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !3053
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 14
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.48, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !3056
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 256
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.92, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !3058
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 50176, i32 3584, i32 256, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.236, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !3070
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 1
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.16, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !3084
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 1
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.17, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !3086
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 256
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.94, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !3089
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 512
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.181, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !3091
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 131072, i32 131072, i32 512, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([204 x i8], [204 x i8]* @.str.237, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !3103
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 512
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.56, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !3117
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !3131
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !3145
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 14
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.57, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !3147
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 14
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.58, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !3150
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 512
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.59, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !3152
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 100352, i32 7168, i32 512, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.60, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_6_compute_(i8* %31, i8* %39, i8* %51, i8* %45, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_6_compute_(i8* noalias align 128 %0, i8* noalias align 128 %1, i8* noalias align 128 %2, i8* noalias align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 200704, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %call_end, %if_end, %entry
  %merge = phi i32 [ -1, %entry ], [ %13, %if_end ], [ %22, %call_end ]
  ret i32 %merge

if_end:                                           ; preds = %entry
  %8 = alloca %22, align 8
  %9 = getelementptr inbounds %22, %22* %8, i64 0, i32 0
  store i8* %6, i8** %9, align 8
  %10 = getelementptr inbounds %22, %22* %8, i64 0, i32 1
  store i8* %0, i8** %10, align 8
  %11 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %12 = bitcast %22* %8 to i8*
  %13 = call i32 %11(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.238, i8* nonnull %12, i32 0)
  %14 = icmp eq i32 %13, 0
  br i1 %14, label %call_end, label %if_then, !prof !5

call_end:                                         ; preds = %if_end
  %15 = alloca %23, align 8
  %16 = getelementptr inbounds %23, %23* %15, i64 0, i32 0
  store i8* %6, i8** %16, align 8
  %17 = getelementptr inbounds %23, %23* %15, i64 0, i32 1
  store i8* %1, i8** %17, align 8
  %18 = getelementptr inbounds %23, %23* %15, i64 0, i32 2
  store i8* %2, i8** %18, align 8
  %19 = getelementptr inbounds %23, %23* %15, i64 0, i32 3
  store i8* %3, i8** %19, align 8
  %20 = load i32 (i32 (i32, %0*, i8*)*, i8*, i32)*, i32 (i32 (i32, %0*, i8*)*, i8*, i32)** @__TVMBackendParallelLaunch, align 8, !tbaa !6
  %21 = bitcast %23* %15 to i8*
  %22 = call i32 %20(i32 (i32, %0*, i8*)* nonnull @__tvm_parallel_lambda.239, i8* nonnull %21, i32 0)
  %23 = icmp eq i32 %22, 0
  br i1 %23, label %call_end2, label %if_then, !prof !5

call_end2:                                        ; preds = %call_end
  %24 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %25 = call i32 %24(i32 1, i32 %4, i8* nonnull %6)
  %.not = icmp ne i32 %25, 0
  %spec.select = sext i1 %.not to i32
  ret i32 %spec.select
}

; Function Attrs: nofree norecurse nounwind
define private i32 @__tvm_parallel_lambda.238(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #2 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %9 = load i32, i32* %8, align 4
  %10 = add nsw i32 %9, 13
  %11 = sdiv i32 %10, %9
  %12 = add nsw i32 %0, 1
  %13 = mul nsw i32 %11, %12
  %14 = icmp slt i32 %13, 14
  %15 = select i1 %14, i32 %13, i32 14
  %16 = mul nsw i32 %11, %0
  %17 = icmp slt i32 %16, 14
  %18 = select i1 %17, i32 %16, i32 14
  %19 = icmp slt i32 %18, %15
  br i1 %19, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %20 = sext i32 %18 to i64
  %wide.trip.count = sext i32 %15 to i64
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_begin1.preheader
  %indvars.iv13 = phi i64 [ %20, %for_begin1.preheader.preheader ], [ %indvars.iv.next14, %for_begin1.preheader ]
  %21 = mul nsw i64 %indvars.iv13, 3584
  %22 = getelementptr inbounds float, float* %7, i64 %21
  %23 = bitcast float* %22 to <4 x float>*
  %wide.load145 = load <4 x float>, <4 x float>* %23, align 4, !tbaa !3164
  %24 = getelementptr inbounds float, float* %22, i64 4
  %25 = bitcast float* %24 to <4 x float>*
  %wide.load146 = load <4 x float>, <4 x float>* %25, align 4, !tbaa !3164
  %26 = getelementptr inbounds float, float* %4, i64 %21
  %27 = bitcast float* %26 to <4 x float>*
  store <4 x float> %wide.load145, <4 x float>* %27, align 4, !tbaa !3167
  %28 = getelementptr inbounds float, float* %26, i64 4
  %29 = bitcast float* %28 to <4 x float>*
  store <4 x float> %wide.load146, <4 x float>* %29, align 4, !tbaa !3167
  %30 = or i64 %21, 8
  %31 = getelementptr inbounds float, float* %7, i64 %30
  %32 = bitcast float* %31 to <4 x float>*
  %wide.load145.1 = load <4 x float>, <4 x float>* %32, align 4, !tbaa !3164
  %33 = getelementptr inbounds float, float* %31, i64 4
  %34 = bitcast float* %33 to <4 x float>*
  %wide.load146.1 = load <4 x float>, <4 x float>* %34, align 4, !tbaa !3164
  %35 = getelementptr inbounds float, float* %4, i64 %30
  %36 = bitcast float* %35 to <4 x float>*
  store <4 x float> %wide.load145.1, <4 x float>* %36, align 4, !tbaa !3167
  %37 = getelementptr inbounds float, float* %35, i64 4
  %38 = bitcast float* %37 to <4 x float>*
  store <4 x float> %wide.load146.1, <4 x float>* %38, align 4, !tbaa !3167
  %39 = or i64 %21, 16
  %40 = getelementptr inbounds float, float* %7, i64 %39
  %41 = bitcast float* %40 to <4 x float>*
  %wide.load145.2 = load <4 x float>, <4 x float>* %41, align 4, !tbaa !3164
  %42 = getelementptr inbounds float, float* %40, i64 4
  %43 = bitcast float* %42 to <4 x float>*
  %wide.load146.2 = load <4 x float>, <4 x float>* %43, align 4, !tbaa !3164
  %44 = getelementptr inbounds float, float* %4, i64 %39
  %45 = bitcast float* %44 to <4 x float>*
  store <4 x float> %wide.load145.2, <4 x float>* %45, align 4, !tbaa !3167
  %46 = getelementptr inbounds float, float* %44, i64 4
  %47 = bitcast float* %46 to <4 x float>*
  store <4 x float> %wide.load146.2, <4 x float>* %47, align 4, !tbaa !3167
  %48 = or i64 %21, 24
  %49 = getelementptr inbounds float, float* %7, i64 %48
  %50 = bitcast float* %49 to <4 x float>*
  %wide.load145.3 = load <4 x float>, <4 x float>* %50, align 4, !tbaa !3164
  %51 = getelementptr inbounds float, float* %49, i64 4
  %52 = bitcast float* %51 to <4 x float>*
  %wide.load146.3 = load <4 x float>, <4 x float>* %52, align 4, !tbaa !3164
  %53 = getelementptr inbounds float, float* %4, i64 %48
  %54 = bitcast float* %53 to <4 x float>*
  store <4 x float> %wide.load145.3, <4 x float>* %54, align 4, !tbaa !3167
  %55 = getelementptr inbounds float, float* %53, i64 4
  %56 = bitcast float* %55 to <4 x float>*
  store <4 x float> %wide.load146.3, <4 x float>* %56, align 4, !tbaa !3167
  %57 = or i64 %21, 32
  %58 = getelementptr inbounds float, float* %7, i64 %57
  %59 = bitcast float* %58 to <4 x float>*
  %wide.load145.4 = load <4 x float>, <4 x float>* %59, align 4, !tbaa !3164
  %60 = getelementptr inbounds float, float* %58, i64 4
  %61 = bitcast float* %60 to <4 x float>*
  %wide.load146.4 = load <4 x float>, <4 x float>* %61, align 4, !tbaa !3164
  %62 = getelementptr inbounds float, float* %4, i64 %57
  %63 = bitcast float* %62 to <4 x float>*
  store <4 x float> %wide.load145.4, <4 x float>* %63, align 4, !tbaa !3167
  %64 = getelementptr inbounds float, float* %62, i64 4
  %65 = bitcast float* %64 to <4 x float>*
  store <4 x float> %wide.load146.4, <4 x float>* %65, align 4, !tbaa !3167
  %66 = or i64 %21, 40
  %67 = getelementptr inbounds float, float* %7, i64 %66
  %68 = bitcast float* %67 to <4 x float>*
  %wide.load145.5 = load <4 x float>, <4 x float>* %68, align 4, !tbaa !3164
  %69 = getelementptr inbounds float, float* %67, i64 4
  %70 = bitcast float* %69 to <4 x float>*
  %wide.load146.5 = load <4 x float>, <4 x float>* %70, align 4, !tbaa !3164
  %71 = getelementptr inbounds float, float* %4, i64 %66
  %72 = bitcast float* %71 to <4 x float>*
  store <4 x float> %wide.load145.5, <4 x float>* %72, align 4, !tbaa !3167
  %73 = getelementptr inbounds float, float* %71, i64 4
  %74 = bitcast float* %73 to <4 x float>*
  store <4 x float> %wide.load146.5, <4 x float>* %74, align 4, !tbaa !3167
  %75 = or i64 %21, 48
  %76 = getelementptr inbounds float, float* %7, i64 %75
  %77 = bitcast float* %76 to <4 x float>*
  %wide.load145.6 = load <4 x float>, <4 x float>* %77, align 4, !tbaa !3164
  %78 = getelementptr inbounds float, float* %76, i64 4
  %79 = bitcast float* %78 to <4 x float>*
  %wide.load146.6 = load <4 x float>, <4 x float>* %79, align 4, !tbaa !3164
  %80 = getelementptr inbounds float, float* %4, i64 %75
  %81 = bitcast float* %80 to <4 x float>*
  store <4 x float> %wide.load145.6, <4 x float>* %81, align 4, !tbaa !3167
  %82 = getelementptr inbounds float, float* %80, i64 4
  %83 = bitcast float* %82 to <4 x float>*
  store <4 x float> %wide.load146.6, <4 x float>* %83, align 4, !tbaa !3167
  %84 = or i64 %21, 56
  %85 = getelementptr inbounds float, float* %7, i64 %84
  %86 = bitcast float* %85 to <4 x float>*
  %wide.load145.7 = load <4 x float>, <4 x float>* %86, align 4, !tbaa !3164
  %87 = getelementptr inbounds float, float* %85, i64 4
  %88 = bitcast float* %87 to <4 x float>*
  %wide.load146.7 = load <4 x float>, <4 x float>* %88, align 4, !tbaa !3164
  %89 = getelementptr inbounds float, float* %4, i64 %84
  %90 = bitcast float* %89 to <4 x float>*
  store <4 x float> %wide.load145.7, <4 x float>* %90, align 4, !tbaa !3167
  %91 = getelementptr inbounds float, float* %89, i64 4
  %92 = bitcast float* %91 to <4 x float>*
  store <4 x float> %wide.load146.7, <4 x float>* %92, align 4, !tbaa !3167
  %93 = or i64 %21, 64
  %94 = getelementptr inbounds float, float* %7, i64 %93
  %95 = bitcast float* %94 to <4 x float>*
  %wide.load145.8 = load <4 x float>, <4 x float>* %95, align 4, !tbaa !3164
  %96 = getelementptr inbounds float, float* %94, i64 4
  %97 = bitcast float* %96 to <4 x float>*
  %wide.load146.8 = load <4 x float>, <4 x float>* %97, align 4, !tbaa !3164
  %98 = getelementptr inbounds float, float* %4, i64 %93
  %99 = bitcast float* %98 to <4 x float>*
  store <4 x float> %wide.load145.8, <4 x float>* %99, align 4, !tbaa !3167
  %100 = getelementptr inbounds float, float* %98, i64 4
  %101 = bitcast float* %100 to <4 x float>*
  store <4 x float> %wide.load146.8, <4 x float>* %101, align 4, !tbaa !3167
  %102 = or i64 %21, 72
  %103 = getelementptr inbounds float, float* %7, i64 %102
  %104 = bitcast float* %103 to <4 x float>*
  %wide.load145.9 = load <4 x float>, <4 x float>* %104, align 4, !tbaa !3164
  %105 = getelementptr inbounds float, float* %103, i64 4
  %106 = bitcast float* %105 to <4 x float>*
  %wide.load146.9 = load <4 x float>, <4 x float>* %106, align 4, !tbaa !3164
  %107 = getelementptr inbounds float, float* %4, i64 %102
  %108 = bitcast float* %107 to <4 x float>*
  store <4 x float> %wide.load145.9, <4 x float>* %108, align 4, !tbaa !3167
  %109 = getelementptr inbounds float, float* %107, i64 4
  %110 = bitcast float* %109 to <4 x float>*
  store <4 x float> %wide.load146.9, <4 x float>* %110, align 4, !tbaa !3167
  %111 = or i64 %21, 80
  %112 = getelementptr inbounds float, float* %7, i64 %111
  %113 = bitcast float* %112 to <4 x float>*
  %wide.load145.10 = load <4 x float>, <4 x float>* %113, align 4, !tbaa !3164
  %114 = getelementptr inbounds float, float* %112, i64 4
  %115 = bitcast float* %114 to <4 x float>*
  %wide.load146.10 = load <4 x float>, <4 x float>* %115, align 4, !tbaa !3164
  %116 = getelementptr inbounds float, float* %4, i64 %111
  %117 = bitcast float* %116 to <4 x float>*
  store <4 x float> %wide.load145.10, <4 x float>* %117, align 4, !tbaa !3167
  %118 = getelementptr inbounds float, float* %116, i64 4
  %119 = bitcast float* %118 to <4 x float>*
  store <4 x float> %wide.load146.10, <4 x float>* %119, align 4, !tbaa !3167
  %120 = or i64 %21, 88
  %121 = getelementptr inbounds float, float* %7, i64 %120
  %122 = bitcast float* %121 to <4 x float>*
  %wide.load145.11 = load <4 x float>, <4 x float>* %122, align 4, !tbaa !3164
  %123 = getelementptr inbounds float, float* %121, i64 4
  %124 = bitcast float* %123 to <4 x float>*
  %wide.load146.11 = load <4 x float>, <4 x float>* %124, align 4, !tbaa !3164
  %125 = getelementptr inbounds float, float* %4, i64 %120
  %126 = bitcast float* %125 to <4 x float>*
  store <4 x float> %wide.load145.11, <4 x float>* %126, align 4, !tbaa !3167
  %127 = getelementptr inbounds float, float* %125, i64 4
  %128 = bitcast float* %127 to <4 x float>*
  store <4 x float> %wide.load146.11, <4 x float>* %128, align 4, !tbaa !3167
  %129 = or i64 %21, 96
  %130 = getelementptr inbounds float, float* %7, i64 %129
  %131 = bitcast float* %130 to <4 x float>*
  %wide.load145.12 = load <4 x float>, <4 x float>* %131, align 4, !tbaa !3164
  %132 = getelementptr inbounds float, float* %130, i64 4
  %133 = bitcast float* %132 to <4 x float>*
  %wide.load146.12 = load <4 x float>, <4 x float>* %133, align 4, !tbaa !3164
  %134 = getelementptr inbounds float, float* %4, i64 %129
  %135 = bitcast float* %134 to <4 x float>*
  store <4 x float> %wide.load145.12, <4 x float>* %135, align 4, !tbaa !3167
  %136 = getelementptr inbounds float, float* %134, i64 4
  %137 = bitcast float* %136 to <4 x float>*
  store <4 x float> %wide.load146.12, <4 x float>* %137, align 4, !tbaa !3167
  %138 = or i64 %21, 104
  %139 = getelementptr inbounds float, float* %7, i64 %138
  %140 = bitcast float* %139 to <4 x float>*
  %wide.load145.13 = load <4 x float>, <4 x float>* %140, align 4, !tbaa !3164
  %141 = getelementptr inbounds float, float* %139, i64 4
  %142 = bitcast float* %141 to <4 x float>*
  %wide.load146.13 = load <4 x float>, <4 x float>* %142, align 4, !tbaa !3164
  %143 = getelementptr inbounds float, float* %4, i64 %138
  %144 = bitcast float* %143 to <4 x float>*
  store <4 x float> %wide.load145.13, <4 x float>* %144, align 4, !tbaa !3167
  %145 = getelementptr inbounds float, float* %143, i64 4
  %146 = bitcast float* %145 to <4 x float>*
  store <4 x float> %wide.load146.13, <4 x float>* %146, align 4, !tbaa !3167
  %147 = or i64 %21, 112
  %148 = getelementptr inbounds float, float* %7, i64 %147
  %149 = bitcast float* %148 to <4 x float>*
  %wide.load145.14 = load <4 x float>, <4 x float>* %149, align 4, !tbaa !3164
  %150 = getelementptr inbounds float, float* %148, i64 4
  %151 = bitcast float* %150 to <4 x float>*
  %wide.load146.14 = load <4 x float>, <4 x float>* %151, align 4, !tbaa !3164
  %152 = getelementptr inbounds float, float* %4, i64 %147
  %153 = bitcast float* %152 to <4 x float>*
  store <4 x float> %wide.load145.14, <4 x float>* %153, align 4, !tbaa !3167
  %154 = getelementptr inbounds float, float* %152, i64 4
  %155 = bitcast float* %154 to <4 x float>*
  store <4 x float> %wide.load146.14, <4 x float>* %155, align 4, !tbaa !3167
  %156 = or i64 %21, 120
  %157 = getelementptr inbounds float, float* %7, i64 %156
  %158 = bitcast float* %157 to <4 x float>*
  %wide.load145.15 = load <4 x float>, <4 x float>* %158, align 4, !tbaa !3164
  %159 = getelementptr inbounds float, float* %157, i64 4
  %160 = bitcast float* %159 to <4 x float>*
  %wide.load146.15 = load <4 x float>, <4 x float>* %160, align 4, !tbaa !3164
  %161 = getelementptr inbounds float, float* %4, i64 %156
  %162 = bitcast float* %161 to <4 x float>*
  store <4 x float> %wide.load145.15, <4 x float>* %162, align 4, !tbaa !3167
  %163 = getelementptr inbounds float, float* %161, i64 4
  %164 = bitcast float* %163 to <4 x float>*
  store <4 x float> %wide.load146.15, <4 x float>* %164, align 4, !tbaa !3167
  %165 = or i64 %21, 128
  %166 = getelementptr inbounds float, float* %7, i64 %165
  %167 = bitcast float* %166 to <4 x float>*
  %wide.load145.16 = load <4 x float>, <4 x float>* %167, align 4, !tbaa !3164
  %168 = getelementptr inbounds float, float* %166, i64 4
  %169 = bitcast float* %168 to <4 x float>*
  %wide.load146.16 = load <4 x float>, <4 x float>* %169, align 4, !tbaa !3164
  %170 = getelementptr inbounds float, float* %4, i64 %165
  %171 = bitcast float* %170 to <4 x float>*
  store <4 x float> %wide.load145.16, <4 x float>* %171, align 4, !tbaa !3167
  %172 = getelementptr inbounds float, float* %170, i64 4
  %173 = bitcast float* %172 to <4 x float>*
  store <4 x float> %wide.load146.16, <4 x float>* %173, align 4, !tbaa !3167
  %174 = or i64 %21, 136
  %175 = getelementptr inbounds float, float* %7, i64 %174
  %176 = bitcast float* %175 to <4 x float>*
  %wide.load145.17 = load <4 x float>, <4 x float>* %176, align 4, !tbaa !3164
  %177 = getelementptr inbounds float, float* %175, i64 4
  %178 = bitcast float* %177 to <4 x float>*
  %wide.load146.17 = load <4 x float>, <4 x float>* %178, align 4, !tbaa !3164
  %179 = getelementptr inbounds float, float* %4, i64 %174
  %180 = bitcast float* %179 to <4 x float>*
  store <4 x float> %wide.load145.17, <4 x float>* %180, align 4, !tbaa !3167
  %181 = getelementptr inbounds float, float* %179, i64 4
  %182 = bitcast float* %181 to <4 x float>*
  store <4 x float> %wide.load146.17, <4 x float>* %182, align 4, !tbaa !3167
  %183 = or i64 %21, 144
  %184 = getelementptr inbounds float, float* %7, i64 %183
  %185 = bitcast float* %184 to <4 x float>*
  %wide.load145.18 = load <4 x float>, <4 x float>* %185, align 4, !tbaa !3164
  %186 = getelementptr inbounds float, float* %184, i64 4
  %187 = bitcast float* %186 to <4 x float>*
  %wide.load146.18 = load <4 x float>, <4 x float>* %187, align 4, !tbaa !3164
  %188 = getelementptr inbounds float, float* %4, i64 %183
  %189 = bitcast float* %188 to <4 x float>*
  store <4 x float> %wide.load145.18, <4 x float>* %189, align 4, !tbaa !3167
  %190 = getelementptr inbounds float, float* %188, i64 4
  %191 = bitcast float* %190 to <4 x float>*
  store <4 x float> %wide.load146.18, <4 x float>* %191, align 4, !tbaa !3167
  %192 = or i64 %21, 152
  %193 = getelementptr inbounds float, float* %7, i64 %192
  %194 = bitcast float* %193 to <4 x float>*
  %wide.load145.19 = load <4 x float>, <4 x float>* %194, align 4, !tbaa !3164
  %195 = getelementptr inbounds float, float* %193, i64 4
  %196 = bitcast float* %195 to <4 x float>*
  %wide.load146.19 = load <4 x float>, <4 x float>* %196, align 4, !tbaa !3164
  %197 = getelementptr inbounds float, float* %4, i64 %192
  %198 = bitcast float* %197 to <4 x float>*
  store <4 x float> %wide.load145.19, <4 x float>* %198, align 4, !tbaa !3167
  %199 = getelementptr inbounds float, float* %197, i64 4
  %200 = bitcast float* %199 to <4 x float>*
  store <4 x float> %wide.load146.19, <4 x float>* %200, align 4, !tbaa !3167
  %201 = or i64 %21, 160
  %202 = getelementptr inbounds float, float* %7, i64 %201
  %203 = bitcast float* %202 to <4 x float>*
  %wide.load145.20 = load <4 x float>, <4 x float>* %203, align 4, !tbaa !3164
  %204 = getelementptr inbounds float, float* %202, i64 4
  %205 = bitcast float* %204 to <4 x float>*
  %wide.load146.20 = load <4 x float>, <4 x float>* %205, align 4, !tbaa !3164
  %206 = getelementptr inbounds float, float* %4, i64 %201
  %207 = bitcast float* %206 to <4 x float>*
  store <4 x float> %wide.load145.20, <4 x float>* %207, align 4, !tbaa !3167
  %208 = getelementptr inbounds float, float* %206, i64 4
  %209 = bitcast float* %208 to <4 x float>*
  store <4 x float> %wide.load146.20, <4 x float>* %209, align 4, !tbaa !3167
  %210 = or i64 %21, 168
  %211 = getelementptr inbounds float, float* %7, i64 %210
  %212 = bitcast float* %211 to <4 x float>*
  %wide.load145.21 = load <4 x float>, <4 x float>* %212, align 4, !tbaa !3164
  %213 = getelementptr inbounds float, float* %211, i64 4
  %214 = bitcast float* %213 to <4 x float>*
  %wide.load146.21 = load <4 x float>, <4 x float>* %214, align 4, !tbaa !3164
  %215 = getelementptr inbounds float, float* %4, i64 %210
  %216 = bitcast float* %215 to <4 x float>*
  store <4 x float> %wide.load145.21, <4 x float>* %216, align 4, !tbaa !3167
  %217 = getelementptr inbounds float, float* %215, i64 4
  %218 = bitcast float* %217 to <4 x float>*
  store <4 x float> %wide.load146.21, <4 x float>* %218, align 4, !tbaa !3167
  %219 = or i64 %21, 176
  %220 = getelementptr inbounds float, float* %7, i64 %219
  %221 = bitcast float* %220 to <4 x float>*
  %wide.load145.22 = load <4 x float>, <4 x float>* %221, align 4, !tbaa !3164
  %222 = getelementptr inbounds float, float* %220, i64 4
  %223 = bitcast float* %222 to <4 x float>*
  %wide.load146.22 = load <4 x float>, <4 x float>* %223, align 4, !tbaa !3164
  %224 = getelementptr inbounds float, float* %4, i64 %219
  %225 = bitcast float* %224 to <4 x float>*
  store <4 x float> %wide.load145.22, <4 x float>* %225, align 4, !tbaa !3167
  %226 = getelementptr inbounds float, float* %224, i64 4
  %227 = bitcast float* %226 to <4 x float>*
  store <4 x float> %wide.load146.22, <4 x float>* %227, align 4, !tbaa !3167
  %228 = or i64 %21, 184
  %229 = getelementptr inbounds float, float* %7, i64 %228
  %230 = bitcast float* %229 to <4 x float>*
  %wide.load145.23 = load <4 x float>, <4 x float>* %230, align 4, !tbaa !3164
  %231 = getelementptr inbounds float, float* %229, i64 4
  %232 = bitcast float* %231 to <4 x float>*
  %wide.load146.23 = load <4 x float>, <4 x float>* %232, align 4, !tbaa !3164
  %233 = getelementptr inbounds float, float* %4, i64 %228
  %234 = bitcast float* %233 to <4 x float>*
  store <4 x float> %wide.load145.23, <4 x float>* %234, align 4, !tbaa !3167
  %235 = getelementptr inbounds float, float* %233, i64 4
  %236 = bitcast float* %235 to <4 x float>*
  store <4 x float> %wide.load146.23, <4 x float>* %236, align 4, !tbaa !3167
  %237 = or i64 %21, 192
  %238 = getelementptr inbounds float, float* %7, i64 %237
  %239 = bitcast float* %238 to <4 x float>*
  %wide.load145.24 = load <4 x float>, <4 x float>* %239, align 4, !tbaa !3164
  %240 = getelementptr inbounds float, float* %238, i64 4
  %241 = bitcast float* %240 to <4 x float>*
  %wide.load146.24 = load <4 x float>, <4 x float>* %241, align 4, !tbaa !3164
  %242 = getelementptr inbounds float, float* %4, i64 %237
  %243 = bitcast float* %242 to <4 x float>*
  store <4 x float> %wide.load145.24, <4 x float>* %243, align 4, !tbaa !3167
  %244 = getelementptr inbounds float, float* %242, i64 4
  %245 = bitcast float* %244 to <4 x float>*
  store <4 x float> %wide.load146.24, <4 x float>* %245, align 4, !tbaa !3167
  %246 = or i64 %21, 200
  %247 = getelementptr inbounds float, float* %7, i64 %246
  %248 = bitcast float* %247 to <4 x float>*
  %wide.load145.25 = load <4 x float>, <4 x float>* %248, align 4, !tbaa !3164
  %249 = getelementptr inbounds float, float* %247, i64 4
  %250 = bitcast float* %249 to <4 x float>*
  %wide.load146.25 = load <4 x float>, <4 x float>* %250, align 4, !tbaa !3164
  %251 = getelementptr inbounds float, float* %4, i64 %246
  %252 = bitcast float* %251 to <4 x float>*
  store <4 x float> %wide.load145.25, <4 x float>* %252, align 4, !tbaa !3167
  %253 = getelementptr inbounds float, float* %251, i64 4
  %254 = bitcast float* %253 to <4 x float>*
  store <4 x float> %wide.load146.25, <4 x float>* %254, align 4, !tbaa !3167
  %255 = or i64 %21, 208
  %256 = getelementptr inbounds float, float* %7, i64 %255
  %257 = bitcast float* %256 to <4 x float>*
  %wide.load145.26 = load <4 x float>, <4 x float>* %257, align 4, !tbaa !3164
  %258 = getelementptr inbounds float, float* %256, i64 4
  %259 = bitcast float* %258 to <4 x float>*
  %wide.load146.26 = load <4 x float>, <4 x float>* %259, align 4, !tbaa !3164
  %260 = getelementptr inbounds float, float* %4, i64 %255
  %261 = bitcast float* %260 to <4 x float>*
  store <4 x float> %wide.load145.26, <4 x float>* %261, align 4, !tbaa !3167
  %262 = getelementptr inbounds float, float* %260, i64 4
  %263 = bitcast float* %262 to <4 x float>*
  store <4 x float> %wide.load146.26, <4 x float>* %263, align 4, !tbaa !3167
  %264 = or i64 %21, 216
  %265 = getelementptr inbounds float, float* %7, i64 %264
  %266 = bitcast float* %265 to <4 x float>*
  %wide.load145.27 = load <4 x float>, <4 x float>* %266, align 4, !tbaa !3164
  %267 = getelementptr inbounds float, float* %265, i64 4
  %268 = bitcast float* %267 to <4 x float>*
  %wide.load146.27 = load <4 x float>, <4 x float>* %268, align 4, !tbaa !3164
  %269 = getelementptr inbounds float, float* %4, i64 %264
  %270 = bitcast float* %269 to <4 x float>*
  store <4 x float> %wide.load145.27, <4 x float>* %270, align 4, !tbaa !3167
  %271 = getelementptr inbounds float, float* %269, i64 4
  %272 = bitcast float* %271 to <4 x float>*
  store <4 x float> %wide.load146.27, <4 x float>* %272, align 4, !tbaa !3167
  %273 = or i64 %21, 224
  %274 = getelementptr inbounds float, float* %7, i64 %273
  %275 = bitcast float* %274 to <4 x float>*
  %wide.load145.28 = load <4 x float>, <4 x float>* %275, align 4, !tbaa !3164
  %276 = getelementptr inbounds float, float* %274, i64 4
  %277 = bitcast float* %276 to <4 x float>*
  %wide.load146.28 = load <4 x float>, <4 x float>* %277, align 4, !tbaa !3164
  %278 = getelementptr inbounds float, float* %4, i64 %273
  %279 = bitcast float* %278 to <4 x float>*
  store <4 x float> %wide.load145.28, <4 x float>* %279, align 4, !tbaa !3167
  %280 = getelementptr inbounds float, float* %278, i64 4
  %281 = bitcast float* %280 to <4 x float>*
  store <4 x float> %wide.load146.28, <4 x float>* %281, align 4, !tbaa !3167
  %282 = or i64 %21, 232
  %283 = getelementptr inbounds float, float* %7, i64 %282
  %284 = bitcast float* %283 to <4 x float>*
  %wide.load145.29 = load <4 x float>, <4 x float>* %284, align 4, !tbaa !3164
  %285 = getelementptr inbounds float, float* %283, i64 4
  %286 = bitcast float* %285 to <4 x float>*
  %wide.load146.29 = load <4 x float>, <4 x float>* %286, align 4, !tbaa !3164
  %287 = getelementptr inbounds float, float* %4, i64 %282
  %288 = bitcast float* %287 to <4 x float>*
  store <4 x float> %wide.load145.29, <4 x float>* %288, align 4, !tbaa !3167
  %289 = getelementptr inbounds float, float* %287, i64 4
  %290 = bitcast float* %289 to <4 x float>*
  store <4 x float> %wide.load146.29, <4 x float>* %290, align 4, !tbaa !3167
  %291 = or i64 %21, 240
  %292 = getelementptr inbounds float, float* %7, i64 %291
  %293 = bitcast float* %292 to <4 x float>*
  %wide.load145.30 = load <4 x float>, <4 x float>* %293, align 4, !tbaa !3164
  %294 = getelementptr inbounds float, float* %292, i64 4
  %295 = bitcast float* %294 to <4 x float>*
  %wide.load146.30 = load <4 x float>, <4 x float>* %295, align 4, !tbaa !3164
  %296 = getelementptr inbounds float, float* %4, i64 %291
  %297 = bitcast float* %296 to <4 x float>*
  store <4 x float> %wide.load145.30, <4 x float>* %297, align 4, !tbaa !3167
  %298 = getelementptr inbounds float, float* %296, i64 4
  %299 = bitcast float* %298 to <4 x float>*
  store <4 x float> %wide.load146.30, <4 x float>* %299, align 4, !tbaa !3167
  %300 = or i64 %21, 248
  %301 = getelementptr inbounds float, float* %7, i64 %300
  %302 = bitcast float* %301 to <4 x float>*
  %wide.load145.31 = load <4 x float>, <4 x float>* %302, align 4, !tbaa !3164
  %303 = getelementptr inbounds float, float* %301, i64 4
  %304 = bitcast float* %303 to <4 x float>*
  %wide.load146.31 = load <4 x float>, <4 x float>* %304, align 4, !tbaa !3164
  %305 = getelementptr inbounds float, float* %4, i64 %300
  %306 = bitcast float* %305 to <4 x float>*
  store <4 x float> %wide.load145.31, <4 x float>* %306, align 4, !tbaa !3167
  %307 = getelementptr inbounds float, float* %305, i64 4
  %308 = bitcast float* %307 to <4 x float>*
  store <4 x float> %wide.load146.31, <4 x float>* %308, align 4, !tbaa !3167
  %309 = or i64 %21, 256
  %310 = getelementptr inbounds float, float* %7, i64 %309
  %311 = bitcast float* %310 to <4 x float>*
  %wide.load135 = load <4 x float>, <4 x float>* %311, align 4, !tbaa !3164
  %312 = getelementptr inbounds float, float* %310, i64 4
  %313 = bitcast float* %312 to <4 x float>*
  %wide.load136 = load <4 x float>, <4 x float>* %313, align 4, !tbaa !3164
  %314 = getelementptr inbounds float, float* %4, i64 %309
  %315 = bitcast float* %314 to <4 x float>*
  store <4 x float> %wide.load135, <4 x float>* %315, align 4, !tbaa !3167
  %316 = getelementptr inbounds float, float* %314, i64 4
  %317 = bitcast float* %316 to <4 x float>*
  store <4 x float> %wide.load136, <4 x float>* %317, align 4, !tbaa !3167
  %318 = or i64 %21, 264
  %319 = getelementptr inbounds float, float* %7, i64 %318
  %320 = bitcast float* %319 to <4 x float>*
  %wide.load135.1 = load <4 x float>, <4 x float>* %320, align 4, !tbaa !3164
  %321 = getelementptr inbounds float, float* %319, i64 4
  %322 = bitcast float* %321 to <4 x float>*
  %wide.load136.1 = load <4 x float>, <4 x float>* %322, align 4, !tbaa !3164
  %323 = getelementptr inbounds float, float* %4, i64 %318
  %324 = bitcast float* %323 to <4 x float>*
  store <4 x float> %wide.load135.1, <4 x float>* %324, align 4, !tbaa !3167
  %325 = getelementptr inbounds float, float* %323, i64 4
  %326 = bitcast float* %325 to <4 x float>*
  store <4 x float> %wide.load136.1, <4 x float>* %326, align 4, !tbaa !3167
  %327 = or i64 %21, 272
  %328 = getelementptr inbounds float, float* %7, i64 %327
  %329 = bitcast float* %328 to <4 x float>*
  %wide.load135.2 = load <4 x float>, <4 x float>* %329, align 4, !tbaa !3164
  %330 = getelementptr inbounds float, float* %328, i64 4
  %331 = bitcast float* %330 to <4 x float>*
  %wide.load136.2 = load <4 x float>, <4 x float>* %331, align 4, !tbaa !3164
  %332 = getelementptr inbounds float, float* %4, i64 %327
  %333 = bitcast float* %332 to <4 x float>*
  store <4 x float> %wide.load135.2, <4 x float>* %333, align 4, !tbaa !3167
  %334 = getelementptr inbounds float, float* %332, i64 4
  %335 = bitcast float* %334 to <4 x float>*
  store <4 x float> %wide.load136.2, <4 x float>* %335, align 4, !tbaa !3167
  %336 = or i64 %21, 280
  %337 = getelementptr inbounds float, float* %7, i64 %336
  %338 = bitcast float* %337 to <4 x float>*
  %wide.load135.3 = load <4 x float>, <4 x float>* %338, align 4, !tbaa !3164
  %339 = getelementptr inbounds float, float* %337, i64 4
  %340 = bitcast float* %339 to <4 x float>*
  %wide.load136.3 = load <4 x float>, <4 x float>* %340, align 4, !tbaa !3164
  %341 = getelementptr inbounds float, float* %4, i64 %336
  %342 = bitcast float* %341 to <4 x float>*
  store <4 x float> %wide.load135.3, <4 x float>* %342, align 4, !tbaa !3167
  %343 = getelementptr inbounds float, float* %341, i64 4
  %344 = bitcast float* %343 to <4 x float>*
  store <4 x float> %wide.load136.3, <4 x float>* %344, align 4, !tbaa !3167
  %345 = or i64 %21, 288
  %346 = getelementptr inbounds float, float* %7, i64 %345
  %347 = bitcast float* %346 to <4 x float>*
  %wide.load135.4 = load <4 x float>, <4 x float>* %347, align 4, !tbaa !3164
  %348 = getelementptr inbounds float, float* %346, i64 4
  %349 = bitcast float* %348 to <4 x float>*
  %wide.load136.4 = load <4 x float>, <4 x float>* %349, align 4, !tbaa !3164
  %350 = getelementptr inbounds float, float* %4, i64 %345
  %351 = bitcast float* %350 to <4 x float>*
  store <4 x float> %wide.load135.4, <4 x float>* %351, align 4, !tbaa !3167
  %352 = getelementptr inbounds float, float* %350, i64 4
  %353 = bitcast float* %352 to <4 x float>*
  store <4 x float> %wide.load136.4, <4 x float>* %353, align 4, !tbaa !3167
  %354 = or i64 %21, 296
  %355 = getelementptr inbounds float, float* %7, i64 %354
  %356 = bitcast float* %355 to <4 x float>*
  %wide.load135.5 = load <4 x float>, <4 x float>* %356, align 4, !tbaa !3164
  %357 = getelementptr inbounds float, float* %355, i64 4
  %358 = bitcast float* %357 to <4 x float>*
  %wide.load136.5 = load <4 x float>, <4 x float>* %358, align 4, !tbaa !3164
  %359 = getelementptr inbounds float, float* %4, i64 %354
  %360 = bitcast float* %359 to <4 x float>*
  store <4 x float> %wide.load135.5, <4 x float>* %360, align 4, !tbaa !3167
  %361 = getelementptr inbounds float, float* %359, i64 4
  %362 = bitcast float* %361 to <4 x float>*
  store <4 x float> %wide.load136.5, <4 x float>* %362, align 4, !tbaa !3167
  %363 = or i64 %21, 304
  %364 = getelementptr inbounds float, float* %7, i64 %363
  %365 = bitcast float* %364 to <4 x float>*
  %wide.load135.6 = load <4 x float>, <4 x float>* %365, align 4, !tbaa !3164
  %366 = getelementptr inbounds float, float* %364, i64 4
  %367 = bitcast float* %366 to <4 x float>*
  %wide.load136.6 = load <4 x float>, <4 x float>* %367, align 4, !tbaa !3164
  %368 = getelementptr inbounds float, float* %4, i64 %363
  %369 = bitcast float* %368 to <4 x float>*
  store <4 x float> %wide.load135.6, <4 x float>* %369, align 4, !tbaa !3167
  %370 = getelementptr inbounds float, float* %368, i64 4
  %371 = bitcast float* %370 to <4 x float>*
  store <4 x float> %wide.load136.6, <4 x float>* %371, align 4, !tbaa !3167
  %372 = or i64 %21, 312
  %373 = getelementptr inbounds float, float* %7, i64 %372
  %374 = bitcast float* %373 to <4 x float>*
  %wide.load135.7 = load <4 x float>, <4 x float>* %374, align 4, !tbaa !3164
  %375 = getelementptr inbounds float, float* %373, i64 4
  %376 = bitcast float* %375 to <4 x float>*
  %wide.load136.7 = load <4 x float>, <4 x float>* %376, align 4, !tbaa !3164
  %377 = getelementptr inbounds float, float* %4, i64 %372
  %378 = bitcast float* %377 to <4 x float>*
  store <4 x float> %wide.load135.7, <4 x float>* %378, align 4, !tbaa !3167
  %379 = getelementptr inbounds float, float* %377, i64 4
  %380 = bitcast float* %379 to <4 x float>*
  store <4 x float> %wide.load136.7, <4 x float>* %380, align 4, !tbaa !3167
  %381 = or i64 %21, 320
  %382 = getelementptr inbounds float, float* %7, i64 %381
  %383 = bitcast float* %382 to <4 x float>*
  %wide.load135.8 = load <4 x float>, <4 x float>* %383, align 4, !tbaa !3164
  %384 = getelementptr inbounds float, float* %382, i64 4
  %385 = bitcast float* %384 to <4 x float>*
  %wide.load136.8 = load <4 x float>, <4 x float>* %385, align 4, !tbaa !3164
  %386 = getelementptr inbounds float, float* %4, i64 %381
  %387 = bitcast float* %386 to <4 x float>*
  store <4 x float> %wide.load135.8, <4 x float>* %387, align 4, !tbaa !3167
  %388 = getelementptr inbounds float, float* %386, i64 4
  %389 = bitcast float* %388 to <4 x float>*
  store <4 x float> %wide.load136.8, <4 x float>* %389, align 4, !tbaa !3167
  %390 = or i64 %21, 328
  %391 = getelementptr inbounds float, float* %7, i64 %390
  %392 = bitcast float* %391 to <4 x float>*
  %wide.load135.9 = load <4 x float>, <4 x float>* %392, align 4, !tbaa !3164
  %393 = getelementptr inbounds float, float* %391, i64 4
  %394 = bitcast float* %393 to <4 x float>*
  %wide.load136.9 = load <4 x float>, <4 x float>* %394, align 4, !tbaa !3164
  %395 = getelementptr inbounds float, float* %4, i64 %390
  %396 = bitcast float* %395 to <4 x float>*
  store <4 x float> %wide.load135.9, <4 x float>* %396, align 4, !tbaa !3167
  %397 = getelementptr inbounds float, float* %395, i64 4
  %398 = bitcast float* %397 to <4 x float>*
  store <4 x float> %wide.load136.9, <4 x float>* %398, align 4, !tbaa !3167
  %399 = or i64 %21, 336
  %400 = getelementptr inbounds float, float* %7, i64 %399
  %401 = bitcast float* %400 to <4 x float>*
  %wide.load135.10 = load <4 x float>, <4 x float>* %401, align 4, !tbaa !3164
  %402 = getelementptr inbounds float, float* %400, i64 4
  %403 = bitcast float* %402 to <4 x float>*
  %wide.load136.10 = load <4 x float>, <4 x float>* %403, align 4, !tbaa !3164
  %404 = getelementptr inbounds float, float* %4, i64 %399
  %405 = bitcast float* %404 to <4 x float>*
  store <4 x float> %wide.load135.10, <4 x float>* %405, align 4, !tbaa !3167
  %406 = getelementptr inbounds float, float* %404, i64 4
  %407 = bitcast float* %406 to <4 x float>*
  store <4 x float> %wide.load136.10, <4 x float>* %407, align 4, !tbaa !3167
  %408 = or i64 %21, 344
  %409 = getelementptr inbounds float, float* %7, i64 %408
  %410 = bitcast float* %409 to <4 x float>*
  %wide.load135.11 = load <4 x float>, <4 x float>* %410, align 4, !tbaa !3164
  %411 = getelementptr inbounds float, float* %409, i64 4
  %412 = bitcast float* %411 to <4 x float>*
  %wide.load136.11 = load <4 x float>, <4 x float>* %412, align 4, !tbaa !3164
  %413 = getelementptr inbounds float, float* %4, i64 %408
  %414 = bitcast float* %413 to <4 x float>*
  store <4 x float> %wide.load135.11, <4 x float>* %414, align 4, !tbaa !3167
  %415 = getelementptr inbounds float, float* %413, i64 4
  %416 = bitcast float* %415 to <4 x float>*
  store <4 x float> %wide.load136.11, <4 x float>* %416, align 4, !tbaa !3167
  %417 = or i64 %21, 352
  %418 = getelementptr inbounds float, float* %7, i64 %417
  %419 = bitcast float* %418 to <4 x float>*
  %wide.load135.12 = load <4 x float>, <4 x float>* %419, align 4, !tbaa !3164
  %420 = getelementptr inbounds float, float* %418, i64 4
  %421 = bitcast float* %420 to <4 x float>*
  %wide.load136.12 = load <4 x float>, <4 x float>* %421, align 4, !tbaa !3164
  %422 = getelementptr inbounds float, float* %4, i64 %417
  %423 = bitcast float* %422 to <4 x float>*
  store <4 x float> %wide.load135.12, <4 x float>* %423, align 4, !tbaa !3167
  %424 = getelementptr inbounds float, float* %422, i64 4
  %425 = bitcast float* %424 to <4 x float>*
  store <4 x float> %wide.load136.12, <4 x float>* %425, align 4, !tbaa !3167
  %426 = or i64 %21, 360
  %427 = getelementptr inbounds float, float* %7, i64 %426
  %428 = bitcast float* %427 to <4 x float>*
  %wide.load135.13 = load <4 x float>, <4 x float>* %428, align 4, !tbaa !3164
  %429 = getelementptr inbounds float, float* %427, i64 4
  %430 = bitcast float* %429 to <4 x float>*
  %wide.load136.13 = load <4 x float>, <4 x float>* %430, align 4, !tbaa !3164
  %431 = getelementptr inbounds float, float* %4, i64 %426
  %432 = bitcast float* %431 to <4 x float>*
  store <4 x float> %wide.load135.13, <4 x float>* %432, align 4, !tbaa !3167
  %433 = getelementptr inbounds float, float* %431, i64 4
  %434 = bitcast float* %433 to <4 x float>*
  store <4 x float> %wide.load136.13, <4 x float>* %434, align 4, !tbaa !3167
  %435 = or i64 %21, 368
  %436 = getelementptr inbounds float, float* %7, i64 %435
  %437 = bitcast float* %436 to <4 x float>*
  %wide.load135.14 = load <4 x float>, <4 x float>* %437, align 4, !tbaa !3164
  %438 = getelementptr inbounds float, float* %436, i64 4
  %439 = bitcast float* %438 to <4 x float>*
  %wide.load136.14 = load <4 x float>, <4 x float>* %439, align 4, !tbaa !3164
  %440 = getelementptr inbounds float, float* %4, i64 %435
  %441 = bitcast float* %440 to <4 x float>*
  store <4 x float> %wide.load135.14, <4 x float>* %441, align 4, !tbaa !3167
  %442 = getelementptr inbounds float, float* %440, i64 4
  %443 = bitcast float* %442 to <4 x float>*
  store <4 x float> %wide.load136.14, <4 x float>* %443, align 4, !tbaa !3167
  %444 = or i64 %21, 376
  %445 = getelementptr inbounds float, float* %7, i64 %444
  %446 = bitcast float* %445 to <4 x float>*
  %wide.load135.15 = load <4 x float>, <4 x float>* %446, align 4, !tbaa !3164
  %447 = getelementptr inbounds float, float* %445, i64 4
  %448 = bitcast float* %447 to <4 x float>*
  %wide.load136.15 = load <4 x float>, <4 x float>* %448, align 4, !tbaa !3164
  %449 = getelementptr inbounds float, float* %4, i64 %444
  %450 = bitcast float* %449 to <4 x float>*
  store <4 x float> %wide.load135.15, <4 x float>* %450, align 4, !tbaa !3167
  %451 = getelementptr inbounds float, float* %449, i64 4
  %452 = bitcast float* %451 to <4 x float>*
  store <4 x float> %wide.load136.15, <4 x float>* %452, align 4, !tbaa !3167
  %453 = or i64 %21, 384
  %454 = getelementptr inbounds float, float* %7, i64 %453
  %455 = bitcast float* %454 to <4 x float>*
  %wide.load135.16 = load <4 x float>, <4 x float>* %455, align 4, !tbaa !3164
  %456 = getelementptr inbounds float, float* %454, i64 4
  %457 = bitcast float* %456 to <4 x float>*
  %wide.load136.16 = load <4 x float>, <4 x float>* %457, align 4, !tbaa !3164
  %458 = getelementptr inbounds float, float* %4, i64 %453
  %459 = bitcast float* %458 to <4 x float>*
  store <4 x float> %wide.load135.16, <4 x float>* %459, align 4, !tbaa !3167
  %460 = getelementptr inbounds float, float* %458, i64 4
  %461 = bitcast float* %460 to <4 x float>*
  store <4 x float> %wide.load136.16, <4 x float>* %461, align 4, !tbaa !3167
  %462 = or i64 %21, 392
  %463 = getelementptr inbounds float, float* %7, i64 %462
  %464 = bitcast float* %463 to <4 x float>*
  %wide.load135.17 = load <4 x float>, <4 x float>* %464, align 4, !tbaa !3164
  %465 = getelementptr inbounds float, float* %463, i64 4
  %466 = bitcast float* %465 to <4 x float>*
  %wide.load136.17 = load <4 x float>, <4 x float>* %466, align 4, !tbaa !3164
  %467 = getelementptr inbounds float, float* %4, i64 %462
  %468 = bitcast float* %467 to <4 x float>*
  store <4 x float> %wide.load135.17, <4 x float>* %468, align 4, !tbaa !3167
  %469 = getelementptr inbounds float, float* %467, i64 4
  %470 = bitcast float* %469 to <4 x float>*
  store <4 x float> %wide.load136.17, <4 x float>* %470, align 4, !tbaa !3167
  %471 = or i64 %21, 400
  %472 = getelementptr inbounds float, float* %7, i64 %471
  %473 = bitcast float* %472 to <4 x float>*
  %wide.load135.18 = load <4 x float>, <4 x float>* %473, align 4, !tbaa !3164
  %474 = getelementptr inbounds float, float* %472, i64 4
  %475 = bitcast float* %474 to <4 x float>*
  %wide.load136.18 = load <4 x float>, <4 x float>* %475, align 4, !tbaa !3164
  %476 = getelementptr inbounds float, float* %4, i64 %471
  %477 = bitcast float* %476 to <4 x float>*
  store <4 x float> %wide.load135.18, <4 x float>* %477, align 4, !tbaa !3167
  %478 = getelementptr inbounds float, float* %476, i64 4
  %479 = bitcast float* %478 to <4 x float>*
  store <4 x float> %wide.load136.18, <4 x float>* %479, align 4, !tbaa !3167
  %480 = or i64 %21, 408
  %481 = getelementptr inbounds float, float* %7, i64 %480
  %482 = bitcast float* %481 to <4 x float>*
  %wide.load135.19 = load <4 x float>, <4 x float>* %482, align 4, !tbaa !3164
  %483 = getelementptr inbounds float, float* %481, i64 4
  %484 = bitcast float* %483 to <4 x float>*
  %wide.load136.19 = load <4 x float>, <4 x float>* %484, align 4, !tbaa !3164
  %485 = getelementptr inbounds float, float* %4, i64 %480
  %486 = bitcast float* %485 to <4 x float>*
  store <4 x float> %wide.load135.19, <4 x float>* %486, align 4, !tbaa !3167
  %487 = getelementptr inbounds float, float* %485, i64 4
  %488 = bitcast float* %487 to <4 x float>*
  store <4 x float> %wide.load136.19, <4 x float>* %488, align 4, !tbaa !3167
  %489 = or i64 %21, 416
  %490 = getelementptr inbounds float, float* %7, i64 %489
  %491 = bitcast float* %490 to <4 x float>*
  %wide.load135.20 = load <4 x float>, <4 x float>* %491, align 4, !tbaa !3164
  %492 = getelementptr inbounds float, float* %490, i64 4
  %493 = bitcast float* %492 to <4 x float>*
  %wide.load136.20 = load <4 x float>, <4 x float>* %493, align 4, !tbaa !3164
  %494 = getelementptr inbounds float, float* %4, i64 %489
  %495 = bitcast float* %494 to <4 x float>*
  store <4 x float> %wide.load135.20, <4 x float>* %495, align 4, !tbaa !3167
  %496 = getelementptr inbounds float, float* %494, i64 4
  %497 = bitcast float* %496 to <4 x float>*
  store <4 x float> %wide.load136.20, <4 x float>* %497, align 4, !tbaa !3167
  %498 = or i64 %21, 424
  %499 = getelementptr inbounds float, float* %7, i64 %498
  %500 = bitcast float* %499 to <4 x float>*
  %wide.load135.21 = load <4 x float>, <4 x float>* %500, align 4, !tbaa !3164
  %501 = getelementptr inbounds float, float* %499, i64 4
  %502 = bitcast float* %501 to <4 x float>*
  %wide.load136.21 = load <4 x float>, <4 x float>* %502, align 4, !tbaa !3164
  %503 = getelementptr inbounds float, float* %4, i64 %498
  %504 = bitcast float* %503 to <4 x float>*
  store <4 x float> %wide.load135.21, <4 x float>* %504, align 4, !tbaa !3167
  %505 = getelementptr inbounds float, float* %503, i64 4
  %506 = bitcast float* %505 to <4 x float>*
  store <4 x float> %wide.load136.21, <4 x float>* %506, align 4, !tbaa !3167
  %507 = or i64 %21, 432
  %508 = getelementptr inbounds float, float* %7, i64 %507
  %509 = bitcast float* %508 to <4 x float>*
  %wide.load135.22 = load <4 x float>, <4 x float>* %509, align 4, !tbaa !3164
  %510 = getelementptr inbounds float, float* %508, i64 4
  %511 = bitcast float* %510 to <4 x float>*
  %wide.load136.22 = load <4 x float>, <4 x float>* %511, align 4, !tbaa !3164
  %512 = getelementptr inbounds float, float* %4, i64 %507
  %513 = bitcast float* %512 to <4 x float>*
  store <4 x float> %wide.load135.22, <4 x float>* %513, align 4, !tbaa !3167
  %514 = getelementptr inbounds float, float* %512, i64 4
  %515 = bitcast float* %514 to <4 x float>*
  store <4 x float> %wide.load136.22, <4 x float>* %515, align 4, !tbaa !3167
  %516 = or i64 %21, 440
  %517 = getelementptr inbounds float, float* %7, i64 %516
  %518 = bitcast float* %517 to <4 x float>*
  %wide.load135.23 = load <4 x float>, <4 x float>* %518, align 4, !tbaa !3164
  %519 = getelementptr inbounds float, float* %517, i64 4
  %520 = bitcast float* %519 to <4 x float>*
  %wide.load136.23 = load <4 x float>, <4 x float>* %520, align 4, !tbaa !3164
  %521 = getelementptr inbounds float, float* %4, i64 %516
  %522 = bitcast float* %521 to <4 x float>*
  store <4 x float> %wide.load135.23, <4 x float>* %522, align 4, !tbaa !3167
  %523 = getelementptr inbounds float, float* %521, i64 4
  %524 = bitcast float* %523 to <4 x float>*
  store <4 x float> %wide.load136.23, <4 x float>* %524, align 4, !tbaa !3167
  %525 = or i64 %21, 448
  %526 = getelementptr inbounds float, float* %7, i64 %525
  %527 = bitcast float* %526 to <4 x float>*
  %wide.load135.24 = load <4 x float>, <4 x float>* %527, align 4, !tbaa !3164
  %528 = getelementptr inbounds float, float* %526, i64 4
  %529 = bitcast float* %528 to <4 x float>*
  %wide.load136.24 = load <4 x float>, <4 x float>* %529, align 4, !tbaa !3164
  %530 = getelementptr inbounds float, float* %4, i64 %525
  %531 = bitcast float* %530 to <4 x float>*
  store <4 x float> %wide.load135.24, <4 x float>* %531, align 4, !tbaa !3167
  %532 = getelementptr inbounds float, float* %530, i64 4
  %533 = bitcast float* %532 to <4 x float>*
  store <4 x float> %wide.load136.24, <4 x float>* %533, align 4, !tbaa !3167
  %534 = or i64 %21, 456
  %535 = getelementptr inbounds float, float* %7, i64 %534
  %536 = bitcast float* %535 to <4 x float>*
  %wide.load135.25 = load <4 x float>, <4 x float>* %536, align 4, !tbaa !3164
  %537 = getelementptr inbounds float, float* %535, i64 4
  %538 = bitcast float* %537 to <4 x float>*
  %wide.load136.25 = load <4 x float>, <4 x float>* %538, align 4, !tbaa !3164
  %539 = getelementptr inbounds float, float* %4, i64 %534
  %540 = bitcast float* %539 to <4 x float>*
  store <4 x float> %wide.load135.25, <4 x float>* %540, align 4, !tbaa !3167
  %541 = getelementptr inbounds float, float* %539, i64 4
  %542 = bitcast float* %541 to <4 x float>*
  store <4 x float> %wide.load136.25, <4 x float>* %542, align 4, !tbaa !3167
  %543 = or i64 %21, 464
  %544 = getelementptr inbounds float, float* %7, i64 %543
  %545 = bitcast float* %544 to <4 x float>*
  %wide.load135.26 = load <4 x float>, <4 x float>* %545, align 4, !tbaa !3164
  %546 = getelementptr inbounds float, float* %544, i64 4
  %547 = bitcast float* %546 to <4 x float>*
  %wide.load136.26 = load <4 x float>, <4 x float>* %547, align 4, !tbaa !3164
  %548 = getelementptr inbounds float, float* %4, i64 %543
  %549 = bitcast float* %548 to <4 x float>*
  store <4 x float> %wide.load135.26, <4 x float>* %549, align 4, !tbaa !3167
  %550 = getelementptr inbounds float, float* %548, i64 4
  %551 = bitcast float* %550 to <4 x float>*
  store <4 x float> %wide.load136.26, <4 x float>* %551, align 4, !tbaa !3167
  %552 = or i64 %21, 472
  %553 = getelementptr inbounds float, float* %7, i64 %552
  %554 = bitcast float* %553 to <4 x float>*
  %wide.load135.27 = load <4 x float>, <4 x float>* %554, align 4, !tbaa !3164
  %555 = getelementptr inbounds float, float* %553, i64 4
  %556 = bitcast float* %555 to <4 x float>*
  %wide.load136.27 = load <4 x float>, <4 x float>* %556, align 4, !tbaa !3164
  %557 = getelementptr inbounds float, float* %4, i64 %552
  %558 = bitcast float* %557 to <4 x float>*
  store <4 x float> %wide.load135.27, <4 x float>* %558, align 4, !tbaa !3167
  %559 = getelementptr inbounds float, float* %557, i64 4
  %560 = bitcast float* %559 to <4 x float>*
  store <4 x float> %wide.load136.27, <4 x float>* %560, align 4, !tbaa !3167
  %561 = or i64 %21, 480
  %562 = getelementptr inbounds float, float* %7, i64 %561
  %563 = bitcast float* %562 to <4 x float>*
  %wide.load135.28 = load <4 x float>, <4 x float>* %563, align 4, !tbaa !3164
  %564 = getelementptr inbounds float, float* %562, i64 4
  %565 = bitcast float* %564 to <4 x float>*
  %wide.load136.28 = load <4 x float>, <4 x float>* %565, align 4, !tbaa !3164
  %566 = getelementptr inbounds float, float* %4, i64 %561
  %567 = bitcast float* %566 to <4 x float>*
  store <4 x float> %wide.load135.28, <4 x float>* %567, align 4, !tbaa !3167
  %568 = getelementptr inbounds float, float* %566, i64 4
  %569 = bitcast float* %568 to <4 x float>*
  store <4 x float> %wide.load136.28, <4 x float>* %569, align 4, !tbaa !3167
  %570 = or i64 %21, 488
  %571 = getelementptr inbounds float, float* %7, i64 %570
  %572 = bitcast float* %571 to <4 x float>*
  %wide.load135.29 = load <4 x float>, <4 x float>* %572, align 4, !tbaa !3164
  %573 = getelementptr inbounds float, float* %571, i64 4
  %574 = bitcast float* %573 to <4 x float>*
  %wide.load136.29 = load <4 x float>, <4 x float>* %574, align 4, !tbaa !3164
  %575 = getelementptr inbounds float, float* %4, i64 %570
  %576 = bitcast float* %575 to <4 x float>*
  store <4 x float> %wide.load135.29, <4 x float>* %576, align 4, !tbaa !3167
  %577 = getelementptr inbounds float, float* %575, i64 4
  %578 = bitcast float* %577 to <4 x float>*
  store <4 x float> %wide.load136.29, <4 x float>* %578, align 4, !tbaa !3167
  %579 = or i64 %21, 496
  %580 = getelementptr inbounds float, float* %7, i64 %579
  %581 = bitcast float* %580 to <4 x float>*
  %wide.load135.30 = load <4 x float>, <4 x float>* %581, align 4, !tbaa !3164
  %582 = getelementptr inbounds float, float* %580, i64 4
  %583 = bitcast float* %582 to <4 x float>*
  %wide.load136.30 = load <4 x float>, <4 x float>* %583, align 4, !tbaa !3164
  %584 = getelementptr inbounds float, float* %4, i64 %579
  %585 = bitcast float* %584 to <4 x float>*
  store <4 x float> %wide.load135.30, <4 x float>* %585, align 4, !tbaa !3167
  %586 = getelementptr inbounds float, float* %584, i64 4
  %587 = bitcast float* %586 to <4 x float>*
  store <4 x float> %wide.load136.30, <4 x float>* %587, align 4, !tbaa !3167
  %588 = or i64 %21, 504
  %589 = getelementptr inbounds float, float* %7, i64 %588
  %590 = bitcast float* %589 to <4 x float>*
  %wide.load135.31 = load <4 x float>, <4 x float>* %590, align 4, !tbaa !3164
  %591 = getelementptr inbounds float, float* %589, i64 4
  %592 = bitcast float* %591 to <4 x float>*
  %wide.load136.31 = load <4 x float>, <4 x float>* %592, align 4, !tbaa !3164
  %593 = getelementptr inbounds float, float* %4, i64 %588
  %594 = bitcast float* %593 to <4 x float>*
  store <4 x float> %wide.load135.31, <4 x float>* %594, align 4, !tbaa !3167
  %595 = getelementptr inbounds float, float* %593, i64 4
  %596 = bitcast float* %595 to <4 x float>*
  store <4 x float> %wide.load136.31, <4 x float>* %596, align 4, !tbaa !3167
  %597 = add nsw i64 %21, 512
  %598 = getelementptr inbounds float, float* %7, i64 %597
  %599 = bitcast float* %598 to <4 x float>*
  %wide.load125 = load <4 x float>, <4 x float>* %599, align 4, !tbaa !3164
  %600 = getelementptr inbounds float, float* %598, i64 4
  %601 = bitcast float* %600 to <4 x float>*
  %wide.load126 = load <4 x float>, <4 x float>* %601, align 4, !tbaa !3164
  %602 = getelementptr inbounds float, float* %4, i64 %597
  %603 = bitcast float* %602 to <4 x float>*
  store <4 x float> %wide.load125, <4 x float>* %603, align 4, !tbaa !3167
  %604 = getelementptr inbounds float, float* %602, i64 4
  %605 = bitcast float* %604 to <4 x float>*
  store <4 x float> %wide.load126, <4 x float>* %605, align 4, !tbaa !3167
  %606 = add nsw i64 %21, 520
  %607 = getelementptr inbounds float, float* %7, i64 %606
  %608 = bitcast float* %607 to <4 x float>*
  %wide.load125.1 = load <4 x float>, <4 x float>* %608, align 4, !tbaa !3164
  %609 = getelementptr inbounds float, float* %607, i64 4
  %610 = bitcast float* %609 to <4 x float>*
  %wide.load126.1 = load <4 x float>, <4 x float>* %610, align 4, !tbaa !3164
  %611 = getelementptr inbounds float, float* %4, i64 %606
  %612 = bitcast float* %611 to <4 x float>*
  store <4 x float> %wide.load125.1, <4 x float>* %612, align 4, !tbaa !3167
  %613 = getelementptr inbounds float, float* %611, i64 4
  %614 = bitcast float* %613 to <4 x float>*
  store <4 x float> %wide.load126.1, <4 x float>* %614, align 4, !tbaa !3167
  %615 = add nsw i64 %21, 528
  %616 = getelementptr inbounds float, float* %7, i64 %615
  %617 = bitcast float* %616 to <4 x float>*
  %wide.load125.2 = load <4 x float>, <4 x float>* %617, align 4, !tbaa !3164
  %618 = getelementptr inbounds float, float* %616, i64 4
  %619 = bitcast float* %618 to <4 x float>*
  %wide.load126.2 = load <4 x float>, <4 x float>* %619, align 4, !tbaa !3164
  %620 = getelementptr inbounds float, float* %4, i64 %615
  %621 = bitcast float* %620 to <4 x float>*
  store <4 x float> %wide.load125.2, <4 x float>* %621, align 4, !tbaa !3167
  %622 = getelementptr inbounds float, float* %620, i64 4
  %623 = bitcast float* %622 to <4 x float>*
  store <4 x float> %wide.load126.2, <4 x float>* %623, align 4, !tbaa !3167
  %624 = add nsw i64 %21, 536
  %625 = getelementptr inbounds float, float* %7, i64 %624
  %626 = bitcast float* %625 to <4 x float>*
  %wide.load125.3 = load <4 x float>, <4 x float>* %626, align 4, !tbaa !3164
  %627 = getelementptr inbounds float, float* %625, i64 4
  %628 = bitcast float* %627 to <4 x float>*
  %wide.load126.3 = load <4 x float>, <4 x float>* %628, align 4, !tbaa !3164
  %629 = getelementptr inbounds float, float* %4, i64 %624
  %630 = bitcast float* %629 to <4 x float>*
  store <4 x float> %wide.load125.3, <4 x float>* %630, align 4, !tbaa !3167
  %631 = getelementptr inbounds float, float* %629, i64 4
  %632 = bitcast float* %631 to <4 x float>*
  store <4 x float> %wide.load126.3, <4 x float>* %632, align 4, !tbaa !3167
  %633 = add nsw i64 %21, 544
  %634 = getelementptr inbounds float, float* %7, i64 %633
  %635 = bitcast float* %634 to <4 x float>*
  %wide.load125.4 = load <4 x float>, <4 x float>* %635, align 4, !tbaa !3164
  %636 = getelementptr inbounds float, float* %634, i64 4
  %637 = bitcast float* %636 to <4 x float>*
  %wide.load126.4 = load <4 x float>, <4 x float>* %637, align 4, !tbaa !3164
  %638 = getelementptr inbounds float, float* %4, i64 %633
  %639 = bitcast float* %638 to <4 x float>*
  store <4 x float> %wide.load125.4, <4 x float>* %639, align 4, !tbaa !3167
  %640 = getelementptr inbounds float, float* %638, i64 4
  %641 = bitcast float* %640 to <4 x float>*
  store <4 x float> %wide.load126.4, <4 x float>* %641, align 4, !tbaa !3167
  %642 = add nsw i64 %21, 552
  %643 = getelementptr inbounds float, float* %7, i64 %642
  %644 = bitcast float* %643 to <4 x float>*
  %wide.load125.5 = load <4 x float>, <4 x float>* %644, align 4, !tbaa !3164
  %645 = getelementptr inbounds float, float* %643, i64 4
  %646 = bitcast float* %645 to <4 x float>*
  %wide.load126.5 = load <4 x float>, <4 x float>* %646, align 4, !tbaa !3164
  %647 = getelementptr inbounds float, float* %4, i64 %642
  %648 = bitcast float* %647 to <4 x float>*
  store <4 x float> %wide.load125.5, <4 x float>* %648, align 4, !tbaa !3167
  %649 = getelementptr inbounds float, float* %647, i64 4
  %650 = bitcast float* %649 to <4 x float>*
  store <4 x float> %wide.load126.5, <4 x float>* %650, align 4, !tbaa !3167
  %651 = add nsw i64 %21, 560
  %652 = getelementptr inbounds float, float* %7, i64 %651
  %653 = bitcast float* %652 to <4 x float>*
  %wide.load125.6 = load <4 x float>, <4 x float>* %653, align 4, !tbaa !3164
  %654 = getelementptr inbounds float, float* %652, i64 4
  %655 = bitcast float* %654 to <4 x float>*
  %wide.load126.6 = load <4 x float>, <4 x float>* %655, align 4, !tbaa !3164
  %656 = getelementptr inbounds float, float* %4, i64 %651
  %657 = bitcast float* %656 to <4 x float>*
  store <4 x float> %wide.load125.6, <4 x float>* %657, align 4, !tbaa !3167
  %658 = getelementptr inbounds float, float* %656, i64 4
  %659 = bitcast float* %658 to <4 x float>*
  store <4 x float> %wide.load126.6, <4 x float>* %659, align 4, !tbaa !3167
  %660 = add nsw i64 %21, 568
  %661 = getelementptr inbounds float, float* %7, i64 %660
  %662 = bitcast float* %661 to <4 x float>*
  %wide.load125.7 = load <4 x float>, <4 x float>* %662, align 4, !tbaa !3164
  %663 = getelementptr inbounds float, float* %661, i64 4
  %664 = bitcast float* %663 to <4 x float>*
  %wide.load126.7 = load <4 x float>, <4 x float>* %664, align 4, !tbaa !3164
  %665 = getelementptr inbounds float, float* %4, i64 %660
  %666 = bitcast float* %665 to <4 x float>*
  store <4 x float> %wide.load125.7, <4 x float>* %666, align 4, !tbaa !3167
  %667 = getelementptr inbounds float, float* %665, i64 4
  %668 = bitcast float* %667 to <4 x float>*
  store <4 x float> %wide.load126.7, <4 x float>* %668, align 4, !tbaa !3167
  %669 = add nsw i64 %21, 576
  %670 = getelementptr inbounds float, float* %7, i64 %669
  %671 = bitcast float* %670 to <4 x float>*
  %wide.load125.8 = load <4 x float>, <4 x float>* %671, align 4, !tbaa !3164
  %672 = getelementptr inbounds float, float* %670, i64 4
  %673 = bitcast float* %672 to <4 x float>*
  %wide.load126.8 = load <4 x float>, <4 x float>* %673, align 4, !tbaa !3164
  %674 = getelementptr inbounds float, float* %4, i64 %669
  %675 = bitcast float* %674 to <4 x float>*
  store <4 x float> %wide.load125.8, <4 x float>* %675, align 4, !tbaa !3167
  %676 = getelementptr inbounds float, float* %674, i64 4
  %677 = bitcast float* %676 to <4 x float>*
  store <4 x float> %wide.load126.8, <4 x float>* %677, align 4, !tbaa !3167
  %678 = add nsw i64 %21, 584
  %679 = getelementptr inbounds float, float* %7, i64 %678
  %680 = bitcast float* %679 to <4 x float>*
  %wide.load125.9 = load <4 x float>, <4 x float>* %680, align 4, !tbaa !3164
  %681 = getelementptr inbounds float, float* %679, i64 4
  %682 = bitcast float* %681 to <4 x float>*
  %wide.load126.9 = load <4 x float>, <4 x float>* %682, align 4, !tbaa !3164
  %683 = getelementptr inbounds float, float* %4, i64 %678
  %684 = bitcast float* %683 to <4 x float>*
  store <4 x float> %wide.load125.9, <4 x float>* %684, align 4, !tbaa !3167
  %685 = getelementptr inbounds float, float* %683, i64 4
  %686 = bitcast float* %685 to <4 x float>*
  store <4 x float> %wide.load126.9, <4 x float>* %686, align 4, !tbaa !3167
  %687 = add nsw i64 %21, 592
  %688 = getelementptr inbounds float, float* %7, i64 %687
  %689 = bitcast float* %688 to <4 x float>*
  %wide.load125.10 = load <4 x float>, <4 x float>* %689, align 4, !tbaa !3164
  %690 = getelementptr inbounds float, float* %688, i64 4
  %691 = bitcast float* %690 to <4 x float>*
  %wide.load126.10 = load <4 x float>, <4 x float>* %691, align 4, !tbaa !3164
  %692 = getelementptr inbounds float, float* %4, i64 %687
  %693 = bitcast float* %692 to <4 x float>*
  store <4 x float> %wide.load125.10, <4 x float>* %693, align 4, !tbaa !3167
  %694 = getelementptr inbounds float, float* %692, i64 4
  %695 = bitcast float* %694 to <4 x float>*
  store <4 x float> %wide.load126.10, <4 x float>* %695, align 4, !tbaa !3167
  %696 = add nsw i64 %21, 600
  %697 = getelementptr inbounds float, float* %7, i64 %696
  %698 = bitcast float* %697 to <4 x float>*
  %wide.load125.11 = load <4 x float>, <4 x float>* %698, align 4, !tbaa !3164
  %699 = getelementptr inbounds float, float* %697, i64 4
  %700 = bitcast float* %699 to <4 x float>*
  %wide.load126.11 = load <4 x float>, <4 x float>* %700, align 4, !tbaa !3164
  %701 = getelementptr inbounds float, float* %4, i64 %696
  %702 = bitcast float* %701 to <4 x float>*
  store <4 x float> %wide.load125.11, <4 x float>* %702, align 4, !tbaa !3167
  %703 = getelementptr inbounds float, float* %701, i64 4
  %704 = bitcast float* %703 to <4 x float>*
  store <4 x float> %wide.load126.11, <4 x float>* %704, align 4, !tbaa !3167
  %705 = add nsw i64 %21, 608
  %706 = getelementptr inbounds float, float* %7, i64 %705
  %707 = bitcast float* %706 to <4 x float>*
  %wide.load125.12 = load <4 x float>, <4 x float>* %707, align 4, !tbaa !3164
  %708 = getelementptr inbounds float, float* %706, i64 4
  %709 = bitcast float* %708 to <4 x float>*
  %wide.load126.12 = load <4 x float>, <4 x float>* %709, align 4, !tbaa !3164
  %710 = getelementptr inbounds float, float* %4, i64 %705
  %711 = bitcast float* %710 to <4 x float>*
  store <4 x float> %wide.load125.12, <4 x float>* %711, align 4, !tbaa !3167
  %712 = getelementptr inbounds float, float* %710, i64 4
  %713 = bitcast float* %712 to <4 x float>*
  store <4 x float> %wide.load126.12, <4 x float>* %713, align 4, !tbaa !3167
  %714 = add nsw i64 %21, 616
  %715 = getelementptr inbounds float, float* %7, i64 %714
  %716 = bitcast float* %715 to <4 x float>*
  %wide.load125.13 = load <4 x float>, <4 x float>* %716, align 4, !tbaa !3164
  %717 = getelementptr inbounds float, float* %715, i64 4
  %718 = bitcast float* %717 to <4 x float>*
  %wide.load126.13 = load <4 x float>, <4 x float>* %718, align 4, !tbaa !3164
  %719 = getelementptr inbounds float, float* %4, i64 %714
  %720 = bitcast float* %719 to <4 x float>*
  store <4 x float> %wide.load125.13, <4 x float>* %720, align 4, !tbaa !3167
  %721 = getelementptr inbounds float, float* %719, i64 4
  %722 = bitcast float* %721 to <4 x float>*
  store <4 x float> %wide.load126.13, <4 x float>* %722, align 4, !tbaa !3167
  %723 = add nsw i64 %21, 624
  %724 = getelementptr inbounds float, float* %7, i64 %723
  %725 = bitcast float* %724 to <4 x float>*
  %wide.load125.14 = load <4 x float>, <4 x float>* %725, align 4, !tbaa !3164
  %726 = getelementptr inbounds float, float* %724, i64 4
  %727 = bitcast float* %726 to <4 x float>*
  %wide.load126.14 = load <4 x float>, <4 x float>* %727, align 4, !tbaa !3164
  %728 = getelementptr inbounds float, float* %4, i64 %723
  %729 = bitcast float* %728 to <4 x float>*
  store <4 x float> %wide.load125.14, <4 x float>* %729, align 4, !tbaa !3167
  %730 = getelementptr inbounds float, float* %728, i64 4
  %731 = bitcast float* %730 to <4 x float>*
  store <4 x float> %wide.load126.14, <4 x float>* %731, align 4, !tbaa !3167
  %732 = add nsw i64 %21, 632
  %733 = getelementptr inbounds float, float* %7, i64 %732
  %734 = bitcast float* %733 to <4 x float>*
  %wide.load125.15 = load <4 x float>, <4 x float>* %734, align 4, !tbaa !3164
  %735 = getelementptr inbounds float, float* %733, i64 4
  %736 = bitcast float* %735 to <4 x float>*
  %wide.load126.15 = load <4 x float>, <4 x float>* %736, align 4, !tbaa !3164
  %737 = getelementptr inbounds float, float* %4, i64 %732
  %738 = bitcast float* %737 to <4 x float>*
  store <4 x float> %wide.load125.15, <4 x float>* %738, align 4, !tbaa !3167
  %739 = getelementptr inbounds float, float* %737, i64 4
  %740 = bitcast float* %739 to <4 x float>*
  store <4 x float> %wide.load126.15, <4 x float>* %740, align 4, !tbaa !3167
  %741 = add nsw i64 %21, 640
  %742 = getelementptr inbounds float, float* %7, i64 %741
  %743 = bitcast float* %742 to <4 x float>*
  %wide.load125.16 = load <4 x float>, <4 x float>* %743, align 4, !tbaa !3164
  %744 = getelementptr inbounds float, float* %742, i64 4
  %745 = bitcast float* %744 to <4 x float>*
  %wide.load126.16 = load <4 x float>, <4 x float>* %745, align 4, !tbaa !3164
  %746 = getelementptr inbounds float, float* %4, i64 %741
  %747 = bitcast float* %746 to <4 x float>*
  store <4 x float> %wide.load125.16, <4 x float>* %747, align 4, !tbaa !3167
  %748 = getelementptr inbounds float, float* %746, i64 4
  %749 = bitcast float* %748 to <4 x float>*
  store <4 x float> %wide.load126.16, <4 x float>* %749, align 4, !tbaa !3167
  %750 = add nsw i64 %21, 648
  %751 = getelementptr inbounds float, float* %7, i64 %750
  %752 = bitcast float* %751 to <4 x float>*
  %wide.load125.17 = load <4 x float>, <4 x float>* %752, align 4, !tbaa !3164
  %753 = getelementptr inbounds float, float* %751, i64 4
  %754 = bitcast float* %753 to <4 x float>*
  %wide.load126.17 = load <4 x float>, <4 x float>* %754, align 4, !tbaa !3164
  %755 = getelementptr inbounds float, float* %4, i64 %750
  %756 = bitcast float* %755 to <4 x float>*
  store <4 x float> %wide.load125.17, <4 x float>* %756, align 4, !tbaa !3167
  %757 = getelementptr inbounds float, float* %755, i64 4
  %758 = bitcast float* %757 to <4 x float>*
  store <4 x float> %wide.load126.17, <4 x float>* %758, align 4, !tbaa !3167
  %759 = add nsw i64 %21, 656
  %760 = getelementptr inbounds float, float* %7, i64 %759
  %761 = bitcast float* %760 to <4 x float>*
  %wide.load125.18 = load <4 x float>, <4 x float>* %761, align 4, !tbaa !3164
  %762 = getelementptr inbounds float, float* %760, i64 4
  %763 = bitcast float* %762 to <4 x float>*
  %wide.load126.18 = load <4 x float>, <4 x float>* %763, align 4, !tbaa !3164
  %764 = getelementptr inbounds float, float* %4, i64 %759
  %765 = bitcast float* %764 to <4 x float>*
  store <4 x float> %wide.load125.18, <4 x float>* %765, align 4, !tbaa !3167
  %766 = getelementptr inbounds float, float* %764, i64 4
  %767 = bitcast float* %766 to <4 x float>*
  store <4 x float> %wide.load126.18, <4 x float>* %767, align 4, !tbaa !3167
  %768 = add nsw i64 %21, 664
  %769 = getelementptr inbounds float, float* %7, i64 %768
  %770 = bitcast float* %769 to <4 x float>*
  %wide.load125.19 = load <4 x float>, <4 x float>* %770, align 4, !tbaa !3164
  %771 = getelementptr inbounds float, float* %769, i64 4
  %772 = bitcast float* %771 to <4 x float>*
  %wide.load126.19 = load <4 x float>, <4 x float>* %772, align 4, !tbaa !3164
  %773 = getelementptr inbounds float, float* %4, i64 %768
  %774 = bitcast float* %773 to <4 x float>*
  store <4 x float> %wide.load125.19, <4 x float>* %774, align 4, !tbaa !3167
  %775 = getelementptr inbounds float, float* %773, i64 4
  %776 = bitcast float* %775 to <4 x float>*
  store <4 x float> %wide.load126.19, <4 x float>* %776, align 4, !tbaa !3167
  %777 = add nsw i64 %21, 672
  %778 = getelementptr inbounds float, float* %7, i64 %777
  %779 = bitcast float* %778 to <4 x float>*
  %wide.load125.20 = load <4 x float>, <4 x float>* %779, align 4, !tbaa !3164
  %780 = getelementptr inbounds float, float* %778, i64 4
  %781 = bitcast float* %780 to <4 x float>*
  %wide.load126.20 = load <4 x float>, <4 x float>* %781, align 4, !tbaa !3164
  %782 = getelementptr inbounds float, float* %4, i64 %777
  %783 = bitcast float* %782 to <4 x float>*
  store <4 x float> %wide.load125.20, <4 x float>* %783, align 4, !tbaa !3167
  %784 = getelementptr inbounds float, float* %782, i64 4
  %785 = bitcast float* %784 to <4 x float>*
  store <4 x float> %wide.load126.20, <4 x float>* %785, align 4, !tbaa !3167
  %786 = add nsw i64 %21, 680
  %787 = getelementptr inbounds float, float* %7, i64 %786
  %788 = bitcast float* %787 to <4 x float>*
  %wide.load125.21 = load <4 x float>, <4 x float>* %788, align 4, !tbaa !3164
  %789 = getelementptr inbounds float, float* %787, i64 4
  %790 = bitcast float* %789 to <4 x float>*
  %wide.load126.21 = load <4 x float>, <4 x float>* %790, align 4, !tbaa !3164
  %791 = getelementptr inbounds float, float* %4, i64 %786
  %792 = bitcast float* %791 to <4 x float>*
  store <4 x float> %wide.load125.21, <4 x float>* %792, align 4, !tbaa !3167
  %793 = getelementptr inbounds float, float* %791, i64 4
  %794 = bitcast float* %793 to <4 x float>*
  store <4 x float> %wide.load126.21, <4 x float>* %794, align 4, !tbaa !3167
  %795 = add nsw i64 %21, 688
  %796 = getelementptr inbounds float, float* %7, i64 %795
  %797 = bitcast float* %796 to <4 x float>*
  %wide.load125.22 = load <4 x float>, <4 x float>* %797, align 4, !tbaa !3164
  %798 = getelementptr inbounds float, float* %796, i64 4
  %799 = bitcast float* %798 to <4 x float>*
  %wide.load126.22 = load <4 x float>, <4 x float>* %799, align 4, !tbaa !3164
  %800 = getelementptr inbounds float, float* %4, i64 %795
  %801 = bitcast float* %800 to <4 x float>*
  store <4 x float> %wide.load125.22, <4 x float>* %801, align 4, !tbaa !3167
  %802 = getelementptr inbounds float, float* %800, i64 4
  %803 = bitcast float* %802 to <4 x float>*
  store <4 x float> %wide.load126.22, <4 x float>* %803, align 4, !tbaa !3167
  %804 = add nsw i64 %21, 696
  %805 = getelementptr inbounds float, float* %7, i64 %804
  %806 = bitcast float* %805 to <4 x float>*
  %wide.load125.23 = load <4 x float>, <4 x float>* %806, align 4, !tbaa !3164
  %807 = getelementptr inbounds float, float* %805, i64 4
  %808 = bitcast float* %807 to <4 x float>*
  %wide.load126.23 = load <4 x float>, <4 x float>* %808, align 4, !tbaa !3164
  %809 = getelementptr inbounds float, float* %4, i64 %804
  %810 = bitcast float* %809 to <4 x float>*
  store <4 x float> %wide.load125.23, <4 x float>* %810, align 4, !tbaa !3167
  %811 = getelementptr inbounds float, float* %809, i64 4
  %812 = bitcast float* %811 to <4 x float>*
  store <4 x float> %wide.load126.23, <4 x float>* %812, align 4, !tbaa !3167
  %813 = add nsw i64 %21, 704
  %814 = getelementptr inbounds float, float* %7, i64 %813
  %815 = bitcast float* %814 to <4 x float>*
  %wide.load125.24 = load <4 x float>, <4 x float>* %815, align 4, !tbaa !3164
  %816 = getelementptr inbounds float, float* %814, i64 4
  %817 = bitcast float* %816 to <4 x float>*
  %wide.load126.24 = load <4 x float>, <4 x float>* %817, align 4, !tbaa !3164
  %818 = getelementptr inbounds float, float* %4, i64 %813
  %819 = bitcast float* %818 to <4 x float>*
  store <4 x float> %wide.load125.24, <4 x float>* %819, align 4, !tbaa !3167
  %820 = getelementptr inbounds float, float* %818, i64 4
  %821 = bitcast float* %820 to <4 x float>*
  store <4 x float> %wide.load126.24, <4 x float>* %821, align 4, !tbaa !3167
  %822 = add nsw i64 %21, 712
  %823 = getelementptr inbounds float, float* %7, i64 %822
  %824 = bitcast float* %823 to <4 x float>*
  %wide.load125.25 = load <4 x float>, <4 x float>* %824, align 4, !tbaa !3164
  %825 = getelementptr inbounds float, float* %823, i64 4
  %826 = bitcast float* %825 to <4 x float>*
  %wide.load126.25 = load <4 x float>, <4 x float>* %826, align 4, !tbaa !3164
  %827 = getelementptr inbounds float, float* %4, i64 %822
  %828 = bitcast float* %827 to <4 x float>*
  store <4 x float> %wide.load125.25, <4 x float>* %828, align 4, !tbaa !3167
  %829 = getelementptr inbounds float, float* %827, i64 4
  %830 = bitcast float* %829 to <4 x float>*
  store <4 x float> %wide.load126.25, <4 x float>* %830, align 4, !tbaa !3167
  %831 = add nsw i64 %21, 720
  %832 = getelementptr inbounds float, float* %7, i64 %831
  %833 = bitcast float* %832 to <4 x float>*
  %wide.load125.26 = load <4 x float>, <4 x float>* %833, align 4, !tbaa !3164
  %834 = getelementptr inbounds float, float* %832, i64 4
  %835 = bitcast float* %834 to <4 x float>*
  %wide.load126.26 = load <4 x float>, <4 x float>* %835, align 4, !tbaa !3164
  %836 = getelementptr inbounds float, float* %4, i64 %831
  %837 = bitcast float* %836 to <4 x float>*
  store <4 x float> %wide.load125.26, <4 x float>* %837, align 4, !tbaa !3167
  %838 = getelementptr inbounds float, float* %836, i64 4
  %839 = bitcast float* %838 to <4 x float>*
  store <4 x float> %wide.load126.26, <4 x float>* %839, align 4, !tbaa !3167
  %840 = add nsw i64 %21, 728
  %841 = getelementptr inbounds float, float* %7, i64 %840
  %842 = bitcast float* %841 to <4 x float>*
  %wide.load125.27 = load <4 x float>, <4 x float>* %842, align 4, !tbaa !3164
  %843 = getelementptr inbounds float, float* %841, i64 4
  %844 = bitcast float* %843 to <4 x float>*
  %wide.load126.27 = load <4 x float>, <4 x float>* %844, align 4, !tbaa !3164
  %845 = getelementptr inbounds float, float* %4, i64 %840
  %846 = bitcast float* %845 to <4 x float>*
  store <4 x float> %wide.load125.27, <4 x float>* %846, align 4, !tbaa !3167
  %847 = getelementptr inbounds float, float* %845, i64 4
  %848 = bitcast float* %847 to <4 x float>*
  store <4 x float> %wide.load126.27, <4 x float>* %848, align 4, !tbaa !3167
  %849 = add nsw i64 %21, 736
  %850 = getelementptr inbounds float, float* %7, i64 %849
  %851 = bitcast float* %850 to <4 x float>*
  %wide.load125.28 = load <4 x float>, <4 x float>* %851, align 4, !tbaa !3164
  %852 = getelementptr inbounds float, float* %850, i64 4
  %853 = bitcast float* %852 to <4 x float>*
  %wide.load126.28 = load <4 x float>, <4 x float>* %853, align 4, !tbaa !3164
  %854 = getelementptr inbounds float, float* %4, i64 %849
  %855 = bitcast float* %854 to <4 x float>*
  store <4 x float> %wide.load125.28, <4 x float>* %855, align 4, !tbaa !3167
  %856 = getelementptr inbounds float, float* %854, i64 4
  %857 = bitcast float* %856 to <4 x float>*
  store <4 x float> %wide.load126.28, <4 x float>* %857, align 4, !tbaa !3167
  %858 = add nsw i64 %21, 744
  %859 = getelementptr inbounds float, float* %7, i64 %858
  %860 = bitcast float* %859 to <4 x float>*
  %wide.load125.29 = load <4 x float>, <4 x float>* %860, align 4, !tbaa !3164
  %861 = getelementptr inbounds float, float* %859, i64 4
  %862 = bitcast float* %861 to <4 x float>*
  %wide.load126.29 = load <4 x float>, <4 x float>* %862, align 4, !tbaa !3164
  %863 = getelementptr inbounds float, float* %4, i64 %858
  %864 = bitcast float* %863 to <4 x float>*
  store <4 x float> %wide.load125.29, <4 x float>* %864, align 4, !tbaa !3167
  %865 = getelementptr inbounds float, float* %863, i64 4
  %866 = bitcast float* %865 to <4 x float>*
  store <4 x float> %wide.load126.29, <4 x float>* %866, align 4, !tbaa !3167
  %867 = add nsw i64 %21, 752
  %868 = getelementptr inbounds float, float* %7, i64 %867
  %869 = bitcast float* %868 to <4 x float>*
  %wide.load125.30 = load <4 x float>, <4 x float>* %869, align 4, !tbaa !3164
  %870 = getelementptr inbounds float, float* %868, i64 4
  %871 = bitcast float* %870 to <4 x float>*
  %wide.load126.30 = load <4 x float>, <4 x float>* %871, align 4, !tbaa !3164
  %872 = getelementptr inbounds float, float* %4, i64 %867
  %873 = bitcast float* %872 to <4 x float>*
  store <4 x float> %wide.load125.30, <4 x float>* %873, align 4, !tbaa !3167
  %874 = getelementptr inbounds float, float* %872, i64 4
  %875 = bitcast float* %874 to <4 x float>*
  store <4 x float> %wide.load126.30, <4 x float>* %875, align 4, !tbaa !3167
  %876 = add nsw i64 %21, 760
  %877 = getelementptr inbounds float, float* %7, i64 %876
  %878 = bitcast float* %877 to <4 x float>*
  %wide.load125.31 = load <4 x float>, <4 x float>* %878, align 4, !tbaa !3164
  %879 = getelementptr inbounds float, float* %877, i64 4
  %880 = bitcast float* %879 to <4 x float>*
  %wide.load126.31 = load <4 x float>, <4 x float>* %880, align 4, !tbaa !3164
  %881 = getelementptr inbounds float, float* %4, i64 %876
  %882 = bitcast float* %881 to <4 x float>*
  store <4 x float> %wide.load125.31, <4 x float>* %882, align 4, !tbaa !3167
  %883 = getelementptr inbounds float, float* %881, i64 4
  %884 = bitcast float* %883 to <4 x float>*
  store <4 x float> %wide.load126.31, <4 x float>* %884, align 4, !tbaa !3167
  %885 = add nsw i64 %21, 768
  %886 = getelementptr inbounds float, float* %7, i64 %885
  %887 = bitcast float* %886 to <4 x float>*
  %wide.load115 = load <4 x float>, <4 x float>* %887, align 4, !tbaa !3164
  %888 = getelementptr inbounds float, float* %886, i64 4
  %889 = bitcast float* %888 to <4 x float>*
  %wide.load116 = load <4 x float>, <4 x float>* %889, align 4, !tbaa !3164
  %890 = getelementptr inbounds float, float* %4, i64 %885
  %891 = bitcast float* %890 to <4 x float>*
  store <4 x float> %wide.load115, <4 x float>* %891, align 4, !tbaa !3167
  %892 = getelementptr inbounds float, float* %890, i64 4
  %893 = bitcast float* %892 to <4 x float>*
  store <4 x float> %wide.load116, <4 x float>* %893, align 4, !tbaa !3167
  %894 = add nsw i64 %21, 776
  %895 = getelementptr inbounds float, float* %7, i64 %894
  %896 = bitcast float* %895 to <4 x float>*
  %wide.load115.1 = load <4 x float>, <4 x float>* %896, align 4, !tbaa !3164
  %897 = getelementptr inbounds float, float* %895, i64 4
  %898 = bitcast float* %897 to <4 x float>*
  %wide.load116.1 = load <4 x float>, <4 x float>* %898, align 4, !tbaa !3164
  %899 = getelementptr inbounds float, float* %4, i64 %894
  %900 = bitcast float* %899 to <4 x float>*
  store <4 x float> %wide.load115.1, <4 x float>* %900, align 4, !tbaa !3167
  %901 = getelementptr inbounds float, float* %899, i64 4
  %902 = bitcast float* %901 to <4 x float>*
  store <4 x float> %wide.load116.1, <4 x float>* %902, align 4, !tbaa !3167
  %903 = add nsw i64 %21, 784
  %904 = getelementptr inbounds float, float* %7, i64 %903
  %905 = bitcast float* %904 to <4 x float>*
  %wide.load115.2 = load <4 x float>, <4 x float>* %905, align 4, !tbaa !3164
  %906 = getelementptr inbounds float, float* %904, i64 4
  %907 = bitcast float* %906 to <4 x float>*
  %wide.load116.2 = load <4 x float>, <4 x float>* %907, align 4, !tbaa !3164
  %908 = getelementptr inbounds float, float* %4, i64 %903
  %909 = bitcast float* %908 to <4 x float>*
  store <4 x float> %wide.load115.2, <4 x float>* %909, align 4, !tbaa !3167
  %910 = getelementptr inbounds float, float* %908, i64 4
  %911 = bitcast float* %910 to <4 x float>*
  store <4 x float> %wide.load116.2, <4 x float>* %911, align 4, !tbaa !3167
  %912 = add nsw i64 %21, 792
  %913 = getelementptr inbounds float, float* %7, i64 %912
  %914 = bitcast float* %913 to <4 x float>*
  %wide.load115.3 = load <4 x float>, <4 x float>* %914, align 4, !tbaa !3164
  %915 = getelementptr inbounds float, float* %913, i64 4
  %916 = bitcast float* %915 to <4 x float>*
  %wide.load116.3 = load <4 x float>, <4 x float>* %916, align 4, !tbaa !3164
  %917 = getelementptr inbounds float, float* %4, i64 %912
  %918 = bitcast float* %917 to <4 x float>*
  store <4 x float> %wide.load115.3, <4 x float>* %918, align 4, !tbaa !3167
  %919 = getelementptr inbounds float, float* %917, i64 4
  %920 = bitcast float* %919 to <4 x float>*
  store <4 x float> %wide.load116.3, <4 x float>* %920, align 4, !tbaa !3167
  %921 = add nsw i64 %21, 800
  %922 = getelementptr inbounds float, float* %7, i64 %921
  %923 = bitcast float* %922 to <4 x float>*
  %wide.load115.4 = load <4 x float>, <4 x float>* %923, align 4, !tbaa !3164
  %924 = getelementptr inbounds float, float* %922, i64 4
  %925 = bitcast float* %924 to <4 x float>*
  %wide.load116.4 = load <4 x float>, <4 x float>* %925, align 4, !tbaa !3164
  %926 = getelementptr inbounds float, float* %4, i64 %921
  %927 = bitcast float* %926 to <4 x float>*
  store <4 x float> %wide.load115.4, <4 x float>* %927, align 4, !tbaa !3167
  %928 = getelementptr inbounds float, float* %926, i64 4
  %929 = bitcast float* %928 to <4 x float>*
  store <4 x float> %wide.load116.4, <4 x float>* %929, align 4, !tbaa !3167
  %930 = add nsw i64 %21, 808
  %931 = getelementptr inbounds float, float* %7, i64 %930
  %932 = bitcast float* %931 to <4 x float>*
  %wide.load115.5 = load <4 x float>, <4 x float>* %932, align 4, !tbaa !3164
  %933 = getelementptr inbounds float, float* %931, i64 4
  %934 = bitcast float* %933 to <4 x float>*
  %wide.load116.5 = load <4 x float>, <4 x float>* %934, align 4, !tbaa !3164
  %935 = getelementptr inbounds float, float* %4, i64 %930
  %936 = bitcast float* %935 to <4 x float>*
  store <4 x float> %wide.load115.5, <4 x float>* %936, align 4, !tbaa !3167
  %937 = getelementptr inbounds float, float* %935, i64 4
  %938 = bitcast float* %937 to <4 x float>*
  store <4 x float> %wide.load116.5, <4 x float>* %938, align 4, !tbaa !3167
  %939 = add nsw i64 %21, 816
  %940 = getelementptr inbounds float, float* %7, i64 %939
  %941 = bitcast float* %940 to <4 x float>*
  %wide.load115.6 = load <4 x float>, <4 x float>* %941, align 4, !tbaa !3164
  %942 = getelementptr inbounds float, float* %940, i64 4
  %943 = bitcast float* %942 to <4 x float>*
  %wide.load116.6 = load <4 x float>, <4 x float>* %943, align 4, !tbaa !3164
  %944 = getelementptr inbounds float, float* %4, i64 %939
  %945 = bitcast float* %944 to <4 x float>*
  store <4 x float> %wide.load115.6, <4 x float>* %945, align 4, !tbaa !3167
  %946 = getelementptr inbounds float, float* %944, i64 4
  %947 = bitcast float* %946 to <4 x float>*
  store <4 x float> %wide.load116.6, <4 x float>* %947, align 4, !tbaa !3167
  %948 = add nsw i64 %21, 824
  %949 = getelementptr inbounds float, float* %7, i64 %948
  %950 = bitcast float* %949 to <4 x float>*
  %wide.load115.7 = load <4 x float>, <4 x float>* %950, align 4, !tbaa !3164
  %951 = getelementptr inbounds float, float* %949, i64 4
  %952 = bitcast float* %951 to <4 x float>*
  %wide.load116.7 = load <4 x float>, <4 x float>* %952, align 4, !tbaa !3164
  %953 = getelementptr inbounds float, float* %4, i64 %948
  %954 = bitcast float* %953 to <4 x float>*
  store <4 x float> %wide.load115.7, <4 x float>* %954, align 4, !tbaa !3167
  %955 = getelementptr inbounds float, float* %953, i64 4
  %956 = bitcast float* %955 to <4 x float>*
  store <4 x float> %wide.load116.7, <4 x float>* %956, align 4, !tbaa !3167
  %957 = add nsw i64 %21, 832
  %958 = getelementptr inbounds float, float* %7, i64 %957
  %959 = bitcast float* %958 to <4 x float>*
  %wide.load115.8 = load <4 x float>, <4 x float>* %959, align 4, !tbaa !3164
  %960 = getelementptr inbounds float, float* %958, i64 4
  %961 = bitcast float* %960 to <4 x float>*
  %wide.load116.8 = load <4 x float>, <4 x float>* %961, align 4, !tbaa !3164
  %962 = getelementptr inbounds float, float* %4, i64 %957
  %963 = bitcast float* %962 to <4 x float>*
  store <4 x float> %wide.load115.8, <4 x float>* %963, align 4, !tbaa !3167
  %964 = getelementptr inbounds float, float* %962, i64 4
  %965 = bitcast float* %964 to <4 x float>*
  store <4 x float> %wide.load116.8, <4 x float>* %965, align 4, !tbaa !3167
  %966 = add nsw i64 %21, 840
  %967 = getelementptr inbounds float, float* %7, i64 %966
  %968 = bitcast float* %967 to <4 x float>*
  %wide.load115.9 = load <4 x float>, <4 x float>* %968, align 4, !tbaa !3164
  %969 = getelementptr inbounds float, float* %967, i64 4
  %970 = bitcast float* %969 to <4 x float>*
  %wide.load116.9 = load <4 x float>, <4 x float>* %970, align 4, !tbaa !3164
  %971 = getelementptr inbounds float, float* %4, i64 %966
  %972 = bitcast float* %971 to <4 x float>*
  store <4 x float> %wide.load115.9, <4 x float>* %972, align 4, !tbaa !3167
  %973 = getelementptr inbounds float, float* %971, i64 4
  %974 = bitcast float* %973 to <4 x float>*
  store <4 x float> %wide.load116.9, <4 x float>* %974, align 4, !tbaa !3167
  %975 = add nsw i64 %21, 848
  %976 = getelementptr inbounds float, float* %7, i64 %975
  %977 = bitcast float* %976 to <4 x float>*
  %wide.load115.10 = load <4 x float>, <4 x float>* %977, align 4, !tbaa !3164
  %978 = getelementptr inbounds float, float* %976, i64 4
  %979 = bitcast float* %978 to <4 x float>*
  %wide.load116.10 = load <4 x float>, <4 x float>* %979, align 4, !tbaa !3164
  %980 = getelementptr inbounds float, float* %4, i64 %975
  %981 = bitcast float* %980 to <4 x float>*
  store <4 x float> %wide.load115.10, <4 x float>* %981, align 4, !tbaa !3167
  %982 = getelementptr inbounds float, float* %980, i64 4
  %983 = bitcast float* %982 to <4 x float>*
  store <4 x float> %wide.load116.10, <4 x float>* %983, align 4, !tbaa !3167
  %984 = add nsw i64 %21, 856
  %985 = getelementptr inbounds float, float* %7, i64 %984
  %986 = bitcast float* %985 to <4 x float>*
  %wide.load115.11 = load <4 x float>, <4 x float>* %986, align 4, !tbaa !3164
  %987 = getelementptr inbounds float, float* %985, i64 4
  %988 = bitcast float* %987 to <4 x float>*
  %wide.load116.11 = load <4 x float>, <4 x float>* %988, align 4, !tbaa !3164
  %989 = getelementptr inbounds float, float* %4, i64 %984
  %990 = bitcast float* %989 to <4 x float>*
  store <4 x float> %wide.load115.11, <4 x float>* %990, align 4, !tbaa !3167
  %991 = getelementptr inbounds float, float* %989, i64 4
  %992 = bitcast float* %991 to <4 x float>*
  store <4 x float> %wide.load116.11, <4 x float>* %992, align 4, !tbaa !3167
  %993 = add nsw i64 %21, 864
  %994 = getelementptr inbounds float, float* %7, i64 %993
  %995 = bitcast float* %994 to <4 x float>*
  %wide.load115.12 = load <4 x float>, <4 x float>* %995, align 4, !tbaa !3164
  %996 = getelementptr inbounds float, float* %994, i64 4
  %997 = bitcast float* %996 to <4 x float>*
  %wide.load116.12 = load <4 x float>, <4 x float>* %997, align 4, !tbaa !3164
  %998 = getelementptr inbounds float, float* %4, i64 %993
  %999 = bitcast float* %998 to <4 x float>*
  store <4 x float> %wide.load115.12, <4 x float>* %999, align 4, !tbaa !3167
  %1000 = getelementptr inbounds float, float* %998, i64 4
  %1001 = bitcast float* %1000 to <4 x float>*
  store <4 x float> %wide.load116.12, <4 x float>* %1001, align 4, !tbaa !3167
  %1002 = add nsw i64 %21, 872
  %1003 = getelementptr inbounds float, float* %7, i64 %1002
  %1004 = bitcast float* %1003 to <4 x float>*
  %wide.load115.13 = load <4 x float>, <4 x float>* %1004, align 4, !tbaa !3164
  %1005 = getelementptr inbounds float, float* %1003, i64 4
  %1006 = bitcast float* %1005 to <4 x float>*
  %wide.load116.13 = load <4 x float>, <4 x float>* %1006, align 4, !tbaa !3164
  %1007 = getelementptr inbounds float, float* %4, i64 %1002
  %1008 = bitcast float* %1007 to <4 x float>*
  store <4 x float> %wide.load115.13, <4 x float>* %1008, align 4, !tbaa !3167
  %1009 = getelementptr inbounds float, float* %1007, i64 4
  %1010 = bitcast float* %1009 to <4 x float>*
  store <4 x float> %wide.load116.13, <4 x float>* %1010, align 4, !tbaa !3167
  %1011 = add nsw i64 %21, 880
  %1012 = getelementptr inbounds float, float* %7, i64 %1011
  %1013 = bitcast float* %1012 to <4 x float>*
  %wide.load115.14 = load <4 x float>, <4 x float>* %1013, align 4, !tbaa !3164
  %1014 = getelementptr inbounds float, float* %1012, i64 4
  %1015 = bitcast float* %1014 to <4 x float>*
  %wide.load116.14 = load <4 x float>, <4 x float>* %1015, align 4, !tbaa !3164
  %1016 = getelementptr inbounds float, float* %4, i64 %1011
  %1017 = bitcast float* %1016 to <4 x float>*
  store <4 x float> %wide.load115.14, <4 x float>* %1017, align 4, !tbaa !3167
  %1018 = getelementptr inbounds float, float* %1016, i64 4
  %1019 = bitcast float* %1018 to <4 x float>*
  store <4 x float> %wide.load116.14, <4 x float>* %1019, align 4, !tbaa !3167
  %1020 = add nsw i64 %21, 888
  %1021 = getelementptr inbounds float, float* %7, i64 %1020
  %1022 = bitcast float* %1021 to <4 x float>*
  %wide.load115.15 = load <4 x float>, <4 x float>* %1022, align 4, !tbaa !3164
  %1023 = getelementptr inbounds float, float* %1021, i64 4
  %1024 = bitcast float* %1023 to <4 x float>*
  %wide.load116.15 = load <4 x float>, <4 x float>* %1024, align 4, !tbaa !3164
  %1025 = getelementptr inbounds float, float* %4, i64 %1020
  %1026 = bitcast float* %1025 to <4 x float>*
  store <4 x float> %wide.load115.15, <4 x float>* %1026, align 4, !tbaa !3167
  %1027 = getelementptr inbounds float, float* %1025, i64 4
  %1028 = bitcast float* %1027 to <4 x float>*
  store <4 x float> %wide.load116.15, <4 x float>* %1028, align 4, !tbaa !3167
  %1029 = add nsw i64 %21, 896
  %1030 = getelementptr inbounds float, float* %7, i64 %1029
  %1031 = bitcast float* %1030 to <4 x float>*
  %wide.load115.16 = load <4 x float>, <4 x float>* %1031, align 4, !tbaa !3164
  %1032 = getelementptr inbounds float, float* %1030, i64 4
  %1033 = bitcast float* %1032 to <4 x float>*
  %wide.load116.16 = load <4 x float>, <4 x float>* %1033, align 4, !tbaa !3164
  %1034 = getelementptr inbounds float, float* %4, i64 %1029
  %1035 = bitcast float* %1034 to <4 x float>*
  store <4 x float> %wide.load115.16, <4 x float>* %1035, align 4, !tbaa !3167
  %1036 = getelementptr inbounds float, float* %1034, i64 4
  %1037 = bitcast float* %1036 to <4 x float>*
  store <4 x float> %wide.load116.16, <4 x float>* %1037, align 4, !tbaa !3167
  %1038 = add nsw i64 %21, 904
  %1039 = getelementptr inbounds float, float* %7, i64 %1038
  %1040 = bitcast float* %1039 to <4 x float>*
  %wide.load115.17 = load <4 x float>, <4 x float>* %1040, align 4, !tbaa !3164
  %1041 = getelementptr inbounds float, float* %1039, i64 4
  %1042 = bitcast float* %1041 to <4 x float>*
  %wide.load116.17 = load <4 x float>, <4 x float>* %1042, align 4, !tbaa !3164
  %1043 = getelementptr inbounds float, float* %4, i64 %1038
  %1044 = bitcast float* %1043 to <4 x float>*
  store <4 x float> %wide.load115.17, <4 x float>* %1044, align 4, !tbaa !3167
  %1045 = getelementptr inbounds float, float* %1043, i64 4
  %1046 = bitcast float* %1045 to <4 x float>*
  store <4 x float> %wide.load116.17, <4 x float>* %1046, align 4, !tbaa !3167
  %1047 = add nsw i64 %21, 912
  %1048 = getelementptr inbounds float, float* %7, i64 %1047
  %1049 = bitcast float* %1048 to <4 x float>*
  %wide.load115.18 = load <4 x float>, <4 x float>* %1049, align 4, !tbaa !3164
  %1050 = getelementptr inbounds float, float* %1048, i64 4
  %1051 = bitcast float* %1050 to <4 x float>*
  %wide.load116.18 = load <4 x float>, <4 x float>* %1051, align 4, !tbaa !3164
  %1052 = getelementptr inbounds float, float* %4, i64 %1047
  %1053 = bitcast float* %1052 to <4 x float>*
  store <4 x float> %wide.load115.18, <4 x float>* %1053, align 4, !tbaa !3167
  %1054 = getelementptr inbounds float, float* %1052, i64 4
  %1055 = bitcast float* %1054 to <4 x float>*
  store <4 x float> %wide.load116.18, <4 x float>* %1055, align 4, !tbaa !3167
  %1056 = add nsw i64 %21, 920
  %1057 = getelementptr inbounds float, float* %7, i64 %1056
  %1058 = bitcast float* %1057 to <4 x float>*
  %wide.load115.19 = load <4 x float>, <4 x float>* %1058, align 4, !tbaa !3164
  %1059 = getelementptr inbounds float, float* %1057, i64 4
  %1060 = bitcast float* %1059 to <4 x float>*
  %wide.load116.19 = load <4 x float>, <4 x float>* %1060, align 4, !tbaa !3164
  %1061 = getelementptr inbounds float, float* %4, i64 %1056
  %1062 = bitcast float* %1061 to <4 x float>*
  store <4 x float> %wide.load115.19, <4 x float>* %1062, align 4, !tbaa !3167
  %1063 = getelementptr inbounds float, float* %1061, i64 4
  %1064 = bitcast float* %1063 to <4 x float>*
  store <4 x float> %wide.load116.19, <4 x float>* %1064, align 4, !tbaa !3167
  %1065 = add nsw i64 %21, 928
  %1066 = getelementptr inbounds float, float* %7, i64 %1065
  %1067 = bitcast float* %1066 to <4 x float>*
  %wide.load115.20 = load <4 x float>, <4 x float>* %1067, align 4, !tbaa !3164
  %1068 = getelementptr inbounds float, float* %1066, i64 4
  %1069 = bitcast float* %1068 to <4 x float>*
  %wide.load116.20 = load <4 x float>, <4 x float>* %1069, align 4, !tbaa !3164
  %1070 = getelementptr inbounds float, float* %4, i64 %1065
  %1071 = bitcast float* %1070 to <4 x float>*
  store <4 x float> %wide.load115.20, <4 x float>* %1071, align 4, !tbaa !3167
  %1072 = getelementptr inbounds float, float* %1070, i64 4
  %1073 = bitcast float* %1072 to <4 x float>*
  store <4 x float> %wide.load116.20, <4 x float>* %1073, align 4, !tbaa !3167
  %1074 = add nsw i64 %21, 936
  %1075 = getelementptr inbounds float, float* %7, i64 %1074
  %1076 = bitcast float* %1075 to <4 x float>*
  %wide.load115.21 = load <4 x float>, <4 x float>* %1076, align 4, !tbaa !3164
  %1077 = getelementptr inbounds float, float* %1075, i64 4
  %1078 = bitcast float* %1077 to <4 x float>*
  %wide.load116.21 = load <4 x float>, <4 x float>* %1078, align 4, !tbaa !3164
  %1079 = getelementptr inbounds float, float* %4, i64 %1074
  %1080 = bitcast float* %1079 to <4 x float>*
  store <4 x float> %wide.load115.21, <4 x float>* %1080, align 4, !tbaa !3167
  %1081 = getelementptr inbounds float, float* %1079, i64 4
  %1082 = bitcast float* %1081 to <4 x float>*
  store <4 x float> %wide.load116.21, <4 x float>* %1082, align 4, !tbaa !3167
  %1083 = add nsw i64 %21, 944
  %1084 = getelementptr inbounds float, float* %7, i64 %1083
  %1085 = bitcast float* %1084 to <4 x float>*
  %wide.load115.22 = load <4 x float>, <4 x float>* %1085, align 4, !tbaa !3164
  %1086 = getelementptr inbounds float, float* %1084, i64 4
  %1087 = bitcast float* %1086 to <4 x float>*
  %wide.load116.22 = load <4 x float>, <4 x float>* %1087, align 4, !tbaa !3164
  %1088 = getelementptr inbounds float, float* %4, i64 %1083
  %1089 = bitcast float* %1088 to <4 x float>*
  store <4 x float> %wide.load115.22, <4 x float>* %1089, align 4, !tbaa !3167
  %1090 = getelementptr inbounds float, float* %1088, i64 4
  %1091 = bitcast float* %1090 to <4 x float>*
  store <4 x float> %wide.load116.22, <4 x float>* %1091, align 4, !tbaa !3167
  %1092 = add nsw i64 %21, 952
  %1093 = getelementptr inbounds float, float* %7, i64 %1092
  %1094 = bitcast float* %1093 to <4 x float>*
  %wide.load115.23 = load <4 x float>, <4 x float>* %1094, align 4, !tbaa !3164
  %1095 = getelementptr inbounds float, float* %1093, i64 4
  %1096 = bitcast float* %1095 to <4 x float>*
  %wide.load116.23 = load <4 x float>, <4 x float>* %1096, align 4, !tbaa !3164
  %1097 = getelementptr inbounds float, float* %4, i64 %1092
  %1098 = bitcast float* %1097 to <4 x float>*
  store <4 x float> %wide.load115.23, <4 x float>* %1098, align 4, !tbaa !3167
  %1099 = getelementptr inbounds float, float* %1097, i64 4
  %1100 = bitcast float* %1099 to <4 x float>*
  store <4 x float> %wide.load116.23, <4 x float>* %1100, align 4, !tbaa !3167
  %1101 = add nsw i64 %21, 960
  %1102 = getelementptr inbounds float, float* %7, i64 %1101
  %1103 = bitcast float* %1102 to <4 x float>*
  %wide.load115.24 = load <4 x float>, <4 x float>* %1103, align 4, !tbaa !3164
  %1104 = getelementptr inbounds float, float* %1102, i64 4
  %1105 = bitcast float* %1104 to <4 x float>*
  %wide.load116.24 = load <4 x float>, <4 x float>* %1105, align 4, !tbaa !3164
  %1106 = getelementptr inbounds float, float* %4, i64 %1101
  %1107 = bitcast float* %1106 to <4 x float>*
  store <4 x float> %wide.load115.24, <4 x float>* %1107, align 4, !tbaa !3167
  %1108 = getelementptr inbounds float, float* %1106, i64 4
  %1109 = bitcast float* %1108 to <4 x float>*
  store <4 x float> %wide.load116.24, <4 x float>* %1109, align 4, !tbaa !3167
  %1110 = add nsw i64 %21, 968
  %1111 = getelementptr inbounds float, float* %7, i64 %1110
  %1112 = bitcast float* %1111 to <4 x float>*
  %wide.load115.25 = load <4 x float>, <4 x float>* %1112, align 4, !tbaa !3164
  %1113 = getelementptr inbounds float, float* %1111, i64 4
  %1114 = bitcast float* %1113 to <4 x float>*
  %wide.load116.25 = load <4 x float>, <4 x float>* %1114, align 4, !tbaa !3164
  %1115 = getelementptr inbounds float, float* %4, i64 %1110
  %1116 = bitcast float* %1115 to <4 x float>*
  store <4 x float> %wide.load115.25, <4 x float>* %1116, align 4, !tbaa !3167
  %1117 = getelementptr inbounds float, float* %1115, i64 4
  %1118 = bitcast float* %1117 to <4 x float>*
  store <4 x float> %wide.load116.25, <4 x float>* %1118, align 4, !tbaa !3167
  %1119 = add nsw i64 %21, 976
  %1120 = getelementptr inbounds float, float* %7, i64 %1119
  %1121 = bitcast float* %1120 to <4 x float>*
  %wide.load115.26 = load <4 x float>, <4 x float>* %1121, align 4, !tbaa !3164
  %1122 = getelementptr inbounds float, float* %1120, i64 4
  %1123 = bitcast float* %1122 to <4 x float>*
  %wide.load116.26 = load <4 x float>, <4 x float>* %1123, align 4, !tbaa !3164
  %1124 = getelementptr inbounds float, float* %4, i64 %1119
  %1125 = bitcast float* %1124 to <4 x float>*
  store <4 x float> %wide.load115.26, <4 x float>* %1125, align 4, !tbaa !3167
  %1126 = getelementptr inbounds float, float* %1124, i64 4
  %1127 = bitcast float* %1126 to <4 x float>*
  store <4 x float> %wide.load116.26, <4 x float>* %1127, align 4, !tbaa !3167
  %1128 = add nsw i64 %21, 984
  %1129 = getelementptr inbounds float, float* %7, i64 %1128
  %1130 = bitcast float* %1129 to <4 x float>*
  %wide.load115.27 = load <4 x float>, <4 x float>* %1130, align 4, !tbaa !3164
  %1131 = getelementptr inbounds float, float* %1129, i64 4
  %1132 = bitcast float* %1131 to <4 x float>*
  %wide.load116.27 = load <4 x float>, <4 x float>* %1132, align 4, !tbaa !3164
  %1133 = getelementptr inbounds float, float* %4, i64 %1128
  %1134 = bitcast float* %1133 to <4 x float>*
  store <4 x float> %wide.load115.27, <4 x float>* %1134, align 4, !tbaa !3167
  %1135 = getelementptr inbounds float, float* %1133, i64 4
  %1136 = bitcast float* %1135 to <4 x float>*
  store <4 x float> %wide.load116.27, <4 x float>* %1136, align 4, !tbaa !3167
  %1137 = add nsw i64 %21, 992
  %1138 = getelementptr inbounds float, float* %7, i64 %1137
  %1139 = bitcast float* %1138 to <4 x float>*
  %wide.load115.28 = load <4 x float>, <4 x float>* %1139, align 4, !tbaa !3164
  %1140 = getelementptr inbounds float, float* %1138, i64 4
  %1141 = bitcast float* %1140 to <4 x float>*
  %wide.load116.28 = load <4 x float>, <4 x float>* %1141, align 4, !tbaa !3164
  %1142 = getelementptr inbounds float, float* %4, i64 %1137
  %1143 = bitcast float* %1142 to <4 x float>*
  store <4 x float> %wide.load115.28, <4 x float>* %1143, align 4, !tbaa !3167
  %1144 = getelementptr inbounds float, float* %1142, i64 4
  %1145 = bitcast float* %1144 to <4 x float>*
  store <4 x float> %wide.load116.28, <4 x float>* %1145, align 4, !tbaa !3167
  %1146 = add nsw i64 %21, 1000
  %1147 = getelementptr inbounds float, float* %7, i64 %1146
  %1148 = bitcast float* %1147 to <4 x float>*
  %wide.load115.29 = load <4 x float>, <4 x float>* %1148, align 4, !tbaa !3164
  %1149 = getelementptr inbounds float, float* %1147, i64 4
  %1150 = bitcast float* %1149 to <4 x float>*
  %wide.load116.29 = load <4 x float>, <4 x float>* %1150, align 4, !tbaa !3164
  %1151 = getelementptr inbounds float, float* %4, i64 %1146
  %1152 = bitcast float* %1151 to <4 x float>*
  store <4 x float> %wide.load115.29, <4 x float>* %1152, align 4, !tbaa !3167
  %1153 = getelementptr inbounds float, float* %1151, i64 4
  %1154 = bitcast float* %1153 to <4 x float>*
  store <4 x float> %wide.load116.29, <4 x float>* %1154, align 4, !tbaa !3167
  %1155 = add nsw i64 %21, 1008
  %1156 = getelementptr inbounds float, float* %7, i64 %1155
  %1157 = bitcast float* %1156 to <4 x float>*
  %wide.load115.30 = load <4 x float>, <4 x float>* %1157, align 4, !tbaa !3164
  %1158 = getelementptr inbounds float, float* %1156, i64 4
  %1159 = bitcast float* %1158 to <4 x float>*
  %wide.load116.30 = load <4 x float>, <4 x float>* %1159, align 4, !tbaa !3164
  %1160 = getelementptr inbounds float, float* %4, i64 %1155
  %1161 = bitcast float* %1160 to <4 x float>*
  store <4 x float> %wide.load115.30, <4 x float>* %1161, align 4, !tbaa !3167
  %1162 = getelementptr inbounds float, float* %1160, i64 4
  %1163 = bitcast float* %1162 to <4 x float>*
  store <4 x float> %wide.load116.30, <4 x float>* %1163, align 4, !tbaa !3167
  %1164 = add nsw i64 %21, 1016
  %1165 = getelementptr inbounds float, float* %7, i64 %1164
  %1166 = bitcast float* %1165 to <4 x float>*
  %wide.load115.31 = load <4 x float>, <4 x float>* %1166, align 4, !tbaa !3164
  %1167 = getelementptr inbounds float, float* %1165, i64 4
  %1168 = bitcast float* %1167 to <4 x float>*
  %wide.load116.31 = load <4 x float>, <4 x float>* %1168, align 4, !tbaa !3164
  %1169 = getelementptr inbounds float, float* %4, i64 %1164
  %1170 = bitcast float* %1169 to <4 x float>*
  store <4 x float> %wide.load115.31, <4 x float>* %1170, align 4, !tbaa !3167
  %1171 = getelementptr inbounds float, float* %1169, i64 4
  %1172 = bitcast float* %1171 to <4 x float>*
  store <4 x float> %wide.load116.31, <4 x float>* %1172, align 4, !tbaa !3167
  %1173 = add nsw i64 %21, 1024
  %1174 = getelementptr inbounds float, float* %7, i64 %1173
  %1175 = bitcast float* %1174 to <4 x float>*
  %wide.load105 = load <4 x float>, <4 x float>* %1175, align 4, !tbaa !3164
  %1176 = getelementptr inbounds float, float* %1174, i64 4
  %1177 = bitcast float* %1176 to <4 x float>*
  %wide.load106 = load <4 x float>, <4 x float>* %1177, align 4, !tbaa !3164
  %1178 = getelementptr inbounds float, float* %4, i64 %1173
  %1179 = bitcast float* %1178 to <4 x float>*
  store <4 x float> %wide.load105, <4 x float>* %1179, align 4, !tbaa !3167
  %1180 = getelementptr inbounds float, float* %1178, i64 4
  %1181 = bitcast float* %1180 to <4 x float>*
  store <4 x float> %wide.load106, <4 x float>* %1181, align 4, !tbaa !3167
  %1182 = add nsw i64 %21, 1032
  %1183 = getelementptr inbounds float, float* %7, i64 %1182
  %1184 = bitcast float* %1183 to <4 x float>*
  %wide.load105.1 = load <4 x float>, <4 x float>* %1184, align 4, !tbaa !3164
  %1185 = getelementptr inbounds float, float* %1183, i64 4
  %1186 = bitcast float* %1185 to <4 x float>*
  %wide.load106.1 = load <4 x float>, <4 x float>* %1186, align 4, !tbaa !3164
  %1187 = getelementptr inbounds float, float* %4, i64 %1182
  %1188 = bitcast float* %1187 to <4 x float>*
  store <4 x float> %wide.load105.1, <4 x float>* %1188, align 4, !tbaa !3167
  %1189 = getelementptr inbounds float, float* %1187, i64 4
  %1190 = bitcast float* %1189 to <4 x float>*
  store <4 x float> %wide.load106.1, <4 x float>* %1190, align 4, !tbaa !3167
  %1191 = add nsw i64 %21, 1040
  %1192 = getelementptr inbounds float, float* %7, i64 %1191
  %1193 = bitcast float* %1192 to <4 x float>*
  %wide.load105.2 = load <4 x float>, <4 x float>* %1193, align 4, !tbaa !3164
  %1194 = getelementptr inbounds float, float* %1192, i64 4
  %1195 = bitcast float* %1194 to <4 x float>*
  %wide.load106.2 = load <4 x float>, <4 x float>* %1195, align 4, !tbaa !3164
  %1196 = getelementptr inbounds float, float* %4, i64 %1191
  %1197 = bitcast float* %1196 to <4 x float>*
  store <4 x float> %wide.load105.2, <4 x float>* %1197, align 4, !tbaa !3167
  %1198 = getelementptr inbounds float, float* %1196, i64 4
  %1199 = bitcast float* %1198 to <4 x float>*
  store <4 x float> %wide.load106.2, <4 x float>* %1199, align 4, !tbaa !3167
  %1200 = add nsw i64 %21, 1048
  %1201 = getelementptr inbounds float, float* %7, i64 %1200
  %1202 = bitcast float* %1201 to <4 x float>*
  %wide.load105.3 = load <4 x float>, <4 x float>* %1202, align 4, !tbaa !3164
  %1203 = getelementptr inbounds float, float* %1201, i64 4
  %1204 = bitcast float* %1203 to <4 x float>*
  %wide.load106.3 = load <4 x float>, <4 x float>* %1204, align 4, !tbaa !3164
  %1205 = getelementptr inbounds float, float* %4, i64 %1200
  %1206 = bitcast float* %1205 to <4 x float>*
  store <4 x float> %wide.load105.3, <4 x float>* %1206, align 4, !tbaa !3167
  %1207 = getelementptr inbounds float, float* %1205, i64 4
  %1208 = bitcast float* %1207 to <4 x float>*
  store <4 x float> %wide.load106.3, <4 x float>* %1208, align 4, !tbaa !3167
  %1209 = add nsw i64 %21, 1056
  %1210 = getelementptr inbounds float, float* %7, i64 %1209
  %1211 = bitcast float* %1210 to <4 x float>*
  %wide.load105.4 = load <4 x float>, <4 x float>* %1211, align 4, !tbaa !3164
  %1212 = getelementptr inbounds float, float* %1210, i64 4
  %1213 = bitcast float* %1212 to <4 x float>*
  %wide.load106.4 = load <4 x float>, <4 x float>* %1213, align 4, !tbaa !3164
  %1214 = getelementptr inbounds float, float* %4, i64 %1209
  %1215 = bitcast float* %1214 to <4 x float>*
  store <4 x float> %wide.load105.4, <4 x float>* %1215, align 4, !tbaa !3167
  %1216 = getelementptr inbounds float, float* %1214, i64 4
  %1217 = bitcast float* %1216 to <4 x float>*
  store <4 x float> %wide.load106.4, <4 x float>* %1217, align 4, !tbaa !3167
  %1218 = add nsw i64 %21, 1064
  %1219 = getelementptr inbounds float, float* %7, i64 %1218
  %1220 = bitcast float* %1219 to <4 x float>*
  %wide.load105.5 = load <4 x float>, <4 x float>* %1220, align 4, !tbaa !3164
  %1221 = getelementptr inbounds float, float* %1219, i64 4
  %1222 = bitcast float* %1221 to <4 x float>*
  %wide.load106.5 = load <4 x float>, <4 x float>* %1222, align 4, !tbaa !3164
  %1223 = getelementptr inbounds float, float* %4, i64 %1218
  %1224 = bitcast float* %1223 to <4 x float>*
  store <4 x float> %wide.load105.5, <4 x float>* %1224, align 4, !tbaa !3167
  %1225 = getelementptr inbounds float, float* %1223, i64 4
  %1226 = bitcast float* %1225 to <4 x float>*
  store <4 x float> %wide.load106.5, <4 x float>* %1226, align 4, !tbaa !3167
  %1227 = add nsw i64 %21, 1072
  %1228 = getelementptr inbounds float, float* %7, i64 %1227
  %1229 = bitcast float* %1228 to <4 x float>*
  %wide.load105.6 = load <4 x float>, <4 x float>* %1229, align 4, !tbaa !3164
  %1230 = getelementptr inbounds float, float* %1228, i64 4
  %1231 = bitcast float* %1230 to <4 x float>*
  %wide.load106.6 = load <4 x float>, <4 x float>* %1231, align 4, !tbaa !3164
  %1232 = getelementptr inbounds float, float* %4, i64 %1227
  %1233 = bitcast float* %1232 to <4 x float>*
  store <4 x float> %wide.load105.6, <4 x float>* %1233, align 4, !tbaa !3167
  %1234 = getelementptr inbounds float, float* %1232, i64 4
  %1235 = bitcast float* %1234 to <4 x float>*
  store <4 x float> %wide.load106.6, <4 x float>* %1235, align 4, !tbaa !3167
  %1236 = add nsw i64 %21, 1080
  %1237 = getelementptr inbounds float, float* %7, i64 %1236
  %1238 = bitcast float* %1237 to <4 x float>*
  %wide.load105.7 = load <4 x float>, <4 x float>* %1238, align 4, !tbaa !3164
  %1239 = getelementptr inbounds float, float* %1237, i64 4
  %1240 = bitcast float* %1239 to <4 x float>*
  %wide.load106.7 = load <4 x float>, <4 x float>* %1240, align 4, !tbaa !3164
  %1241 = getelementptr inbounds float, float* %4, i64 %1236
  %1242 = bitcast float* %1241 to <4 x float>*
  store <4 x float> %wide.load105.7, <4 x float>* %1242, align 4, !tbaa !3167
  %1243 = getelementptr inbounds float, float* %1241, i64 4
  %1244 = bitcast float* %1243 to <4 x float>*
  store <4 x float> %wide.load106.7, <4 x float>* %1244, align 4, !tbaa !3167
  %1245 = add nsw i64 %21, 1088
  %1246 = getelementptr inbounds float, float* %7, i64 %1245
  %1247 = bitcast float* %1246 to <4 x float>*
  %wide.load105.8 = load <4 x float>, <4 x float>* %1247, align 4, !tbaa !3164
  %1248 = getelementptr inbounds float, float* %1246, i64 4
  %1249 = bitcast float* %1248 to <4 x float>*
  %wide.load106.8 = load <4 x float>, <4 x float>* %1249, align 4, !tbaa !3164
  %1250 = getelementptr inbounds float, float* %4, i64 %1245
  %1251 = bitcast float* %1250 to <4 x float>*
  store <4 x float> %wide.load105.8, <4 x float>* %1251, align 4, !tbaa !3167
  %1252 = getelementptr inbounds float, float* %1250, i64 4
  %1253 = bitcast float* %1252 to <4 x float>*
  store <4 x float> %wide.load106.8, <4 x float>* %1253, align 4, !tbaa !3167
  %1254 = add nsw i64 %21, 1096
  %1255 = getelementptr inbounds float, float* %7, i64 %1254
  %1256 = bitcast float* %1255 to <4 x float>*
  %wide.load105.9 = load <4 x float>, <4 x float>* %1256, align 4, !tbaa !3164
  %1257 = getelementptr inbounds float, float* %1255, i64 4
  %1258 = bitcast float* %1257 to <4 x float>*
  %wide.load106.9 = load <4 x float>, <4 x float>* %1258, align 4, !tbaa !3164
  %1259 = getelementptr inbounds float, float* %4, i64 %1254
  %1260 = bitcast float* %1259 to <4 x float>*
  store <4 x float> %wide.load105.9, <4 x float>* %1260, align 4, !tbaa !3167
  %1261 = getelementptr inbounds float, float* %1259, i64 4
  %1262 = bitcast float* %1261 to <4 x float>*
  store <4 x float> %wide.load106.9, <4 x float>* %1262, align 4, !tbaa !3167
  %1263 = add nsw i64 %21, 1104
  %1264 = getelementptr inbounds float, float* %7, i64 %1263
  %1265 = bitcast float* %1264 to <4 x float>*
  %wide.load105.10 = load <4 x float>, <4 x float>* %1265, align 4, !tbaa !3164
  %1266 = getelementptr inbounds float, float* %1264, i64 4
  %1267 = bitcast float* %1266 to <4 x float>*
  %wide.load106.10 = load <4 x float>, <4 x float>* %1267, align 4, !tbaa !3164
  %1268 = getelementptr inbounds float, float* %4, i64 %1263
  %1269 = bitcast float* %1268 to <4 x float>*
  store <4 x float> %wide.load105.10, <4 x float>* %1269, align 4, !tbaa !3167
  %1270 = getelementptr inbounds float, float* %1268, i64 4
  %1271 = bitcast float* %1270 to <4 x float>*
  store <4 x float> %wide.load106.10, <4 x float>* %1271, align 4, !tbaa !3167
  %1272 = add nsw i64 %21, 1112
  %1273 = getelementptr inbounds float, float* %7, i64 %1272
  %1274 = bitcast float* %1273 to <4 x float>*
  %wide.load105.11 = load <4 x float>, <4 x float>* %1274, align 4, !tbaa !3164
  %1275 = getelementptr inbounds float, float* %1273, i64 4
  %1276 = bitcast float* %1275 to <4 x float>*
  %wide.load106.11 = load <4 x float>, <4 x float>* %1276, align 4, !tbaa !3164
  %1277 = getelementptr inbounds float, float* %4, i64 %1272
  %1278 = bitcast float* %1277 to <4 x float>*
  store <4 x float> %wide.load105.11, <4 x float>* %1278, align 4, !tbaa !3167
  %1279 = getelementptr inbounds float, float* %1277, i64 4
  %1280 = bitcast float* %1279 to <4 x float>*
  store <4 x float> %wide.load106.11, <4 x float>* %1280, align 4, !tbaa !3167
  %1281 = add nsw i64 %21, 1120
  %1282 = getelementptr inbounds float, float* %7, i64 %1281
  %1283 = bitcast float* %1282 to <4 x float>*
  %wide.load105.12 = load <4 x float>, <4 x float>* %1283, align 4, !tbaa !3164
  %1284 = getelementptr inbounds float, float* %1282, i64 4
  %1285 = bitcast float* %1284 to <4 x float>*
  %wide.load106.12 = load <4 x float>, <4 x float>* %1285, align 4, !tbaa !3164
  %1286 = getelementptr inbounds float, float* %4, i64 %1281
  %1287 = bitcast float* %1286 to <4 x float>*
  store <4 x float> %wide.load105.12, <4 x float>* %1287, align 4, !tbaa !3167
  %1288 = getelementptr inbounds float, float* %1286, i64 4
  %1289 = bitcast float* %1288 to <4 x float>*
  store <4 x float> %wide.load106.12, <4 x float>* %1289, align 4, !tbaa !3167
  %1290 = add nsw i64 %21, 1128
  %1291 = getelementptr inbounds float, float* %7, i64 %1290
  %1292 = bitcast float* %1291 to <4 x float>*
  %wide.load105.13 = load <4 x float>, <4 x float>* %1292, align 4, !tbaa !3164
  %1293 = getelementptr inbounds float, float* %1291, i64 4
  %1294 = bitcast float* %1293 to <4 x float>*
  %wide.load106.13 = load <4 x float>, <4 x float>* %1294, align 4, !tbaa !3164
  %1295 = getelementptr inbounds float, float* %4, i64 %1290
  %1296 = bitcast float* %1295 to <4 x float>*
  store <4 x float> %wide.load105.13, <4 x float>* %1296, align 4, !tbaa !3167
  %1297 = getelementptr inbounds float, float* %1295, i64 4
  %1298 = bitcast float* %1297 to <4 x float>*
  store <4 x float> %wide.load106.13, <4 x float>* %1298, align 4, !tbaa !3167
  %1299 = add nsw i64 %21, 1136
  %1300 = getelementptr inbounds float, float* %7, i64 %1299
  %1301 = bitcast float* %1300 to <4 x float>*
  %wide.load105.14 = load <4 x float>, <4 x float>* %1301, align 4, !tbaa !3164
  %1302 = getelementptr inbounds float, float* %1300, i64 4
  %1303 = bitcast float* %1302 to <4 x float>*
  %wide.load106.14 = load <4 x float>, <4 x float>* %1303, align 4, !tbaa !3164
  %1304 = getelementptr inbounds float, float* %4, i64 %1299
  %1305 = bitcast float* %1304 to <4 x float>*
  store <4 x float> %wide.load105.14, <4 x float>* %1305, align 4, !tbaa !3167
  %1306 = getelementptr inbounds float, float* %1304, i64 4
  %1307 = bitcast float* %1306 to <4 x float>*
  store <4 x float> %wide.load106.14, <4 x float>* %1307, align 4, !tbaa !3167
  %1308 = add nsw i64 %21, 1144
  %1309 = getelementptr inbounds float, float* %7, i64 %1308
  %1310 = bitcast float* %1309 to <4 x float>*
  %wide.load105.15 = load <4 x float>, <4 x float>* %1310, align 4, !tbaa !3164
  %1311 = getelementptr inbounds float, float* %1309, i64 4
  %1312 = bitcast float* %1311 to <4 x float>*
  %wide.load106.15 = load <4 x float>, <4 x float>* %1312, align 4, !tbaa !3164
  %1313 = getelementptr inbounds float, float* %4, i64 %1308
  %1314 = bitcast float* %1313 to <4 x float>*
  store <4 x float> %wide.load105.15, <4 x float>* %1314, align 4, !tbaa !3167
  %1315 = getelementptr inbounds float, float* %1313, i64 4
  %1316 = bitcast float* %1315 to <4 x float>*
  store <4 x float> %wide.load106.15, <4 x float>* %1316, align 4, !tbaa !3167
  %1317 = add nsw i64 %21, 1152
  %1318 = getelementptr inbounds float, float* %7, i64 %1317
  %1319 = bitcast float* %1318 to <4 x float>*
  %wide.load105.16 = load <4 x float>, <4 x float>* %1319, align 4, !tbaa !3164
  %1320 = getelementptr inbounds float, float* %1318, i64 4
  %1321 = bitcast float* %1320 to <4 x float>*
  %wide.load106.16 = load <4 x float>, <4 x float>* %1321, align 4, !tbaa !3164
  %1322 = getelementptr inbounds float, float* %4, i64 %1317
  %1323 = bitcast float* %1322 to <4 x float>*
  store <4 x float> %wide.load105.16, <4 x float>* %1323, align 4, !tbaa !3167
  %1324 = getelementptr inbounds float, float* %1322, i64 4
  %1325 = bitcast float* %1324 to <4 x float>*
  store <4 x float> %wide.load106.16, <4 x float>* %1325, align 4, !tbaa !3167
  %1326 = add nsw i64 %21, 1160
  %1327 = getelementptr inbounds float, float* %7, i64 %1326
  %1328 = bitcast float* %1327 to <4 x float>*
  %wide.load105.17 = load <4 x float>, <4 x float>* %1328, align 4, !tbaa !3164
  %1329 = getelementptr inbounds float, float* %1327, i64 4
  %1330 = bitcast float* %1329 to <4 x float>*
  %wide.load106.17 = load <4 x float>, <4 x float>* %1330, align 4, !tbaa !3164
  %1331 = getelementptr inbounds float, float* %4, i64 %1326
  %1332 = bitcast float* %1331 to <4 x float>*
  store <4 x float> %wide.load105.17, <4 x float>* %1332, align 4, !tbaa !3167
  %1333 = getelementptr inbounds float, float* %1331, i64 4
  %1334 = bitcast float* %1333 to <4 x float>*
  store <4 x float> %wide.load106.17, <4 x float>* %1334, align 4, !tbaa !3167
  %1335 = add nsw i64 %21, 1168
  %1336 = getelementptr inbounds float, float* %7, i64 %1335
  %1337 = bitcast float* %1336 to <4 x float>*
  %wide.load105.18 = load <4 x float>, <4 x float>* %1337, align 4, !tbaa !3164
  %1338 = getelementptr inbounds float, float* %1336, i64 4
  %1339 = bitcast float* %1338 to <4 x float>*
  %wide.load106.18 = load <4 x float>, <4 x float>* %1339, align 4, !tbaa !3164
  %1340 = getelementptr inbounds float, float* %4, i64 %1335
  %1341 = bitcast float* %1340 to <4 x float>*
  store <4 x float> %wide.load105.18, <4 x float>* %1341, align 4, !tbaa !3167
  %1342 = getelementptr inbounds float, float* %1340, i64 4
  %1343 = bitcast float* %1342 to <4 x float>*
  store <4 x float> %wide.load106.18, <4 x float>* %1343, align 4, !tbaa !3167
  %1344 = add nsw i64 %21, 1176
  %1345 = getelementptr inbounds float, float* %7, i64 %1344
  %1346 = bitcast float* %1345 to <4 x float>*
  %wide.load105.19 = load <4 x float>, <4 x float>* %1346, align 4, !tbaa !3164
  %1347 = getelementptr inbounds float, float* %1345, i64 4
  %1348 = bitcast float* %1347 to <4 x float>*
  %wide.load106.19 = load <4 x float>, <4 x float>* %1348, align 4, !tbaa !3164
  %1349 = getelementptr inbounds float, float* %4, i64 %1344
  %1350 = bitcast float* %1349 to <4 x float>*
  store <4 x float> %wide.load105.19, <4 x float>* %1350, align 4, !tbaa !3167
  %1351 = getelementptr inbounds float, float* %1349, i64 4
  %1352 = bitcast float* %1351 to <4 x float>*
  store <4 x float> %wide.load106.19, <4 x float>* %1352, align 4, !tbaa !3167
  %1353 = add nsw i64 %21, 1184
  %1354 = getelementptr inbounds float, float* %7, i64 %1353
  %1355 = bitcast float* %1354 to <4 x float>*
  %wide.load105.20 = load <4 x float>, <4 x float>* %1355, align 4, !tbaa !3164
  %1356 = getelementptr inbounds float, float* %1354, i64 4
  %1357 = bitcast float* %1356 to <4 x float>*
  %wide.load106.20 = load <4 x float>, <4 x float>* %1357, align 4, !tbaa !3164
  %1358 = getelementptr inbounds float, float* %4, i64 %1353
  %1359 = bitcast float* %1358 to <4 x float>*
  store <4 x float> %wide.load105.20, <4 x float>* %1359, align 4, !tbaa !3167
  %1360 = getelementptr inbounds float, float* %1358, i64 4
  %1361 = bitcast float* %1360 to <4 x float>*
  store <4 x float> %wide.load106.20, <4 x float>* %1361, align 4, !tbaa !3167
  %1362 = add nsw i64 %21, 1192
  %1363 = getelementptr inbounds float, float* %7, i64 %1362
  %1364 = bitcast float* %1363 to <4 x float>*
  %wide.load105.21 = load <4 x float>, <4 x float>* %1364, align 4, !tbaa !3164
  %1365 = getelementptr inbounds float, float* %1363, i64 4
  %1366 = bitcast float* %1365 to <4 x float>*
  %wide.load106.21 = load <4 x float>, <4 x float>* %1366, align 4, !tbaa !3164
  %1367 = getelementptr inbounds float, float* %4, i64 %1362
  %1368 = bitcast float* %1367 to <4 x float>*
  store <4 x float> %wide.load105.21, <4 x float>* %1368, align 4, !tbaa !3167
  %1369 = getelementptr inbounds float, float* %1367, i64 4
  %1370 = bitcast float* %1369 to <4 x float>*
  store <4 x float> %wide.load106.21, <4 x float>* %1370, align 4, !tbaa !3167
  %1371 = add nsw i64 %21, 1200
  %1372 = getelementptr inbounds float, float* %7, i64 %1371
  %1373 = bitcast float* %1372 to <4 x float>*
  %wide.load105.22 = load <4 x float>, <4 x float>* %1373, align 4, !tbaa !3164
  %1374 = getelementptr inbounds float, float* %1372, i64 4
  %1375 = bitcast float* %1374 to <4 x float>*
  %wide.load106.22 = load <4 x float>, <4 x float>* %1375, align 4, !tbaa !3164
  %1376 = getelementptr inbounds float, float* %4, i64 %1371
  %1377 = bitcast float* %1376 to <4 x float>*
  store <4 x float> %wide.load105.22, <4 x float>* %1377, align 4, !tbaa !3167
  %1378 = getelementptr inbounds float, float* %1376, i64 4
  %1379 = bitcast float* %1378 to <4 x float>*
  store <4 x float> %wide.load106.22, <4 x float>* %1379, align 4, !tbaa !3167
  %1380 = add nsw i64 %21, 1208
  %1381 = getelementptr inbounds float, float* %7, i64 %1380
  %1382 = bitcast float* %1381 to <4 x float>*
  %wide.load105.23 = load <4 x float>, <4 x float>* %1382, align 4, !tbaa !3164
  %1383 = getelementptr inbounds float, float* %1381, i64 4
  %1384 = bitcast float* %1383 to <4 x float>*
  %wide.load106.23 = load <4 x float>, <4 x float>* %1384, align 4, !tbaa !3164
  %1385 = getelementptr inbounds float, float* %4, i64 %1380
  %1386 = bitcast float* %1385 to <4 x float>*
  store <4 x float> %wide.load105.23, <4 x float>* %1386, align 4, !tbaa !3167
  %1387 = getelementptr inbounds float, float* %1385, i64 4
  %1388 = bitcast float* %1387 to <4 x float>*
  store <4 x float> %wide.load106.23, <4 x float>* %1388, align 4, !tbaa !3167
  %1389 = add nsw i64 %21, 1216
  %1390 = getelementptr inbounds float, float* %7, i64 %1389
  %1391 = bitcast float* %1390 to <4 x float>*
  %wide.load105.24 = load <4 x float>, <4 x float>* %1391, align 4, !tbaa !3164
  %1392 = getelementptr inbounds float, float* %1390, i64 4
  %1393 = bitcast float* %1392 to <4 x float>*
  %wide.load106.24 = load <4 x float>, <4 x float>* %1393, align 4, !tbaa !3164
  %1394 = getelementptr inbounds float, float* %4, i64 %1389
  %1395 = bitcast float* %1394 to <4 x float>*
  store <4 x float> %wide.load105.24, <4 x float>* %1395, align 4, !tbaa !3167
  %1396 = getelementptr inbounds float, float* %1394, i64 4
  %1397 = bitcast float* %1396 to <4 x float>*
  store <4 x float> %wide.load106.24, <4 x float>* %1397, align 4, !tbaa !3167
  %1398 = add nsw i64 %21, 1224
  %1399 = getelementptr inbounds float, float* %7, i64 %1398
  %1400 = bitcast float* %1399 to <4 x float>*
  %wide.load105.25 = load <4 x float>, <4 x float>* %1400, align 4, !tbaa !3164
  %1401 = getelementptr inbounds float, float* %1399, i64 4
  %1402 = bitcast float* %1401 to <4 x float>*
  %wide.load106.25 = load <4 x float>, <4 x float>* %1402, align 4, !tbaa !3164
  %1403 = getelementptr inbounds float, float* %4, i64 %1398
  %1404 = bitcast float* %1403 to <4 x float>*
  store <4 x float> %wide.load105.25, <4 x float>* %1404, align 4, !tbaa !3167
  %1405 = getelementptr inbounds float, float* %1403, i64 4
  %1406 = bitcast float* %1405 to <4 x float>*
  store <4 x float> %wide.load106.25, <4 x float>* %1406, align 4, !tbaa !3167
  %1407 = add nsw i64 %21, 1232
  %1408 = getelementptr inbounds float, float* %7, i64 %1407
  %1409 = bitcast float* %1408 to <4 x float>*
  %wide.load105.26 = load <4 x float>, <4 x float>* %1409, align 4, !tbaa !3164
  %1410 = getelementptr inbounds float, float* %1408, i64 4
  %1411 = bitcast float* %1410 to <4 x float>*
  %wide.load106.26 = load <4 x float>, <4 x float>* %1411, align 4, !tbaa !3164
  %1412 = getelementptr inbounds float, float* %4, i64 %1407
  %1413 = bitcast float* %1412 to <4 x float>*
  store <4 x float> %wide.load105.26, <4 x float>* %1413, align 4, !tbaa !3167
  %1414 = getelementptr inbounds float, float* %1412, i64 4
  %1415 = bitcast float* %1414 to <4 x float>*
  store <4 x float> %wide.load106.26, <4 x float>* %1415, align 4, !tbaa !3167
  %1416 = add nsw i64 %21, 1240
  %1417 = getelementptr inbounds float, float* %7, i64 %1416
  %1418 = bitcast float* %1417 to <4 x float>*
  %wide.load105.27 = load <4 x float>, <4 x float>* %1418, align 4, !tbaa !3164
  %1419 = getelementptr inbounds float, float* %1417, i64 4
  %1420 = bitcast float* %1419 to <4 x float>*
  %wide.load106.27 = load <4 x float>, <4 x float>* %1420, align 4, !tbaa !3164
  %1421 = getelementptr inbounds float, float* %4, i64 %1416
  %1422 = bitcast float* %1421 to <4 x float>*
  store <4 x float> %wide.load105.27, <4 x float>* %1422, align 4, !tbaa !3167
  %1423 = getelementptr inbounds float, float* %1421, i64 4
  %1424 = bitcast float* %1423 to <4 x float>*
  store <4 x float> %wide.load106.27, <4 x float>* %1424, align 4, !tbaa !3167
  %1425 = add nsw i64 %21, 1248
  %1426 = getelementptr inbounds float, float* %7, i64 %1425
  %1427 = bitcast float* %1426 to <4 x float>*
  %wide.load105.28 = load <4 x float>, <4 x float>* %1427, align 4, !tbaa !3164
  %1428 = getelementptr inbounds float, float* %1426, i64 4
  %1429 = bitcast float* %1428 to <4 x float>*
  %wide.load106.28 = load <4 x float>, <4 x float>* %1429, align 4, !tbaa !3164
  %1430 = getelementptr inbounds float, float* %4, i64 %1425
  %1431 = bitcast float* %1430 to <4 x float>*
  store <4 x float> %wide.load105.28, <4 x float>* %1431, align 4, !tbaa !3167
  %1432 = getelementptr inbounds float, float* %1430, i64 4
  %1433 = bitcast float* %1432 to <4 x float>*
  store <4 x float> %wide.load106.28, <4 x float>* %1433, align 4, !tbaa !3167
  %1434 = add nsw i64 %21, 1256
  %1435 = getelementptr inbounds float, float* %7, i64 %1434
  %1436 = bitcast float* %1435 to <4 x float>*
  %wide.load105.29 = load <4 x float>, <4 x float>* %1436, align 4, !tbaa !3164
  %1437 = getelementptr inbounds float, float* %1435, i64 4
  %1438 = bitcast float* %1437 to <4 x float>*
  %wide.load106.29 = load <4 x float>, <4 x float>* %1438, align 4, !tbaa !3164
  %1439 = getelementptr inbounds float, float* %4, i64 %1434
  %1440 = bitcast float* %1439 to <4 x float>*
  store <4 x float> %wide.load105.29, <4 x float>* %1440, align 4, !tbaa !3167
  %1441 = getelementptr inbounds float, float* %1439, i64 4
  %1442 = bitcast float* %1441 to <4 x float>*
  store <4 x float> %wide.load106.29, <4 x float>* %1442, align 4, !tbaa !3167
  %1443 = add nsw i64 %21, 1264
  %1444 = getelementptr inbounds float, float* %7, i64 %1443
  %1445 = bitcast float* %1444 to <4 x float>*
  %wide.load105.30 = load <4 x float>, <4 x float>* %1445, align 4, !tbaa !3164
  %1446 = getelementptr inbounds float, float* %1444, i64 4
  %1447 = bitcast float* %1446 to <4 x float>*
  %wide.load106.30 = load <4 x float>, <4 x float>* %1447, align 4, !tbaa !3164
  %1448 = getelementptr inbounds float, float* %4, i64 %1443
  %1449 = bitcast float* %1448 to <4 x float>*
  store <4 x float> %wide.load105.30, <4 x float>* %1449, align 4, !tbaa !3167
  %1450 = getelementptr inbounds float, float* %1448, i64 4
  %1451 = bitcast float* %1450 to <4 x float>*
  store <4 x float> %wide.load106.30, <4 x float>* %1451, align 4, !tbaa !3167
  %1452 = add nsw i64 %21, 1272
  %1453 = getelementptr inbounds float, float* %7, i64 %1452
  %1454 = bitcast float* %1453 to <4 x float>*
  %wide.load105.31 = load <4 x float>, <4 x float>* %1454, align 4, !tbaa !3164
  %1455 = getelementptr inbounds float, float* %1453, i64 4
  %1456 = bitcast float* %1455 to <4 x float>*
  %wide.load106.31 = load <4 x float>, <4 x float>* %1456, align 4, !tbaa !3164
  %1457 = getelementptr inbounds float, float* %4, i64 %1452
  %1458 = bitcast float* %1457 to <4 x float>*
  store <4 x float> %wide.load105.31, <4 x float>* %1458, align 4, !tbaa !3167
  %1459 = getelementptr inbounds float, float* %1457, i64 4
  %1460 = bitcast float* %1459 to <4 x float>*
  store <4 x float> %wide.load106.31, <4 x float>* %1460, align 4, !tbaa !3167
  %1461 = add nsw i64 %21, 1280
  %1462 = getelementptr inbounds float, float* %7, i64 %1461
  %1463 = bitcast float* %1462 to <4 x float>*
  %wide.load95 = load <4 x float>, <4 x float>* %1463, align 4, !tbaa !3164
  %1464 = getelementptr inbounds float, float* %1462, i64 4
  %1465 = bitcast float* %1464 to <4 x float>*
  %wide.load96 = load <4 x float>, <4 x float>* %1465, align 4, !tbaa !3164
  %1466 = getelementptr inbounds float, float* %4, i64 %1461
  %1467 = bitcast float* %1466 to <4 x float>*
  store <4 x float> %wide.load95, <4 x float>* %1467, align 4, !tbaa !3167
  %1468 = getelementptr inbounds float, float* %1466, i64 4
  %1469 = bitcast float* %1468 to <4 x float>*
  store <4 x float> %wide.load96, <4 x float>* %1469, align 4, !tbaa !3167
  %1470 = add nsw i64 %21, 1288
  %1471 = getelementptr inbounds float, float* %7, i64 %1470
  %1472 = bitcast float* %1471 to <4 x float>*
  %wide.load95.1 = load <4 x float>, <4 x float>* %1472, align 4, !tbaa !3164
  %1473 = getelementptr inbounds float, float* %1471, i64 4
  %1474 = bitcast float* %1473 to <4 x float>*
  %wide.load96.1 = load <4 x float>, <4 x float>* %1474, align 4, !tbaa !3164
  %1475 = getelementptr inbounds float, float* %4, i64 %1470
  %1476 = bitcast float* %1475 to <4 x float>*
  store <4 x float> %wide.load95.1, <4 x float>* %1476, align 4, !tbaa !3167
  %1477 = getelementptr inbounds float, float* %1475, i64 4
  %1478 = bitcast float* %1477 to <4 x float>*
  store <4 x float> %wide.load96.1, <4 x float>* %1478, align 4, !tbaa !3167
  %1479 = add nsw i64 %21, 1296
  %1480 = getelementptr inbounds float, float* %7, i64 %1479
  %1481 = bitcast float* %1480 to <4 x float>*
  %wide.load95.2 = load <4 x float>, <4 x float>* %1481, align 4, !tbaa !3164
  %1482 = getelementptr inbounds float, float* %1480, i64 4
  %1483 = bitcast float* %1482 to <4 x float>*
  %wide.load96.2 = load <4 x float>, <4 x float>* %1483, align 4, !tbaa !3164
  %1484 = getelementptr inbounds float, float* %4, i64 %1479
  %1485 = bitcast float* %1484 to <4 x float>*
  store <4 x float> %wide.load95.2, <4 x float>* %1485, align 4, !tbaa !3167
  %1486 = getelementptr inbounds float, float* %1484, i64 4
  %1487 = bitcast float* %1486 to <4 x float>*
  store <4 x float> %wide.load96.2, <4 x float>* %1487, align 4, !tbaa !3167
  %1488 = add nsw i64 %21, 1304
  %1489 = getelementptr inbounds float, float* %7, i64 %1488
  %1490 = bitcast float* %1489 to <4 x float>*
  %wide.load95.3 = load <4 x float>, <4 x float>* %1490, align 4, !tbaa !3164
  %1491 = getelementptr inbounds float, float* %1489, i64 4
  %1492 = bitcast float* %1491 to <4 x float>*
  %wide.load96.3 = load <4 x float>, <4 x float>* %1492, align 4, !tbaa !3164
  %1493 = getelementptr inbounds float, float* %4, i64 %1488
  %1494 = bitcast float* %1493 to <4 x float>*
  store <4 x float> %wide.load95.3, <4 x float>* %1494, align 4, !tbaa !3167
  %1495 = getelementptr inbounds float, float* %1493, i64 4
  %1496 = bitcast float* %1495 to <4 x float>*
  store <4 x float> %wide.load96.3, <4 x float>* %1496, align 4, !tbaa !3167
  %1497 = add nsw i64 %21, 1312
  %1498 = getelementptr inbounds float, float* %7, i64 %1497
  %1499 = bitcast float* %1498 to <4 x float>*
  %wide.load95.4 = load <4 x float>, <4 x float>* %1499, align 4, !tbaa !3164
  %1500 = getelementptr inbounds float, float* %1498, i64 4
  %1501 = bitcast float* %1500 to <4 x float>*
  %wide.load96.4 = load <4 x float>, <4 x float>* %1501, align 4, !tbaa !3164
  %1502 = getelementptr inbounds float, float* %4, i64 %1497
  %1503 = bitcast float* %1502 to <4 x float>*
  store <4 x float> %wide.load95.4, <4 x float>* %1503, align 4, !tbaa !3167
  %1504 = getelementptr inbounds float, float* %1502, i64 4
  %1505 = bitcast float* %1504 to <4 x float>*
  store <4 x float> %wide.load96.4, <4 x float>* %1505, align 4, !tbaa !3167
  %1506 = add nsw i64 %21, 1320
  %1507 = getelementptr inbounds float, float* %7, i64 %1506
  %1508 = bitcast float* %1507 to <4 x float>*
  %wide.load95.5 = load <4 x float>, <4 x float>* %1508, align 4, !tbaa !3164
  %1509 = getelementptr inbounds float, float* %1507, i64 4
  %1510 = bitcast float* %1509 to <4 x float>*
  %wide.load96.5 = load <4 x float>, <4 x float>* %1510, align 4, !tbaa !3164
  %1511 = getelementptr inbounds float, float* %4, i64 %1506
  %1512 = bitcast float* %1511 to <4 x float>*
  store <4 x float> %wide.load95.5, <4 x float>* %1512, align 4, !tbaa !3167
  %1513 = getelementptr inbounds float, float* %1511, i64 4
  %1514 = bitcast float* %1513 to <4 x float>*
  store <4 x float> %wide.load96.5, <4 x float>* %1514, align 4, !tbaa !3167
  %1515 = add nsw i64 %21, 1328
  %1516 = getelementptr inbounds float, float* %7, i64 %1515
  %1517 = bitcast float* %1516 to <4 x float>*
  %wide.load95.6 = load <4 x float>, <4 x float>* %1517, align 4, !tbaa !3164
  %1518 = getelementptr inbounds float, float* %1516, i64 4
  %1519 = bitcast float* %1518 to <4 x float>*
  %wide.load96.6 = load <4 x float>, <4 x float>* %1519, align 4, !tbaa !3164
  %1520 = getelementptr inbounds float, float* %4, i64 %1515
  %1521 = bitcast float* %1520 to <4 x float>*
  store <4 x float> %wide.load95.6, <4 x float>* %1521, align 4, !tbaa !3167
  %1522 = getelementptr inbounds float, float* %1520, i64 4
  %1523 = bitcast float* %1522 to <4 x float>*
  store <4 x float> %wide.load96.6, <4 x float>* %1523, align 4, !tbaa !3167
  %1524 = add nsw i64 %21, 1336
  %1525 = getelementptr inbounds float, float* %7, i64 %1524
  %1526 = bitcast float* %1525 to <4 x float>*
  %wide.load95.7 = load <4 x float>, <4 x float>* %1526, align 4, !tbaa !3164
  %1527 = getelementptr inbounds float, float* %1525, i64 4
  %1528 = bitcast float* %1527 to <4 x float>*
  %wide.load96.7 = load <4 x float>, <4 x float>* %1528, align 4, !tbaa !3164
  %1529 = getelementptr inbounds float, float* %4, i64 %1524
  %1530 = bitcast float* %1529 to <4 x float>*
  store <4 x float> %wide.load95.7, <4 x float>* %1530, align 4, !tbaa !3167
  %1531 = getelementptr inbounds float, float* %1529, i64 4
  %1532 = bitcast float* %1531 to <4 x float>*
  store <4 x float> %wide.load96.7, <4 x float>* %1532, align 4, !tbaa !3167
  %1533 = add nsw i64 %21, 1344
  %1534 = getelementptr inbounds float, float* %7, i64 %1533
  %1535 = bitcast float* %1534 to <4 x float>*
  %wide.load95.8 = load <4 x float>, <4 x float>* %1535, align 4, !tbaa !3164
  %1536 = getelementptr inbounds float, float* %1534, i64 4
  %1537 = bitcast float* %1536 to <4 x float>*
  %wide.load96.8 = load <4 x float>, <4 x float>* %1537, align 4, !tbaa !3164
  %1538 = getelementptr inbounds float, float* %4, i64 %1533
  %1539 = bitcast float* %1538 to <4 x float>*
  store <4 x float> %wide.load95.8, <4 x float>* %1539, align 4, !tbaa !3167
  %1540 = getelementptr inbounds float, float* %1538, i64 4
  %1541 = bitcast float* %1540 to <4 x float>*
  store <4 x float> %wide.load96.8, <4 x float>* %1541, align 4, !tbaa !3167
  %1542 = add nsw i64 %21, 1352
  %1543 = getelementptr inbounds float, float* %7, i64 %1542
  %1544 = bitcast float* %1543 to <4 x float>*
  %wide.load95.9 = load <4 x float>, <4 x float>* %1544, align 4, !tbaa !3164
  %1545 = getelementptr inbounds float, float* %1543, i64 4
  %1546 = bitcast float* %1545 to <4 x float>*
  %wide.load96.9 = load <4 x float>, <4 x float>* %1546, align 4, !tbaa !3164
  %1547 = getelementptr inbounds float, float* %4, i64 %1542
  %1548 = bitcast float* %1547 to <4 x float>*
  store <4 x float> %wide.load95.9, <4 x float>* %1548, align 4, !tbaa !3167
  %1549 = getelementptr inbounds float, float* %1547, i64 4
  %1550 = bitcast float* %1549 to <4 x float>*
  store <4 x float> %wide.load96.9, <4 x float>* %1550, align 4, !tbaa !3167
  %1551 = add nsw i64 %21, 1360
  %1552 = getelementptr inbounds float, float* %7, i64 %1551
  %1553 = bitcast float* %1552 to <4 x float>*
  %wide.load95.10 = load <4 x float>, <4 x float>* %1553, align 4, !tbaa !3164
  %1554 = getelementptr inbounds float, float* %1552, i64 4
  %1555 = bitcast float* %1554 to <4 x float>*
  %wide.load96.10 = load <4 x float>, <4 x float>* %1555, align 4, !tbaa !3164
  %1556 = getelementptr inbounds float, float* %4, i64 %1551
  %1557 = bitcast float* %1556 to <4 x float>*
  store <4 x float> %wide.load95.10, <4 x float>* %1557, align 4, !tbaa !3167
  %1558 = getelementptr inbounds float, float* %1556, i64 4
  %1559 = bitcast float* %1558 to <4 x float>*
  store <4 x float> %wide.load96.10, <4 x float>* %1559, align 4, !tbaa !3167
  %1560 = add nsw i64 %21, 1368
  %1561 = getelementptr inbounds float, float* %7, i64 %1560
  %1562 = bitcast float* %1561 to <4 x float>*
  %wide.load95.11 = load <4 x float>, <4 x float>* %1562, align 4, !tbaa !3164
  %1563 = getelementptr inbounds float, float* %1561, i64 4
  %1564 = bitcast float* %1563 to <4 x float>*
  %wide.load96.11 = load <4 x float>, <4 x float>* %1564, align 4, !tbaa !3164
  %1565 = getelementptr inbounds float, float* %4, i64 %1560
  %1566 = bitcast float* %1565 to <4 x float>*
  store <4 x float> %wide.load95.11, <4 x float>* %1566, align 4, !tbaa !3167
  %1567 = getelementptr inbounds float, float* %1565, i64 4
  %1568 = bitcast float* %1567 to <4 x float>*
  store <4 x float> %wide.load96.11, <4 x float>* %1568, align 4, !tbaa !3167
  %1569 = add nsw i64 %21, 1376
  %1570 = getelementptr inbounds float, float* %7, i64 %1569
  %1571 = bitcast float* %1570 to <4 x float>*
  %wide.load95.12 = load <4 x float>, <4 x float>* %1571, align 4, !tbaa !3164
  %1572 = getelementptr inbounds float, float* %1570, i64 4
  %1573 = bitcast float* %1572 to <4 x float>*
  %wide.load96.12 = load <4 x float>, <4 x float>* %1573, align 4, !tbaa !3164
  %1574 = getelementptr inbounds float, float* %4, i64 %1569
  %1575 = bitcast float* %1574 to <4 x float>*
  store <4 x float> %wide.load95.12, <4 x float>* %1575, align 4, !tbaa !3167
  %1576 = getelementptr inbounds float, float* %1574, i64 4
  %1577 = bitcast float* %1576 to <4 x float>*
  store <4 x float> %wide.load96.12, <4 x float>* %1577, align 4, !tbaa !3167
  %1578 = add nsw i64 %21, 1384
  %1579 = getelementptr inbounds float, float* %7, i64 %1578
  %1580 = bitcast float* %1579 to <4 x float>*
  %wide.load95.13 = load <4 x float>, <4 x float>* %1580, align 4, !tbaa !3164
  %1581 = getelementptr inbounds float, float* %1579, i64 4
  %1582 = bitcast float* %1581 to <4 x float>*
  %wide.load96.13 = load <4 x float>, <4 x float>* %1582, align 4, !tbaa !3164
  %1583 = getelementptr inbounds float, float* %4, i64 %1578
  %1584 = bitcast float* %1583 to <4 x float>*
  store <4 x float> %wide.load95.13, <4 x float>* %1584, align 4, !tbaa !3167
  %1585 = getelementptr inbounds float, float* %1583, i64 4
  %1586 = bitcast float* %1585 to <4 x float>*
  store <4 x float> %wide.load96.13, <4 x float>* %1586, align 4, !tbaa !3167
  %1587 = add nsw i64 %21, 1392
  %1588 = getelementptr inbounds float, float* %7, i64 %1587
  %1589 = bitcast float* %1588 to <4 x float>*
  %wide.load95.14 = load <4 x float>, <4 x float>* %1589, align 4, !tbaa !3164
  %1590 = getelementptr inbounds float, float* %1588, i64 4
  %1591 = bitcast float* %1590 to <4 x float>*
  %wide.load96.14 = load <4 x float>, <4 x float>* %1591, align 4, !tbaa !3164
  %1592 = getelementptr inbounds float, float* %4, i64 %1587
  %1593 = bitcast float* %1592 to <4 x float>*
  store <4 x float> %wide.load95.14, <4 x float>* %1593, align 4, !tbaa !3167
  %1594 = getelementptr inbounds float, float* %1592, i64 4
  %1595 = bitcast float* %1594 to <4 x float>*
  store <4 x float> %wide.load96.14, <4 x float>* %1595, align 4, !tbaa !3167
  %1596 = add nsw i64 %21, 1400
  %1597 = getelementptr inbounds float, float* %7, i64 %1596
  %1598 = bitcast float* %1597 to <4 x float>*
  %wide.load95.15 = load <4 x float>, <4 x float>* %1598, align 4, !tbaa !3164
  %1599 = getelementptr inbounds float, float* %1597, i64 4
  %1600 = bitcast float* %1599 to <4 x float>*
  %wide.load96.15 = load <4 x float>, <4 x float>* %1600, align 4, !tbaa !3164
  %1601 = getelementptr inbounds float, float* %4, i64 %1596
  %1602 = bitcast float* %1601 to <4 x float>*
  store <4 x float> %wide.load95.15, <4 x float>* %1602, align 4, !tbaa !3167
  %1603 = getelementptr inbounds float, float* %1601, i64 4
  %1604 = bitcast float* %1603 to <4 x float>*
  store <4 x float> %wide.load96.15, <4 x float>* %1604, align 4, !tbaa !3167
  %1605 = add nsw i64 %21, 1408
  %1606 = getelementptr inbounds float, float* %7, i64 %1605
  %1607 = bitcast float* %1606 to <4 x float>*
  %wide.load95.16 = load <4 x float>, <4 x float>* %1607, align 4, !tbaa !3164
  %1608 = getelementptr inbounds float, float* %1606, i64 4
  %1609 = bitcast float* %1608 to <4 x float>*
  %wide.load96.16 = load <4 x float>, <4 x float>* %1609, align 4, !tbaa !3164
  %1610 = getelementptr inbounds float, float* %4, i64 %1605
  %1611 = bitcast float* %1610 to <4 x float>*
  store <4 x float> %wide.load95.16, <4 x float>* %1611, align 4, !tbaa !3167
  %1612 = getelementptr inbounds float, float* %1610, i64 4
  %1613 = bitcast float* %1612 to <4 x float>*
  store <4 x float> %wide.load96.16, <4 x float>* %1613, align 4, !tbaa !3167
  %1614 = add nsw i64 %21, 1416
  %1615 = getelementptr inbounds float, float* %7, i64 %1614
  %1616 = bitcast float* %1615 to <4 x float>*
  %wide.load95.17 = load <4 x float>, <4 x float>* %1616, align 4, !tbaa !3164
  %1617 = getelementptr inbounds float, float* %1615, i64 4
  %1618 = bitcast float* %1617 to <4 x float>*
  %wide.load96.17 = load <4 x float>, <4 x float>* %1618, align 4, !tbaa !3164
  %1619 = getelementptr inbounds float, float* %4, i64 %1614
  %1620 = bitcast float* %1619 to <4 x float>*
  store <4 x float> %wide.load95.17, <4 x float>* %1620, align 4, !tbaa !3167
  %1621 = getelementptr inbounds float, float* %1619, i64 4
  %1622 = bitcast float* %1621 to <4 x float>*
  store <4 x float> %wide.load96.17, <4 x float>* %1622, align 4, !tbaa !3167
  %1623 = add nsw i64 %21, 1424
  %1624 = getelementptr inbounds float, float* %7, i64 %1623
  %1625 = bitcast float* %1624 to <4 x float>*
  %wide.load95.18 = load <4 x float>, <4 x float>* %1625, align 4, !tbaa !3164
  %1626 = getelementptr inbounds float, float* %1624, i64 4
  %1627 = bitcast float* %1626 to <4 x float>*
  %wide.load96.18 = load <4 x float>, <4 x float>* %1627, align 4, !tbaa !3164
  %1628 = getelementptr inbounds float, float* %4, i64 %1623
  %1629 = bitcast float* %1628 to <4 x float>*
  store <4 x float> %wide.load95.18, <4 x float>* %1629, align 4, !tbaa !3167
  %1630 = getelementptr inbounds float, float* %1628, i64 4
  %1631 = bitcast float* %1630 to <4 x float>*
  store <4 x float> %wide.load96.18, <4 x float>* %1631, align 4, !tbaa !3167
  %1632 = add nsw i64 %21, 1432
  %1633 = getelementptr inbounds float, float* %7, i64 %1632
  %1634 = bitcast float* %1633 to <4 x float>*
  %wide.load95.19 = load <4 x float>, <4 x float>* %1634, align 4, !tbaa !3164
  %1635 = getelementptr inbounds float, float* %1633, i64 4
  %1636 = bitcast float* %1635 to <4 x float>*
  %wide.load96.19 = load <4 x float>, <4 x float>* %1636, align 4, !tbaa !3164
  %1637 = getelementptr inbounds float, float* %4, i64 %1632
  %1638 = bitcast float* %1637 to <4 x float>*
  store <4 x float> %wide.load95.19, <4 x float>* %1638, align 4, !tbaa !3167
  %1639 = getelementptr inbounds float, float* %1637, i64 4
  %1640 = bitcast float* %1639 to <4 x float>*
  store <4 x float> %wide.load96.19, <4 x float>* %1640, align 4, !tbaa !3167
  %1641 = add nsw i64 %21, 1440
  %1642 = getelementptr inbounds float, float* %7, i64 %1641
  %1643 = bitcast float* %1642 to <4 x float>*
  %wide.load95.20 = load <4 x float>, <4 x float>* %1643, align 4, !tbaa !3164
  %1644 = getelementptr inbounds float, float* %1642, i64 4
  %1645 = bitcast float* %1644 to <4 x float>*
  %wide.load96.20 = load <4 x float>, <4 x float>* %1645, align 4, !tbaa !3164
  %1646 = getelementptr inbounds float, float* %4, i64 %1641
  %1647 = bitcast float* %1646 to <4 x float>*
  store <4 x float> %wide.load95.20, <4 x float>* %1647, align 4, !tbaa !3167
  %1648 = getelementptr inbounds float, float* %1646, i64 4
  %1649 = bitcast float* %1648 to <4 x float>*
  store <4 x float> %wide.load96.20, <4 x float>* %1649, align 4, !tbaa !3167
  %1650 = add nsw i64 %21, 1448
  %1651 = getelementptr inbounds float, float* %7, i64 %1650
  %1652 = bitcast float* %1651 to <4 x float>*
  %wide.load95.21 = load <4 x float>, <4 x float>* %1652, align 4, !tbaa !3164
  %1653 = getelementptr inbounds float, float* %1651, i64 4
  %1654 = bitcast float* %1653 to <4 x float>*
  %wide.load96.21 = load <4 x float>, <4 x float>* %1654, align 4, !tbaa !3164
  %1655 = getelementptr inbounds float, float* %4, i64 %1650
  %1656 = bitcast float* %1655 to <4 x float>*
  store <4 x float> %wide.load95.21, <4 x float>* %1656, align 4, !tbaa !3167
  %1657 = getelementptr inbounds float, float* %1655, i64 4
  %1658 = bitcast float* %1657 to <4 x float>*
  store <4 x float> %wide.load96.21, <4 x float>* %1658, align 4, !tbaa !3167
  %1659 = add nsw i64 %21, 1456
  %1660 = getelementptr inbounds float, float* %7, i64 %1659
  %1661 = bitcast float* %1660 to <4 x float>*
  %wide.load95.22 = load <4 x float>, <4 x float>* %1661, align 4, !tbaa !3164
  %1662 = getelementptr inbounds float, float* %1660, i64 4
  %1663 = bitcast float* %1662 to <4 x float>*
  %wide.load96.22 = load <4 x float>, <4 x float>* %1663, align 4, !tbaa !3164
  %1664 = getelementptr inbounds float, float* %4, i64 %1659
  %1665 = bitcast float* %1664 to <4 x float>*
  store <4 x float> %wide.load95.22, <4 x float>* %1665, align 4, !tbaa !3167
  %1666 = getelementptr inbounds float, float* %1664, i64 4
  %1667 = bitcast float* %1666 to <4 x float>*
  store <4 x float> %wide.load96.22, <4 x float>* %1667, align 4, !tbaa !3167
  %1668 = add nsw i64 %21, 1464
  %1669 = getelementptr inbounds float, float* %7, i64 %1668
  %1670 = bitcast float* %1669 to <4 x float>*
  %wide.load95.23 = load <4 x float>, <4 x float>* %1670, align 4, !tbaa !3164
  %1671 = getelementptr inbounds float, float* %1669, i64 4
  %1672 = bitcast float* %1671 to <4 x float>*
  %wide.load96.23 = load <4 x float>, <4 x float>* %1672, align 4, !tbaa !3164
  %1673 = getelementptr inbounds float, float* %4, i64 %1668
  %1674 = bitcast float* %1673 to <4 x float>*
  store <4 x float> %wide.load95.23, <4 x float>* %1674, align 4, !tbaa !3167
  %1675 = getelementptr inbounds float, float* %1673, i64 4
  %1676 = bitcast float* %1675 to <4 x float>*
  store <4 x float> %wide.load96.23, <4 x float>* %1676, align 4, !tbaa !3167
  %1677 = add nsw i64 %21, 1472
  %1678 = getelementptr inbounds float, float* %7, i64 %1677
  %1679 = bitcast float* %1678 to <4 x float>*
  %wide.load95.24 = load <4 x float>, <4 x float>* %1679, align 4, !tbaa !3164
  %1680 = getelementptr inbounds float, float* %1678, i64 4
  %1681 = bitcast float* %1680 to <4 x float>*
  %wide.load96.24 = load <4 x float>, <4 x float>* %1681, align 4, !tbaa !3164
  %1682 = getelementptr inbounds float, float* %4, i64 %1677
  %1683 = bitcast float* %1682 to <4 x float>*
  store <4 x float> %wide.load95.24, <4 x float>* %1683, align 4, !tbaa !3167
  %1684 = getelementptr inbounds float, float* %1682, i64 4
  %1685 = bitcast float* %1684 to <4 x float>*
  store <4 x float> %wide.load96.24, <4 x float>* %1685, align 4, !tbaa !3167
  %1686 = add nsw i64 %21, 1480
  %1687 = getelementptr inbounds float, float* %7, i64 %1686
  %1688 = bitcast float* %1687 to <4 x float>*
  %wide.load95.25 = load <4 x float>, <4 x float>* %1688, align 4, !tbaa !3164
  %1689 = getelementptr inbounds float, float* %1687, i64 4
  %1690 = bitcast float* %1689 to <4 x float>*
  %wide.load96.25 = load <4 x float>, <4 x float>* %1690, align 4, !tbaa !3164
  %1691 = getelementptr inbounds float, float* %4, i64 %1686
  %1692 = bitcast float* %1691 to <4 x float>*
  store <4 x float> %wide.load95.25, <4 x float>* %1692, align 4, !tbaa !3167
  %1693 = getelementptr inbounds float, float* %1691, i64 4
  %1694 = bitcast float* %1693 to <4 x float>*
  store <4 x float> %wide.load96.25, <4 x float>* %1694, align 4, !tbaa !3167
  %1695 = add nsw i64 %21, 1488
  %1696 = getelementptr inbounds float, float* %7, i64 %1695
  %1697 = bitcast float* %1696 to <4 x float>*
  %wide.load95.26 = load <4 x float>, <4 x float>* %1697, align 4, !tbaa !3164
  %1698 = getelementptr inbounds float, float* %1696, i64 4
  %1699 = bitcast float* %1698 to <4 x float>*
  %wide.load96.26 = load <4 x float>, <4 x float>* %1699, align 4, !tbaa !3164
  %1700 = getelementptr inbounds float, float* %4, i64 %1695
  %1701 = bitcast float* %1700 to <4 x float>*
  store <4 x float> %wide.load95.26, <4 x float>* %1701, align 4, !tbaa !3167
  %1702 = getelementptr inbounds float, float* %1700, i64 4
  %1703 = bitcast float* %1702 to <4 x float>*
  store <4 x float> %wide.load96.26, <4 x float>* %1703, align 4, !tbaa !3167
  %1704 = add nsw i64 %21, 1496
  %1705 = getelementptr inbounds float, float* %7, i64 %1704
  %1706 = bitcast float* %1705 to <4 x float>*
  %wide.load95.27 = load <4 x float>, <4 x float>* %1706, align 4, !tbaa !3164
  %1707 = getelementptr inbounds float, float* %1705, i64 4
  %1708 = bitcast float* %1707 to <4 x float>*
  %wide.load96.27 = load <4 x float>, <4 x float>* %1708, align 4, !tbaa !3164
  %1709 = getelementptr inbounds float, float* %4, i64 %1704
  %1710 = bitcast float* %1709 to <4 x float>*
  store <4 x float> %wide.load95.27, <4 x float>* %1710, align 4, !tbaa !3167
  %1711 = getelementptr inbounds float, float* %1709, i64 4
  %1712 = bitcast float* %1711 to <4 x float>*
  store <4 x float> %wide.load96.27, <4 x float>* %1712, align 4, !tbaa !3167
  %1713 = add nsw i64 %21, 1504
  %1714 = getelementptr inbounds float, float* %7, i64 %1713
  %1715 = bitcast float* %1714 to <4 x float>*
  %wide.load95.28 = load <4 x float>, <4 x float>* %1715, align 4, !tbaa !3164
  %1716 = getelementptr inbounds float, float* %1714, i64 4
  %1717 = bitcast float* %1716 to <4 x float>*
  %wide.load96.28 = load <4 x float>, <4 x float>* %1717, align 4, !tbaa !3164
  %1718 = getelementptr inbounds float, float* %4, i64 %1713
  %1719 = bitcast float* %1718 to <4 x float>*
  store <4 x float> %wide.load95.28, <4 x float>* %1719, align 4, !tbaa !3167
  %1720 = getelementptr inbounds float, float* %1718, i64 4
  %1721 = bitcast float* %1720 to <4 x float>*
  store <4 x float> %wide.load96.28, <4 x float>* %1721, align 4, !tbaa !3167
  %1722 = add nsw i64 %21, 1512
  %1723 = getelementptr inbounds float, float* %7, i64 %1722
  %1724 = bitcast float* %1723 to <4 x float>*
  %wide.load95.29 = load <4 x float>, <4 x float>* %1724, align 4, !tbaa !3164
  %1725 = getelementptr inbounds float, float* %1723, i64 4
  %1726 = bitcast float* %1725 to <4 x float>*
  %wide.load96.29 = load <4 x float>, <4 x float>* %1726, align 4, !tbaa !3164
  %1727 = getelementptr inbounds float, float* %4, i64 %1722
  %1728 = bitcast float* %1727 to <4 x float>*
  store <4 x float> %wide.load95.29, <4 x float>* %1728, align 4, !tbaa !3167
  %1729 = getelementptr inbounds float, float* %1727, i64 4
  %1730 = bitcast float* %1729 to <4 x float>*
  store <4 x float> %wide.load96.29, <4 x float>* %1730, align 4, !tbaa !3167
  %1731 = add nsw i64 %21, 1520
  %1732 = getelementptr inbounds float, float* %7, i64 %1731
  %1733 = bitcast float* %1732 to <4 x float>*
  %wide.load95.30 = load <4 x float>, <4 x float>* %1733, align 4, !tbaa !3164
  %1734 = getelementptr inbounds float, float* %1732, i64 4
  %1735 = bitcast float* %1734 to <4 x float>*
  %wide.load96.30 = load <4 x float>, <4 x float>* %1735, align 4, !tbaa !3164
  %1736 = getelementptr inbounds float, float* %4, i64 %1731
  %1737 = bitcast float* %1736 to <4 x float>*
  store <4 x float> %wide.load95.30, <4 x float>* %1737, align 4, !tbaa !3167
  %1738 = getelementptr inbounds float, float* %1736, i64 4
  %1739 = bitcast float* %1738 to <4 x float>*
  store <4 x float> %wide.load96.30, <4 x float>* %1739, align 4, !tbaa !3167
  %1740 = add nsw i64 %21, 1528
  %1741 = getelementptr inbounds float, float* %7, i64 %1740
  %1742 = bitcast float* %1741 to <4 x float>*
  %wide.load95.31 = load <4 x float>, <4 x float>* %1742, align 4, !tbaa !3164
  %1743 = getelementptr inbounds float, float* %1741, i64 4
  %1744 = bitcast float* %1743 to <4 x float>*
  %wide.load96.31 = load <4 x float>, <4 x float>* %1744, align 4, !tbaa !3164
  %1745 = getelementptr inbounds float, float* %4, i64 %1740
  %1746 = bitcast float* %1745 to <4 x float>*
  store <4 x float> %wide.load95.31, <4 x float>* %1746, align 4, !tbaa !3167
  %1747 = getelementptr inbounds float, float* %1745, i64 4
  %1748 = bitcast float* %1747 to <4 x float>*
  store <4 x float> %wide.load96.31, <4 x float>* %1748, align 4, !tbaa !3167
  %1749 = add nsw i64 %21, 1536
  %1750 = getelementptr inbounds float, float* %7, i64 %1749
  %1751 = bitcast float* %1750 to <4 x float>*
  %wide.load85 = load <4 x float>, <4 x float>* %1751, align 4, !tbaa !3164
  %1752 = getelementptr inbounds float, float* %1750, i64 4
  %1753 = bitcast float* %1752 to <4 x float>*
  %wide.load86 = load <4 x float>, <4 x float>* %1753, align 4, !tbaa !3164
  %1754 = getelementptr inbounds float, float* %4, i64 %1749
  %1755 = bitcast float* %1754 to <4 x float>*
  store <4 x float> %wide.load85, <4 x float>* %1755, align 4, !tbaa !3167
  %1756 = getelementptr inbounds float, float* %1754, i64 4
  %1757 = bitcast float* %1756 to <4 x float>*
  store <4 x float> %wide.load86, <4 x float>* %1757, align 4, !tbaa !3167
  %1758 = add nsw i64 %21, 1544
  %1759 = getelementptr inbounds float, float* %7, i64 %1758
  %1760 = bitcast float* %1759 to <4 x float>*
  %wide.load85.1 = load <4 x float>, <4 x float>* %1760, align 4, !tbaa !3164
  %1761 = getelementptr inbounds float, float* %1759, i64 4
  %1762 = bitcast float* %1761 to <4 x float>*
  %wide.load86.1 = load <4 x float>, <4 x float>* %1762, align 4, !tbaa !3164
  %1763 = getelementptr inbounds float, float* %4, i64 %1758
  %1764 = bitcast float* %1763 to <4 x float>*
  store <4 x float> %wide.load85.1, <4 x float>* %1764, align 4, !tbaa !3167
  %1765 = getelementptr inbounds float, float* %1763, i64 4
  %1766 = bitcast float* %1765 to <4 x float>*
  store <4 x float> %wide.load86.1, <4 x float>* %1766, align 4, !tbaa !3167
  %1767 = add nsw i64 %21, 1552
  %1768 = getelementptr inbounds float, float* %7, i64 %1767
  %1769 = bitcast float* %1768 to <4 x float>*
  %wide.load85.2 = load <4 x float>, <4 x float>* %1769, align 4, !tbaa !3164
  %1770 = getelementptr inbounds float, float* %1768, i64 4
  %1771 = bitcast float* %1770 to <4 x float>*
  %wide.load86.2 = load <4 x float>, <4 x float>* %1771, align 4, !tbaa !3164
  %1772 = getelementptr inbounds float, float* %4, i64 %1767
  %1773 = bitcast float* %1772 to <4 x float>*
  store <4 x float> %wide.load85.2, <4 x float>* %1773, align 4, !tbaa !3167
  %1774 = getelementptr inbounds float, float* %1772, i64 4
  %1775 = bitcast float* %1774 to <4 x float>*
  store <4 x float> %wide.load86.2, <4 x float>* %1775, align 4, !tbaa !3167
  %1776 = add nsw i64 %21, 1560
  %1777 = getelementptr inbounds float, float* %7, i64 %1776
  %1778 = bitcast float* %1777 to <4 x float>*
  %wide.load85.3 = load <4 x float>, <4 x float>* %1778, align 4, !tbaa !3164
  %1779 = getelementptr inbounds float, float* %1777, i64 4
  %1780 = bitcast float* %1779 to <4 x float>*
  %wide.load86.3 = load <4 x float>, <4 x float>* %1780, align 4, !tbaa !3164
  %1781 = getelementptr inbounds float, float* %4, i64 %1776
  %1782 = bitcast float* %1781 to <4 x float>*
  store <4 x float> %wide.load85.3, <4 x float>* %1782, align 4, !tbaa !3167
  %1783 = getelementptr inbounds float, float* %1781, i64 4
  %1784 = bitcast float* %1783 to <4 x float>*
  store <4 x float> %wide.load86.3, <4 x float>* %1784, align 4, !tbaa !3167
  %1785 = add nsw i64 %21, 1568
  %1786 = getelementptr inbounds float, float* %7, i64 %1785
  %1787 = bitcast float* %1786 to <4 x float>*
  %wide.load85.4 = load <4 x float>, <4 x float>* %1787, align 4, !tbaa !3164
  %1788 = getelementptr inbounds float, float* %1786, i64 4
  %1789 = bitcast float* %1788 to <4 x float>*
  %wide.load86.4 = load <4 x float>, <4 x float>* %1789, align 4, !tbaa !3164
  %1790 = getelementptr inbounds float, float* %4, i64 %1785
  %1791 = bitcast float* %1790 to <4 x float>*
  store <4 x float> %wide.load85.4, <4 x float>* %1791, align 4, !tbaa !3167
  %1792 = getelementptr inbounds float, float* %1790, i64 4
  %1793 = bitcast float* %1792 to <4 x float>*
  store <4 x float> %wide.load86.4, <4 x float>* %1793, align 4, !tbaa !3167
  %1794 = add nsw i64 %21, 1576
  %1795 = getelementptr inbounds float, float* %7, i64 %1794
  %1796 = bitcast float* %1795 to <4 x float>*
  %wide.load85.5 = load <4 x float>, <4 x float>* %1796, align 4, !tbaa !3164
  %1797 = getelementptr inbounds float, float* %1795, i64 4
  %1798 = bitcast float* %1797 to <4 x float>*
  %wide.load86.5 = load <4 x float>, <4 x float>* %1798, align 4, !tbaa !3164
  %1799 = getelementptr inbounds float, float* %4, i64 %1794
  %1800 = bitcast float* %1799 to <4 x float>*
  store <4 x float> %wide.load85.5, <4 x float>* %1800, align 4, !tbaa !3167
  %1801 = getelementptr inbounds float, float* %1799, i64 4
  %1802 = bitcast float* %1801 to <4 x float>*
  store <4 x float> %wide.load86.5, <4 x float>* %1802, align 4, !tbaa !3167
  %1803 = add nsw i64 %21, 1584
  %1804 = getelementptr inbounds float, float* %7, i64 %1803
  %1805 = bitcast float* %1804 to <4 x float>*
  %wide.load85.6 = load <4 x float>, <4 x float>* %1805, align 4, !tbaa !3164
  %1806 = getelementptr inbounds float, float* %1804, i64 4
  %1807 = bitcast float* %1806 to <4 x float>*
  %wide.load86.6 = load <4 x float>, <4 x float>* %1807, align 4, !tbaa !3164
  %1808 = getelementptr inbounds float, float* %4, i64 %1803
  %1809 = bitcast float* %1808 to <4 x float>*
  store <4 x float> %wide.load85.6, <4 x float>* %1809, align 4, !tbaa !3167
  %1810 = getelementptr inbounds float, float* %1808, i64 4
  %1811 = bitcast float* %1810 to <4 x float>*
  store <4 x float> %wide.load86.6, <4 x float>* %1811, align 4, !tbaa !3167
  %1812 = add nsw i64 %21, 1592
  %1813 = getelementptr inbounds float, float* %7, i64 %1812
  %1814 = bitcast float* %1813 to <4 x float>*
  %wide.load85.7 = load <4 x float>, <4 x float>* %1814, align 4, !tbaa !3164
  %1815 = getelementptr inbounds float, float* %1813, i64 4
  %1816 = bitcast float* %1815 to <4 x float>*
  %wide.load86.7 = load <4 x float>, <4 x float>* %1816, align 4, !tbaa !3164
  %1817 = getelementptr inbounds float, float* %4, i64 %1812
  %1818 = bitcast float* %1817 to <4 x float>*
  store <4 x float> %wide.load85.7, <4 x float>* %1818, align 4, !tbaa !3167
  %1819 = getelementptr inbounds float, float* %1817, i64 4
  %1820 = bitcast float* %1819 to <4 x float>*
  store <4 x float> %wide.load86.7, <4 x float>* %1820, align 4, !tbaa !3167
  %1821 = add nsw i64 %21, 1600
  %1822 = getelementptr inbounds float, float* %7, i64 %1821
  %1823 = bitcast float* %1822 to <4 x float>*
  %wide.load85.8 = load <4 x float>, <4 x float>* %1823, align 4, !tbaa !3164
  %1824 = getelementptr inbounds float, float* %1822, i64 4
  %1825 = bitcast float* %1824 to <4 x float>*
  %wide.load86.8 = load <4 x float>, <4 x float>* %1825, align 4, !tbaa !3164
  %1826 = getelementptr inbounds float, float* %4, i64 %1821
  %1827 = bitcast float* %1826 to <4 x float>*
  store <4 x float> %wide.load85.8, <4 x float>* %1827, align 4, !tbaa !3167
  %1828 = getelementptr inbounds float, float* %1826, i64 4
  %1829 = bitcast float* %1828 to <4 x float>*
  store <4 x float> %wide.load86.8, <4 x float>* %1829, align 4, !tbaa !3167
  %1830 = add nsw i64 %21, 1608
  %1831 = getelementptr inbounds float, float* %7, i64 %1830
  %1832 = bitcast float* %1831 to <4 x float>*
  %wide.load85.9 = load <4 x float>, <4 x float>* %1832, align 4, !tbaa !3164
  %1833 = getelementptr inbounds float, float* %1831, i64 4
  %1834 = bitcast float* %1833 to <4 x float>*
  %wide.load86.9 = load <4 x float>, <4 x float>* %1834, align 4, !tbaa !3164
  %1835 = getelementptr inbounds float, float* %4, i64 %1830
  %1836 = bitcast float* %1835 to <4 x float>*
  store <4 x float> %wide.load85.9, <4 x float>* %1836, align 4, !tbaa !3167
  %1837 = getelementptr inbounds float, float* %1835, i64 4
  %1838 = bitcast float* %1837 to <4 x float>*
  store <4 x float> %wide.load86.9, <4 x float>* %1838, align 4, !tbaa !3167
  %1839 = add nsw i64 %21, 1616
  %1840 = getelementptr inbounds float, float* %7, i64 %1839
  %1841 = bitcast float* %1840 to <4 x float>*
  %wide.load85.10 = load <4 x float>, <4 x float>* %1841, align 4, !tbaa !3164
  %1842 = getelementptr inbounds float, float* %1840, i64 4
  %1843 = bitcast float* %1842 to <4 x float>*
  %wide.load86.10 = load <4 x float>, <4 x float>* %1843, align 4, !tbaa !3164
  %1844 = getelementptr inbounds float, float* %4, i64 %1839
  %1845 = bitcast float* %1844 to <4 x float>*
  store <4 x float> %wide.load85.10, <4 x float>* %1845, align 4, !tbaa !3167
  %1846 = getelementptr inbounds float, float* %1844, i64 4
  %1847 = bitcast float* %1846 to <4 x float>*
  store <4 x float> %wide.load86.10, <4 x float>* %1847, align 4, !tbaa !3167
  %1848 = add nsw i64 %21, 1624
  %1849 = getelementptr inbounds float, float* %7, i64 %1848
  %1850 = bitcast float* %1849 to <4 x float>*
  %wide.load85.11 = load <4 x float>, <4 x float>* %1850, align 4, !tbaa !3164
  %1851 = getelementptr inbounds float, float* %1849, i64 4
  %1852 = bitcast float* %1851 to <4 x float>*
  %wide.load86.11 = load <4 x float>, <4 x float>* %1852, align 4, !tbaa !3164
  %1853 = getelementptr inbounds float, float* %4, i64 %1848
  %1854 = bitcast float* %1853 to <4 x float>*
  store <4 x float> %wide.load85.11, <4 x float>* %1854, align 4, !tbaa !3167
  %1855 = getelementptr inbounds float, float* %1853, i64 4
  %1856 = bitcast float* %1855 to <4 x float>*
  store <4 x float> %wide.load86.11, <4 x float>* %1856, align 4, !tbaa !3167
  %1857 = add nsw i64 %21, 1632
  %1858 = getelementptr inbounds float, float* %7, i64 %1857
  %1859 = bitcast float* %1858 to <4 x float>*
  %wide.load85.12 = load <4 x float>, <4 x float>* %1859, align 4, !tbaa !3164
  %1860 = getelementptr inbounds float, float* %1858, i64 4
  %1861 = bitcast float* %1860 to <4 x float>*
  %wide.load86.12 = load <4 x float>, <4 x float>* %1861, align 4, !tbaa !3164
  %1862 = getelementptr inbounds float, float* %4, i64 %1857
  %1863 = bitcast float* %1862 to <4 x float>*
  store <4 x float> %wide.load85.12, <4 x float>* %1863, align 4, !tbaa !3167
  %1864 = getelementptr inbounds float, float* %1862, i64 4
  %1865 = bitcast float* %1864 to <4 x float>*
  store <4 x float> %wide.load86.12, <4 x float>* %1865, align 4, !tbaa !3167
  %1866 = add nsw i64 %21, 1640
  %1867 = getelementptr inbounds float, float* %7, i64 %1866
  %1868 = bitcast float* %1867 to <4 x float>*
  %wide.load85.13 = load <4 x float>, <4 x float>* %1868, align 4, !tbaa !3164
  %1869 = getelementptr inbounds float, float* %1867, i64 4
  %1870 = bitcast float* %1869 to <4 x float>*
  %wide.load86.13 = load <4 x float>, <4 x float>* %1870, align 4, !tbaa !3164
  %1871 = getelementptr inbounds float, float* %4, i64 %1866
  %1872 = bitcast float* %1871 to <4 x float>*
  store <4 x float> %wide.load85.13, <4 x float>* %1872, align 4, !tbaa !3167
  %1873 = getelementptr inbounds float, float* %1871, i64 4
  %1874 = bitcast float* %1873 to <4 x float>*
  store <4 x float> %wide.load86.13, <4 x float>* %1874, align 4, !tbaa !3167
  %1875 = add nsw i64 %21, 1648
  %1876 = getelementptr inbounds float, float* %7, i64 %1875
  %1877 = bitcast float* %1876 to <4 x float>*
  %wide.load85.14 = load <4 x float>, <4 x float>* %1877, align 4, !tbaa !3164
  %1878 = getelementptr inbounds float, float* %1876, i64 4
  %1879 = bitcast float* %1878 to <4 x float>*
  %wide.load86.14 = load <4 x float>, <4 x float>* %1879, align 4, !tbaa !3164
  %1880 = getelementptr inbounds float, float* %4, i64 %1875
  %1881 = bitcast float* %1880 to <4 x float>*
  store <4 x float> %wide.load85.14, <4 x float>* %1881, align 4, !tbaa !3167
  %1882 = getelementptr inbounds float, float* %1880, i64 4
  %1883 = bitcast float* %1882 to <4 x float>*
  store <4 x float> %wide.load86.14, <4 x float>* %1883, align 4, !tbaa !3167
  %1884 = add nsw i64 %21, 1656
  %1885 = getelementptr inbounds float, float* %7, i64 %1884
  %1886 = bitcast float* %1885 to <4 x float>*
  %wide.load85.15 = load <4 x float>, <4 x float>* %1886, align 4, !tbaa !3164
  %1887 = getelementptr inbounds float, float* %1885, i64 4
  %1888 = bitcast float* %1887 to <4 x float>*
  %wide.load86.15 = load <4 x float>, <4 x float>* %1888, align 4, !tbaa !3164
  %1889 = getelementptr inbounds float, float* %4, i64 %1884
  %1890 = bitcast float* %1889 to <4 x float>*
  store <4 x float> %wide.load85.15, <4 x float>* %1890, align 4, !tbaa !3167
  %1891 = getelementptr inbounds float, float* %1889, i64 4
  %1892 = bitcast float* %1891 to <4 x float>*
  store <4 x float> %wide.load86.15, <4 x float>* %1892, align 4, !tbaa !3167
  %1893 = add nsw i64 %21, 1664
  %1894 = getelementptr inbounds float, float* %7, i64 %1893
  %1895 = bitcast float* %1894 to <4 x float>*
  %wide.load85.16 = load <4 x float>, <4 x float>* %1895, align 4, !tbaa !3164
  %1896 = getelementptr inbounds float, float* %1894, i64 4
  %1897 = bitcast float* %1896 to <4 x float>*
  %wide.load86.16 = load <4 x float>, <4 x float>* %1897, align 4, !tbaa !3164
  %1898 = getelementptr inbounds float, float* %4, i64 %1893
  %1899 = bitcast float* %1898 to <4 x float>*
  store <4 x float> %wide.load85.16, <4 x float>* %1899, align 4, !tbaa !3167
  %1900 = getelementptr inbounds float, float* %1898, i64 4
  %1901 = bitcast float* %1900 to <4 x float>*
  store <4 x float> %wide.load86.16, <4 x float>* %1901, align 4, !tbaa !3167
  %1902 = add nsw i64 %21, 1672
  %1903 = getelementptr inbounds float, float* %7, i64 %1902
  %1904 = bitcast float* %1903 to <4 x float>*
  %wide.load85.17 = load <4 x float>, <4 x float>* %1904, align 4, !tbaa !3164
  %1905 = getelementptr inbounds float, float* %1903, i64 4
  %1906 = bitcast float* %1905 to <4 x float>*
  %wide.load86.17 = load <4 x float>, <4 x float>* %1906, align 4, !tbaa !3164
  %1907 = getelementptr inbounds float, float* %4, i64 %1902
  %1908 = bitcast float* %1907 to <4 x float>*
  store <4 x float> %wide.load85.17, <4 x float>* %1908, align 4, !tbaa !3167
  %1909 = getelementptr inbounds float, float* %1907, i64 4
  %1910 = bitcast float* %1909 to <4 x float>*
  store <4 x float> %wide.load86.17, <4 x float>* %1910, align 4, !tbaa !3167
  %1911 = add nsw i64 %21, 1680
  %1912 = getelementptr inbounds float, float* %7, i64 %1911
  %1913 = bitcast float* %1912 to <4 x float>*
  %wide.load85.18 = load <4 x float>, <4 x float>* %1913, align 4, !tbaa !3164
  %1914 = getelementptr inbounds float, float* %1912, i64 4
  %1915 = bitcast float* %1914 to <4 x float>*
  %wide.load86.18 = load <4 x float>, <4 x float>* %1915, align 4, !tbaa !3164
  %1916 = getelementptr inbounds float, float* %4, i64 %1911
  %1917 = bitcast float* %1916 to <4 x float>*
  store <4 x float> %wide.load85.18, <4 x float>* %1917, align 4, !tbaa !3167
  %1918 = getelementptr inbounds float, float* %1916, i64 4
  %1919 = bitcast float* %1918 to <4 x float>*
  store <4 x float> %wide.load86.18, <4 x float>* %1919, align 4, !tbaa !3167
  %1920 = add nsw i64 %21, 1688
  %1921 = getelementptr inbounds float, float* %7, i64 %1920
  %1922 = bitcast float* %1921 to <4 x float>*
  %wide.load85.19 = load <4 x float>, <4 x float>* %1922, align 4, !tbaa !3164
  %1923 = getelementptr inbounds float, float* %1921, i64 4
  %1924 = bitcast float* %1923 to <4 x float>*
  %wide.load86.19 = load <4 x float>, <4 x float>* %1924, align 4, !tbaa !3164
  %1925 = getelementptr inbounds float, float* %4, i64 %1920
  %1926 = bitcast float* %1925 to <4 x float>*
  store <4 x float> %wide.load85.19, <4 x float>* %1926, align 4, !tbaa !3167
  %1927 = getelementptr inbounds float, float* %1925, i64 4
  %1928 = bitcast float* %1927 to <4 x float>*
  store <4 x float> %wide.load86.19, <4 x float>* %1928, align 4, !tbaa !3167
  %1929 = add nsw i64 %21, 1696
  %1930 = getelementptr inbounds float, float* %7, i64 %1929
  %1931 = bitcast float* %1930 to <4 x float>*
  %wide.load85.20 = load <4 x float>, <4 x float>* %1931, align 4, !tbaa !3164
  %1932 = getelementptr inbounds float, float* %1930, i64 4
  %1933 = bitcast float* %1932 to <4 x float>*
  %wide.load86.20 = load <4 x float>, <4 x float>* %1933, align 4, !tbaa !3164
  %1934 = getelementptr inbounds float, float* %4, i64 %1929
  %1935 = bitcast float* %1934 to <4 x float>*
  store <4 x float> %wide.load85.20, <4 x float>* %1935, align 4, !tbaa !3167
  %1936 = getelementptr inbounds float, float* %1934, i64 4
  %1937 = bitcast float* %1936 to <4 x float>*
  store <4 x float> %wide.load86.20, <4 x float>* %1937, align 4, !tbaa !3167
  %1938 = add nsw i64 %21, 1704
  %1939 = getelementptr inbounds float, float* %7, i64 %1938
  %1940 = bitcast float* %1939 to <4 x float>*
  %wide.load85.21 = load <4 x float>, <4 x float>* %1940, align 4, !tbaa !3164
  %1941 = getelementptr inbounds float, float* %1939, i64 4
  %1942 = bitcast float* %1941 to <4 x float>*
  %wide.load86.21 = load <4 x float>, <4 x float>* %1942, align 4, !tbaa !3164
  %1943 = getelementptr inbounds float, float* %4, i64 %1938
  %1944 = bitcast float* %1943 to <4 x float>*
  store <4 x float> %wide.load85.21, <4 x float>* %1944, align 4, !tbaa !3167
  %1945 = getelementptr inbounds float, float* %1943, i64 4
  %1946 = bitcast float* %1945 to <4 x float>*
  store <4 x float> %wide.load86.21, <4 x float>* %1946, align 4, !tbaa !3167
  %1947 = add nsw i64 %21, 1712
  %1948 = getelementptr inbounds float, float* %7, i64 %1947
  %1949 = bitcast float* %1948 to <4 x float>*
  %wide.load85.22 = load <4 x float>, <4 x float>* %1949, align 4, !tbaa !3164
  %1950 = getelementptr inbounds float, float* %1948, i64 4
  %1951 = bitcast float* %1950 to <4 x float>*
  %wide.load86.22 = load <4 x float>, <4 x float>* %1951, align 4, !tbaa !3164
  %1952 = getelementptr inbounds float, float* %4, i64 %1947
  %1953 = bitcast float* %1952 to <4 x float>*
  store <4 x float> %wide.load85.22, <4 x float>* %1953, align 4, !tbaa !3167
  %1954 = getelementptr inbounds float, float* %1952, i64 4
  %1955 = bitcast float* %1954 to <4 x float>*
  store <4 x float> %wide.load86.22, <4 x float>* %1955, align 4, !tbaa !3167
  %1956 = add nsw i64 %21, 1720
  %1957 = getelementptr inbounds float, float* %7, i64 %1956
  %1958 = bitcast float* %1957 to <4 x float>*
  %wide.load85.23 = load <4 x float>, <4 x float>* %1958, align 4, !tbaa !3164
  %1959 = getelementptr inbounds float, float* %1957, i64 4
  %1960 = bitcast float* %1959 to <4 x float>*
  %wide.load86.23 = load <4 x float>, <4 x float>* %1960, align 4, !tbaa !3164
  %1961 = getelementptr inbounds float, float* %4, i64 %1956
  %1962 = bitcast float* %1961 to <4 x float>*
  store <4 x float> %wide.load85.23, <4 x float>* %1962, align 4, !tbaa !3167
  %1963 = getelementptr inbounds float, float* %1961, i64 4
  %1964 = bitcast float* %1963 to <4 x float>*
  store <4 x float> %wide.load86.23, <4 x float>* %1964, align 4, !tbaa !3167
  %1965 = add nsw i64 %21, 1728
  %1966 = getelementptr inbounds float, float* %7, i64 %1965
  %1967 = bitcast float* %1966 to <4 x float>*
  %wide.load85.24 = load <4 x float>, <4 x float>* %1967, align 4, !tbaa !3164
  %1968 = getelementptr inbounds float, float* %1966, i64 4
  %1969 = bitcast float* %1968 to <4 x float>*
  %wide.load86.24 = load <4 x float>, <4 x float>* %1969, align 4, !tbaa !3164
  %1970 = getelementptr inbounds float, float* %4, i64 %1965
  %1971 = bitcast float* %1970 to <4 x float>*
  store <4 x float> %wide.load85.24, <4 x float>* %1971, align 4, !tbaa !3167
  %1972 = getelementptr inbounds float, float* %1970, i64 4
  %1973 = bitcast float* %1972 to <4 x float>*
  store <4 x float> %wide.load86.24, <4 x float>* %1973, align 4, !tbaa !3167
  %1974 = add nsw i64 %21, 1736
  %1975 = getelementptr inbounds float, float* %7, i64 %1974
  %1976 = bitcast float* %1975 to <4 x float>*
  %wide.load85.25 = load <4 x float>, <4 x float>* %1976, align 4, !tbaa !3164
  %1977 = getelementptr inbounds float, float* %1975, i64 4
  %1978 = bitcast float* %1977 to <4 x float>*
  %wide.load86.25 = load <4 x float>, <4 x float>* %1978, align 4, !tbaa !3164
  %1979 = getelementptr inbounds float, float* %4, i64 %1974
  %1980 = bitcast float* %1979 to <4 x float>*
  store <4 x float> %wide.load85.25, <4 x float>* %1980, align 4, !tbaa !3167
  %1981 = getelementptr inbounds float, float* %1979, i64 4
  %1982 = bitcast float* %1981 to <4 x float>*
  store <4 x float> %wide.load86.25, <4 x float>* %1982, align 4, !tbaa !3167
  %1983 = add nsw i64 %21, 1744
  %1984 = getelementptr inbounds float, float* %7, i64 %1983
  %1985 = bitcast float* %1984 to <4 x float>*
  %wide.load85.26 = load <4 x float>, <4 x float>* %1985, align 4, !tbaa !3164
  %1986 = getelementptr inbounds float, float* %1984, i64 4
  %1987 = bitcast float* %1986 to <4 x float>*
  %wide.load86.26 = load <4 x float>, <4 x float>* %1987, align 4, !tbaa !3164
  %1988 = getelementptr inbounds float, float* %4, i64 %1983
  %1989 = bitcast float* %1988 to <4 x float>*
  store <4 x float> %wide.load85.26, <4 x float>* %1989, align 4, !tbaa !3167
  %1990 = getelementptr inbounds float, float* %1988, i64 4
  %1991 = bitcast float* %1990 to <4 x float>*
  store <4 x float> %wide.load86.26, <4 x float>* %1991, align 4, !tbaa !3167
  %1992 = add nsw i64 %21, 1752
  %1993 = getelementptr inbounds float, float* %7, i64 %1992
  %1994 = bitcast float* %1993 to <4 x float>*
  %wide.load85.27 = load <4 x float>, <4 x float>* %1994, align 4, !tbaa !3164
  %1995 = getelementptr inbounds float, float* %1993, i64 4
  %1996 = bitcast float* %1995 to <4 x float>*
  %wide.load86.27 = load <4 x float>, <4 x float>* %1996, align 4, !tbaa !3164
  %1997 = getelementptr inbounds float, float* %4, i64 %1992
  %1998 = bitcast float* %1997 to <4 x float>*
  store <4 x float> %wide.load85.27, <4 x float>* %1998, align 4, !tbaa !3167
  %1999 = getelementptr inbounds float, float* %1997, i64 4
  %2000 = bitcast float* %1999 to <4 x float>*
  store <4 x float> %wide.load86.27, <4 x float>* %2000, align 4, !tbaa !3167
  %2001 = add nsw i64 %21, 1760
  %2002 = getelementptr inbounds float, float* %7, i64 %2001
  %2003 = bitcast float* %2002 to <4 x float>*
  %wide.load85.28 = load <4 x float>, <4 x float>* %2003, align 4, !tbaa !3164
  %2004 = getelementptr inbounds float, float* %2002, i64 4
  %2005 = bitcast float* %2004 to <4 x float>*
  %wide.load86.28 = load <4 x float>, <4 x float>* %2005, align 4, !tbaa !3164
  %2006 = getelementptr inbounds float, float* %4, i64 %2001
  %2007 = bitcast float* %2006 to <4 x float>*
  store <4 x float> %wide.load85.28, <4 x float>* %2007, align 4, !tbaa !3167
  %2008 = getelementptr inbounds float, float* %2006, i64 4
  %2009 = bitcast float* %2008 to <4 x float>*
  store <4 x float> %wide.load86.28, <4 x float>* %2009, align 4, !tbaa !3167
  %2010 = add nsw i64 %21, 1768
  %2011 = getelementptr inbounds float, float* %7, i64 %2010
  %2012 = bitcast float* %2011 to <4 x float>*
  %wide.load85.29 = load <4 x float>, <4 x float>* %2012, align 4, !tbaa !3164
  %2013 = getelementptr inbounds float, float* %2011, i64 4
  %2014 = bitcast float* %2013 to <4 x float>*
  %wide.load86.29 = load <4 x float>, <4 x float>* %2014, align 4, !tbaa !3164
  %2015 = getelementptr inbounds float, float* %4, i64 %2010
  %2016 = bitcast float* %2015 to <4 x float>*
  store <4 x float> %wide.load85.29, <4 x float>* %2016, align 4, !tbaa !3167
  %2017 = getelementptr inbounds float, float* %2015, i64 4
  %2018 = bitcast float* %2017 to <4 x float>*
  store <4 x float> %wide.load86.29, <4 x float>* %2018, align 4, !tbaa !3167
  %2019 = add nsw i64 %21, 1776
  %2020 = getelementptr inbounds float, float* %7, i64 %2019
  %2021 = bitcast float* %2020 to <4 x float>*
  %wide.load85.30 = load <4 x float>, <4 x float>* %2021, align 4, !tbaa !3164
  %2022 = getelementptr inbounds float, float* %2020, i64 4
  %2023 = bitcast float* %2022 to <4 x float>*
  %wide.load86.30 = load <4 x float>, <4 x float>* %2023, align 4, !tbaa !3164
  %2024 = getelementptr inbounds float, float* %4, i64 %2019
  %2025 = bitcast float* %2024 to <4 x float>*
  store <4 x float> %wide.load85.30, <4 x float>* %2025, align 4, !tbaa !3167
  %2026 = getelementptr inbounds float, float* %2024, i64 4
  %2027 = bitcast float* %2026 to <4 x float>*
  store <4 x float> %wide.load86.30, <4 x float>* %2027, align 4, !tbaa !3167
  %2028 = add nsw i64 %21, 1784
  %2029 = getelementptr inbounds float, float* %7, i64 %2028
  %2030 = bitcast float* %2029 to <4 x float>*
  %wide.load85.31 = load <4 x float>, <4 x float>* %2030, align 4, !tbaa !3164
  %2031 = getelementptr inbounds float, float* %2029, i64 4
  %2032 = bitcast float* %2031 to <4 x float>*
  %wide.load86.31 = load <4 x float>, <4 x float>* %2032, align 4, !tbaa !3164
  %2033 = getelementptr inbounds float, float* %4, i64 %2028
  %2034 = bitcast float* %2033 to <4 x float>*
  store <4 x float> %wide.load85.31, <4 x float>* %2034, align 4, !tbaa !3167
  %2035 = getelementptr inbounds float, float* %2033, i64 4
  %2036 = bitcast float* %2035 to <4 x float>*
  store <4 x float> %wide.load86.31, <4 x float>* %2036, align 4, !tbaa !3167
  %2037 = add nsw i64 %21, 1792
  %2038 = getelementptr inbounds float, float* %7, i64 %2037
  %2039 = bitcast float* %2038 to <4 x float>*
  %wide.load75 = load <4 x float>, <4 x float>* %2039, align 4, !tbaa !3164
  %2040 = getelementptr inbounds float, float* %2038, i64 4
  %2041 = bitcast float* %2040 to <4 x float>*
  %wide.load76 = load <4 x float>, <4 x float>* %2041, align 4, !tbaa !3164
  %2042 = getelementptr inbounds float, float* %4, i64 %2037
  %2043 = bitcast float* %2042 to <4 x float>*
  store <4 x float> %wide.load75, <4 x float>* %2043, align 4, !tbaa !3167
  %2044 = getelementptr inbounds float, float* %2042, i64 4
  %2045 = bitcast float* %2044 to <4 x float>*
  store <4 x float> %wide.load76, <4 x float>* %2045, align 4, !tbaa !3167
  %2046 = add nsw i64 %21, 1800
  %2047 = getelementptr inbounds float, float* %7, i64 %2046
  %2048 = bitcast float* %2047 to <4 x float>*
  %wide.load75.1 = load <4 x float>, <4 x float>* %2048, align 4, !tbaa !3164
  %2049 = getelementptr inbounds float, float* %2047, i64 4
  %2050 = bitcast float* %2049 to <4 x float>*
  %wide.load76.1 = load <4 x float>, <4 x float>* %2050, align 4, !tbaa !3164
  %2051 = getelementptr inbounds float, float* %4, i64 %2046
  %2052 = bitcast float* %2051 to <4 x float>*
  store <4 x float> %wide.load75.1, <4 x float>* %2052, align 4, !tbaa !3167
  %2053 = getelementptr inbounds float, float* %2051, i64 4
  %2054 = bitcast float* %2053 to <4 x float>*
  store <4 x float> %wide.load76.1, <4 x float>* %2054, align 4, !tbaa !3167
  %2055 = add nsw i64 %21, 1808
  %2056 = getelementptr inbounds float, float* %7, i64 %2055
  %2057 = bitcast float* %2056 to <4 x float>*
  %wide.load75.2 = load <4 x float>, <4 x float>* %2057, align 4, !tbaa !3164
  %2058 = getelementptr inbounds float, float* %2056, i64 4
  %2059 = bitcast float* %2058 to <4 x float>*
  %wide.load76.2 = load <4 x float>, <4 x float>* %2059, align 4, !tbaa !3164
  %2060 = getelementptr inbounds float, float* %4, i64 %2055
  %2061 = bitcast float* %2060 to <4 x float>*
  store <4 x float> %wide.load75.2, <4 x float>* %2061, align 4, !tbaa !3167
  %2062 = getelementptr inbounds float, float* %2060, i64 4
  %2063 = bitcast float* %2062 to <4 x float>*
  store <4 x float> %wide.load76.2, <4 x float>* %2063, align 4, !tbaa !3167
  %2064 = add nsw i64 %21, 1816
  %2065 = getelementptr inbounds float, float* %7, i64 %2064
  %2066 = bitcast float* %2065 to <4 x float>*
  %wide.load75.3 = load <4 x float>, <4 x float>* %2066, align 4, !tbaa !3164
  %2067 = getelementptr inbounds float, float* %2065, i64 4
  %2068 = bitcast float* %2067 to <4 x float>*
  %wide.load76.3 = load <4 x float>, <4 x float>* %2068, align 4, !tbaa !3164
  %2069 = getelementptr inbounds float, float* %4, i64 %2064
  %2070 = bitcast float* %2069 to <4 x float>*
  store <4 x float> %wide.load75.3, <4 x float>* %2070, align 4, !tbaa !3167
  %2071 = getelementptr inbounds float, float* %2069, i64 4
  %2072 = bitcast float* %2071 to <4 x float>*
  store <4 x float> %wide.load76.3, <4 x float>* %2072, align 4, !tbaa !3167
  %2073 = add nsw i64 %21, 1824
  %2074 = getelementptr inbounds float, float* %7, i64 %2073
  %2075 = bitcast float* %2074 to <4 x float>*
  %wide.load75.4 = load <4 x float>, <4 x float>* %2075, align 4, !tbaa !3164
  %2076 = getelementptr inbounds float, float* %2074, i64 4
  %2077 = bitcast float* %2076 to <4 x float>*
  %wide.load76.4 = load <4 x float>, <4 x float>* %2077, align 4, !tbaa !3164
  %2078 = getelementptr inbounds float, float* %4, i64 %2073
  %2079 = bitcast float* %2078 to <4 x float>*
  store <4 x float> %wide.load75.4, <4 x float>* %2079, align 4, !tbaa !3167
  %2080 = getelementptr inbounds float, float* %2078, i64 4
  %2081 = bitcast float* %2080 to <4 x float>*
  store <4 x float> %wide.load76.4, <4 x float>* %2081, align 4, !tbaa !3167
  %2082 = add nsw i64 %21, 1832
  %2083 = getelementptr inbounds float, float* %7, i64 %2082
  %2084 = bitcast float* %2083 to <4 x float>*
  %wide.load75.5 = load <4 x float>, <4 x float>* %2084, align 4, !tbaa !3164
  %2085 = getelementptr inbounds float, float* %2083, i64 4
  %2086 = bitcast float* %2085 to <4 x float>*
  %wide.load76.5 = load <4 x float>, <4 x float>* %2086, align 4, !tbaa !3164
  %2087 = getelementptr inbounds float, float* %4, i64 %2082
  %2088 = bitcast float* %2087 to <4 x float>*
  store <4 x float> %wide.load75.5, <4 x float>* %2088, align 4, !tbaa !3167
  %2089 = getelementptr inbounds float, float* %2087, i64 4
  %2090 = bitcast float* %2089 to <4 x float>*
  store <4 x float> %wide.load76.5, <4 x float>* %2090, align 4, !tbaa !3167
  %2091 = add nsw i64 %21, 1840
  %2092 = getelementptr inbounds float, float* %7, i64 %2091
  %2093 = bitcast float* %2092 to <4 x float>*
  %wide.load75.6 = load <4 x float>, <4 x float>* %2093, align 4, !tbaa !3164
  %2094 = getelementptr inbounds float, float* %2092, i64 4
  %2095 = bitcast float* %2094 to <4 x float>*
  %wide.load76.6 = load <4 x float>, <4 x float>* %2095, align 4, !tbaa !3164
  %2096 = getelementptr inbounds float, float* %4, i64 %2091
  %2097 = bitcast float* %2096 to <4 x float>*
  store <4 x float> %wide.load75.6, <4 x float>* %2097, align 4, !tbaa !3167
  %2098 = getelementptr inbounds float, float* %2096, i64 4
  %2099 = bitcast float* %2098 to <4 x float>*
  store <4 x float> %wide.load76.6, <4 x float>* %2099, align 4, !tbaa !3167
  %2100 = add nsw i64 %21, 1848
  %2101 = getelementptr inbounds float, float* %7, i64 %2100
  %2102 = bitcast float* %2101 to <4 x float>*
  %wide.load75.7 = load <4 x float>, <4 x float>* %2102, align 4, !tbaa !3164
  %2103 = getelementptr inbounds float, float* %2101, i64 4
  %2104 = bitcast float* %2103 to <4 x float>*
  %wide.load76.7 = load <4 x float>, <4 x float>* %2104, align 4, !tbaa !3164
  %2105 = getelementptr inbounds float, float* %4, i64 %2100
  %2106 = bitcast float* %2105 to <4 x float>*
  store <4 x float> %wide.load75.7, <4 x float>* %2106, align 4, !tbaa !3167
  %2107 = getelementptr inbounds float, float* %2105, i64 4
  %2108 = bitcast float* %2107 to <4 x float>*
  store <4 x float> %wide.load76.7, <4 x float>* %2108, align 4, !tbaa !3167
  %2109 = add nsw i64 %21, 1856
  %2110 = getelementptr inbounds float, float* %7, i64 %2109
  %2111 = bitcast float* %2110 to <4 x float>*
  %wide.load75.8 = load <4 x float>, <4 x float>* %2111, align 4, !tbaa !3164
  %2112 = getelementptr inbounds float, float* %2110, i64 4
  %2113 = bitcast float* %2112 to <4 x float>*
  %wide.load76.8 = load <4 x float>, <4 x float>* %2113, align 4, !tbaa !3164
  %2114 = getelementptr inbounds float, float* %4, i64 %2109
  %2115 = bitcast float* %2114 to <4 x float>*
  store <4 x float> %wide.load75.8, <4 x float>* %2115, align 4, !tbaa !3167
  %2116 = getelementptr inbounds float, float* %2114, i64 4
  %2117 = bitcast float* %2116 to <4 x float>*
  store <4 x float> %wide.load76.8, <4 x float>* %2117, align 4, !tbaa !3167
  %2118 = add nsw i64 %21, 1864
  %2119 = getelementptr inbounds float, float* %7, i64 %2118
  %2120 = bitcast float* %2119 to <4 x float>*
  %wide.load75.9 = load <4 x float>, <4 x float>* %2120, align 4, !tbaa !3164
  %2121 = getelementptr inbounds float, float* %2119, i64 4
  %2122 = bitcast float* %2121 to <4 x float>*
  %wide.load76.9 = load <4 x float>, <4 x float>* %2122, align 4, !tbaa !3164
  %2123 = getelementptr inbounds float, float* %4, i64 %2118
  %2124 = bitcast float* %2123 to <4 x float>*
  store <4 x float> %wide.load75.9, <4 x float>* %2124, align 4, !tbaa !3167
  %2125 = getelementptr inbounds float, float* %2123, i64 4
  %2126 = bitcast float* %2125 to <4 x float>*
  store <4 x float> %wide.load76.9, <4 x float>* %2126, align 4, !tbaa !3167
  %2127 = add nsw i64 %21, 1872
  %2128 = getelementptr inbounds float, float* %7, i64 %2127
  %2129 = bitcast float* %2128 to <4 x float>*
  %wide.load75.10 = load <4 x float>, <4 x float>* %2129, align 4, !tbaa !3164
  %2130 = getelementptr inbounds float, float* %2128, i64 4
  %2131 = bitcast float* %2130 to <4 x float>*
  %wide.load76.10 = load <4 x float>, <4 x float>* %2131, align 4, !tbaa !3164
  %2132 = getelementptr inbounds float, float* %4, i64 %2127
  %2133 = bitcast float* %2132 to <4 x float>*
  store <4 x float> %wide.load75.10, <4 x float>* %2133, align 4, !tbaa !3167
  %2134 = getelementptr inbounds float, float* %2132, i64 4
  %2135 = bitcast float* %2134 to <4 x float>*
  store <4 x float> %wide.load76.10, <4 x float>* %2135, align 4, !tbaa !3167
  %2136 = add nsw i64 %21, 1880
  %2137 = getelementptr inbounds float, float* %7, i64 %2136
  %2138 = bitcast float* %2137 to <4 x float>*
  %wide.load75.11 = load <4 x float>, <4 x float>* %2138, align 4, !tbaa !3164
  %2139 = getelementptr inbounds float, float* %2137, i64 4
  %2140 = bitcast float* %2139 to <4 x float>*
  %wide.load76.11 = load <4 x float>, <4 x float>* %2140, align 4, !tbaa !3164
  %2141 = getelementptr inbounds float, float* %4, i64 %2136
  %2142 = bitcast float* %2141 to <4 x float>*
  store <4 x float> %wide.load75.11, <4 x float>* %2142, align 4, !tbaa !3167
  %2143 = getelementptr inbounds float, float* %2141, i64 4
  %2144 = bitcast float* %2143 to <4 x float>*
  store <4 x float> %wide.load76.11, <4 x float>* %2144, align 4, !tbaa !3167
  %2145 = add nsw i64 %21, 1888
  %2146 = getelementptr inbounds float, float* %7, i64 %2145
  %2147 = bitcast float* %2146 to <4 x float>*
  %wide.load75.12 = load <4 x float>, <4 x float>* %2147, align 4, !tbaa !3164
  %2148 = getelementptr inbounds float, float* %2146, i64 4
  %2149 = bitcast float* %2148 to <4 x float>*
  %wide.load76.12 = load <4 x float>, <4 x float>* %2149, align 4, !tbaa !3164
  %2150 = getelementptr inbounds float, float* %4, i64 %2145
  %2151 = bitcast float* %2150 to <4 x float>*
  store <4 x float> %wide.load75.12, <4 x float>* %2151, align 4, !tbaa !3167
  %2152 = getelementptr inbounds float, float* %2150, i64 4
  %2153 = bitcast float* %2152 to <4 x float>*
  store <4 x float> %wide.load76.12, <4 x float>* %2153, align 4, !tbaa !3167
  %2154 = add nsw i64 %21, 1896
  %2155 = getelementptr inbounds float, float* %7, i64 %2154
  %2156 = bitcast float* %2155 to <4 x float>*
  %wide.load75.13 = load <4 x float>, <4 x float>* %2156, align 4, !tbaa !3164
  %2157 = getelementptr inbounds float, float* %2155, i64 4
  %2158 = bitcast float* %2157 to <4 x float>*
  %wide.load76.13 = load <4 x float>, <4 x float>* %2158, align 4, !tbaa !3164
  %2159 = getelementptr inbounds float, float* %4, i64 %2154
  %2160 = bitcast float* %2159 to <4 x float>*
  store <4 x float> %wide.load75.13, <4 x float>* %2160, align 4, !tbaa !3167
  %2161 = getelementptr inbounds float, float* %2159, i64 4
  %2162 = bitcast float* %2161 to <4 x float>*
  store <4 x float> %wide.load76.13, <4 x float>* %2162, align 4, !tbaa !3167
  %2163 = add nsw i64 %21, 1904
  %2164 = getelementptr inbounds float, float* %7, i64 %2163
  %2165 = bitcast float* %2164 to <4 x float>*
  %wide.load75.14 = load <4 x float>, <4 x float>* %2165, align 4, !tbaa !3164
  %2166 = getelementptr inbounds float, float* %2164, i64 4
  %2167 = bitcast float* %2166 to <4 x float>*
  %wide.load76.14 = load <4 x float>, <4 x float>* %2167, align 4, !tbaa !3164
  %2168 = getelementptr inbounds float, float* %4, i64 %2163
  %2169 = bitcast float* %2168 to <4 x float>*
  store <4 x float> %wide.load75.14, <4 x float>* %2169, align 4, !tbaa !3167
  %2170 = getelementptr inbounds float, float* %2168, i64 4
  %2171 = bitcast float* %2170 to <4 x float>*
  store <4 x float> %wide.load76.14, <4 x float>* %2171, align 4, !tbaa !3167
  %2172 = add nsw i64 %21, 1912
  %2173 = getelementptr inbounds float, float* %7, i64 %2172
  %2174 = bitcast float* %2173 to <4 x float>*
  %wide.load75.15 = load <4 x float>, <4 x float>* %2174, align 4, !tbaa !3164
  %2175 = getelementptr inbounds float, float* %2173, i64 4
  %2176 = bitcast float* %2175 to <4 x float>*
  %wide.load76.15 = load <4 x float>, <4 x float>* %2176, align 4, !tbaa !3164
  %2177 = getelementptr inbounds float, float* %4, i64 %2172
  %2178 = bitcast float* %2177 to <4 x float>*
  store <4 x float> %wide.load75.15, <4 x float>* %2178, align 4, !tbaa !3167
  %2179 = getelementptr inbounds float, float* %2177, i64 4
  %2180 = bitcast float* %2179 to <4 x float>*
  store <4 x float> %wide.load76.15, <4 x float>* %2180, align 4, !tbaa !3167
  %2181 = add nsw i64 %21, 1920
  %2182 = getelementptr inbounds float, float* %7, i64 %2181
  %2183 = bitcast float* %2182 to <4 x float>*
  %wide.load75.16 = load <4 x float>, <4 x float>* %2183, align 4, !tbaa !3164
  %2184 = getelementptr inbounds float, float* %2182, i64 4
  %2185 = bitcast float* %2184 to <4 x float>*
  %wide.load76.16 = load <4 x float>, <4 x float>* %2185, align 4, !tbaa !3164
  %2186 = getelementptr inbounds float, float* %4, i64 %2181
  %2187 = bitcast float* %2186 to <4 x float>*
  store <4 x float> %wide.load75.16, <4 x float>* %2187, align 4, !tbaa !3167
  %2188 = getelementptr inbounds float, float* %2186, i64 4
  %2189 = bitcast float* %2188 to <4 x float>*
  store <4 x float> %wide.load76.16, <4 x float>* %2189, align 4, !tbaa !3167
  %2190 = add nsw i64 %21, 1928
  %2191 = getelementptr inbounds float, float* %7, i64 %2190
  %2192 = bitcast float* %2191 to <4 x float>*
  %wide.load75.17 = load <4 x float>, <4 x float>* %2192, align 4, !tbaa !3164
  %2193 = getelementptr inbounds float, float* %2191, i64 4
  %2194 = bitcast float* %2193 to <4 x float>*
  %wide.load76.17 = load <4 x float>, <4 x float>* %2194, align 4, !tbaa !3164
  %2195 = getelementptr inbounds float, float* %4, i64 %2190
  %2196 = bitcast float* %2195 to <4 x float>*
  store <4 x float> %wide.load75.17, <4 x float>* %2196, align 4, !tbaa !3167
  %2197 = getelementptr inbounds float, float* %2195, i64 4
  %2198 = bitcast float* %2197 to <4 x float>*
  store <4 x float> %wide.load76.17, <4 x float>* %2198, align 4, !tbaa !3167
  %2199 = add nsw i64 %21, 1936
  %2200 = getelementptr inbounds float, float* %7, i64 %2199
  %2201 = bitcast float* %2200 to <4 x float>*
  %wide.load75.18 = load <4 x float>, <4 x float>* %2201, align 4, !tbaa !3164
  %2202 = getelementptr inbounds float, float* %2200, i64 4
  %2203 = bitcast float* %2202 to <4 x float>*
  %wide.load76.18 = load <4 x float>, <4 x float>* %2203, align 4, !tbaa !3164
  %2204 = getelementptr inbounds float, float* %4, i64 %2199
  %2205 = bitcast float* %2204 to <4 x float>*
  store <4 x float> %wide.load75.18, <4 x float>* %2205, align 4, !tbaa !3167
  %2206 = getelementptr inbounds float, float* %2204, i64 4
  %2207 = bitcast float* %2206 to <4 x float>*
  store <4 x float> %wide.load76.18, <4 x float>* %2207, align 4, !tbaa !3167
  %2208 = add nsw i64 %21, 1944
  %2209 = getelementptr inbounds float, float* %7, i64 %2208
  %2210 = bitcast float* %2209 to <4 x float>*
  %wide.load75.19 = load <4 x float>, <4 x float>* %2210, align 4, !tbaa !3164
  %2211 = getelementptr inbounds float, float* %2209, i64 4
  %2212 = bitcast float* %2211 to <4 x float>*
  %wide.load76.19 = load <4 x float>, <4 x float>* %2212, align 4, !tbaa !3164
  %2213 = getelementptr inbounds float, float* %4, i64 %2208
  %2214 = bitcast float* %2213 to <4 x float>*
  store <4 x float> %wide.load75.19, <4 x float>* %2214, align 4, !tbaa !3167
  %2215 = getelementptr inbounds float, float* %2213, i64 4
  %2216 = bitcast float* %2215 to <4 x float>*
  store <4 x float> %wide.load76.19, <4 x float>* %2216, align 4, !tbaa !3167
  %2217 = add nsw i64 %21, 1952
  %2218 = getelementptr inbounds float, float* %7, i64 %2217
  %2219 = bitcast float* %2218 to <4 x float>*
  %wide.load75.20 = load <4 x float>, <4 x float>* %2219, align 4, !tbaa !3164
  %2220 = getelementptr inbounds float, float* %2218, i64 4
  %2221 = bitcast float* %2220 to <4 x float>*
  %wide.load76.20 = load <4 x float>, <4 x float>* %2221, align 4, !tbaa !3164
  %2222 = getelementptr inbounds float, float* %4, i64 %2217
  %2223 = bitcast float* %2222 to <4 x float>*
  store <4 x float> %wide.load75.20, <4 x float>* %2223, align 4, !tbaa !3167
  %2224 = getelementptr inbounds float, float* %2222, i64 4
  %2225 = bitcast float* %2224 to <4 x float>*
  store <4 x float> %wide.load76.20, <4 x float>* %2225, align 4, !tbaa !3167
  %2226 = add nsw i64 %21, 1960
  %2227 = getelementptr inbounds float, float* %7, i64 %2226
  %2228 = bitcast float* %2227 to <4 x float>*
  %wide.load75.21 = load <4 x float>, <4 x float>* %2228, align 4, !tbaa !3164
  %2229 = getelementptr inbounds float, float* %2227, i64 4
  %2230 = bitcast float* %2229 to <4 x float>*
  %wide.load76.21 = load <4 x float>, <4 x float>* %2230, align 4, !tbaa !3164
  %2231 = getelementptr inbounds float, float* %4, i64 %2226
  %2232 = bitcast float* %2231 to <4 x float>*
  store <4 x float> %wide.load75.21, <4 x float>* %2232, align 4, !tbaa !3167
  %2233 = getelementptr inbounds float, float* %2231, i64 4
  %2234 = bitcast float* %2233 to <4 x float>*
  store <4 x float> %wide.load76.21, <4 x float>* %2234, align 4, !tbaa !3167
  %2235 = add nsw i64 %21, 1968
  %2236 = getelementptr inbounds float, float* %7, i64 %2235
  %2237 = bitcast float* %2236 to <4 x float>*
  %wide.load75.22 = load <4 x float>, <4 x float>* %2237, align 4, !tbaa !3164
  %2238 = getelementptr inbounds float, float* %2236, i64 4
  %2239 = bitcast float* %2238 to <4 x float>*
  %wide.load76.22 = load <4 x float>, <4 x float>* %2239, align 4, !tbaa !3164
  %2240 = getelementptr inbounds float, float* %4, i64 %2235
  %2241 = bitcast float* %2240 to <4 x float>*
  store <4 x float> %wide.load75.22, <4 x float>* %2241, align 4, !tbaa !3167
  %2242 = getelementptr inbounds float, float* %2240, i64 4
  %2243 = bitcast float* %2242 to <4 x float>*
  store <4 x float> %wide.load76.22, <4 x float>* %2243, align 4, !tbaa !3167
  %2244 = add nsw i64 %21, 1976
  %2245 = getelementptr inbounds float, float* %7, i64 %2244
  %2246 = bitcast float* %2245 to <4 x float>*
  %wide.load75.23 = load <4 x float>, <4 x float>* %2246, align 4, !tbaa !3164
  %2247 = getelementptr inbounds float, float* %2245, i64 4
  %2248 = bitcast float* %2247 to <4 x float>*
  %wide.load76.23 = load <4 x float>, <4 x float>* %2248, align 4, !tbaa !3164
  %2249 = getelementptr inbounds float, float* %4, i64 %2244
  %2250 = bitcast float* %2249 to <4 x float>*
  store <4 x float> %wide.load75.23, <4 x float>* %2250, align 4, !tbaa !3167
  %2251 = getelementptr inbounds float, float* %2249, i64 4
  %2252 = bitcast float* %2251 to <4 x float>*
  store <4 x float> %wide.load76.23, <4 x float>* %2252, align 4, !tbaa !3167
  %2253 = add nsw i64 %21, 1984
  %2254 = getelementptr inbounds float, float* %7, i64 %2253
  %2255 = bitcast float* %2254 to <4 x float>*
  %wide.load75.24 = load <4 x float>, <4 x float>* %2255, align 4, !tbaa !3164
  %2256 = getelementptr inbounds float, float* %2254, i64 4
  %2257 = bitcast float* %2256 to <4 x float>*
  %wide.load76.24 = load <4 x float>, <4 x float>* %2257, align 4, !tbaa !3164
  %2258 = getelementptr inbounds float, float* %4, i64 %2253
  %2259 = bitcast float* %2258 to <4 x float>*
  store <4 x float> %wide.load75.24, <4 x float>* %2259, align 4, !tbaa !3167
  %2260 = getelementptr inbounds float, float* %2258, i64 4
  %2261 = bitcast float* %2260 to <4 x float>*
  store <4 x float> %wide.load76.24, <4 x float>* %2261, align 4, !tbaa !3167
  %2262 = add nsw i64 %21, 1992
  %2263 = getelementptr inbounds float, float* %7, i64 %2262
  %2264 = bitcast float* %2263 to <4 x float>*
  %wide.load75.25 = load <4 x float>, <4 x float>* %2264, align 4, !tbaa !3164
  %2265 = getelementptr inbounds float, float* %2263, i64 4
  %2266 = bitcast float* %2265 to <4 x float>*
  %wide.load76.25 = load <4 x float>, <4 x float>* %2266, align 4, !tbaa !3164
  %2267 = getelementptr inbounds float, float* %4, i64 %2262
  %2268 = bitcast float* %2267 to <4 x float>*
  store <4 x float> %wide.load75.25, <4 x float>* %2268, align 4, !tbaa !3167
  %2269 = getelementptr inbounds float, float* %2267, i64 4
  %2270 = bitcast float* %2269 to <4 x float>*
  store <4 x float> %wide.load76.25, <4 x float>* %2270, align 4, !tbaa !3167
  %2271 = add nsw i64 %21, 2000
  %2272 = getelementptr inbounds float, float* %7, i64 %2271
  %2273 = bitcast float* %2272 to <4 x float>*
  %wide.load75.26 = load <4 x float>, <4 x float>* %2273, align 4, !tbaa !3164
  %2274 = getelementptr inbounds float, float* %2272, i64 4
  %2275 = bitcast float* %2274 to <4 x float>*
  %wide.load76.26 = load <4 x float>, <4 x float>* %2275, align 4, !tbaa !3164
  %2276 = getelementptr inbounds float, float* %4, i64 %2271
  %2277 = bitcast float* %2276 to <4 x float>*
  store <4 x float> %wide.load75.26, <4 x float>* %2277, align 4, !tbaa !3167
  %2278 = getelementptr inbounds float, float* %2276, i64 4
  %2279 = bitcast float* %2278 to <4 x float>*
  store <4 x float> %wide.load76.26, <4 x float>* %2279, align 4, !tbaa !3167
  %2280 = add nsw i64 %21, 2008
  %2281 = getelementptr inbounds float, float* %7, i64 %2280
  %2282 = bitcast float* %2281 to <4 x float>*
  %wide.load75.27 = load <4 x float>, <4 x float>* %2282, align 4, !tbaa !3164
  %2283 = getelementptr inbounds float, float* %2281, i64 4
  %2284 = bitcast float* %2283 to <4 x float>*
  %wide.load76.27 = load <4 x float>, <4 x float>* %2284, align 4, !tbaa !3164
  %2285 = getelementptr inbounds float, float* %4, i64 %2280
  %2286 = bitcast float* %2285 to <4 x float>*
  store <4 x float> %wide.load75.27, <4 x float>* %2286, align 4, !tbaa !3167
  %2287 = getelementptr inbounds float, float* %2285, i64 4
  %2288 = bitcast float* %2287 to <4 x float>*
  store <4 x float> %wide.load76.27, <4 x float>* %2288, align 4, !tbaa !3167
  %2289 = add nsw i64 %21, 2016
  %2290 = getelementptr inbounds float, float* %7, i64 %2289
  %2291 = bitcast float* %2290 to <4 x float>*
  %wide.load75.28 = load <4 x float>, <4 x float>* %2291, align 4, !tbaa !3164
  %2292 = getelementptr inbounds float, float* %2290, i64 4
  %2293 = bitcast float* %2292 to <4 x float>*
  %wide.load76.28 = load <4 x float>, <4 x float>* %2293, align 4, !tbaa !3164
  %2294 = getelementptr inbounds float, float* %4, i64 %2289
  %2295 = bitcast float* %2294 to <4 x float>*
  store <4 x float> %wide.load75.28, <4 x float>* %2295, align 4, !tbaa !3167
  %2296 = getelementptr inbounds float, float* %2294, i64 4
  %2297 = bitcast float* %2296 to <4 x float>*
  store <4 x float> %wide.load76.28, <4 x float>* %2297, align 4, !tbaa !3167
  %2298 = add nsw i64 %21, 2024
  %2299 = getelementptr inbounds float, float* %7, i64 %2298
  %2300 = bitcast float* %2299 to <4 x float>*
  %wide.load75.29 = load <4 x float>, <4 x float>* %2300, align 4, !tbaa !3164
  %2301 = getelementptr inbounds float, float* %2299, i64 4
  %2302 = bitcast float* %2301 to <4 x float>*
  %wide.load76.29 = load <4 x float>, <4 x float>* %2302, align 4, !tbaa !3164
  %2303 = getelementptr inbounds float, float* %4, i64 %2298
  %2304 = bitcast float* %2303 to <4 x float>*
  store <4 x float> %wide.load75.29, <4 x float>* %2304, align 4, !tbaa !3167
  %2305 = getelementptr inbounds float, float* %2303, i64 4
  %2306 = bitcast float* %2305 to <4 x float>*
  store <4 x float> %wide.load76.29, <4 x float>* %2306, align 4, !tbaa !3167
  %2307 = add nsw i64 %21, 2032
  %2308 = getelementptr inbounds float, float* %7, i64 %2307
  %2309 = bitcast float* %2308 to <4 x float>*
  %wide.load75.30 = load <4 x float>, <4 x float>* %2309, align 4, !tbaa !3164
  %2310 = getelementptr inbounds float, float* %2308, i64 4
  %2311 = bitcast float* %2310 to <4 x float>*
  %wide.load76.30 = load <4 x float>, <4 x float>* %2311, align 4, !tbaa !3164
  %2312 = getelementptr inbounds float, float* %4, i64 %2307
  %2313 = bitcast float* %2312 to <4 x float>*
  store <4 x float> %wide.load75.30, <4 x float>* %2313, align 4, !tbaa !3167
  %2314 = getelementptr inbounds float, float* %2312, i64 4
  %2315 = bitcast float* %2314 to <4 x float>*
  store <4 x float> %wide.load76.30, <4 x float>* %2315, align 4, !tbaa !3167
  %2316 = add nsw i64 %21, 2040
  %2317 = getelementptr inbounds float, float* %7, i64 %2316
  %2318 = bitcast float* %2317 to <4 x float>*
  %wide.load75.31 = load <4 x float>, <4 x float>* %2318, align 4, !tbaa !3164
  %2319 = getelementptr inbounds float, float* %2317, i64 4
  %2320 = bitcast float* %2319 to <4 x float>*
  %wide.load76.31 = load <4 x float>, <4 x float>* %2320, align 4, !tbaa !3164
  %2321 = getelementptr inbounds float, float* %4, i64 %2316
  %2322 = bitcast float* %2321 to <4 x float>*
  store <4 x float> %wide.load75.31, <4 x float>* %2322, align 4, !tbaa !3167
  %2323 = getelementptr inbounds float, float* %2321, i64 4
  %2324 = bitcast float* %2323 to <4 x float>*
  store <4 x float> %wide.load76.31, <4 x float>* %2324, align 4, !tbaa !3167
  %2325 = add nsw i64 %21, 2048
  %2326 = getelementptr inbounds float, float* %7, i64 %2325
  %2327 = bitcast float* %2326 to <4 x float>*
  %wide.load65 = load <4 x float>, <4 x float>* %2327, align 4, !tbaa !3164
  %2328 = getelementptr inbounds float, float* %2326, i64 4
  %2329 = bitcast float* %2328 to <4 x float>*
  %wide.load66 = load <4 x float>, <4 x float>* %2329, align 4, !tbaa !3164
  %2330 = getelementptr inbounds float, float* %4, i64 %2325
  %2331 = bitcast float* %2330 to <4 x float>*
  store <4 x float> %wide.load65, <4 x float>* %2331, align 4, !tbaa !3167
  %2332 = getelementptr inbounds float, float* %2330, i64 4
  %2333 = bitcast float* %2332 to <4 x float>*
  store <4 x float> %wide.load66, <4 x float>* %2333, align 4, !tbaa !3167
  %2334 = add nsw i64 %21, 2056
  %2335 = getelementptr inbounds float, float* %7, i64 %2334
  %2336 = bitcast float* %2335 to <4 x float>*
  %wide.load65.1 = load <4 x float>, <4 x float>* %2336, align 4, !tbaa !3164
  %2337 = getelementptr inbounds float, float* %2335, i64 4
  %2338 = bitcast float* %2337 to <4 x float>*
  %wide.load66.1 = load <4 x float>, <4 x float>* %2338, align 4, !tbaa !3164
  %2339 = getelementptr inbounds float, float* %4, i64 %2334
  %2340 = bitcast float* %2339 to <4 x float>*
  store <4 x float> %wide.load65.1, <4 x float>* %2340, align 4, !tbaa !3167
  %2341 = getelementptr inbounds float, float* %2339, i64 4
  %2342 = bitcast float* %2341 to <4 x float>*
  store <4 x float> %wide.load66.1, <4 x float>* %2342, align 4, !tbaa !3167
  %2343 = add nsw i64 %21, 2064
  %2344 = getelementptr inbounds float, float* %7, i64 %2343
  %2345 = bitcast float* %2344 to <4 x float>*
  %wide.load65.2 = load <4 x float>, <4 x float>* %2345, align 4, !tbaa !3164
  %2346 = getelementptr inbounds float, float* %2344, i64 4
  %2347 = bitcast float* %2346 to <4 x float>*
  %wide.load66.2 = load <4 x float>, <4 x float>* %2347, align 4, !tbaa !3164
  %2348 = getelementptr inbounds float, float* %4, i64 %2343
  %2349 = bitcast float* %2348 to <4 x float>*
  store <4 x float> %wide.load65.2, <4 x float>* %2349, align 4, !tbaa !3167
  %2350 = getelementptr inbounds float, float* %2348, i64 4
  %2351 = bitcast float* %2350 to <4 x float>*
  store <4 x float> %wide.load66.2, <4 x float>* %2351, align 4, !tbaa !3167
  %2352 = add nsw i64 %21, 2072
  %2353 = getelementptr inbounds float, float* %7, i64 %2352
  %2354 = bitcast float* %2353 to <4 x float>*
  %wide.load65.3 = load <4 x float>, <4 x float>* %2354, align 4, !tbaa !3164
  %2355 = getelementptr inbounds float, float* %2353, i64 4
  %2356 = bitcast float* %2355 to <4 x float>*
  %wide.load66.3 = load <4 x float>, <4 x float>* %2356, align 4, !tbaa !3164
  %2357 = getelementptr inbounds float, float* %4, i64 %2352
  %2358 = bitcast float* %2357 to <4 x float>*
  store <4 x float> %wide.load65.3, <4 x float>* %2358, align 4, !tbaa !3167
  %2359 = getelementptr inbounds float, float* %2357, i64 4
  %2360 = bitcast float* %2359 to <4 x float>*
  store <4 x float> %wide.load66.3, <4 x float>* %2360, align 4, !tbaa !3167
  %2361 = add nsw i64 %21, 2080
  %2362 = getelementptr inbounds float, float* %7, i64 %2361
  %2363 = bitcast float* %2362 to <4 x float>*
  %wide.load65.4 = load <4 x float>, <4 x float>* %2363, align 4, !tbaa !3164
  %2364 = getelementptr inbounds float, float* %2362, i64 4
  %2365 = bitcast float* %2364 to <4 x float>*
  %wide.load66.4 = load <4 x float>, <4 x float>* %2365, align 4, !tbaa !3164
  %2366 = getelementptr inbounds float, float* %4, i64 %2361
  %2367 = bitcast float* %2366 to <4 x float>*
  store <4 x float> %wide.load65.4, <4 x float>* %2367, align 4, !tbaa !3167
  %2368 = getelementptr inbounds float, float* %2366, i64 4
  %2369 = bitcast float* %2368 to <4 x float>*
  store <4 x float> %wide.load66.4, <4 x float>* %2369, align 4, !tbaa !3167
  %2370 = add nsw i64 %21, 2088
  %2371 = getelementptr inbounds float, float* %7, i64 %2370
  %2372 = bitcast float* %2371 to <4 x float>*
  %wide.load65.5 = load <4 x float>, <4 x float>* %2372, align 4, !tbaa !3164
  %2373 = getelementptr inbounds float, float* %2371, i64 4
  %2374 = bitcast float* %2373 to <4 x float>*
  %wide.load66.5 = load <4 x float>, <4 x float>* %2374, align 4, !tbaa !3164
  %2375 = getelementptr inbounds float, float* %4, i64 %2370
  %2376 = bitcast float* %2375 to <4 x float>*
  store <4 x float> %wide.load65.5, <4 x float>* %2376, align 4, !tbaa !3167
  %2377 = getelementptr inbounds float, float* %2375, i64 4
  %2378 = bitcast float* %2377 to <4 x float>*
  store <4 x float> %wide.load66.5, <4 x float>* %2378, align 4, !tbaa !3167
  %2379 = add nsw i64 %21, 2096
  %2380 = getelementptr inbounds float, float* %7, i64 %2379
  %2381 = bitcast float* %2380 to <4 x float>*
  %wide.load65.6 = load <4 x float>, <4 x float>* %2381, align 4, !tbaa !3164
  %2382 = getelementptr inbounds float, float* %2380, i64 4
  %2383 = bitcast float* %2382 to <4 x float>*
  %wide.load66.6 = load <4 x float>, <4 x float>* %2383, align 4, !tbaa !3164
  %2384 = getelementptr inbounds float, float* %4, i64 %2379
  %2385 = bitcast float* %2384 to <4 x float>*
  store <4 x float> %wide.load65.6, <4 x float>* %2385, align 4, !tbaa !3167
  %2386 = getelementptr inbounds float, float* %2384, i64 4
  %2387 = bitcast float* %2386 to <4 x float>*
  store <4 x float> %wide.load66.6, <4 x float>* %2387, align 4, !tbaa !3167
  %2388 = add nsw i64 %21, 2104
  %2389 = getelementptr inbounds float, float* %7, i64 %2388
  %2390 = bitcast float* %2389 to <4 x float>*
  %wide.load65.7 = load <4 x float>, <4 x float>* %2390, align 4, !tbaa !3164
  %2391 = getelementptr inbounds float, float* %2389, i64 4
  %2392 = bitcast float* %2391 to <4 x float>*
  %wide.load66.7 = load <4 x float>, <4 x float>* %2392, align 4, !tbaa !3164
  %2393 = getelementptr inbounds float, float* %4, i64 %2388
  %2394 = bitcast float* %2393 to <4 x float>*
  store <4 x float> %wide.load65.7, <4 x float>* %2394, align 4, !tbaa !3167
  %2395 = getelementptr inbounds float, float* %2393, i64 4
  %2396 = bitcast float* %2395 to <4 x float>*
  store <4 x float> %wide.load66.7, <4 x float>* %2396, align 4, !tbaa !3167
  %2397 = add nsw i64 %21, 2112
  %2398 = getelementptr inbounds float, float* %7, i64 %2397
  %2399 = bitcast float* %2398 to <4 x float>*
  %wide.load65.8 = load <4 x float>, <4 x float>* %2399, align 4, !tbaa !3164
  %2400 = getelementptr inbounds float, float* %2398, i64 4
  %2401 = bitcast float* %2400 to <4 x float>*
  %wide.load66.8 = load <4 x float>, <4 x float>* %2401, align 4, !tbaa !3164
  %2402 = getelementptr inbounds float, float* %4, i64 %2397
  %2403 = bitcast float* %2402 to <4 x float>*
  store <4 x float> %wide.load65.8, <4 x float>* %2403, align 4, !tbaa !3167
  %2404 = getelementptr inbounds float, float* %2402, i64 4
  %2405 = bitcast float* %2404 to <4 x float>*
  store <4 x float> %wide.load66.8, <4 x float>* %2405, align 4, !tbaa !3167
  %2406 = add nsw i64 %21, 2120
  %2407 = getelementptr inbounds float, float* %7, i64 %2406
  %2408 = bitcast float* %2407 to <4 x float>*
  %wide.load65.9 = load <4 x float>, <4 x float>* %2408, align 4, !tbaa !3164
  %2409 = getelementptr inbounds float, float* %2407, i64 4
  %2410 = bitcast float* %2409 to <4 x float>*
  %wide.load66.9 = load <4 x float>, <4 x float>* %2410, align 4, !tbaa !3164
  %2411 = getelementptr inbounds float, float* %4, i64 %2406
  %2412 = bitcast float* %2411 to <4 x float>*
  store <4 x float> %wide.load65.9, <4 x float>* %2412, align 4, !tbaa !3167
  %2413 = getelementptr inbounds float, float* %2411, i64 4
  %2414 = bitcast float* %2413 to <4 x float>*
  store <4 x float> %wide.load66.9, <4 x float>* %2414, align 4, !tbaa !3167
  %2415 = add nsw i64 %21, 2128
  %2416 = getelementptr inbounds float, float* %7, i64 %2415
  %2417 = bitcast float* %2416 to <4 x float>*
  %wide.load65.10 = load <4 x float>, <4 x float>* %2417, align 4, !tbaa !3164
  %2418 = getelementptr inbounds float, float* %2416, i64 4
  %2419 = bitcast float* %2418 to <4 x float>*
  %wide.load66.10 = load <4 x float>, <4 x float>* %2419, align 4, !tbaa !3164
  %2420 = getelementptr inbounds float, float* %4, i64 %2415
  %2421 = bitcast float* %2420 to <4 x float>*
  store <4 x float> %wide.load65.10, <4 x float>* %2421, align 4, !tbaa !3167
  %2422 = getelementptr inbounds float, float* %2420, i64 4
  %2423 = bitcast float* %2422 to <4 x float>*
  store <4 x float> %wide.load66.10, <4 x float>* %2423, align 4, !tbaa !3167
  %2424 = add nsw i64 %21, 2136
  %2425 = getelementptr inbounds float, float* %7, i64 %2424
  %2426 = bitcast float* %2425 to <4 x float>*
  %wide.load65.11 = load <4 x float>, <4 x float>* %2426, align 4, !tbaa !3164
  %2427 = getelementptr inbounds float, float* %2425, i64 4
  %2428 = bitcast float* %2427 to <4 x float>*
  %wide.load66.11 = load <4 x float>, <4 x float>* %2428, align 4, !tbaa !3164
  %2429 = getelementptr inbounds float, float* %4, i64 %2424
  %2430 = bitcast float* %2429 to <4 x float>*
  store <4 x float> %wide.load65.11, <4 x float>* %2430, align 4, !tbaa !3167
  %2431 = getelementptr inbounds float, float* %2429, i64 4
  %2432 = bitcast float* %2431 to <4 x float>*
  store <4 x float> %wide.load66.11, <4 x float>* %2432, align 4, !tbaa !3167
  %2433 = add nsw i64 %21, 2144
  %2434 = getelementptr inbounds float, float* %7, i64 %2433
  %2435 = bitcast float* %2434 to <4 x float>*
  %wide.load65.12 = load <4 x float>, <4 x float>* %2435, align 4, !tbaa !3164
  %2436 = getelementptr inbounds float, float* %2434, i64 4
  %2437 = bitcast float* %2436 to <4 x float>*
  %wide.load66.12 = load <4 x float>, <4 x float>* %2437, align 4, !tbaa !3164
  %2438 = getelementptr inbounds float, float* %4, i64 %2433
  %2439 = bitcast float* %2438 to <4 x float>*
  store <4 x float> %wide.load65.12, <4 x float>* %2439, align 4, !tbaa !3167
  %2440 = getelementptr inbounds float, float* %2438, i64 4
  %2441 = bitcast float* %2440 to <4 x float>*
  store <4 x float> %wide.load66.12, <4 x float>* %2441, align 4, !tbaa !3167
  %2442 = add nsw i64 %21, 2152
  %2443 = getelementptr inbounds float, float* %7, i64 %2442
  %2444 = bitcast float* %2443 to <4 x float>*
  %wide.load65.13 = load <4 x float>, <4 x float>* %2444, align 4, !tbaa !3164
  %2445 = getelementptr inbounds float, float* %2443, i64 4
  %2446 = bitcast float* %2445 to <4 x float>*
  %wide.load66.13 = load <4 x float>, <4 x float>* %2446, align 4, !tbaa !3164
  %2447 = getelementptr inbounds float, float* %4, i64 %2442
  %2448 = bitcast float* %2447 to <4 x float>*
  store <4 x float> %wide.load65.13, <4 x float>* %2448, align 4, !tbaa !3167
  %2449 = getelementptr inbounds float, float* %2447, i64 4
  %2450 = bitcast float* %2449 to <4 x float>*
  store <4 x float> %wide.load66.13, <4 x float>* %2450, align 4, !tbaa !3167
  %2451 = add nsw i64 %21, 2160
  %2452 = getelementptr inbounds float, float* %7, i64 %2451
  %2453 = bitcast float* %2452 to <4 x float>*
  %wide.load65.14 = load <4 x float>, <4 x float>* %2453, align 4, !tbaa !3164
  %2454 = getelementptr inbounds float, float* %2452, i64 4
  %2455 = bitcast float* %2454 to <4 x float>*
  %wide.load66.14 = load <4 x float>, <4 x float>* %2455, align 4, !tbaa !3164
  %2456 = getelementptr inbounds float, float* %4, i64 %2451
  %2457 = bitcast float* %2456 to <4 x float>*
  store <4 x float> %wide.load65.14, <4 x float>* %2457, align 4, !tbaa !3167
  %2458 = getelementptr inbounds float, float* %2456, i64 4
  %2459 = bitcast float* %2458 to <4 x float>*
  store <4 x float> %wide.load66.14, <4 x float>* %2459, align 4, !tbaa !3167
  %2460 = add nsw i64 %21, 2168
  %2461 = getelementptr inbounds float, float* %7, i64 %2460
  %2462 = bitcast float* %2461 to <4 x float>*
  %wide.load65.15 = load <4 x float>, <4 x float>* %2462, align 4, !tbaa !3164
  %2463 = getelementptr inbounds float, float* %2461, i64 4
  %2464 = bitcast float* %2463 to <4 x float>*
  %wide.load66.15 = load <4 x float>, <4 x float>* %2464, align 4, !tbaa !3164
  %2465 = getelementptr inbounds float, float* %4, i64 %2460
  %2466 = bitcast float* %2465 to <4 x float>*
  store <4 x float> %wide.load65.15, <4 x float>* %2466, align 4, !tbaa !3167
  %2467 = getelementptr inbounds float, float* %2465, i64 4
  %2468 = bitcast float* %2467 to <4 x float>*
  store <4 x float> %wide.load66.15, <4 x float>* %2468, align 4, !tbaa !3167
  %2469 = add nsw i64 %21, 2176
  %2470 = getelementptr inbounds float, float* %7, i64 %2469
  %2471 = bitcast float* %2470 to <4 x float>*
  %wide.load65.16 = load <4 x float>, <4 x float>* %2471, align 4, !tbaa !3164
  %2472 = getelementptr inbounds float, float* %2470, i64 4
  %2473 = bitcast float* %2472 to <4 x float>*
  %wide.load66.16 = load <4 x float>, <4 x float>* %2473, align 4, !tbaa !3164
  %2474 = getelementptr inbounds float, float* %4, i64 %2469
  %2475 = bitcast float* %2474 to <4 x float>*
  store <4 x float> %wide.load65.16, <4 x float>* %2475, align 4, !tbaa !3167
  %2476 = getelementptr inbounds float, float* %2474, i64 4
  %2477 = bitcast float* %2476 to <4 x float>*
  store <4 x float> %wide.load66.16, <4 x float>* %2477, align 4, !tbaa !3167
  %2478 = add nsw i64 %21, 2184
  %2479 = getelementptr inbounds float, float* %7, i64 %2478
  %2480 = bitcast float* %2479 to <4 x float>*
  %wide.load65.17 = load <4 x float>, <4 x float>* %2480, align 4, !tbaa !3164
  %2481 = getelementptr inbounds float, float* %2479, i64 4
  %2482 = bitcast float* %2481 to <4 x float>*
  %wide.load66.17 = load <4 x float>, <4 x float>* %2482, align 4, !tbaa !3164
  %2483 = getelementptr inbounds float, float* %4, i64 %2478
  %2484 = bitcast float* %2483 to <4 x float>*
  store <4 x float> %wide.load65.17, <4 x float>* %2484, align 4, !tbaa !3167
  %2485 = getelementptr inbounds float, float* %2483, i64 4
  %2486 = bitcast float* %2485 to <4 x float>*
  store <4 x float> %wide.load66.17, <4 x float>* %2486, align 4, !tbaa !3167
  %2487 = add nsw i64 %21, 2192
  %2488 = getelementptr inbounds float, float* %7, i64 %2487
  %2489 = bitcast float* %2488 to <4 x float>*
  %wide.load65.18 = load <4 x float>, <4 x float>* %2489, align 4, !tbaa !3164
  %2490 = getelementptr inbounds float, float* %2488, i64 4
  %2491 = bitcast float* %2490 to <4 x float>*
  %wide.load66.18 = load <4 x float>, <4 x float>* %2491, align 4, !tbaa !3164
  %2492 = getelementptr inbounds float, float* %4, i64 %2487
  %2493 = bitcast float* %2492 to <4 x float>*
  store <4 x float> %wide.load65.18, <4 x float>* %2493, align 4, !tbaa !3167
  %2494 = getelementptr inbounds float, float* %2492, i64 4
  %2495 = bitcast float* %2494 to <4 x float>*
  store <4 x float> %wide.load66.18, <4 x float>* %2495, align 4, !tbaa !3167
  %2496 = add nsw i64 %21, 2200
  %2497 = getelementptr inbounds float, float* %7, i64 %2496
  %2498 = bitcast float* %2497 to <4 x float>*
  %wide.load65.19 = load <4 x float>, <4 x float>* %2498, align 4, !tbaa !3164
  %2499 = getelementptr inbounds float, float* %2497, i64 4
  %2500 = bitcast float* %2499 to <4 x float>*
  %wide.load66.19 = load <4 x float>, <4 x float>* %2500, align 4, !tbaa !3164
  %2501 = getelementptr inbounds float, float* %4, i64 %2496
  %2502 = bitcast float* %2501 to <4 x float>*
  store <4 x float> %wide.load65.19, <4 x float>* %2502, align 4, !tbaa !3167
  %2503 = getelementptr inbounds float, float* %2501, i64 4
  %2504 = bitcast float* %2503 to <4 x float>*
  store <4 x float> %wide.load66.19, <4 x float>* %2504, align 4, !tbaa !3167
  %2505 = add nsw i64 %21, 2208
  %2506 = getelementptr inbounds float, float* %7, i64 %2505
  %2507 = bitcast float* %2506 to <4 x float>*
  %wide.load65.20 = load <4 x float>, <4 x float>* %2507, align 4, !tbaa !3164
  %2508 = getelementptr inbounds float, float* %2506, i64 4
  %2509 = bitcast float* %2508 to <4 x float>*
  %wide.load66.20 = load <4 x float>, <4 x float>* %2509, align 4, !tbaa !3164
  %2510 = getelementptr inbounds float, float* %4, i64 %2505
  %2511 = bitcast float* %2510 to <4 x float>*
  store <4 x float> %wide.load65.20, <4 x float>* %2511, align 4, !tbaa !3167
  %2512 = getelementptr inbounds float, float* %2510, i64 4
  %2513 = bitcast float* %2512 to <4 x float>*
  store <4 x float> %wide.load66.20, <4 x float>* %2513, align 4, !tbaa !3167
  %2514 = add nsw i64 %21, 2216
  %2515 = getelementptr inbounds float, float* %7, i64 %2514
  %2516 = bitcast float* %2515 to <4 x float>*
  %wide.load65.21 = load <4 x float>, <4 x float>* %2516, align 4, !tbaa !3164
  %2517 = getelementptr inbounds float, float* %2515, i64 4
  %2518 = bitcast float* %2517 to <4 x float>*
  %wide.load66.21 = load <4 x float>, <4 x float>* %2518, align 4, !tbaa !3164
  %2519 = getelementptr inbounds float, float* %4, i64 %2514
  %2520 = bitcast float* %2519 to <4 x float>*
  store <4 x float> %wide.load65.21, <4 x float>* %2520, align 4, !tbaa !3167
  %2521 = getelementptr inbounds float, float* %2519, i64 4
  %2522 = bitcast float* %2521 to <4 x float>*
  store <4 x float> %wide.load66.21, <4 x float>* %2522, align 4, !tbaa !3167
  %2523 = add nsw i64 %21, 2224
  %2524 = getelementptr inbounds float, float* %7, i64 %2523
  %2525 = bitcast float* %2524 to <4 x float>*
  %wide.load65.22 = load <4 x float>, <4 x float>* %2525, align 4, !tbaa !3164
  %2526 = getelementptr inbounds float, float* %2524, i64 4
  %2527 = bitcast float* %2526 to <4 x float>*
  %wide.load66.22 = load <4 x float>, <4 x float>* %2527, align 4, !tbaa !3164
  %2528 = getelementptr inbounds float, float* %4, i64 %2523
  %2529 = bitcast float* %2528 to <4 x float>*
  store <4 x float> %wide.load65.22, <4 x float>* %2529, align 4, !tbaa !3167
  %2530 = getelementptr inbounds float, float* %2528, i64 4
  %2531 = bitcast float* %2530 to <4 x float>*
  store <4 x float> %wide.load66.22, <4 x float>* %2531, align 4, !tbaa !3167
  %2532 = add nsw i64 %21, 2232
  %2533 = getelementptr inbounds float, float* %7, i64 %2532
  %2534 = bitcast float* %2533 to <4 x float>*
  %wide.load65.23 = load <4 x float>, <4 x float>* %2534, align 4, !tbaa !3164
  %2535 = getelementptr inbounds float, float* %2533, i64 4
  %2536 = bitcast float* %2535 to <4 x float>*
  %wide.load66.23 = load <4 x float>, <4 x float>* %2536, align 4, !tbaa !3164
  %2537 = getelementptr inbounds float, float* %4, i64 %2532
  %2538 = bitcast float* %2537 to <4 x float>*
  store <4 x float> %wide.load65.23, <4 x float>* %2538, align 4, !tbaa !3167
  %2539 = getelementptr inbounds float, float* %2537, i64 4
  %2540 = bitcast float* %2539 to <4 x float>*
  store <4 x float> %wide.load66.23, <4 x float>* %2540, align 4, !tbaa !3167
  %2541 = add nsw i64 %21, 2240
  %2542 = getelementptr inbounds float, float* %7, i64 %2541
  %2543 = bitcast float* %2542 to <4 x float>*
  %wide.load65.24 = load <4 x float>, <4 x float>* %2543, align 4, !tbaa !3164
  %2544 = getelementptr inbounds float, float* %2542, i64 4
  %2545 = bitcast float* %2544 to <4 x float>*
  %wide.load66.24 = load <4 x float>, <4 x float>* %2545, align 4, !tbaa !3164
  %2546 = getelementptr inbounds float, float* %4, i64 %2541
  %2547 = bitcast float* %2546 to <4 x float>*
  store <4 x float> %wide.load65.24, <4 x float>* %2547, align 4, !tbaa !3167
  %2548 = getelementptr inbounds float, float* %2546, i64 4
  %2549 = bitcast float* %2548 to <4 x float>*
  store <4 x float> %wide.load66.24, <4 x float>* %2549, align 4, !tbaa !3167
  %2550 = add nsw i64 %21, 2248
  %2551 = getelementptr inbounds float, float* %7, i64 %2550
  %2552 = bitcast float* %2551 to <4 x float>*
  %wide.load65.25 = load <4 x float>, <4 x float>* %2552, align 4, !tbaa !3164
  %2553 = getelementptr inbounds float, float* %2551, i64 4
  %2554 = bitcast float* %2553 to <4 x float>*
  %wide.load66.25 = load <4 x float>, <4 x float>* %2554, align 4, !tbaa !3164
  %2555 = getelementptr inbounds float, float* %4, i64 %2550
  %2556 = bitcast float* %2555 to <4 x float>*
  store <4 x float> %wide.load65.25, <4 x float>* %2556, align 4, !tbaa !3167
  %2557 = getelementptr inbounds float, float* %2555, i64 4
  %2558 = bitcast float* %2557 to <4 x float>*
  store <4 x float> %wide.load66.25, <4 x float>* %2558, align 4, !tbaa !3167
  %2559 = add nsw i64 %21, 2256
  %2560 = getelementptr inbounds float, float* %7, i64 %2559
  %2561 = bitcast float* %2560 to <4 x float>*
  %wide.load65.26 = load <4 x float>, <4 x float>* %2561, align 4, !tbaa !3164
  %2562 = getelementptr inbounds float, float* %2560, i64 4
  %2563 = bitcast float* %2562 to <4 x float>*
  %wide.load66.26 = load <4 x float>, <4 x float>* %2563, align 4, !tbaa !3164
  %2564 = getelementptr inbounds float, float* %4, i64 %2559
  %2565 = bitcast float* %2564 to <4 x float>*
  store <4 x float> %wide.load65.26, <4 x float>* %2565, align 4, !tbaa !3167
  %2566 = getelementptr inbounds float, float* %2564, i64 4
  %2567 = bitcast float* %2566 to <4 x float>*
  store <4 x float> %wide.load66.26, <4 x float>* %2567, align 4, !tbaa !3167
  %2568 = add nsw i64 %21, 2264
  %2569 = getelementptr inbounds float, float* %7, i64 %2568
  %2570 = bitcast float* %2569 to <4 x float>*
  %wide.load65.27 = load <4 x float>, <4 x float>* %2570, align 4, !tbaa !3164
  %2571 = getelementptr inbounds float, float* %2569, i64 4
  %2572 = bitcast float* %2571 to <4 x float>*
  %wide.load66.27 = load <4 x float>, <4 x float>* %2572, align 4, !tbaa !3164
  %2573 = getelementptr inbounds float, float* %4, i64 %2568
  %2574 = bitcast float* %2573 to <4 x float>*
  store <4 x float> %wide.load65.27, <4 x float>* %2574, align 4, !tbaa !3167
  %2575 = getelementptr inbounds float, float* %2573, i64 4
  %2576 = bitcast float* %2575 to <4 x float>*
  store <4 x float> %wide.load66.27, <4 x float>* %2576, align 4, !tbaa !3167
  %2577 = add nsw i64 %21, 2272
  %2578 = getelementptr inbounds float, float* %7, i64 %2577
  %2579 = bitcast float* %2578 to <4 x float>*
  %wide.load65.28 = load <4 x float>, <4 x float>* %2579, align 4, !tbaa !3164
  %2580 = getelementptr inbounds float, float* %2578, i64 4
  %2581 = bitcast float* %2580 to <4 x float>*
  %wide.load66.28 = load <4 x float>, <4 x float>* %2581, align 4, !tbaa !3164
  %2582 = getelementptr inbounds float, float* %4, i64 %2577
  %2583 = bitcast float* %2582 to <4 x float>*
  store <4 x float> %wide.load65.28, <4 x float>* %2583, align 4, !tbaa !3167
  %2584 = getelementptr inbounds float, float* %2582, i64 4
  %2585 = bitcast float* %2584 to <4 x float>*
  store <4 x float> %wide.load66.28, <4 x float>* %2585, align 4, !tbaa !3167
  %2586 = add nsw i64 %21, 2280
  %2587 = getelementptr inbounds float, float* %7, i64 %2586
  %2588 = bitcast float* %2587 to <4 x float>*
  %wide.load65.29 = load <4 x float>, <4 x float>* %2588, align 4, !tbaa !3164
  %2589 = getelementptr inbounds float, float* %2587, i64 4
  %2590 = bitcast float* %2589 to <4 x float>*
  %wide.load66.29 = load <4 x float>, <4 x float>* %2590, align 4, !tbaa !3164
  %2591 = getelementptr inbounds float, float* %4, i64 %2586
  %2592 = bitcast float* %2591 to <4 x float>*
  store <4 x float> %wide.load65.29, <4 x float>* %2592, align 4, !tbaa !3167
  %2593 = getelementptr inbounds float, float* %2591, i64 4
  %2594 = bitcast float* %2593 to <4 x float>*
  store <4 x float> %wide.load66.29, <4 x float>* %2594, align 4, !tbaa !3167
  %2595 = add nsw i64 %21, 2288
  %2596 = getelementptr inbounds float, float* %7, i64 %2595
  %2597 = bitcast float* %2596 to <4 x float>*
  %wide.load65.30 = load <4 x float>, <4 x float>* %2597, align 4, !tbaa !3164
  %2598 = getelementptr inbounds float, float* %2596, i64 4
  %2599 = bitcast float* %2598 to <4 x float>*
  %wide.load66.30 = load <4 x float>, <4 x float>* %2599, align 4, !tbaa !3164
  %2600 = getelementptr inbounds float, float* %4, i64 %2595
  %2601 = bitcast float* %2600 to <4 x float>*
  store <4 x float> %wide.load65.30, <4 x float>* %2601, align 4, !tbaa !3167
  %2602 = getelementptr inbounds float, float* %2600, i64 4
  %2603 = bitcast float* %2602 to <4 x float>*
  store <4 x float> %wide.load66.30, <4 x float>* %2603, align 4, !tbaa !3167
  %2604 = add nsw i64 %21, 2296
  %2605 = getelementptr inbounds float, float* %7, i64 %2604
  %2606 = bitcast float* %2605 to <4 x float>*
  %wide.load65.31 = load <4 x float>, <4 x float>* %2606, align 4, !tbaa !3164
  %2607 = getelementptr inbounds float, float* %2605, i64 4
  %2608 = bitcast float* %2607 to <4 x float>*
  %wide.load66.31 = load <4 x float>, <4 x float>* %2608, align 4, !tbaa !3164
  %2609 = getelementptr inbounds float, float* %4, i64 %2604
  %2610 = bitcast float* %2609 to <4 x float>*
  store <4 x float> %wide.load65.31, <4 x float>* %2610, align 4, !tbaa !3167
  %2611 = getelementptr inbounds float, float* %2609, i64 4
  %2612 = bitcast float* %2611 to <4 x float>*
  store <4 x float> %wide.load66.31, <4 x float>* %2612, align 4, !tbaa !3167
  %2613 = add nsw i64 %21, 2304
  %2614 = getelementptr inbounds float, float* %7, i64 %2613
  %2615 = bitcast float* %2614 to <4 x float>*
  %wide.load55 = load <4 x float>, <4 x float>* %2615, align 4, !tbaa !3164
  %2616 = getelementptr inbounds float, float* %2614, i64 4
  %2617 = bitcast float* %2616 to <4 x float>*
  %wide.load56 = load <4 x float>, <4 x float>* %2617, align 4, !tbaa !3164
  %2618 = getelementptr inbounds float, float* %4, i64 %2613
  %2619 = bitcast float* %2618 to <4 x float>*
  store <4 x float> %wide.load55, <4 x float>* %2619, align 4, !tbaa !3167
  %2620 = getelementptr inbounds float, float* %2618, i64 4
  %2621 = bitcast float* %2620 to <4 x float>*
  store <4 x float> %wide.load56, <4 x float>* %2621, align 4, !tbaa !3167
  %2622 = add nsw i64 %21, 2312
  %2623 = getelementptr inbounds float, float* %7, i64 %2622
  %2624 = bitcast float* %2623 to <4 x float>*
  %wide.load55.1 = load <4 x float>, <4 x float>* %2624, align 4, !tbaa !3164
  %2625 = getelementptr inbounds float, float* %2623, i64 4
  %2626 = bitcast float* %2625 to <4 x float>*
  %wide.load56.1 = load <4 x float>, <4 x float>* %2626, align 4, !tbaa !3164
  %2627 = getelementptr inbounds float, float* %4, i64 %2622
  %2628 = bitcast float* %2627 to <4 x float>*
  store <4 x float> %wide.load55.1, <4 x float>* %2628, align 4, !tbaa !3167
  %2629 = getelementptr inbounds float, float* %2627, i64 4
  %2630 = bitcast float* %2629 to <4 x float>*
  store <4 x float> %wide.load56.1, <4 x float>* %2630, align 4, !tbaa !3167
  %2631 = add nsw i64 %21, 2320
  %2632 = getelementptr inbounds float, float* %7, i64 %2631
  %2633 = bitcast float* %2632 to <4 x float>*
  %wide.load55.2 = load <4 x float>, <4 x float>* %2633, align 4, !tbaa !3164
  %2634 = getelementptr inbounds float, float* %2632, i64 4
  %2635 = bitcast float* %2634 to <4 x float>*
  %wide.load56.2 = load <4 x float>, <4 x float>* %2635, align 4, !tbaa !3164
  %2636 = getelementptr inbounds float, float* %4, i64 %2631
  %2637 = bitcast float* %2636 to <4 x float>*
  store <4 x float> %wide.load55.2, <4 x float>* %2637, align 4, !tbaa !3167
  %2638 = getelementptr inbounds float, float* %2636, i64 4
  %2639 = bitcast float* %2638 to <4 x float>*
  store <4 x float> %wide.load56.2, <4 x float>* %2639, align 4, !tbaa !3167
  %2640 = add nsw i64 %21, 2328
  %2641 = getelementptr inbounds float, float* %7, i64 %2640
  %2642 = bitcast float* %2641 to <4 x float>*
  %wide.load55.3 = load <4 x float>, <4 x float>* %2642, align 4, !tbaa !3164
  %2643 = getelementptr inbounds float, float* %2641, i64 4
  %2644 = bitcast float* %2643 to <4 x float>*
  %wide.load56.3 = load <4 x float>, <4 x float>* %2644, align 4, !tbaa !3164
  %2645 = getelementptr inbounds float, float* %4, i64 %2640
  %2646 = bitcast float* %2645 to <4 x float>*
  store <4 x float> %wide.load55.3, <4 x float>* %2646, align 4, !tbaa !3167
  %2647 = getelementptr inbounds float, float* %2645, i64 4
  %2648 = bitcast float* %2647 to <4 x float>*
  store <4 x float> %wide.load56.3, <4 x float>* %2648, align 4, !tbaa !3167
  %2649 = add nsw i64 %21, 2336
  %2650 = getelementptr inbounds float, float* %7, i64 %2649
  %2651 = bitcast float* %2650 to <4 x float>*
  %wide.load55.4 = load <4 x float>, <4 x float>* %2651, align 4, !tbaa !3164
  %2652 = getelementptr inbounds float, float* %2650, i64 4
  %2653 = bitcast float* %2652 to <4 x float>*
  %wide.load56.4 = load <4 x float>, <4 x float>* %2653, align 4, !tbaa !3164
  %2654 = getelementptr inbounds float, float* %4, i64 %2649
  %2655 = bitcast float* %2654 to <4 x float>*
  store <4 x float> %wide.load55.4, <4 x float>* %2655, align 4, !tbaa !3167
  %2656 = getelementptr inbounds float, float* %2654, i64 4
  %2657 = bitcast float* %2656 to <4 x float>*
  store <4 x float> %wide.load56.4, <4 x float>* %2657, align 4, !tbaa !3167
  %2658 = add nsw i64 %21, 2344
  %2659 = getelementptr inbounds float, float* %7, i64 %2658
  %2660 = bitcast float* %2659 to <4 x float>*
  %wide.load55.5 = load <4 x float>, <4 x float>* %2660, align 4, !tbaa !3164
  %2661 = getelementptr inbounds float, float* %2659, i64 4
  %2662 = bitcast float* %2661 to <4 x float>*
  %wide.load56.5 = load <4 x float>, <4 x float>* %2662, align 4, !tbaa !3164
  %2663 = getelementptr inbounds float, float* %4, i64 %2658
  %2664 = bitcast float* %2663 to <4 x float>*
  store <4 x float> %wide.load55.5, <4 x float>* %2664, align 4, !tbaa !3167
  %2665 = getelementptr inbounds float, float* %2663, i64 4
  %2666 = bitcast float* %2665 to <4 x float>*
  store <4 x float> %wide.load56.5, <4 x float>* %2666, align 4, !tbaa !3167
  %2667 = add nsw i64 %21, 2352
  %2668 = getelementptr inbounds float, float* %7, i64 %2667
  %2669 = bitcast float* %2668 to <4 x float>*
  %wide.load55.6 = load <4 x float>, <4 x float>* %2669, align 4, !tbaa !3164
  %2670 = getelementptr inbounds float, float* %2668, i64 4
  %2671 = bitcast float* %2670 to <4 x float>*
  %wide.load56.6 = load <4 x float>, <4 x float>* %2671, align 4, !tbaa !3164
  %2672 = getelementptr inbounds float, float* %4, i64 %2667
  %2673 = bitcast float* %2672 to <4 x float>*
  store <4 x float> %wide.load55.6, <4 x float>* %2673, align 4, !tbaa !3167
  %2674 = getelementptr inbounds float, float* %2672, i64 4
  %2675 = bitcast float* %2674 to <4 x float>*
  store <4 x float> %wide.load56.6, <4 x float>* %2675, align 4, !tbaa !3167
  %2676 = add nsw i64 %21, 2360
  %2677 = getelementptr inbounds float, float* %7, i64 %2676
  %2678 = bitcast float* %2677 to <4 x float>*
  %wide.load55.7 = load <4 x float>, <4 x float>* %2678, align 4, !tbaa !3164
  %2679 = getelementptr inbounds float, float* %2677, i64 4
  %2680 = bitcast float* %2679 to <4 x float>*
  %wide.load56.7 = load <4 x float>, <4 x float>* %2680, align 4, !tbaa !3164
  %2681 = getelementptr inbounds float, float* %4, i64 %2676
  %2682 = bitcast float* %2681 to <4 x float>*
  store <4 x float> %wide.load55.7, <4 x float>* %2682, align 4, !tbaa !3167
  %2683 = getelementptr inbounds float, float* %2681, i64 4
  %2684 = bitcast float* %2683 to <4 x float>*
  store <4 x float> %wide.load56.7, <4 x float>* %2684, align 4, !tbaa !3167
  %2685 = add nsw i64 %21, 2368
  %2686 = getelementptr inbounds float, float* %7, i64 %2685
  %2687 = bitcast float* %2686 to <4 x float>*
  %wide.load55.8 = load <4 x float>, <4 x float>* %2687, align 4, !tbaa !3164
  %2688 = getelementptr inbounds float, float* %2686, i64 4
  %2689 = bitcast float* %2688 to <4 x float>*
  %wide.load56.8 = load <4 x float>, <4 x float>* %2689, align 4, !tbaa !3164
  %2690 = getelementptr inbounds float, float* %4, i64 %2685
  %2691 = bitcast float* %2690 to <4 x float>*
  store <4 x float> %wide.load55.8, <4 x float>* %2691, align 4, !tbaa !3167
  %2692 = getelementptr inbounds float, float* %2690, i64 4
  %2693 = bitcast float* %2692 to <4 x float>*
  store <4 x float> %wide.load56.8, <4 x float>* %2693, align 4, !tbaa !3167
  %2694 = add nsw i64 %21, 2376
  %2695 = getelementptr inbounds float, float* %7, i64 %2694
  %2696 = bitcast float* %2695 to <4 x float>*
  %wide.load55.9 = load <4 x float>, <4 x float>* %2696, align 4, !tbaa !3164
  %2697 = getelementptr inbounds float, float* %2695, i64 4
  %2698 = bitcast float* %2697 to <4 x float>*
  %wide.load56.9 = load <4 x float>, <4 x float>* %2698, align 4, !tbaa !3164
  %2699 = getelementptr inbounds float, float* %4, i64 %2694
  %2700 = bitcast float* %2699 to <4 x float>*
  store <4 x float> %wide.load55.9, <4 x float>* %2700, align 4, !tbaa !3167
  %2701 = getelementptr inbounds float, float* %2699, i64 4
  %2702 = bitcast float* %2701 to <4 x float>*
  store <4 x float> %wide.load56.9, <4 x float>* %2702, align 4, !tbaa !3167
  %2703 = add nsw i64 %21, 2384
  %2704 = getelementptr inbounds float, float* %7, i64 %2703
  %2705 = bitcast float* %2704 to <4 x float>*
  %wide.load55.10 = load <4 x float>, <4 x float>* %2705, align 4, !tbaa !3164
  %2706 = getelementptr inbounds float, float* %2704, i64 4
  %2707 = bitcast float* %2706 to <4 x float>*
  %wide.load56.10 = load <4 x float>, <4 x float>* %2707, align 4, !tbaa !3164
  %2708 = getelementptr inbounds float, float* %4, i64 %2703
  %2709 = bitcast float* %2708 to <4 x float>*
  store <4 x float> %wide.load55.10, <4 x float>* %2709, align 4, !tbaa !3167
  %2710 = getelementptr inbounds float, float* %2708, i64 4
  %2711 = bitcast float* %2710 to <4 x float>*
  store <4 x float> %wide.load56.10, <4 x float>* %2711, align 4, !tbaa !3167
  %2712 = add nsw i64 %21, 2392
  %2713 = getelementptr inbounds float, float* %7, i64 %2712
  %2714 = bitcast float* %2713 to <4 x float>*
  %wide.load55.11 = load <4 x float>, <4 x float>* %2714, align 4, !tbaa !3164
  %2715 = getelementptr inbounds float, float* %2713, i64 4
  %2716 = bitcast float* %2715 to <4 x float>*
  %wide.load56.11 = load <4 x float>, <4 x float>* %2716, align 4, !tbaa !3164
  %2717 = getelementptr inbounds float, float* %4, i64 %2712
  %2718 = bitcast float* %2717 to <4 x float>*
  store <4 x float> %wide.load55.11, <4 x float>* %2718, align 4, !tbaa !3167
  %2719 = getelementptr inbounds float, float* %2717, i64 4
  %2720 = bitcast float* %2719 to <4 x float>*
  store <4 x float> %wide.load56.11, <4 x float>* %2720, align 4, !tbaa !3167
  %2721 = add nsw i64 %21, 2400
  %2722 = getelementptr inbounds float, float* %7, i64 %2721
  %2723 = bitcast float* %2722 to <4 x float>*
  %wide.load55.12 = load <4 x float>, <4 x float>* %2723, align 4, !tbaa !3164
  %2724 = getelementptr inbounds float, float* %2722, i64 4
  %2725 = bitcast float* %2724 to <4 x float>*
  %wide.load56.12 = load <4 x float>, <4 x float>* %2725, align 4, !tbaa !3164
  %2726 = getelementptr inbounds float, float* %4, i64 %2721
  %2727 = bitcast float* %2726 to <4 x float>*
  store <4 x float> %wide.load55.12, <4 x float>* %2727, align 4, !tbaa !3167
  %2728 = getelementptr inbounds float, float* %2726, i64 4
  %2729 = bitcast float* %2728 to <4 x float>*
  store <4 x float> %wide.load56.12, <4 x float>* %2729, align 4, !tbaa !3167
  %2730 = add nsw i64 %21, 2408
  %2731 = getelementptr inbounds float, float* %7, i64 %2730
  %2732 = bitcast float* %2731 to <4 x float>*
  %wide.load55.13 = load <4 x float>, <4 x float>* %2732, align 4, !tbaa !3164
  %2733 = getelementptr inbounds float, float* %2731, i64 4
  %2734 = bitcast float* %2733 to <4 x float>*
  %wide.load56.13 = load <4 x float>, <4 x float>* %2734, align 4, !tbaa !3164
  %2735 = getelementptr inbounds float, float* %4, i64 %2730
  %2736 = bitcast float* %2735 to <4 x float>*
  store <4 x float> %wide.load55.13, <4 x float>* %2736, align 4, !tbaa !3167
  %2737 = getelementptr inbounds float, float* %2735, i64 4
  %2738 = bitcast float* %2737 to <4 x float>*
  store <4 x float> %wide.load56.13, <4 x float>* %2738, align 4, !tbaa !3167
  %2739 = add nsw i64 %21, 2416
  %2740 = getelementptr inbounds float, float* %7, i64 %2739
  %2741 = bitcast float* %2740 to <4 x float>*
  %wide.load55.14 = load <4 x float>, <4 x float>* %2741, align 4, !tbaa !3164
  %2742 = getelementptr inbounds float, float* %2740, i64 4
  %2743 = bitcast float* %2742 to <4 x float>*
  %wide.load56.14 = load <4 x float>, <4 x float>* %2743, align 4, !tbaa !3164
  %2744 = getelementptr inbounds float, float* %4, i64 %2739
  %2745 = bitcast float* %2744 to <4 x float>*
  store <4 x float> %wide.load55.14, <4 x float>* %2745, align 4, !tbaa !3167
  %2746 = getelementptr inbounds float, float* %2744, i64 4
  %2747 = bitcast float* %2746 to <4 x float>*
  store <4 x float> %wide.load56.14, <4 x float>* %2747, align 4, !tbaa !3167
  %2748 = add nsw i64 %21, 2424
  %2749 = getelementptr inbounds float, float* %7, i64 %2748
  %2750 = bitcast float* %2749 to <4 x float>*
  %wide.load55.15 = load <4 x float>, <4 x float>* %2750, align 4, !tbaa !3164
  %2751 = getelementptr inbounds float, float* %2749, i64 4
  %2752 = bitcast float* %2751 to <4 x float>*
  %wide.load56.15 = load <4 x float>, <4 x float>* %2752, align 4, !tbaa !3164
  %2753 = getelementptr inbounds float, float* %4, i64 %2748
  %2754 = bitcast float* %2753 to <4 x float>*
  store <4 x float> %wide.load55.15, <4 x float>* %2754, align 4, !tbaa !3167
  %2755 = getelementptr inbounds float, float* %2753, i64 4
  %2756 = bitcast float* %2755 to <4 x float>*
  store <4 x float> %wide.load56.15, <4 x float>* %2756, align 4, !tbaa !3167
  %2757 = add nsw i64 %21, 2432
  %2758 = getelementptr inbounds float, float* %7, i64 %2757
  %2759 = bitcast float* %2758 to <4 x float>*
  %wide.load55.16 = load <4 x float>, <4 x float>* %2759, align 4, !tbaa !3164
  %2760 = getelementptr inbounds float, float* %2758, i64 4
  %2761 = bitcast float* %2760 to <4 x float>*
  %wide.load56.16 = load <4 x float>, <4 x float>* %2761, align 4, !tbaa !3164
  %2762 = getelementptr inbounds float, float* %4, i64 %2757
  %2763 = bitcast float* %2762 to <4 x float>*
  store <4 x float> %wide.load55.16, <4 x float>* %2763, align 4, !tbaa !3167
  %2764 = getelementptr inbounds float, float* %2762, i64 4
  %2765 = bitcast float* %2764 to <4 x float>*
  store <4 x float> %wide.load56.16, <4 x float>* %2765, align 4, !tbaa !3167
  %2766 = add nsw i64 %21, 2440
  %2767 = getelementptr inbounds float, float* %7, i64 %2766
  %2768 = bitcast float* %2767 to <4 x float>*
  %wide.load55.17 = load <4 x float>, <4 x float>* %2768, align 4, !tbaa !3164
  %2769 = getelementptr inbounds float, float* %2767, i64 4
  %2770 = bitcast float* %2769 to <4 x float>*
  %wide.load56.17 = load <4 x float>, <4 x float>* %2770, align 4, !tbaa !3164
  %2771 = getelementptr inbounds float, float* %4, i64 %2766
  %2772 = bitcast float* %2771 to <4 x float>*
  store <4 x float> %wide.load55.17, <4 x float>* %2772, align 4, !tbaa !3167
  %2773 = getelementptr inbounds float, float* %2771, i64 4
  %2774 = bitcast float* %2773 to <4 x float>*
  store <4 x float> %wide.load56.17, <4 x float>* %2774, align 4, !tbaa !3167
  %2775 = add nsw i64 %21, 2448
  %2776 = getelementptr inbounds float, float* %7, i64 %2775
  %2777 = bitcast float* %2776 to <4 x float>*
  %wide.load55.18 = load <4 x float>, <4 x float>* %2777, align 4, !tbaa !3164
  %2778 = getelementptr inbounds float, float* %2776, i64 4
  %2779 = bitcast float* %2778 to <4 x float>*
  %wide.load56.18 = load <4 x float>, <4 x float>* %2779, align 4, !tbaa !3164
  %2780 = getelementptr inbounds float, float* %4, i64 %2775
  %2781 = bitcast float* %2780 to <4 x float>*
  store <4 x float> %wide.load55.18, <4 x float>* %2781, align 4, !tbaa !3167
  %2782 = getelementptr inbounds float, float* %2780, i64 4
  %2783 = bitcast float* %2782 to <4 x float>*
  store <4 x float> %wide.load56.18, <4 x float>* %2783, align 4, !tbaa !3167
  %2784 = add nsw i64 %21, 2456
  %2785 = getelementptr inbounds float, float* %7, i64 %2784
  %2786 = bitcast float* %2785 to <4 x float>*
  %wide.load55.19 = load <4 x float>, <4 x float>* %2786, align 4, !tbaa !3164
  %2787 = getelementptr inbounds float, float* %2785, i64 4
  %2788 = bitcast float* %2787 to <4 x float>*
  %wide.load56.19 = load <4 x float>, <4 x float>* %2788, align 4, !tbaa !3164
  %2789 = getelementptr inbounds float, float* %4, i64 %2784
  %2790 = bitcast float* %2789 to <4 x float>*
  store <4 x float> %wide.load55.19, <4 x float>* %2790, align 4, !tbaa !3167
  %2791 = getelementptr inbounds float, float* %2789, i64 4
  %2792 = bitcast float* %2791 to <4 x float>*
  store <4 x float> %wide.load56.19, <4 x float>* %2792, align 4, !tbaa !3167
  %2793 = add nsw i64 %21, 2464
  %2794 = getelementptr inbounds float, float* %7, i64 %2793
  %2795 = bitcast float* %2794 to <4 x float>*
  %wide.load55.20 = load <4 x float>, <4 x float>* %2795, align 4, !tbaa !3164
  %2796 = getelementptr inbounds float, float* %2794, i64 4
  %2797 = bitcast float* %2796 to <4 x float>*
  %wide.load56.20 = load <4 x float>, <4 x float>* %2797, align 4, !tbaa !3164
  %2798 = getelementptr inbounds float, float* %4, i64 %2793
  %2799 = bitcast float* %2798 to <4 x float>*
  store <4 x float> %wide.load55.20, <4 x float>* %2799, align 4, !tbaa !3167
  %2800 = getelementptr inbounds float, float* %2798, i64 4
  %2801 = bitcast float* %2800 to <4 x float>*
  store <4 x float> %wide.load56.20, <4 x float>* %2801, align 4, !tbaa !3167
  %2802 = add nsw i64 %21, 2472
  %2803 = getelementptr inbounds float, float* %7, i64 %2802
  %2804 = bitcast float* %2803 to <4 x float>*
  %wide.load55.21 = load <4 x float>, <4 x float>* %2804, align 4, !tbaa !3164
  %2805 = getelementptr inbounds float, float* %2803, i64 4
  %2806 = bitcast float* %2805 to <4 x float>*
  %wide.load56.21 = load <4 x float>, <4 x float>* %2806, align 4, !tbaa !3164
  %2807 = getelementptr inbounds float, float* %4, i64 %2802
  %2808 = bitcast float* %2807 to <4 x float>*
  store <4 x float> %wide.load55.21, <4 x float>* %2808, align 4, !tbaa !3167
  %2809 = getelementptr inbounds float, float* %2807, i64 4
  %2810 = bitcast float* %2809 to <4 x float>*
  store <4 x float> %wide.load56.21, <4 x float>* %2810, align 4, !tbaa !3167
  %2811 = add nsw i64 %21, 2480
  %2812 = getelementptr inbounds float, float* %7, i64 %2811
  %2813 = bitcast float* %2812 to <4 x float>*
  %wide.load55.22 = load <4 x float>, <4 x float>* %2813, align 4, !tbaa !3164
  %2814 = getelementptr inbounds float, float* %2812, i64 4
  %2815 = bitcast float* %2814 to <4 x float>*
  %wide.load56.22 = load <4 x float>, <4 x float>* %2815, align 4, !tbaa !3164
  %2816 = getelementptr inbounds float, float* %4, i64 %2811
  %2817 = bitcast float* %2816 to <4 x float>*
  store <4 x float> %wide.load55.22, <4 x float>* %2817, align 4, !tbaa !3167
  %2818 = getelementptr inbounds float, float* %2816, i64 4
  %2819 = bitcast float* %2818 to <4 x float>*
  store <4 x float> %wide.load56.22, <4 x float>* %2819, align 4, !tbaa !3167
  %2820 = add nsw i64 %21, 2488
  %2821 = getelementptr inbounds float, float* %7, i64 %2820
  %2822 = bitcast float* %2821 to <4 x float>*
  %wide.load55.23 = load <4 x float>, <4 x float>* %2822, align 4, !tbaa !3164
  %2823 = getelementptr inbounds float, float* %2821, i64 4
  %2824 = bitcast float* %2823 to <4 x float>*
  %wide.load56.23 = load <4 x float>, <4 x float>* %2824, align 4, !tbaa !3164
  %2825 = getelementptr inbounds float, float* %4, i64 %2820
  %2826 = bitcast float* %2825 to <4 x float>*
  store <4 x float> %wide.load55.23, <4 x float>* %2826, align 4, !tbaa !3167
  %2827 = getelementptr inbounds float, float* %2825, i64 4
  %2828 = bitcast float* %2827 to <4 x float>*
  store <4 x float> %wide.load56.23, <4 x float>* %2828, align 4, !tbaa !3167
  %2829 = add nsw i64 %21, 2496
  %2830 = getelementptr inbounds float, float* %7, i64 %2829
  %2831 = bitcast float* %2830 to <4 x float>*
  %wide.load55.24 = load <4 x float>, <4 x float>* %2831, align 4, !tbaa !3164
  %2832 = getelementptr inbounds float, float* %2830, i64 4
  %2833 = bitcast float* %2832 to <4 x float>*
  %wide.load56.24 = load <4 x float>, <4 x float>* %2833, align 4, !tbaa !3164
  %2834 = getelementptr inbounds float, float* %4, i64 %2829
  %2835 = bitcast float* %2834 to <4 x float>*
  store <4 x float> %wide.load55.24, <4 x float>* %2835, align 4, !tbaa !3167
  %2836 = getelementptr inbounds float, float* %2834, i64 4
  %2837 = bitcast float* %2836 to <4 x float>*
  store <4 x float> %wide.load56.24, <4 x float>* %2837, align 4, !tbaa !3167
  %2838 = add nsw i64 %21, 2504
  %2839 = getelementptr inbounds float, float* %7, i64 %2838
  %2840 = bitcast float* %2839 to <4 x float>*
  %wide.load55.25 = load <4 x float>, <4 x float>* %2840, align 4, !tbaa !3164
  %2841 = getelementptr inbounds float, float* %2839, i64 4
  %2842 = bitcast float* %2841 to <4 x float>*
  %wide.load56.25 = load <4 x float>, <4 x float>* %2842, align 4, !tbaa !3164
  %2843 = getelementptr inbounds float, float* %4, i64 %2838
  %2844 = bitcast float* %2843 to <4 x float>*
  store <4 x float> %wide.load55.25, <4 x float>* %2844, align 4, !tbaa !3167
  %2845 = getelementptr inbounds float, float* %2843, i64 4
  %2846 = bitcast float* %2845 to <4 x float>*
  store <4 x float> %wide.load56.25, <4 x float>* %2846, align 4, !tbaa !3167
  %2847 = add nsw i64 %21, 2512
  %2848 = getelementptr inbounds float, float* %7, i64 %2847
  %2849 = bitcast float* %2848 to <4 x float>*
  %wide.load55.26 = load <4 x float>, <4 x float>* %2849, align 4, !tbaa !3164
  %2850 = getelementptr inbounds float, float* %2848, i64 4
  %2851 = bitcast float* %2850 to <4 x float>*
  %wide.load56.26 = load <4 x float>, <4 x float>* %2851, align 4, !tbaa !3164
  %2852 = getelementptr inbounds float, float* %4, i64 %2847
  %2853 = bitcast float* %2852 to <4 x float>*
  store <4 x float> %wide.load55.26, <4 x float>* %2853, align 4, !tbaa !3167
  %2854 = getelementptr inbounds float, float* %2852, i64 4
  %2855 = bitcast float* %2854 to <4 x float>*
  store <4 x float> %wide.load56.26, <4 x float>* %2855, align 4, !tbaa !3167
  %2856 = add nsw i64 %21, 2520
  %2857 = getelementptr inbounds float, float* %7, i64 %2856
  %2858 = bitcast float* %2857 to <4 x float>*
  %wide.load55.27 = load <4 x float>, <4 x float>* %2858, align 4, !tbaa !3164
  %2859 = getelementptr inbounds float, float* %2857, i64 4
  %2860 = bitcast float* %2859 to <4 x float>*
  %wide.load56.27 = load <4 x float>, <4 x float>* %2860, align 4, !tbaa !3164
  %2861 = getelementptr inbounds float, float* %4, i64 %2856
  %2862 = bitcast float* %2861 to <4 x float>*
  store <4 x float> %wide.load55.27, <4 x float>* %2862, align 4, !tbaa !3167
  %2863 = getelementptr inbounds float, float* %2861, i64 4
  %2864 = bitcast float* %2863 to <4 x float>*
  store <4 x float> %wide.load56.27, <4 x float>* %2864, align 4, !tbaa !3167
  %2865 = add nsw i64 %21, 2528
  %2866 = getelementptr inbounds float, float* %7, i64 %2865
  %2867 = bitcast float* %2866 to <4 x float>*
  %wide.load55.28 = load <4 x float>, <4 x float>* %2867, align 4, !tbaa !3164
  %2868 = getelementptr inbounds float, float* %2866, i64 4
  %2869 = bitcast float* %2868 to <4 x float>*
  %wide.load56.28 = load <4 x float>, <4 x float>* %2869, align 4, !tbaa !3164
  %2870 = getelementptr inbounds float, float* %4, i64 %2865
  %2871 = bitcast float* %2870 to <4 x float>*
  store <4 x float> %wide.load55.28, <4 x float>* %2871, align 4, !tbaa !3167
  %2872 = getelementptr inbounds float, float* %2870, i64 4
  %2873 = bitcast float* %2872 to <4 x float>*
  store <4 x float> %wide.load56.28, <4 x float>* %2873, align 4, !tbaa !3167
  %2874 = add nsw i64 %21, 2536
  %2875 = getelementptr inbounds float, float* %7, i64 %2874
  %2876 = bitcast float* %2875 to <4 x float>*
  %wide.load55.29 = load <4 x float>, <4 x float>* %2876, align 4, !tbaa !3164
  %2877 = getelementptr inbounds float, float* %2875, i64 4
  %2878 = bitcast float* %2877 to <4 x float>*
  %wide.load56.29 = load <4 x float>, <4 x float>* %2878, align 4, !tbaa !3164
  %2879 = getelementptr inbounds float, float* %4, i64 %2874
  %2880 = bitcast float* %2879 to <4 x float>*
  store <4 x float> %wide.load55.29, <4 x float>* %2880, align 4, !tbaa !3167
  %2881 = getelementptr inbounds float, float* %2879, i64 4
  %2882 = bitcast float* %2881 to <4 x float>*
  store <4 x float> %wide.load56.29, <4 x float>* %2882, align 4, !tbaa !3167
  %2883 = add nsw i64 %21, 2544
  %2884 = getelementptr inbounds float, float* %7, i64 %2883
  %2885 = bitcast float* %2884 to <4 x float>*
  %wide.load55.30 = load <4 x float>, <4 x float>* %2885, align 4, !tbaa !3164
  %2886 = getelementptr inbounds float, float* %2884, i64 4
  %2887 = bitcast float* %2886 to <4 x float>*
  %wide.load56.30 = load <4 x float>, <4 x float>* %2887, align 4, !tbaa !3164
  %2888 = getelementptr inbounds float, float* %4, i64 %2883
  %2889 = bitcast float* %2888 to <4 x float>*
  store <4 x float> %wide.load55.30, <4 x float>* %2889, align 4, !tbaa !3167
  %2890 = getelementptr inbounds float, float* %2888, i64 4
  %2891 = bitcast float* %2890 to <4 x float>*
  store <4 x float> %wide.load56.30, <4 x float>* %2891, align 4, !tbaa !3167
  %2892 = add nsw i64 %21, 2552
  %2893 = getelementptr inbounds float, float* %7, i64 %2892
  %2894 = bitcast float* %2893 to <4 x float>*
  %wide.load55.31 = load <4 x float>, <4 x float>* %2894, align 4, !tbaa !3164
  %2895 = getelementptr inbounds float, float* %2893, i64 4
  %2896 = bitcast float* %2895 to <4 x float>*
  %wide.load56.31 = load <4 x float>, <4 x float>* %2896, align 4, !tbaa !3164
  %2897 = getelementptr inbounds float, float* %4, i64 %2892
  %2898 = bitcast float* %2897 to <4 x float>*
  store <4 x float> %wide.load55.31, <4 x float>* %2898, align 4, !tbaa !3167
  %2899 = getelementptr inbounds float, float* %2897, i64 4
  %2900 = bitcast float* %2899 to <4 x float>*
  store <4 x float> %wide.load56.31, <4 x float>* %2900, align 4, !tbaa !3167
  %2901 = add nsw i64 %21, 2560
  %2902 = getelementptr inbounds float, float* %7, i64 %2901
  %2903 = bitcast float* %2902 to <4 x float>*
  %wide.load45 = load <4 x float>, <4 x float>* %2903, align 4, !tbaa !3164
  %2904 = getelementptr inbounds float, float* %2902, i64 4
  %2905 = bitcast float* %2904 to <4 x float>*
  %wide.load46 = load <4 x float>, <4 x float>* %2905, align 4, !tbaa !3164
  %2906 = getelementptr inbounds float, float* %4, i64 %2901
  %2907 = bitcast float* %2906 to <4 x float>*
  store <4 x float> %wide.load45, <4 x float>* %2907, align 4, !tbaa !3167
  %2908 = getelementptr inbounds float, float* %2906, i64 4
  %2909 = bitcast float* %2908 to <4 x float>*
  store <4 x float> %wide.load46, <4 x float>* %2909, align 4, !tbaa !3167
  %2910 = add nsw i64 %21, 2568
  %2911 = getelementptr inbounds float, float* %7, i64 %2910
  %2912 = bitcast float* %2911 to <4 x float>*
  %wide.load45.1 = load <4 x float>, <4 x float>* %2912, align 4, !tbaa !3164
  %2913 = getelementptr inbounds float, float* %2911, i64 4
  %2914 = bitcast float* %2913 to <4 x float>*
  %wide.load46.1 = load <4 x float>, <4 x float>* %2914, align 4, !tbaa !3164
  %2915 = getelementptr inbounds float, float* %4, i64 %2910
  %2916 = bitcast float* %2915 to <4 x float>*
  store <4 x float> %wide.load45.1, <4 x float>* %2916, align 4, !tbaa !3167
  %2917 = getelementptr inbounds float, float* %2915, i64 4
  %2918 = bitcast float* %2917 to <4 x float>*
  store <4 x float> %wide.load46.1, <4 x float>* %2918, align 4, !tbaa !3167
  %2919 = add nsw i64 %21, 2576
  %2920 = getelementptr inbounds float, float* %7, i64 %2919
  %2921 = bitcast float* %2920 to <4 x float>*
  %wide.load45.2 = load <4 x float>, <4 x float>* %2921, align 4, !tbaa !3164
  %2922 = getelementptr inbounds float, float* %2920, i64 4
  %2923 = bitcast float* %2922 to <4 x float>*
  %wide.load46.2 = load <4 x float>, <4 x float>* %2923, align 4, !tbaa !3164
  %2924 = getelementptr inbounds float, float* %4, i64 %2919
  %2925 = bitcast float* %2924 to <4 x float>*
  store <4 x float> %wide.load45.2, <4 x float>* %2925, align 4, !tbaa !3167
  %2926 = getelementptr inbounds float, float* %2924, i64 4
  %2927 = bitcast float* %2926 to <4 x float>*
  store <4 x float> %wide.load46.2, <4 x float>* %2927, align 4, !tbaa !3167
  %2928 = add nsw i64 %21, 2584
  %2929 = getelementptr inbounds float, float* %7, i64 %2928
  %2930 = bitcast float* %2929 to <4 x float>*
  %wide.load45.3 = load <4 x float>, <4 x float>* %2930, align 4, !tbaa !3164
  %2931 = getelementptr inbounds float, float* %2929, i64 4
  %2932 = bitcast float* %2931 to <4 x float>*
  %wide.load46.3 = load <4 x float>, <4 x float>* %2932, align 4, !tbaa !3164
  %2933 = getelementptr inbounds float, float* %4, i64 %2928
  %2934 = bitcast float* %2933 to <4 x float>*
  store <4 x float> %wide.load45.3, <4 x float>* %2934, align 4, !tbaa !3167
  %2935 = getelementptr inbounds float, float* %2933, i64 4
  %2936 = bitcast float* %2935 to <4 x float>*
  store <4 x float> %wide.load46.3, <4 x float>* %2936, align 4, !tbaa !3167
  %2937 = add nsw i64 %21, 2592
  %2938 = getelementptr inbounds float, float* %7, i64 %2937
  %2939 = bitcast float* %2938 to <4 x float>*
  %wide.load45.4 = load <4 x float>, <4 x float>* %2939, align 4, !tbaa !3164
  %2940 = getelementptr inbounds float, float* %2938, i64 4
  %2941 = bitcast float* %2940 to <4 x float>*
  %wide.load46.4 = load <4 x float>, <4 x float>* %2941, align 4, !tbaa !3164
  %2942 = getelementptr inbounds float, float* %4, i64 %2937
  %2943 = bitcast float* %2942 to <4 x float>*
  store <4 x float> %wide.load45.4, <4 x float>* %2943, align 4, !tbaa !3167
  %2944 = getelementptr inbounds float, float* %2942, i64 4
  %2945 = bitcast float* %2944 to <4 x float>*
  store <4 x float> %wide.load46.4, <4 x float>* %2945, align 4, !tbaa !3167
  %2946 = add nsw i64 %21, 2600
  %2947 = getelementptr inbounds float, float* %7, i64 %2946
  %2948 = bitcast float* %2947 to <4 x float>*
  %wide.load45.5 = load <4 x float>, <4 x float>* %2948, align 4, !tbaa !3164
  %2949 = getelementptr inbounds float, float* %2947, i64 4
  %2950 = bitcast float* %2949 to <4 x float>*
  %wide.load46.5 = load <4 x float>, <4 x float>* %2950, align 4, !tbaa !3164
  %2951 = getelementptr inbounds float, float* %4, i64 %2946
  %2952 = bitcast float* %2951 to <4 x float>*
  store <4 x float> %wide.load45.5, <4 x float>* %2952, align 4, !tbaa !3167
  %2953 = getelementptr inbounds float, float* %2951, i64 4
  %2954 = bitcast float* %2953 to <4 x float>*
  store <4 x float> %wide.load46.5, <4 x float>* %2954, align 4, !tbaa !3167
  %2955 = add nsw i64 %21, 2608
  %2956 = getelementptr inbounds float, float* %7, i64 %2955
  %2957 = bitcast float* %2956 to <4 x float>*
  %wide.load45.6 = load <4 x float>, <4 x float>* %2957, align 4, !tbaa !3164
  %2958 = getelementptr inbounds float, float* %2956, i64 4
  %2959 = bitcast float* %2958 to <4 x float>*
  %wide.load46.6 = load <4 x float>, <4 x float>* %2959, align 4, !tbaa !3164
  %2960 = getelementptr inbounds float, float* %4, i64 %2955
  %2961 = bitcast float* %2960 to <4 x float>*
  store <4 x float> %wide.load45.6, <4 x float>* %2961, align 4, !tbaa !3167
  %2962 = getelementptr inbounds float, float* %2960, i64 4
  %2963 = bitcast float* %2962 to <4 x float>*
  store <4 x float> %wide.load46.6, <4 x float>* %2963, align 4, !tbaa !3167
  %2964 = add nsw i64 %21, 2616
  %2965 = getelementptr inbounds float, float* %7, i64 %2964
  %2966 = bitcast float* %2965 to <4 x float>*
  %wide.load45.7 = load <4 x float>, <4 x float>* %2966, align 4, !tbaa !3164
  %2967 = getelementptr inbounds float, float* %2965, i64 4
  %2968 = bitcast float* %2967 to <4 x float>*
  %wide.load46.7 = load <4 x float>, <4 x float>* %2968, align 4, !tbaa !3164
  %2969 = getelementptr inbounds float, float* %4, i64 %2964
  %2970 = bitcast float* %2969 to <4 x float>*
  store <4 x float> %wide.load45.7, <4 x float>* %2970, align 4, !tbaa !3167
  %2971 = getelementptr inbounds float, float* %2969, i64 4
  %2972 = bitcast float* %2971 to <4 x float>*
  store <4 x float> %wide.load46.7, <4 x float>* %2972, align 4, !tbaa !3167
  %2973 = add nsw i64 %21, 2624
  %2974 = getelementptr inbounds float, float* %7, i64 %2973
  %2975 = bitcast float* %2974 to <4 x float>*
  %wide.load45.8 = load <4 x float>, <4 x float>* %2975, align 4, !tbaa !3164
  %2976 = getelementptr inbounds float, float* %2974, i64 4
  %2977 = bitcast float* %2976 to <4 x float>*
  %wide.load46.8 = load <4 x float>, <4 x float>* %2977, align 4, !tbaa !3164
  %2978 = getelementptr inbounds float, float* %4, i64 %2973
  %2979 = bitcast float* %2978 to <4 x float>*
  store <4 x float> %wide.load45.8, <4 x float>* %2979, align 4, !tbaa !3167
  %2980 = getelementptr inbounds float, float* %2978, i64 4
  %2981 = bitcast float* %2980 to <4 x float>*
  store <4 x float> %wide.load46.8, <4 x float>* %2981, align 4, !tbaa !3167
  %2982 = add nsw i64 %21, 2632
  %2983 = getelementptr inbounds float, float* %7, i64 %2982
  %2984 = bitcast float* %2983 to <4 x float>*
  %wide.load45.9 = load <4 x float>, <4 x float>* %2984, align 4, !tbaa !3164
  %2985 = getelementptr inbounds float, float* %2983, i64 4
  %2986 = bitcast float* %2985 to <4 x float>*
  %wide.load46.9 = load <4 x float>, <4 x float>* %2986, align 4, !tbaa !3164
  %2987 = getelementptr inbounds float, float* %4, i64 %2982
  %2988 = bitcast float* %2987 to <4 x float>*
  store <4 x float> %wide.load45.9, <4 x float>* %2988, align 4, !tbaa !3167
  %2989 = getelementptr inbounds float, float* %2987, i64 4
  %2990 = bitcast float* %2989 to <4 x float>*
  store <4 x float> %wide.load46.9, <4 x float>* %2990, align 4, !tbaa !3167
  %2991 = add nsw i64 %21, 2640
  %2992 = getelementptr inbounds float, float* %7, i64 %2991
  %2993 = bitcast float* %2992 to <4 x float>*
  %wide.load45.10 = load <4 x float>, <4 x float>* %2993, align 4, !tbaa !3164
  %2994 = getelementptr inbounds float, float* %2992, i64 4
  %2995 = bitcast float* %2994 to <4 x float>*
  %wide.load46.10 = load <4 x float>, <4 x float>* %2995, align 4, !tbaa !3164
  %2996 = getelementptr inbounds float, float* %4, i64 %2991
  %2997 = bitcast float* %2996 to <4 x float>*
  store <4 x float> %wide.load45.10, <4 x float>* %2997, align 4, !tbaa !3167
  %2998 = getelementptr inbounds float, float* %2996, i64 4
  %2999 = bitcast float* %2998 to <4 x float>*
  store <4 x float> %wide.load46.10, <4 x float>* %2999, align 4, !tbaa !3167
  %3000 = add nsw i64 %21, 2648
  %3001 = getelementptr inbounds float, float* %7, i64 %3000
  %3002 = bitcast float* %3001 to <4 x float>*
  %wide.load45.11 = load <4 x float>, <4 x float>* %3002, align 4, !tbaa !3164
  %3003 = getelementptr inbounds float, float* %3001, i64 4
  %3004 = bitcast float* %3003 to <4 x float>*
  %wide.load46.11 = load <4 x float>, <4 x float>* %3004, align 4, !tbaa !3164
  %3005 = getelementptr inbounds float, float* %4, i64 %3000
  %3006 = bitcast float* %3005 to <4 x float>*
  store <4 x float> %wide.load45.11, <4 x float>* %3006, align 4, !tbaa !3167
  %3007 = getelementptr inbounds float, float* %3005, i64 4
  %3008 = bitcast float* %3007 to <4 x float>*
  store <4 x float> %wide.load46.11, <4 x float>* %3008, align 4, !tbaa !3167
  %3009 = add nsw i64 %21, 2656
  %3010 = getelementptr inbounds float, float* %7, i64 %3009
  %3011 = bitcast float* %3010 to <4 x float>*
  %wide.load45.12 = load <4 x float>, <4 x float>* %3011, align 4, !tbaa !3164
  %3012 = getelementptr inbounds float, float* %3010, i64 4
  %3013 = bitcast float* %3012 to <4 x float>*
  %wide.load46.12 = load <4 x float>, <4 x float>* %3013, align 4, !tbaa !3164
  %3014 = getelementptr inbounds float, float* %4, i64 %3009
  %3015 = bitcast float* %3014 to <4 x float>*
  store <4 x float> %wide.load45.12, <4 x float>* %3015, align 4, !tbaa !3167
  %3016 = getelementptr inbounds float, float* %3014, i64 4
  %3017 = bitcast float* %3016 to <4 x float>*
  store <4 x float> %wide.load46.12, <4 x float>* %3017, align 4, !tbaa !3167
  %3018 = add nsw i64 %21, 2664
  %3019 = getelementptr inbounds float, float* %7, i64 %3018
  %3020 = bitcast float* %3019 to <4 x float>*
  %wide.load45.13 = load <4 x float>, <4 x float>* %3020, align 4, !tbaa !3164
  %3021 = getelementptr inbounds float, float* %3019, i64 4
  %3022 = bitcast float* %3021 to <4 x float>*
  %wide.load46.13 = load <4 x float>, <4 x float>* %3022, align 4, !tbaa !3164
  %3023 = getelementptr inbounds float, float* %4, i64 %3018
  %3024 = bitcast float* %3023 to <4 x float>*
  store <4 x float> %wide.load45.13, <4 x float>* %3024, align 4, !tbaa !3167
  %3025 = getelementptr inbounds float, float* %3023, i64 4
  %3026 = bitcast float* %3025 to <4 x float>*
  store <4 x float> %wide.load46.13, <4 x float>* %3026, align 4, !tbaa !3167
  %3027 = add nsw i64 %21, 2672
  %3028 = getelementptr inbounds float, float* %7, i64 %3027
  %3029 = bitcast float* %3028 to <4 x float>*
  %wide.load45.14 = load <4 x float>, <4 x float>* %3029, align 4, !tbaa !3164
  %3030 = getelementptr inbounds float, float* %3028, i64 4
  %3031 = bitcast float* %3030 to <4 x float>*
  %wide.load46.14 = load <4 x float>, <4 x float>* %3031, align 4, !tbaa !3164
  %3032 = getelementptr inbounds float, float* %4, i64 %3027
  %3033 = bitcast float* %3032 to <4 x float>*
  store <4 x float> %wide.load45.14, <4 x float>* %3033, align 4, !tbaa !3167
  %3034 = getelementptr inbounds float, float* %3032, i64 4
  %3035 = bitcast float* %3034 to <4 x float>*
  store <4 x float> %wide.load46.14, <4 x float>* %3035, align 4, !tbaa !3167
  %3036 = add nsw i64 %21, 2680
  %3037 = getelementptr inbounds float, float* %7, i64 %3036
  %3038 = bitcast float* %3037 to <4 x float>*
  %wide.load45.15 = load <4 x float>, <4 x float>* %3038, align 4, !tbaa !3164
  %3039 = getelementptr inbounds float, float* %3037, i64 4
  %3040 = bitcast float* %3039 to <4 x float>*
  %wide.load46.15 = load <4 x float>, <4 x float>* %3040, align 4, !tbaa !3164
  %3041 = getelementptr inbounds float, float* %4, i64 %3036
  %3042 = bitcast float* %3041 to <4 x float>*
  store <4 x float> %wide.load45.15, <4 x float>* %3042, align 4, !tbaa !3167
  %3043 = getelementptr inbounds float, float* %3041, i64 4
  %3044 = bitcast float* %3043 to <4 x float>*
  store <4 x float> %wide.load46.15, <4 x float>* %3044, align 4, !tbaa !3167
  %3045 = add nsw i64 %21, 2688
  %3046 = getelementptr inbounds float, float* %7, i64 %3045
  %3047 = bitcast float* %3046 to <4 x float>*
  %wide.load45.16 = load <4 x float>, <4 x float>* %3047, align 4, !tbaa !3164
  %3048 = getelementptr inbounds float, float* %3046, i64 4
  %3049 = bitcast float* %3048 to <4 x float>*
  %wide.load46.16 = load <4 x float>, <4 x float>* %3049, align 4, !tbaa !3164
  %3050 = getelementptr inbounds float, float* %4, i64 %3045
  %3051 = bitcast float* %3050 to <4 x float>*
  store <4 x float> %wide.load45.16, <4 x float>* %3051, align 4, !tbaa !3167
  %3052 = getelementptr inbounds float, float* %3050, i64 4
  %3053 = bitcast float* %3052 to <4 x float>*
  store <4 x float> %wide.load46.16, <4 x float>* %3053, align 4, !tbaa !3167
  %3054 = add nsw i64 %21, 2696
  %3055 = getelementptr inbounds float, float* %7, i64 %3054
  %3056 = bitcast float* %3055 to <4 x float>*
  %wide.load45.17 = load <4 x float>, <4 x float>* %3056, align 4, !tbaa !3164
  %3057 = getelementptr inbounds float, float* %3055, i64 4
  %3058 = bitcast float* %3057 to <4 x float>*
  %wide.load46.17 = load <4 x float>, <4 x float>* %3058, align 4, !tbaa !3164
  %3059 = getelementptr inbounds float, float* %4, i64 %3054
  %3060 = bitcast float* %3059 to <4 x float>*
  store <4 x float> %wide.load45.17, <4 x float>* %3060, align 4, !tbaa !3167
  %3061 = getelementptr inbounds float, float* %3059, i64 4
  %3062 = bitcast float* %3061 to <4 x float>*
  store <4 x float> %wide.load46.17, <4 x float>* %3062, align 4, !tbaa !3167
  %3063 = add nsw i64 %21, 2704
  %3064 = getelementptr inbounds float, float* %7, i64 %3063
  %3065 = bitcast float* %3064 to <4 x float>*
  %wide.load45.18 = load <4 x float>, <4 x float>* %3065, align 4, !tbaa !3164
  %3066 = getelementptr inbounds float, float* %3064, i64 4
  %3067 = bitcast float* %3066 to <4 x float>*
  %wide.load46.18 = load <4 x float>, <4 x float>* %3067, align 4, !tbaa !3164
  %3068 = getelementptr inbounds float, float* %4, i64 %3063
  %3069 = bitcast float* %3068 to <4 x float>*
  store <4 x float> %wide.load45.18, <4 x float>* %3069, align 4, !tbaa !3167
  %3070 = getelementptr inbounds float, float* %3068, i64 4
  %3071 = bitcast float* %3070 to <4 x float>*
  store <4 x float> %wide.load46.18, <4 x float>* %3071, align 4, !tbaa !3167
  %3072 = add nsw i64 %21, 2712
  %3073 = getelementptr inbounds float, float* %7, i64 %3072
  %3074 = bitcast float* %3073 to <4 x float>*
  %wide.load45.19 = load <4 x float>, <4 x float>* %3074, align 4, !tbaa !3164
  %3075 = getelementptr inbounds float, float* %3073, i64 4
  %3076 = bitcast float* %3075 to <4 x float>*
  %wide.load46.19 = load <4 x float>, <4 x float>* %3076, align 4, !tbaa !3164
  %3077 = getelementptr inbounds float, float* %4, i64 %3072
  %3078 = bitcast float* %3077 to <4 x float>*
  store <4 x float> %wide.load45.19, <4 x float>* %3078, align 4, !tbaa !3167
  %3079 = getelementptr inbounds float, float* %3077, i64 4
  %3080 = bitcast float* %3079 to <4 x float>*
  store <4 x float> %wide.load46.19, <4 x float>* %3080, align 4, !tbaa !3167
  %3081 = add nsw i64 %21, 2720
  %3082 = getelementptr inbounds float, float* %7, i64 %3081
  %3083 = bitcast float* %3082 to <4 x float>*
  %wide.load45.20 = load <4 x float>, <4 x float>* %3083, align 4, !tbaa !3164
  %3084 = getelementptr inbounds float, float* %3082, i64 4
  %3085 = bitcast float* %3084 to <4 x float>*
  %wide.load46.20 = load <4 x float>, <4 x float>* %3085, align 4, !tbaa !3164
  %3086 = getelementptr inbounds float, float* %4, i64 %3081
  %3087 = bitcast float* %3086 to <4 x float>*
  store <4 x float> %wide.load45.20, <4 x float>* %3087, align 4, !tbaa !3167
  %3088 = getelementptr inbounds float, float* %3086, i64 4
  %3089 = bitcast float* %3088 to <4 x float>*
  store <4 x float> %wide.load46.20, <4 x float>* %3089, align 4, !tbaa !3167
  %3090 = add nsw i64 %21, 2728
  %3091 = getelementptr inbounds float, float* %7, i64 %3090
  %3092 = bitcast float* %3091 to <4 x float>*
  %wide.load45.21 = load <4 x float>, <4 x float>* %3092, align 4, !tbaa !3164
  %3093 = getelementptr inbounds float, float* %3091, i64 4
  %3094 = bitcast float* %3093 to <4 x float>*
  %wide.load46.21 = load <4 x float>, <4 x float>* %3094, align 4, !tbaa !3164
  %3095 = getelementptr inbounds float, float* %4, i64 %3090
  %3096 = bitcast float* %3095 to <4 x float>*
  store <4 x float> %wide.load45.21, <4 x float>* %3096, align 4, !tbaa !3167
  %3097 = getelementptr inbounds float, float* %3095, i64 4
  %3098 = bitcast float* %3097 to <4 x float>*
  store <4 x float> %wide.load46.21, <4 x float>* %3098, align 4, !tbaa !3167
  %3099 = add nsw i64 %21, 2736
  %3100 = getelementptr inbounds float, float* %7, i64 %3099
  %3101 = bitcast float* %3100 to <4 x float>*
  %wide.load45.22 = load <4 x float>, <4 x float>* %3101, align 4, !tbaa !3164
  %3102 = getelementptr inbounds float, float* %3100, i64 4
  %3103 = bitcast float* %3102 to <4 x float>*
  %wide.load46.22 = load <4 x float>, <4 x float>* %3103, align 4, !tbaa !3164
  %3104 = getelementptr inbounds float, float* %4, i64 %3099
  %3105 = bitcast float* %3104 to <4 x float>*
  store <4 x float> %wide.load45.22, <4 x float>* %3105, align 4, !tbaa !3167
  %3106 = getelementptr inbounds float, float* %3104, i64 4
  %3107 = bitcast float* %3106 to <4 x float>*
  store <4 x float> %wide.load46.22, <4 x float>* %3107, align 4, !tbaa !3167
  %3108 = add nsw i64 %21, 2744
  %3109 = getelementptr inbounds float, float* %7, i64 %3108
  %3110 = bitcast float* %3109 to <4 x float>*
  %wide.load45.23 = load <4 x float>, <4 x float>* %3110, align 4, !tbaa !3164
  %3111 = getelementptr inbounds float, float* %3109, i64 4
  %3112 = bitcast float* %3111 to <4 x float>*
  %wide.load46.23 = load <4 x float>, <4 x float>* %3112, align 4, !tbaa !3164
  %3113 = getelementptr inbounds float, float* %4, i64 %3108
  %3114 = bitcast float* %3113 to <4 x float>*
  store <4 x float> %wide.load45.23, <4 x float>* %3114, align 4, !tbaa !3167
  %3115 = getelementptr inbounds float, float* %3113, i64 4
  %3116 = bitcast float* %3115 to <4 x float>*
  store <4 x float> %wide.load46.23, <4 x float>* %3116, align 4, !tbaa !3167
  %3117 = add nsw i64 %21, 2752
  %3118 = getelementptr inbounds float, float* %7, i64 %3117
  %3119 = bitcast float* %3118 to <4 x float>*
  %wide.load45.24 = load <4 x float>, <4 x float>* %3119, align 4, !tbaa !3164
  %3120 = getelementptr inbounds float, float* %3118, i64 4
  %3121 = bitcast float* %3120 to <4 x float>*
  %wide.load46.24 = load <4 x float>, <4 x float>* %3121, align 4, !tbaa !3164
  %3122 = getelementptr inbounds float, float* %4, i64 %3117
  %3123 = bitcast float* %3122 to <4 x float>*
  store <4 x float> %wide.load45.24, <4 x float>* %3123, align 4, !tbaa !3167
  %3124 = getelementptr inbounds float, float* %3122, i64 4
  %3125 = bitcast float* %3124 to <4 x float>*
  store <4 x float> %wide.load46.24, <4 x float>* %3125, align 4, !tbaa !3167
  %3126 = add nsw i64 %21, 2760
  %3127 = getelementptr inbounds float, float* %7, i64 %3126
  %3128 = bitcast float* %3127 to <4 x float>*
  %wide.load45.25 = load <4 x float>, <4 x float>* %3128, align 4, !tbaa !3164
  %3129 = getelementptr inbounds float, float* %3127, i64 4
  %3130 = bitcast float* %3129 to <4 x float>*
  %wide.load46.25 = load <4 x float>, <4 x float>* %3130, align 4, !tbaa !3164
  %3131 = getelementptr inbounds float, float* %4, i64 %3126
  %3132 = bitcast float* %3131 to <4 x float>*
  store <4 x float> %wide.load45.25, <4 x float>* %3132, align 4, !tbaa !3167
  %3133 = getelementptr inbounds float, float* %3131, i64 4
  %3134 = bitcast float* %3133 to <4 x float>*
  store <4 x float> %wide.load46.25, <4 x float>* %3134, align 4, !tbaa !3167
  %3135 = add nsw i64 %21, 2768
  %3136 = getelementptr inbounds float, float* %7, i64 %3135
  %3137 = bitcast float* %3136 to <4 x float>*
  %wide.load45.26 = load <4 x float>, <4 x float>* %3137, align 4, !tbaa !3164
  %3138 = getelementptr inbounds float, float* %3136, i64 4
  %3139 = bitcast float* %3138 to <4 x float>*
  %wide.load46.26 = load <4 x float>, <4 x float>* %3139, align 4, !tbaa !3164
  %3140 = getelementptr inbounds float, float* %4, i64 %3135
  %3141 = bitcast float* %3140 to <4 x float>*
  store <4 x float> %wide.load45.26, <4 x float>* %3141, align 4, !tbaa !3167
  %3142 = getelementptr inbounds float, float* %3140, i64 4
  %3143 = bitcast float* %3142 to <4 x float>*
  store <4 x float> %wide.load46.26, <4 x float>* %3143, align 4, !tbaa !3167
  %3144 = add nsw i64 %21, 2776
  %3145 = getelementptr inbounds float, float* %7, i64 %3144
  %3146 = bitcast float* %3145 to <4 x float>*
  %wide.load45.27 = load <4 x float>, <4 x float>* %3146, align 4, !tbaa !3164
  %3147 = getelementptr inbounds float, float* %3145, i64 4
  %3148 = bitcast float* %3147 to <4 x float>*
  %wide.load46.27 = load <4 x float>, <4 x float>* %3148, align 4, !tbaa !3164
  %3149 = getelementptr inbounds float, float* %4, i64 %3144
  %3150 = bitcast float* %3149 to <4 x float>*
  store <4 x float> %wide.load45.27, <4 x float>* %3150, align 4, !tbaa !3167
  %3151 = getelementptr inbounds float, float* %3149, i64 4
  %3152 = bitcast float* %3151 to <4 x float>*
  store <4 x float> %wide.load46.27, <4 x float>* %3152, align 4, !tbaa !3167
  %3153 = add nsw i64 %21, 2784
  %3154 = getelementptr inbounds float, float* %7, i64 %3153
  %3155 = bitcast float* %3154 to <4 x float>*
  %wide.load45.28 = load <4 x float>, <4 x float>* %3155, align 4, !tbaa !3164
  %3156 = getelementptr inbounds float, float* %3154, i64 4
  %3157 = bitcast float* %3156 to <4 x float>*
  %wide.load46.28 = load <4 x float>, <4 x float>* %3157, align 4, !tbaa !3164
  %3158 = getelementptr inbounds float, float* %4, i64 %3153
  %3159 = bitcast float* %3158 to <4 x float>*
  store <4 x float> %wide.load45.28, <4 x float>* %3159, align 4, !tbaa !3167
  %3160 = getelementptr inbounds float, float* %3158, i64 4
  %3161 = bitcast float* %3160 to <4 x float>*
  store <4 x float> %wide.load46.28, <4 x float>* %3161, align 4, !tbaa !3167
  %3162 = add nsw i64 %21, 2792
  %3163 = getelementptr inbounds float, float* %7, i64 %3162
  %3164 = bitcast float* %3163 to <4 x float>*
  %wide.load45.29 = load <4 x float>, <4 x float>* %3164, align 4, !tbaa !3164
  %3165 = getelementptr inbounds float, float* %3163, i64 4
  %3166 = bitcast float* %3165 to <4 x float>*
  %wide.load46.29 = load <4 x float>, <4 x float>* %3166, align 4, !tbaa !3164
  %3167 = getelementptr inbounds float, float* %4, i64 %3162
  %3168 = bitcast float* %3167 to <4 x float>*
  store <4 x float> %wide.load45.29, <4 x float>* %3168, align 4, !tbaa !3167
  %3169 = getelementptr inbounds float, float* %3167, i64 4
  %3170 = bitcast float* %3169 to <4 x float>*
  store <4 x float> %wide.load46.29, <4 x float>* %3170, align 4, !tbaa !3167
  %3171 = add nsw i64 %21, 2800
  %3172 = getelementptr inbounds float, float* %7, i64 %3171
  %3173 = bitcast float* %3172 to <4 x float>*
  %wide.load45.30 = load <4 x float>, <4 x float>* %3173, align 4, !tbaa !3164
  %3174 = getelementptr inbounds float, float* %3172, i64 4
  %3175 = bitcast float* %3174 to <4 x float>*
  %wide.load46.30 = load <4 x float>, <4 x float>* %3175, align 4, !tbaa !3164
  %3176 = getelementptr inbounds float, float* %4, i64 %3171
  %3177 = bitcast float* %3176 to <4 x float>*
  store <4 x float> %wide.load45.30, <4 x float>* %3177, align 4, !tbaa !3167
  %3178 = getelementptr inbounds float, float* %3176, i64 4
  %3179 = bitcast float* %3178 to <4 x float>*
  store <4 x float> %wide.load46.30, <4 x float>* %3179, align 4, !tbaa !3167
  %3180 = add nsw i64 %21, 2808
  %3181 = getelementptr inbounds float, float* %7, i64 %3180
  %3182 = bitcast float* %3181 to <4 x float>*
  %wide.load45.31 = load <4 x float>, <4 x float>* %3182, align 4, !tbaa !3164
  %3183 = getelementptr inbounds float, float* %3181, i64 4
  %3184 = bitcast float* %3183 to <4 x float>*
  %wide.load46.31 = load <4 x float>, <4 x float>* %3184, align 4, !tbaa !3164
  %3185 = getelementptr inbounds float, float* %4, i64 %3180
  %3186 = bitcast float* %3185 to <4 x float>*
  store <4 x float> %wide.load45.31, <4 x float>* %3186, align 4, !tbaa !3167
  %3187 = getelementptr inbounds float, float* %3185, i64 4
  %3188 = bitcast float* %3187 to <4 x float>*
  store <4 x float> %wide.load46.31, <4 x float>* %3188, align 4, !tbaa !3167
  %3189 = add nsw i64 %21, 2816
  %3190 = getelementptr inbounds float, float* %7, i64 %3189
  %3191 = bitcast float* %3190 to <4 x float>*
  %wide.load35 = load <4 x float>, <4 x float>* %3191, align 4, !tbaa !3164
  %3192 = getelementptr inbounds float, float* %3190, i64 4
  %3193 = bitcast float* %3192 to <4 x float>*
  %wide.load36 = load <4 x float>, <4 x float>* %3193, align 4, !tbaa !3164
  %3194 = getelementptr inbounds float, float* %4, i64 %3189
  %3195 = bitcast float* %3194 to <4 x float>*
  store <4 x float> %wide.load35, <4 x float>* %3195, align 4, !tbaa !3167
  %3196 = getelementptr inbounds float, float* %3194, i64 4
  %3197 = bitcast float* %3196 to <4 x float>*
  store <4 x float> %wide.load36, <4 x float>* %3197, align 4, !tbaa !3167
  %3198 = add nsw i64 %21, 2824
  %3199 = getelementptr inbounds float, float* %7, i64 %3198
  %3200 = bitcast float* %3199 to <4 x float>*
  %wide.load35.1 = load <4 x float>, <4 x float>* %3200, align 4, !tbaa !3164
  %3201 = getelementptr inbounds float, float* %3199, i64 4
  %3202 = bitcast float* %3201 to <4 x float>*
  %wide.load36.1 = load <4 x float>, <4 x float>* %3202, align 4, !tbaa !3164
  %3203 = getelementptr inbounds float, float* %4, i64 %3198
  %3204 = bitcast float* %3203 to <4 x float>*
  store <4 x float> %wide.load35.1, <4 x float>* %3204, align 4, !tbaa !3167
  %3205 = getelementptr inbounds float, float* %3203, i64 4
  %3206 = bitcast float* %3205 to <4 x float>*
  store <4 x float> %wide.load36.1, <4 x float>* %3206, align 4, !tbaa !3167
  %3207 = add nsw i64 %21, 2832
  %3208 = getelementptr inbounds float, float* %7, i64 %3207
  %3209 = bitcast float* %3208 to <4 x float>*
  %wide.load35.2 = load <4 x float>, <4 x float>* %3209, align 4, !tbaa !3164
  %3210 = getelementptr inbounds float, float* %3208, i64 4
  %3211 = bitcast float* %3210 to <4 x float>*
  %wide.load36.2 = load <4 x float>, <4 x float>* %3211, align 4, !tbaa !3164
  %3212 = getelementptr inbounds float, float* %4, i64 %3207
  %3213 = bitcast float* %3212 to <4 x float>*
  store <4 x float> %wide.load35.2, <4 x float>* %3213, align 4, !tbaa !3167
  %3214 = getelementptr inbounds float, float* %3212, i64 4
  %3215 = bitcast float* %3214 to <4 x float>*
  store <4 x float> %wide.load36.2, <4 x float>* %3215, align 4, !tbaa !3167
  %3216 = add nsw i64 %21, 2840
  %3217 = getelementptr inbounds float, float* %7, i64 %3216
  %3218 = bitcast float* %3217 to <4 x float>*
  %wide.load35.3 = load <4 x float>, <4 x float>* %3218, align 4, !tbaa !3164
  %3219 = getelementptr inbounds float, float* %3217, i64 4
  %3220 = bitcast float* %3219 to <4 x float>*
  %wide.load36.3 = load <4 x float>, <4 x float>* %3220, align 4, !tbaa !3164
  %3221 = getelementptr inbounds float, float* %4, i64 %3216
  %3222 = bitcast float* %3221 to <4 x float>*
  store <4 x float> %wide.load35.3, <4 x float>* %3222, align 4, !tbaa !3167
  %3223 = getelementptr inbounds float, float* %3221, i64 4
  %3224 = bitcast float* %3223 to <4 x float>*
  store <4 x float> %wide.load36.3, <4 x float>* %3224, align 4, !tbaa !3167
  %3225 = add nsw i64 %21, 2848
  %3226 = getelementptr inbounds float, float* %7, i64 %3225
  %3227 = bitcast float* %3226 to <4 x float>*
  %wide.load35.4 = load <4 x float>, <4 x float>* %3227, align 4, !tbaa !3164
  %3228 = getelementptr inbounds float, float* %3226, i64 4
  %3229 = bitcast float* %3228 to <4 x float>*
  %wide.load36.4 = load <4 x float>, <4 x float>* %3229, align 4, !tbaa !3164
  %3230 = getelementptr inbounds float, float* %4, i64 %3225
  %3231 = bitcast float* %3230 to <4 x float>*
  store <4 x float> %wide.load35.4, <4 x float>* %3231, align 4, !tbaa !3167
  %3232 = getelementptr inbounds float, float* %3230, i64 4
  %3233 = bitcast float* %3232 to <4 x float>*
  store <4 x float> %wide.load36.4, <4 x float>* %3233, align 4, !tbaa !3167
  %3234 = add nsw i64 %21, 2856
  %3235 = getelementptr inbounds float, float* %7, i64 %3234
  %3236 = bitcast float* %3235 to <4 x float>*
  %wide.load35.5 = load <4 x float>, <4 x float>* %3236, align 4, !tbaa !3164
  %3237 = getelementptr inbounds float, float* %3235, i64 4
  %3238 = bitcast float* %3237 to <4 x float>*
  %wide.load36.5 = load <4 x float>, <4 x float>* %3238, align 4, !tbaa !3164
  %3239 = getelementptr inbounds float, float* %4, i64 %3234
  %3240 = bitcast float* %3239 to <4 x float>*
  store <4 x float> %wide.load35.5, <4 x float>* %3240, align 4, !tbaa !3167
  %3241 = getelementptr inbounds float, float* %3239, i64 4
  %3242 = bitcast float* %3241 to <4 x float>*
  store <4 x float> %wide.load36.5, <4 x float>* %3242, align 4, !tbaa !3167
  %3243 = add nsw i64 %21, 2864
  %3244 = getelementptr inbounds float, float* %7, i64 %3243
  %3245 = bitcast float* %3244 to <4 x float>*
  %wide.load35.6 = load <4 x float>, <4 x float>* %3245, align 4, !tbaa !3164
  %3246 = getelementptr inbounds float, float* %3244, i64 4
  %3247 = bitcast float* %3246 to <4 x float>*
  %wide.load36.6 = load <4 x float>, <4 x float>* %3247, align 4, !tbaa !3164
  %3248 = getelementptr inbounds float, float* %4, i64 %3243
  %3249 = bitcast float* %3248 to <4 x float>*
  store <4 x float> %wide.load35.6, <4 x float>* %3249, align 4, !tbaa !3167
  %3250 = getelementptr inbounds float, float* %3248, i64 4
  %3251 = bitcast float* %3250 to <4 x float>*
  store <4 x float> %wide.load36.6, <4 x float>* %3251, align 4, !tbaa !3167
  %3252 = add nsw i64 %21, 2872
  %3253 = getelementptr inbounds float, float* %7, i64 %3252
  %3254 = bitcast float* %3253 to <4 x float>*
  %wide.load35.7 = load <4 x float>, <4 x float>* %3254, align 4, !tbaa !3164
  %3255 = getelementptr inbounds float, float* %3253, i64 4
  %3256 = bitcast float* %3255 to <4 x float>*
  %wide.load36.7 = load <4 x float>, <4 x float>* %3256, align 4, !tbaa !3164
  %3257 = getelementptr inbounds float, float* %4, i64 %3252
  %3258 = bitcast float* %3257 to <4 x float>*
  store <4 x float> %wide.load35.7, <4 x float>* %3258, align 4, !tbaa !3167
  %3259 = getelementptr inbounds float, float* %3257, i64 4
  %3260 = bitcast float* %3259 to <4 x float>*
  store <4 x float> %wide.load36.7, <4 x float>* %3260, align 4, !tbaa !3167
  %3261 = add nsw i64 %21, 2880
  %3262 = getelementptr inbounds float, float* %7, i64 %3261
  %3263 = bitcast float* %3262 to <4 x float>*
  %wide.load35.8 = load <4 x float>, <4 x float>* %3263, align 4, !tbaa !3164
  %3264 = getelementptr inbounds float, float* %3262, i64 4
  %3265 = bitcast float* %3264 to <4 x float>*
  %wide.load36.8 = load <4 x float>, <4 x float>* %3265, align 4, !tbaa !3164
  %3266 = getelementptr inbounds float, float* %4, i64 %3261
  %3267 = bitcast float* %3266 to <4 x float>*
  store <4 x float> %wide.load35.8, <4 x float>* %3267, align 4, !tbaa !3167
  %3268 = getelementptr inbounds float, float* %3266, i64 4
  %3269 = bitcast float* %3268 to <4 x float>*
  store <4 x float> %wide.load36.8, <4 x float>* %3269, align 4, !tbaa !3167
  %3270 = add nsw i64 %21, 2888
  %3271 = getelementptr inbounds float, float* %7, i64 %3270
  %3272 = bitcast float* %3271 to <4 x float>*
  %wide.load35.9 = load <4 x float>, <4 x float>* %3272, align 4, !tbaa !3164
  %3273 = getelementptr inbounds float, float* %3271, i64 4
  %3274 = bitcast float* %3273 to <4 x float>*
  %wide.load36.9 = load <4 x float>, <4 x float>* %3274, align 4, !tbaa !3164
  %3275 = getelementptr inbounds float, float* %4, i64 %3270
  %3276 = bitcast float* %3275 to <4 x float>*
  store <4 x float> %wide.load35.9, <4 x float>* %3276, align 4, !tbaa !3167
  %3277 = getelementptr inbounds float, float* %3275, i64 4
  %3278 = bitcast float* %3277 to <4 x float>*
  store <4 x float> %wide.load36.9, <4 x float>* %3278, align 4, !tbaa !3167
  %3279 = add nsw i64 %21, 2896
  %3280 = getelementptr inbounds float, float* %7, i64 %3279
  %3281 = bitcast float* %3280 to <4 x float>*
  %wide.load35.10 = load <4 x float>, <4 x float>* %3281, align 4, !tbaa !3164
  %3282 = getelementptr inbounds float, float* %3280, i64 4
  %3283 = bitcast float* %3282 to <4 x float>*
  %wide.load36.10 = load <4 x float>, <4 x float>* %3283, align 4, !tbaa !3164
  %3284 = getelementptr inbounds float, float* %4, i64 %3279
  %3285 = bitcast float* %3284 to <4 x float>*
  store <4 x float> %wide.load35.10, <4 x float>* %3285, align 4, !tbaa !3167
  %3286 = getelementptr inbounds float, float* %3284, i64 4
  %3287 = bitcast float* %3286 to <4 x float>*
  store <4 x float> %wide.load36.10, <4 x float>* %3287, align 4, !tbaa !3167
  %3288 = add nsw i64 %21, 2904
  %3289 = getelementptr inbounds float, float* %7, i64 %3288
  %3290 = bitcast float* %3289 to <4 x float>*
  %wide.load35.11 = load <4 x float>, <4 x float>* %3290, align 4, !tbaa !3164
  %3291 = getelementptr inbounds float, float* %3289, i64 4
  %3292 = bitcast float* %3291 to <4 x float>*
  %wide.load36.11 = load <4 x float>, <4 x float>* %3292, align 4, !tbaa !3164
  %3293 = getelementptr inbounds float, float* %4, i64 %3288
  %3294 = bitcast float* %3293 to <4 x float>*
  store <4 x float> %wide.load35.11, <4 x float>* %3294, align 4, !tbaa !3167
  %3295 = getelementptr inbounds float, float* %3293, i64 4
  %3296 = bitcast float* %3295 to <4 x float>*
  store <4 x float> %wide.load36.11, <4 x float>* %3296, align 4, !tbaa !3167
  %3297 = add nsw i64 %21, 2912
  %3298 = getelementptr inbounds float, float* %7, i64 %3297
  %3299 = bitcast float* %3298 to <4 x float>*
  %wide.load35.12 = load <4 x float>, <4 x float>* %3299, align 4, !tbaa !3164
  %3300 = getelementptr inbounds float, float* %3298, i64 4
  %3301 = bitcast float* %3300 to <4 x float>*
  %wide.load36.12 = load <4 x float>, <4 x float>* %3301, align 4, !tbaa !3164
  %3302 = getelementptr inbounds float, float* %4, i64 %3297
  %3303 = bitcast float* %3302 to <4 x float>*
  store <4 x float> %wide.load35.12, <4 x float>* %3303, align 4, !tbaa !3167
  %3304 = getelementptr inbounds float, float* %3302, i64 4
  %3305 = bitcast float* %3304 to <4 x float>*
  store <4 x float> %wide.load36.12, <4 x float>* %3305, align 4, !tbaa !3167
  %3306 = add nsw i64 %21, 2920
  %3307 = getelementptr inbounds float, float* %7, i64 %3306
  %3308 = bitcast float* %3307 to <4 x float>*
  %wide.load35.13 = load <4 x float>, <4 x float>* %3308, align 4, !tbaa !3164
  %3309 = getelementptr inbounds float, float* %3307, i64 4
  %3310 = bitcast float* %3309 to <4 x float>*
  %wide.load36.13 = load <4 x float>, <4 x float>* %3310, align 4, !tbaa !3164
  %3311 = getelementptr inbounds float, float* %4, i64 %3306
  %3312 = bitcast float* %3311 to <4 x float>*
  store <4 x float> %wide.load35.13, <4 x float>* %3312, align 4, !tbaa !3167
  %3313 = getelementptr inbounds float, float* %3311, i64 4
  %3314 = bitcast float* %3313 to <4 x float>*
  store <4 x float> %wide.load36.13, <4 x float>* %3314, align 4, !tbaa !3167
  %3315 = add nsw i64 %21, 2928
  %3316 = getelementptr inbounds float, float* %7, i64 %3315
  %3317 = bitcast float* %3316 to <4 x float>*
  %wide.load35.14 = load <4 x float>, <4 x float>* %3317, align 4, !tbaa !3164
  %3318 = getelementptr inbounds float, float* %3316, i64 4
  %3319 = bitcast float* %3318 to <4 x float>*
  %wide.load36.14 = load <4 x float>, <4 x float>* %3319, align 4, !tbaa !3164
  %3320 = getelementptr inbounds float, float* %4, i64 %3315
  %3321 = bitcast float* %3320 to <4 x float>*
  store <4 x float> %wide.load35.14, <4 x float>* %3321, align 4, !tbaa !3167
  %3322 = getelementptr inbounds float, float* %3320, i64 4
  %3323 = bitcast float* %3322 to <4 x float>*
  store <4 x float> %wide.load36.14, <4 x float>* %3323, align 4, !tbaa !3167
  %3324 = add nsw i64 %21, 2936
  %3325 = getelementptr inbounds float, float* %7, i64 %3324
  %3326 = bitcast float* %3325 to <4 x float>*
  %wide.load35.15 = load <4 x float>, <4 x float>* %3326, align 4, !tbaa !3164
  %3327 = getelementptr inbounds float, float* %3325, i64 4
  %3328 = bitcast float* %3327 to <4 x float>*
  %wide.load36.15 = load <4 x float>, <4 x float>* %3328, align 4, !tbaa !3164
  %3329 = getelementptr inbounds float, float* %4, i64 %3324
  %3330 = bitcast float* %3329 to <4 x float>*
  store <4 x float> %wide.load35.15, <4 x float>* %3330, align 4, !tbaa !3167
  %3331 = getelementptr inbounds float, float* %3329, i64 4
  %3332 = bitcast float* %3331 to <4 x float>*
  store <4 x float> %wide.load36.15, <4 x float>* %3332, align 4, !tbaa !3167
  %3333 = add nsw i64 %21, 2944
  %3334 = getelementptr inbounds float, float* %7, i64 %3333
  %3335 = bitcast float* %3334 to <4 x float>*
  %wide.load35.16 = load <4 x float>, <4 x float>* %3335, align 4, !tbaa !3164
  %3336 = getelementptr inbounds float, float* %3334, i64 4
  %3337 = bitcast float* %3336 to <4 x float>*
  %wide.load36.16 = load <4 x float>, <4 x float>* %3337, align 4, !tbaa !3164
  %3338 = getelementptr inbounds float, float* %4, i64 %3333
  %3339 = bitcast float* %3338 to <4 x float>*
  store <4 x float> %wide.load35.16, <4 x float>* %3339, align 4, !tbaa !3167
  %3340 = getelementptr inbounds float, float* %3338, i64 4
  %3341 = bitcast float* %3340 to <4 x float>*
  store <4 x float> %wide.load36.16, <4 x float>* %3341, align 4, !tbaa !3167
  %3342 = add nsw i64 %21, 2952
  %3343 = getelementptr inbounds float, float* %7, i64 %3342
  %3344 = bitcast float* %3343 to <4 x float>*
  %wide.load35.17 = load <4 x float>, <4 x float>* %3344, align 4, !tbaa !3164
  %3345 = getelementptr inbounds float, float* %3343, i64 4
  %3346 = bitcast float* %3345 to <4 x float>*
  %wide.load36.17 = load <4 x float>, <4 x float>* %3346, align 4, !tbaa !3164
  %3347 = getelementptr inbounds float, float* %4, i64 %3342
  %3348 = bitcast float* %3347 to <4 x float>*
  store <4 x float> %wide.load35.17, <4 x float>* %3348, align 4, !tbaa !3167
  %3349 = getelementptr inbounds float, float* %3347, i64 4
  %3350 = bitcast float* %3349 to <4 x float>*
  store <4 x float> %wide.load36.17, <4 x float>* %3350, align 4, !tbaa !3167
  %3351 = add nsw i64 %21, 2960
  %3352 = getelementptr inbounds float, float* %7, i64 %3351
  %3353 = bitcast float* %3352 to <4 x float>*
  %wide.load35.18 = load <4 x float>, <4 x float>* %3353, align 4, !tbaa !3164
  %3354 = getelementptr inbounds float, float* %3352, i64 4
  %3355 = bitcast float* %3354 to <4 x float>*
  %wide.load36.18 = load <4 x float>, <4 x float>* %3355, align 4, !tbaa !3164
  %3356 = getelementptr inbounds float, float* %4, i64 %3351
  %3357 = bitcast float* %3356 to <4 x float>*
  store <4 x float> %wide.load35.18, <4 x float>* %3357, align 4, !tbaa !3167
  %3358 = getelementptr inbounds float, float* %3356, i64 4
  %3359 = bitcast float* %3358 to <4 x float>*
  store <4 x float> %wide.load36.18, <4 x float>* %3359, align 4, !tbaa !3167
  %3360 = add nsw i64 %21, 2968
  %3361 = getelementptr inbounds float, float* %7, i64 %3360
  %3362 = bitcast float* %3361 to <4 x float>*
  %wide.load35.19 = load <4 x float>, <4 x float>* %3362, align 4, !tbaa !3164
  %3363 = getelementptr inbounds float, float* %3361, i64 4
  %3364 = bitcast float* %3363 to <4 x float>*
  %wide.load36.19 = load <4 x float>, <4 x float>* %3364, align 4, !tbaa !3164
  %3365 = getelementptr inbounds float, float* %4, i64 %3360
  %3366 = bitcast float* %3365 to <4 x float>*
  store <4 x float> %wide.load35.19, <4 x float>* %3366, align 4, !tbaa !3167
  %3367 = getelementptr inbounds float, float* %3365, i64 4
  %3368 = bitcast float* %3367 to <4 x float>*
  store <4 x float> %wide.load36.19, <4 x float>* %3368, align 4, !tbaa !3167
  %3369 = add nsw i64 %21, 2976
  %3370 = getelementptr inbounds float, float* %7, i64 %3369
  %3371 = bitcast float* %3370 to <4 x float>*
  %wide.load35.20 = load <4 x float>, <4 x float>* %3371, align 4, !tbaa !3164
  %3372 = getelementptr inbounds float, float* %3370, i64 4
  %3373 = bitcast float* %3372 to <4 x float>*
  %wide.load36.20 = load <4 x float>, <4 x float>* %3373, align 4, !tbaa !3164
  %3374 = getelementptr inbounds float, float* %4, i64 %3369
  %3375 = bitcast float* %3374 to <4 x float>*
  store <4 x float> %wide.load35.20, <4 x float>* %3375, align 4, !tbaa !3167
  %3376 = getelementptr inbounds float, float* %3374, i64 4
  %3377 = bitcast float* %3376 to <4 x float>*
  store <4 x float> %wide.load36.20, <4 x float>* %3377, align 4, !tbaa !3167
  %3378 = add nsw i64 %21, 2984
  %3379 = getelementptr inbounds float, float* %7, i64 %3378
  %3380 = bitcast float* %3379 to <4 x float>*
  %wide.load35.21 = load <4 x float>, <4 x float>* %3380, align 4, !tbaa !3164
  %3381 = getelementptr inbounds float, float* %3379, i64 4
  %3382 = bitcast float* %3381 to <4 x float>*
  %wide.load36.21 = load <4 x float>, <4 x float>* %3382, align 4, !tbaa !3164
  %3383 = getelementptr inbounds float, float* %4, i64 %3378
  %3384 = bitcast float* %3383 to <4 x float>*
  store <4 x float> %wide.load35.21, <4 x float>* %3384, align 4, !tbaa !3167
  %3385 = getelementptr inbounds float, float* %3383, i64 4
  %3386 = bitcast float* %3385 to <4 x float>*
  store <4 x float> %wide.load36.21, <4 x float>* %3386, align 4, !tbaa !3167
  %3387 = add nsw i64 %21, 2992
  %3388 = getelementptr inbounds float, float* %7, i64 %3387
  %3389 = bitcast float* %3388 to <4 x float>*
  %wide.load35.22 = load <4 x float>, <4 x float>* %3389, align 4, !tbaa !3164
  %3390 = getelementptr inbounds float, float* %3388, i64 4
  %3391 = bitcast float* %3390 to <4 x float>*
  %wide.load36.22 = load <4 x float>, <4 x float>* %3391, align 4, !tbaa !3164
  %3392 = getelementptr inbounds float, float* %4, i64 %3387
  %3393 = bitcast float* %3392 to <4 x float>*
  store <4 x float> %wide.load35.22, <4 x float>* %3393, align 4, !tbaa !3167
  %3394 = getelementptr inbounds float, float* %3392, i64 4
  %3395 = bitcast float* %3394 to <4 x float>*
  store <4 x float> %wide.load36.22, <4 x float>* %3395, align 4, !tbaa !3167
  %3396 = add nsw i64 %21, 3000
  %3397 = getelementptr inbounds float, float* %7, i64 %3396
  %3398 = bitcast float* %3397 to <4 x float>*
  %wide.load35.23 = load <4 x float>, <4 x float>* %3398, align 4, !tbaa !3164
  %3399 = getelementptr inbounds float, float* %3397, i64 4
  %3400 = bitcast float* %3399 to <4 x float>*
  %wide.load36.23 = load <4 x float>, <4 x float>* %3400, align 4, !tbaa !3164
  %3401 = getelementptr inbounds float, float* %4, i64 %3396
  %3402 = bitcast float* %3401 to <4 x float>*
  store <4 x float> %wide.load35.23, <4 x float>* %3402, align 4, !tbaa !3167
  %3403 = getelementptr inbounds float, float* %3401, i64 4
  %3404 = bitcast float* %3403 to <4 x float>*
  store <4 x float> %wide.load36.23, <4 x float>* %3404, align 4, !tbaa !3167
  %3405 = add nsw i64 %21, 3008
  %3406 = getelementptr inbounds float, float* %7, i64 %3405
  %3407 = bitcast float* %3406 to <4 x float>*
  %wide.load35.24 = load <4 x float>, <4 x float>* %3407, align 4, !tbaa !3164
  %3408 = getelementptr inbounds float, float* %3406, i64 4
  %3409 = bitcast float* %3408 to <4 x float>*
  %wide.load36.24 = load <4 x float>, <4 x float>* %3409, align 4, !tbaa !3164
  %3410 = getelementptr inbounds float, float* %4, i64 %3405
  %3411 = bitcast float* %3410 to <4 x float>*
  store <4 x float> %wide.load35.24, <4 x float>* %3411, align 4, !tbaa !3167
  %3412 = getelementptr inbounds float, float* %3410, i64 4
  %3413 = bitcast float* %3412 to <4 x float>*
  store <4 x float> %wide.load36.24, <4 x float>* %3413, align 4, !tbaa !3167
  %3414 = add nsw i64 %21, 3016
  %3415 = getelementptr inbounds float, float* %7, i64 %3414
  %3416 = bitcast float* %3415 to <4 x float>*
  %wide.load35.25 = load <4 x float>, <4 x float>* %3416, align 4, !tbaa !3164
  %3417 = getelementptr inbounds float, float* %3415, i64 4
  %3418 = bitcast float* %3417 to <4 x float>*
  %wide.load36.25 = load <4 x float>, <4 x float>* %3418, align 4, !tbaa !3164
  %3419 = getelementptr inbounds float, float* %4, i64 %3414
  %3420 = bitcast float* %3419 to <4 x float>*
  store <4 x float> %wide.load35.25, <4 x float>* %3420, align 4, !tbaa !3167
  %3421 = getelementptr inbounds float, float* %3419, i64 4
  %3422 = bitcast float* %3421 to <4 x float>*
  store <4 x float> %wide.load36.25, <4 x float>* %3422, align 4, !tbaa !3167
  %3423 = add nsw i64 %21, 3024
  %3424 = getelementptr inbounds float, float* %7, i64 %3423
  %3425 = bitcast float* %3424 to <4 x float>*
  %wide.load35.26 = load <4 x float>, <4 x float>* %3425, align 4, !tbaa !3164
  %3426 = getelementptr inbounds float, float* %3424, i64 4
  %3427 = bitcast float* %3426 to <4 x float>*
  %wide.load36.26 = load <4 x float>, <4 x float>* %3427, align 4, !tbaa !3164
  %3428 = getelementptr inbounds float, float* %4, i64 %3423
  %3429 = bitcast float* %3428 to <4 x float>*
  store <4 x float> %wide.load35.26, <4 x float>* %3429, align 4, !tbaa !3167
  %3430 = getelementptr inbounds float, float* %3428, i64 4
  %3431 = bitcast float* %3430 to <4 x float>*
  store <4 x float> %wide.load36.26, <4 x float>* %3431, align 4, !tbaa !3167
  %3432 = add nsw i64 %21, 3032
  %3433 = getelementptr inbounds float, float* %7, i64 %3432
  %3434 = bitcast float* %3433 to <4 x float>*
  %wide.load35.27 = load <4 x float>, <4 x float>* %3434, align 4, !tbaa !3164
  %3435 = getelementptr inbounds float, float* %3433, i64 4
  %3436 = bitcast float* %3435 to <4 x float>*
  %wide.load36.27 = load <4 x float>, <4 x float>* %3436, align 4, !tbaa !3164
  %3437 = getelementptr inbounds float, float* %4, i64 %3432
  %3438 = bitcast float* %3437 to <4 x float>*
  store <4 x float> %wide.load35.27, <4 x float>* %3438, align 4, !tbaa !3167
  %3439 = getelementptr inbounds float, float* %3437, i64 4
  %3440 = bitcast float* %3439 to <4 x float>*
  store <4 x float> %wide.load36.27, <4 x float>* %3440, align 4, !tbaa !3167
  %3441 = add nsw i64 %21, 3040
  %3442 = getelementptr inbounds float, float* %7, i64 %3441
  %3443 = bitcast float* %3442 to <4 x float>*
  %wide.load35.28 = load <4 x float>, <4 x float>* %3443, align 4, !tbaa !3164
  %3444 = getelementptr inbounds float, float* %3442, i64 4
  %3445 = bitcast float* %3444 to <4 x float>*
  %wide.load36.28 = load <4 x float>, <4 x float>* %3445, align 4, !tbaa !3164
  %3446 = getelementptr inbounds float, float* %4, i64 %3441
  %3447 = bitcast float* %3446 to <4 x float>*
  store <4 x float> %wide.load35.28, <4 x float>* %3447, align 4, !tbaa !3167
  %3448 = getelementptr inbounds float, float* %3446, i64 4
  %3449 = bitcast float* %3448 to <4 x float>*
  store <4 x float> %wide.load36.28, <4 x float>* %3449, align 4, !tbaa !3167
  %3450 = add nsw i64 %21, 3048
  %3451 = getelementptr inbounds float, float* %7, i64 %3450
  %3452 = bitcast float* %3451 to <4 x float>*
  %wide.load35.29 = load <4 x float>, <4 x float>* %3452, align 4, !tbaa !3164
  %3453 = getelementptr inbounds float, float* %3451, i64 4
  %3454 = bitcast float* %3453 to <4 x float>*
  %wide.load36.29 = load <4 x float>, <4 x float>* %3454, align 4, !tbaa !3164
  %3455 = getelementptr inbounds float, float* %4, i64 %3450
  %3456 = bitcast float* %3455 to <4 x float>*
  store <4 x float> %wide.load35.29, <4 x float>* %3456, align 4, !tbaa !3167
  %3457 = getelementptr inbounds float, float* %3455, i64 4
  %3458 = bitcast float* %3457 to <4 x float>*
  store <4 x float> %wide.load36.29, <4 x float>* %3458, align 4, !tbaa !3167
  %3459 = add nsw i64 %21, 3056
  %3460 = getelementptr inbounds float, float* %7, i64 %3459
  %3461 = bitcast float* %3460 to <4 x float>*
  %wide.load35.30 = load <4 x float>, <4 x float>* %3461, align 4, !tbaa !3164
  %3462 = getelementptr inbounds float, float* %3460, i64 4
  %3463 = bitcast float* %3462 to <4 x float>*
  %wide.load36.30 = load <4 x float>, <4 x float>* %3463, align 4, !tbaa !3164
  %3464 = getelementptr inbounds float, float* %4, i64 %3459
  %3465 = bitcast float* %3464 to <4 x float>*
  store <4 x float> %wide.load35.30, <4 x float>* %3465, align 4, !tbaa !3167
  %3466 = getelementptr inbounds float, float* %3464, i64 4
  %3467 = bitcast float* %3466 to <4 x float>*
  store <4 x float> %wide.load36.30, <4 x float>* %3467, align 4, !tbaa !3167
  %3468 = add nsw i64 %21, 3064
  %3469 = getelementptr inbounds float, float* %7, i64 %3468
  %3470 = bitcast float* %3469 to <4 x float>*
  %wide.load35.31 = load <4 x float>, <4 x float>* %3470, align 4, !tbaa !3164
  %3471 = getelementptr inbounds float, float* %3469, i64 4
  %3472 = bitcast float* %3471 to <4 x float>*
  %wide.load36.31 = load <4 x float>, <4 x float>* %3472, align 4, !tbaa !3164
  %3473 = getelementptr inbounds float, float* %4, i64 %3468
  %3474 = bitcast float* %3473 to <4 x float>*
  store <4 x float> %wide.load35.31, <4 x float>* %3474, align 4, !tbaa !3167
  %3475 = getelementptr inbounds float, float* %3473, i64 4
  %3476 = bitcast float* %3475 to <4 x float>*
  store <4 x float> %wide.load36.31, <4 x float>* %3476, align 4, !tbaa !3167
  %3477 = add nsw i64 %21, 3072
  %3478 = getelementptr inbounds float, float* %7, i64 %3477
  %3479 = bitcast float* %3478 to <4 x float>*
  %wide.load25 = load <4 x float>, <4 x float>* %3479, align 4, !tbaa !3164
  %3480 = getelementptr inbounds float, float* %3478, i64 4
  %3481 = bitcast float* %3480 to <4 x float>*
  %wide.load26 = load <4 x float>, <4 x float>* %3481, align 4, !tbaa !3164
  %3482 = getelementptr inbounds float, float* %4, i64 %3477
  %3483 = bitcast float* %3482 to <4 x float>*
  store <4 x float> %wide.load25, <4 x float>* %3483, align 4, !tbaa !3167
  %3484 = getelementptr inbounds float, float* %3482, i64 4
  %3485 = bitcast float* %3484 to <4 x float>*
  store <4 x float> %wide.load26, <4 x float>* %3485, align 4, !tbaa !3167
  %3486 = add nsw i64 %21, 3080
  %3487 = getelementptr inbounds float, float* %7, i64 %3486
  %3488 = bitcast float* %3487 to <4 x float>*
  %wide.load25.1 = load <4 x float>, <4 x float>* %3488, align 4, !tbaa !3164
  %3489 = getelementptr inbounds float, float* %3487, i64 4
  %3490 = bitcast float* %3489 to <4 x float>*
  %wide.load26.1 = load <4 x float>, <4 x float>* %3490, align 4, !tbaa !3164
  %3491 = getelementptr inbounds float, float* %4, i64 %3486
  %3492 = bitcast float* %3491 to <4 x float>*
  store <4 x float> %wide.load25.1, <4 x float>* %3492, align 4, !tbaa !3167
  %3493 = getelementptr inbounds float, float* %3491, i64 4
  %3494 = bitcast float* %3493 to <4 x float>*
  store <4 x float> %wide.load26.1, <4 x float>* %3494, align 4, !tbaa !3167
  %3495 = add nsw i64 %21, 3088
  %3496 = getelementptr inbounds float, float* %7, i64 %3495
  %3497 = bitcast float* %3496 to <4 x float>*
  %wide.load25.2 = load <4 x float>, <4 x float>* %3497, align 4, !tbaa !3164
  %3498 = getelementptr inbounds float, float* %3496, i64 4
  %3499 = bitcast float* %3498 to <4 x float>*
  %wide.load26.2 = load <4 x float>, <4 x float>* %3499, align 4, !tbaa !3164
  %3500 = getelementptr inbounds float, float* %4, i64 %3495
  %3501 = bitcast float* %3500 to <4 x float>*
  store <4 x float> %wide.load25.2, <4 x float>* %3501, align 4, !tbaa !3167
  %3502 = getelementptr inbounds float, float* %3500, i64 4
  %3503 = bitcast float* %3502 to <4 x float>*
  store <4 x float> %wide.load26.2, <4 x float>* %3503, align 4, !tbaa !3167
  %3504 = add nsw i64 %21, 3096
  %3505 = getelementptr inbounds float, float* %7, i64 %3504
  %3506 = bitcast float* %3505 to <4 x float>*
  %wide.load25.3 = load <4 x float>, <4 x float>* %3506, align 4, !tbaa !3164
  %3507 = getelementptr inbounds float, float* %3505, i64 4
  %3508 = bitcast float* %3507 to <4 x float>*
  %wide.load26.3 = load <4 x float>, <4 x float>* %3508, align 4, !tbaa !3164
  %3509 = getelementptr inbounds float, float* %4, i64 %3504
  %3510 = bitcast float* %3509 to <4 x float>*
  store <4 x float> %wide.load25.3, <4 x float>* %3510, align 4, !tbaa !3167
  %3511 = getelementptr inbounds float, float* %3509, i64 4
  %3512 = bitcast float* %3511 to <4 x float>*
  store <4 x float> %wide.load26.3, <4 x float>* %3512, align 4, !tbaa !3167
  %3513 = add nsw i64 %21, 3104
  %3514 = getelementptr inbounds float, float* %7, i64 %3513
  %3515 = bitcast float* %3514 to <4 x float>*
  %wide.load25.4 = load <4 x float>, <4 x float>* %3515, align 4, !tbaa !3164
  %3516 = getelementptr inbounds float, float* %3514, i64 4
  %3517 = bitcast float* %3516 to <4 x float>*
  %wide.load26.4 = load <4 x float>, <4 x float>* %3517, align 4, !tbaa !3164
  %3518 = getelementptr inbounds float, float* %4, i64 %3513
  %3519 = bitcast float* %3518 to <4 x float>*
  store <4 x float> %wide.load25.4, <4 x float>* %3519, align 4, !tbaa !3167
  %3520 = getelementptr inbounds float, float* %3518, i64 4
  %3521 = bitcast float* %3520 to <4 x float>*
  store <4 x float> %wide.load26.4, <4 x float>* %3521, align 4, !tbaa !3167
  %3522 = add nsw i64 %21, 3112
  %3523 = getelementptr inbounds float, float* %7, i64 %3522
  %3524 = bitcast float* %3523 to <4 x float>*
  %wide.load25.5 = load <4 x float>, <4 x float>* %3524, align 4, !tbaa !3164
  %3525 = getelementptr inbounds float, float* %3523, i64 4
  %3526 = bitcast float* %3525 to <4 x float>*
  %wide.load26.5 = load <4 x float>, <4 x float>* %3526, align 4, !tbaa !3164
  %3527 = getelementptr inbounds float, float* %4, i64 %3522
  %3528 = bitcast float* %3527 to <4 x float>*
  store <4 x float> %wide.load25.5, <4 x float>* %3528, align 4, !tbaa !3167
  %3529 = getelementptr inbounds float, float* %3527, i64 4
  %3530 = bitcast float* %3529 to <4 x float>*
  store <4 x float> %wide.load26.5, <4 x float>* %3530, align 4, !tbaa !3167
  %3531 = add nsw i64 %21, 3120
  %3532 = getelementptr inbounds float, float* %7, i64 %3531
  %3533 = bitcast float* %3532 to <4 x float>*
  %wide.load25.6 = load <4 x float>, <4 x float>* %3533, align 4, !tbaa !3164
  %3534 = getelementptr inbounds float, float* %3532, i64 4
  %3535 = bitcast float* %3534 to <4 x float>*
  %wide.load26.6 = load <4 x float>, <4 x float>* %3535, align 4, !tbaa !3164
  %3536 = getelementptr inbounds float, float* %4, i64 %3531
  %3537 = bitcast float* %3536 to <4 x float>*
  store <4 x float> %wide.load25.6, <4 x float>* %3537, align 4, !tbaa !3167
  %3538 = getelementptr inbounds float, float* %3536, i64 4
  %3539 = bitcast float* %3538 to <4 x float>*
  store <4 x float> %wide.load26.6, <4 x float>* %3539, align 4, !tbaa !3167
  %3540 = add nsw i64 %21, 3128
  %3541 = getelementptr inbounds float, float* %7, i64 %3540
  %3542 = bitcast float* %3541 to <4 x float>*
  %wide.load25.7 = load <4 x float>, <4 x float>* %3542, align 4, !tbaa !3164
  %3543 = getelementptr inbounds float, float* %3541, i64 4
  %3544 = bitcast float* %3543 to <4 x float>*
  %wide.load26.7 = load <4 x float>, <4 x float>* %3544, align 4, !tbaa !3164
  %3545 = getelementptr inbounds float, float* %4, i64 %3540
  %3546 = bitcast float* %3545 to <4 x float>*
  store <4 x float> %wide.load25.7, <4 x float>* %3546, align 4, !tbaa !3167
  %3547 = getelementptr inbounds float, float* %3545, i64 4
  %3548 = bitcast float* %3547 to <4 x float>*
  store <4 x float> %wide.load26.7, <4 x float>* %3548, align 4, !tbaa !3167
  %3549 = add nsw i64 %21, 3136
  %3550 = getelementptr inbounds float, float* %7, i64 %3549
  %3551 = bitcast float* %3550 to <4 x float>*
  %wide.load25.8 = load <4 x float>, <4 x float>* %3551, align 4, !tbaa !3164
  %3552 = getelementptr inbounds float, float* %3550, i64 4
  %3553 = bitcast float* %3552 to <4 x float>*
  %wide.load26.8 = load <4 x float>, <4 x float>* %3553, align 4, !tbaa !3164
  %3554 = getelementptr inbounds float, float* %4, i64 %3549
  %3555 = bitcast float* %3554 to <4 x float>*
  store <4 x float> %wide.load25.8, <4 x float>* %3555, align 4, !tbaa !3167
  %3556 = getelementptr inbounds float, float* %3554, i64 4
  %3557 = bitcast float* %3556 to <4 x float>*
  store <4 x float> %wide.load26.8, <4 x float>* %3557, align 4, !tbaa !3167
  %3558 = add nsw i64 %21, 3144
  %3559 = getelementptr inbounds float, float* %7, i64 %3558
  %3560 = bitcast float* %3559 to <4 x float>*
  %wide.load25.9 = load <4 x float>, <4 x float>* %3560, align 4, !tbaa !3164
  %3561 = getelementptr inbounds float, float* %3559, i64 4
  %3562 = bitcast float* %3561 to <4 x float>*
  %wide.load26.9 = load <4 x float>, <4 x float>* %3562, align 4, !tbaa !3164
  %3563 = getelementptr inbounds float, float* %4, i64 %3558
  %3564 = bitcast float* %3563 to <4 x float>*
  store <4 x float> %wide.load25.9, <4 x float>* %3564, align 4, !tbaa !3167
  %3565 = getelementptr inbounds float, float* %3563, i64 4
  %3566 = bitcast float* %3565 to <4 x float>*
  store <4 x float> %wide.load26.9, <4 x float>* %3566, align 4, !tbaa !3167
  %3567 = add nsw i64 %21, 3152
  %3568 = getelementptr inbounds float, float* %7, i64 %3567
  %3569 = bitcast float* %3568 to <4 x float>*
  %wide.load25.10 = load <4 x float>, <4 x float>* %3569, align 4, !tbaa !3164
  %3570 = getelementptr inbounds float, float* %3568, i64 4
  %3571 = bitcast float* %3570 to <4 x float>*
  %wide.load26.10 = load <4 x float>, <4 x float>* %3571, align 4, !tbaa !3164
  %3572 = getelementptr inbounds float, float* %4, i64 %3567
  %3573 = bitcast float* %3572 to <4 x float>*
  store <4 x float> %wide.load25.10, <4 x float>* %3573, align 4, !tbaa !3167
  %3574 = getelementptr inbounds float, float* %3572, i64 4
  %3575 = bitcast float* %3574 to <4 x float>*
  store <4 x float> %wide.load26.10, <4 x float>* %3575, align 4, !tbaa !3167
  %3576 = add nsw i64 %21, 3160
  %3577 = getelementptr inbounds float, float* %7, i64 %3576
  %3578 = bitcast float* %3577 to <4 x float>*
  %wide.load25.11 = load <4 x float>, <4 x float>* %3578, align 4, !tbaa !3164
  %3579 = getelementptr inbounds float, float* %3577, i64 4
  %3580 = bitcast float* %3579 to <4 x float>*
  %wide.load26.11 = load <4 x float>, <4 x float>* %3580, align 4, !tbaa !3164
  %3581 = getelementptr inbounds float, float* %4, i64 %3576
  %3582 = bitcast float* %3581 to <4 x float>*
  store <4 x float> %wide.load25.11, <4 x float>* %3582, align 4, !tbaa !3167
  %3583 = getelementptr inbounds float, float* %3581, i64 4
  %3584 = bitcast float* %3583 to <4 x float>*
  store <4 x float> %wide.load26.11, <4 x float>* %3584, align 4, !tbaa !3167
  %3585 = add nsw i64 %21, 3168
  %3586 = getelementptr inbounds float, float* %7, i64 %3585
  %3587 = bitcast float* %3586 to <4 x float>*
  %wide.load25.12 = load <4 x float>, <4 x float>* %3587, align 4, !tbaa !3164
  %3588 = getelementptr inbounds float, float* %3586, i64 4
  %3589 = bitcast float* %3588 to <4 x float>*
  %wide.load26.12 = load <4 x float>, <4 x float>* %3589, align 4, !tbaa !3164
  %3590 = getelementptr inbounds float, float* %4, i64 %3585
  %3591 = bitcast float* %3590 to <4 x float>*
  store <4 x float> %wide.load25.12, <4 x float>* %3591, align 4, !tbaa !3167
  %3592 = getelementptr inbounds float, float* %3590, i64 4
  %3593 = bitcast float* %3592 to <4 x float>*
  store <4 x float> %wide.load26.12, <4 x float>* %3593, align 4, !tbaa !3167
  %3594 = add nsw i64 %21, 3176
  %3595 = getelementptr inbounds float, float* %7, i64 %3594
  %3596 = bitcast float* %3595 to <4 x float>*
  %wide.load25.13 = load <4 x float>, <4 x float>* %3596, align 4, !tbaa !3164
  %3597 = getelementptr inbounds float, float* %3595, i64 4
  %3598 = bitcast float* %3597 to <4 x float>*
  %wide.load26.13 = load <4 x float>, <4 x float>* %3598, align 4, !tbaa !3164
  %3599 = getelementptr inbounds float, float* %4, i64 %3594
  %3600 = bitcast float* %3599 to <4 x float>*
  store <4 x float> %wide.load25.13, <4 x float>* %3600, align 4, !tbaa !3167
  %3601 = getelementptr inbounds float, float* %3599, i64 4
  %3602 = bitcast float* %3601 to <4 x float>*
  store <4 x float> %wide.load26.13, <4 x float>* %3602, align 4, !tbaa !3167
  %3603 = add nsw i64 %21, 3184
  %3604 = getelementptr inbounds float, float* %7, i64 %3603
  %3605 = bitcast float* %3604 to <4 x float>*
  %wide.load25.14 = load <4 x float>, <4 x float>* %3605, align 4, !tbaa !3164
  %3606 = getelementptr inbounds float, float* %3604, i64 4
  %3607 = bitcast float* %3606 to <4 x float>*
  %wide.load26.14 = load <4 x float>, <4 x float>* %3607, align 4, !tbaa !3164
  %3608 = getelementptr inbounds float, float* %4, i64 %3603
  %3609 = bitcast float* %3608 to <4 x float>*
  store <4 x float> %wide.load25.14, <4 x float>* %3609, align 4, !tbaa !3167
  %3610 = getelementptr inbounds float, float* %3608, i64 4
  %3611 = bitcast float* %3610 to <4 x float>*
  store <4 x float> %wide.load26.14, <4 x float>* %3611, align 4, !tbaa !3167
  %3612 = add nsw i64 %21, 3192
  %3613 = getelementptr inbounds float, float* %7, i64 %3612
  %3614 = bitcast float* %3613 to <4 x float>*
  %wide.load25.15 = load <4 x float>, <4 x float>* %3614, align 4, !tbaa !3164
  %3615 = getelementptr inbounds float, float* %3613, i64 4
  %3616 = bitcast float* %3615 to <4 x float>*
  %wide.load26.15 = load <4 x float>, <4 x float>* %3616, align 4, !tbaa !3164
  %3617 = getelementptr inbounds float, float* %4, i64 %3612
  %3618 = bitcast float* %3617 to <4 x float>*
  store <4 x float> %wide.load25.15, <4 x float>* %3618, align 4, !tbaa !3167
  %3619 = getelementptr inbounds float, float* %3617, i64 4
  %3620 = bitcast float* %3619 to <4 x float>*
  store <4 x float> %wide.load26.15, <4 x float>* %3620, align 4, !tbaa !3167
  %3621 = add nsw i64 %21, 3200
  %3622 = getelementptr inbounds float, float* %7, i64 %3621
  %3623 = bitcast float* %3622 to <4 x float>*
  %wide.load25.16 = load <4 x float>, <4 x float>* %3623, align 4, !tbaa !3164
  %3624 = getelementptr inbounds float, float* %3622, i64 4
  %3625 = bitcast float* %3624 to <4 x float>*
  %wide.load26.16 = load <4 x float>, <4 x float>* %3625, align 4, !tbaa !3164
  %3626 = getelementptr inbounds float, float* %4, i64 %3621
  %3627 = bitcast float* %3626 to <4 x float>*
  store <4 x float> %wide.load25.16, <4 x float>* %3627, align 4, !tbaa !3167
  %3628 = getelementptr inbounds float, float* %3626, i64 4
  %3629 = bitcast float* %3628 to <4 x float>*
  store <4 x float> %wide.load26.16, <4 x float>* %3629, align 4, !tbaa !3167
  %3630 = add nsw i64 %21, 3208
  %3631 = getelementptr inbounds float, float* %7, i64 %3630
  %3632 = bitcast float* %3631 to <4 x float>*
  %wide.load25.17 = load <4 x float>, <4 x float>* %3632, align 4, !tbaa !3164
  %3633 = getelementptr inbounds float, float* %3631, i64 4
  %3634 = bitcast float* %3633 to <4 x float>*
  %wide.load26.17 = load <4 x float>, <4 x float>* %3634, align 4, !tbaa !3164
  %3635 = getelementptr inbounds float, float* %4, i64 %3630
  %3636 = bitcast float* %3635 to <4 x float>*
  store <4 x float> %wide.load25.17, <4 x float>* %3636, align 4, !tbaa !3167
  %3637 = getelementptr inbounds float, float* %3635, i64 4
  %3638 = bitcast float* %3637 to <4 x float>*
  store <4 x float> %wide.load26.17, <4 x float>* %3638, align 4, !tbaa !3167
  %3639 = add nsw i64 %21, 3216
  %3640 = getelementptr inbounds float, float* %7, i64 %3639
  %3641 = bitcast float* %3640 to <4 x float>*
  %wide.load25.18 = load <4 x float>, <4 x float>* %3641, align 4, !tbaa !3164
  %3642 = getelementptr inbounds float, float* %3640, i64 4
  %3643 = bitcast float* %3642 to <4 x float>*
  %wide.load26.18 = load <4 x float>, <4 x float>* %3643, align 4, !tbaa !3164
  %3644 = getelementptr inbounds float, float* %4, i64 %3639
  %3645 = bitcast float* %3644 to <4 x float>*
  store <4 x float> %wide.load25.18, <4 x float>* %3645, align 4, !tbaa !3167
  %3646 = getelementptr inbounds float, float* %3644, i64 4
  %3647 = bitcast float* %3646 to <4 x float>*
  store <4 x float> %wide.load26.18, <4 x float>* %3647, align 4, !tbaa !3167
  %3648 = add nsw i64 %21, 3224
  %3649 = getelementptr inbounds float, float* %7, i64 %3648
  %3650 = bitcast float* %3649 to <4 x float>*
  %wide.load25.19 = load <4 x float>, <4 x float>* %3650, align 4, !tbaa !3164
  %3651 = getelementptr inbounds float, float* %3649, i64 4
  %3652 = bitcast float* %3651 to <4 x float>*
  %wide.load26.19 = load <4 x float>, <4 x float>* %3652, align 4, !tbaa !3164
  %3653 = getelementptr inbounds float, float* %4, i64 %3648
  %3654 = bitcast float* %3653 to <4 x float>*
  store <4 x float> %wide.load25.19, <4 x float>* %3654, align 4, !tbaa !3167
  %3655 = getelementptr inbounds float, float* %3653, i64 4
  %3656 = bitcast float* %3655 to <4 x float>*
  store <4 x float> %wide.load26.19, <4 x float>* %3656, align 4, !tbaa !3167
  %3657 = add nsw i64 %21, 3232
  %3658 = getelementptr inbounds float, float* %7, i64 %3657
  %3659 = bitcast float* %3658 to <4 x float>*
  %wide.load25.20 = load <4 x float>, <4 x float>* %3659, align 4, !tbaa !3164
  %3660 = getelementptr inbounds float, float* %3658, i64 4
  %3661 = bitcast float* %3660 to <4 x float>*
  %wide.load26.20 = load <4 x float>, <4 x float>* %3661, align 4, !tbaa !3164
  %3662 = getelementptr inbounds float, float* %4, i64 %3657
  %3663 = bitcast float* %3662 to <4 x float>*
  store <4 x float> %wide.load25.20, <4 x float>* %3663, align 4, !tbaa !3167
  %3664 = getelementptr inbounds float, float* %3662, i64 4
  %3665 = bitcast float* %3664 to <4 x float>*
  store <4 x float> %wide.load26.20, <4 x float>* %3665, align 4, !tbaa !3167
  %3666 = add nsw i64 %21, 3240
  %3667 = getelementptr inbounds float, float* %7, i64 %3666
  %3668 = bitcast float* %3667 to <4 x float>*
  %wide.load25.21 = load <4 x float>, <4 x float>* %3668, align 4, !tbaa !3164
  %3669 = getelementptr inbounds float, float* %3667, i64 4
  %3670 = bitcast float* %3669 to <4 x float>*
  %wide.load26.21 = load <4 x float>, <4 x float>* %3670, align 4, !tbaa !3164
  %3671 = getelementptr inbounds float, float* %4, i64 %3666
  %3672 = bitcast float* %3671 to <4 x float>*
  store <4 x float> %wide.load25.21, <4 x float>* %3672, align 4, !tbaa !3167
  %3673 = getelementptr inbounds float, float* %3671, i64 4
  %3674 = bitcast float* %3673 to <4 x float>*
  store <4 x float> %wide.load26.21, <4 x float>* %3674, align 4, !tbaa !3167
  %3675 = add nsw i64 %21, 3248
  %3676 = getelementptr inbounds float, float* %7, i64 %3675
  %3677 = bitcast float* %3676 to <4 x float>*
  %wide.load25.22 = load <4 x float>, <4 x float>* %3677, align 4, !tbaa !3164
  %3678 = getelementptr inbounds float, float* %3676, i64 4
  %3679 = bitcast float* %3678 to <4 x float>*
  %wide.load26.22 = load <4 x float>, <4 x float>* %3679, align 4, !tbaa !3164
  %3680 = getelementptr inbounds float, float* %4, i64 %3675
  %3681 = bitcast float* %3680 to <4 x float>*
  store <4 x float> %wide.load25.22, <4 x float>* %3681, align 4, !tbaa !3167
  %3682 = getelementptr inbounds float, float* %3680, i64 4
  %3683 = bitcast float* %3682 to <4 x float>*
  store <4 x float> %wide.load26.22, <4 x float>* %3683, align 4, !tbaa !3167
  %3684 = add nsw i64 %21, 3256
  %3685 = getelementptr inbounds float, float* %7, i64 %3684
  %3686 = bitcast float* %3685 to <4 x float>*
  %wide.load25.23 = load <4 x float>, <4 x float>* %3686, align 4, !tbaa !3164
  %3687 = getelementptr inbounds float, float* %3685, i64 4
  %3688 = bitcast float* %3687 to <4 x float>*
  %wide.load26.23 = load <4 x float>, <4 x float>* %3688, align 4, !tbaa !3164
  %3689 = getelementptr inbounds float, float* %4, i64 %3684
  %3690 = bitcast float* %3689 to <4 x float>*
  store <4 x float> %wide.load25.23, <4 x float>* %3690, align 4, !tbaa !3167
  %3691 = getelementptr inbounds float, float* %3689, i64 4
  %3692 = bitcast float* %3691 to <4 x float>*
  store <4 x float> %wide.load26.23, <4 x float>* %3692, align 4, !tbaa !3167
  %3693 = add nsw i64 %21, 3264
  %3694 = getelementptr inbounds float, float* %7, i64 %3693
  %3695 = bitcast float* %3694 to <4 x float>*
  %wide.load25.24 = load <4 x float>, <4 x float>* %3695, align 4, !tbaa !3164
  %3696 = getelementptr inbounds float, float* %3694, i64 4
  %3697 = bitcast float* %3696 to <4 x float>*
  %wide.load26.24 = load <4 x float>, <4 x float>* %3697, align 4, !tbaa !3164
  %3698 = getelementptr inbounds float, float* %4, i64 %3693
  %3699 = bitcast float* %3698 to <4 x float>*
  store <4 x float> %wide.load25.24, <4 x float>* %3699, align 4, !tbaa !3167
  %3700 = getelementptr inbounds float, float* %3698, i64 4
  %3701 = bitcast float* %3700 to <4 x float>*
  store <4 x float> %wide.load26.24, <4 x float>* %3701, align 4, !tbaa !3167
  %3702 = add nsw i64 %21, 3272
  %3703 = getelementptr inbounds float, float* %7, i64 %3702
  %3704 = bitcast float* %3703 to <4 x float>*
  %wide.load25.25 = load <4 x float>, <4 x float>* %3704, align 4, !tbaa !3164
  %3705 = getelementptr inbounds float, float* %3703, i64 4
  %3706 = bitcast float* %3705 to <4 x float>*
  %wide.load26.25 = load <4 x float>, <4 x float>* %3706, align 4, !tbaa !3164
  %3707 = getelementptr inbounds float, float* %4, i64 %3702
  %3708 = bitcast float* %3707 to <4 x float>*
  store <4 x float> %wide.load25.25, <4 x float>* %3708, align 4, !tbaa !3167
  %3709 = getelementptr inbounds float, float* %3707, i64 4
  %3710 = bitcast float* %3709 to <4 x float>*
  store <4 x float> %wide.load26.25, <4 x float>* %3710, align 4, !tbaa !3167
  %3711 = add nsw i64 %21, 3280
  %3712 = getelementptr inbounds float, float* %7, i64 %3711
  %3713 = bitcast float* %3712 to <4 x float>*
  %wide.load25.26 = load <4 x float>, <4 x float>* %3713, align 4, !tbaa !3164
  %3714 = getelementptr inbounds float, float* %3712, i64 4
  %3715 = bitcast float* %3714 to <4 x float>*
  %wide.load26.26 = load <4 x float>, <4 x float>* %3715, align 4, !tbaa !3164
  %3716 = getelementptr inbounds float, float* %4, i64 %3711
  %3717 = bitcast float* %3716 to <4 x float>*
  store <4 x float> %wide.load25.26, <4 x float>* %3717, align 4, !tbaa !3167
  %3718 = getelementptr inbounds float, float* %3716, i64 4
  %3719 = bitcast float* %3718 to <4 x float>*
  store <4 x float> %wide.load26.26, <4 x float>* %3719, align 4, !tbaa !3167
  %3720 = add nsw i64 %21, 3288
  %3721 = getelementptr inbounds float, float* %7, i64 %3720
  %3722 = bitcast float* %3721 to <4 x float>*
  %wide.load25.27 = load <4 x float>, <4 x float>* %3722, align 4, !tbaa !3164
  %3723 = getelementptr inbounds float, float* %3721, i64 4
  %3724 = bitcast float* %3723 to <4 x float>*
  %wide.load26.27 = load <4 x float>, <4 x float>* %3724, align 4, !tbaa !3164
  %3725 = getelementptr inbounds float, float* %4, i64 %3720
  %3726 = bitcast float* %3725 to <4 x float>*
  store <4 x float> %wide.load25.27, <4 x float>* %3726, align 4, !tbaa !3167
  %3727 = getelementptr inbounds float, float* %3725, i64 4
  %3728 = bitcast float* %3727 to <4 x float>*
  store <4 x float> %wide.load26.27, <4 x float>* %3728, align 4, !tbaa !3167
  %3729 = add nsw i64 %21, 3296
  %3730 = getelementptr inbounds float, float* %7, i64 %3729
  %3731 = bitcast float* %3730 to <4 x float>*
  %wide.load25.28 = load <4 x float>, <4 x float>* %3731, align 4, !tbaa !3164
  %3732 = getelementptr inbounds float, float* %3730, i64 4
  %3733 = bitcast float* %3732 to <4 x float>*
  %wide.load26.28 = load <4 x float>, <4 x float>* %3733, align 4, !tbaa !3164
  %3734 = getelementptr inbounds float, float* %4, i64 %3729
  %3735 = bitcast float* %3734 to <4 x float>*
  store <4 x float> %wide.load25.28, <4 x float>* %3735, align 4, !tbaa !3167
  %3736 = getelementptr inbounds float, float* %3734, i64 4
  %3737 = bitcast float* %3736 to <4 x float>*
  store <4 x float> %wide.load26.28, <4 x float>* %3737, align 4, !tbaa !3167
  %3738 = add nsw i64 %21, 3304
  %3739 = getelementptr inbounds float, float* %7, i64 %3738
  %3740 = bitcast float* %3739 to <4 x float>*
  %wide.load25.29 = load <4 x float>, <4 x float>* %3740, align 4, !tbaa !3164
  %3741 = getelementptr inbounds float, float* %3739, i64 4
  %3742 = bitcast float* %3741 to <4 x float>*
  %wide.load26.29 = load <4 x float>, <4 x float>* %3742, align 4, !tbaa !3164
  %3743 = getelementptr inbounds float, float* %4, i64 %3738
  %3744 = bitcast float* %3743 to <4 x float>*
  store <4 x float> %wide.load25.29, <4 x float>* %3744, align 4, !tbaa !3167
  %3745 = getelementptr inbounds float, float* %3743, i64 4
  %3746 = bitcast float* %3745 to <4 x float>*
  store <4 x float> %wide.load26.29, <4 x float>* %3746, align 4, !tbaa !3167
  %3747 = add nsw i64 %21, 3312
  %3748 = getelementptr inbounds float, float* %7, i64 %3747
  %3749 = bitcast float* %3748 to <4 x float>*
  %wide.load25.30 = load <4 x float>, <4 x float>* %3749, align 4, !tbaa !3164
  %3750 = getelementptr inbounds float, float* %3748, i64 4
  %3751 = bitcast float* %3750 to <4 x float>*
  %wide.load26.30 = load <4 x float>, <4 x float>* %3751, align 4, !tbaa !3164
  %3752 = getelementptr inbounds float, float* %4, i64 %3747
  %3753 = bitcast float* %3752 to <4 x float>*
  store <4 x float> %wide.load25.30, <4 x float>* %3753, align 4, !tbaa !3167
  %3754 = getelementptr inbounds float, float* %3752, i64 4
  %3755 = bitcast float* %3754 to <4 x float>*
  store <4 x float> %wide.load26.30, <4 x float>* %3755, align 4, !tbaa !3167
  %3756 = add nsw i64 %21, 3320
  %3757 = getelementptr inbounds float, float* %7, i64 %3756
  %3758 = bitcast float* %3757 to <4 x float>*
  %wide.load25.31 = load <4 x float>, <4 x float>* %3758, align 4, !tbaa !3164
  %3759 = getelementptr inbounds float, float* %3757, i64 4
  %3760 = bitcast float* %3759 to <4 x float>*
  %wide.load26.31 = load <4 x float>, <4 x float>* %3760, align 4, !tbaa !3164
  %3761 = getelementptr inbounds float, float* %4, i64 %3756
  %3762 = bitcast float* %3761 to <4 x float>*
  store <4 x float> %wide.load25.31, <4 x float>* %3762, align 4, !tbaa !3167
  %3763 = getelementptr inbounds float, float* %3761, i64 4
  %3764 = bitcast float* %3763 to <4 x float>*
  store <4 x float> %wide.load26.31, <4 x float>* %3764, align 4, !tbaa !3167
  %3765 = add nsw i64 %21, 3328
  %3766 = getelementptr inbounds float, float* %7, i64 %3765
  %3767 = bitcast float* %3766 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %3767, align 4, !tbaa !3164
  %3768 = getelementptr inbounds float, float* %3766, i64 4
  %3769 = bitcast float* %3768 to <4 x float>*
  %wide.load16 = load <4 x float>, <4 x float>* %3769, align 4, !tbaa !3164
  %3770 = getelementptr inbounds float, float* %4, i64 %3765
  %3771 = bitcast float* %3770 to <4 x float>*
  store <4 x float> %wide.load, <4 x float>* %3771, align 4, !tbaa !3167
  %3772 = getelementptr inbounds float, float* %3770, i64 4
  %3773 = bitcast float* %3772 to <4 x float>*
  store <4 x float> %wide.load16, <4 x float>* %3773, align 4, !tbaa !3167
  %3774 = add nsw i64 %21, 3336
  %3775 = getelementptr inbounds float, float* %7, i64 %3774
  %3776 = bitcast float* %3775 to <4 x float>*
  %wide.load.1 = load <4 x float>, <4 x float>* %3776, align 4, !tbaa !3164
  %3777 = getelementptr inbounds float, float* %3775, i64 4
  %3778 = bitcast float* %3777 to <4 x float>*
  %wide.load16.1 = load <4 x float>, <4 x float>* %3778, align 4, !tbaa !3164
  %3779 = getelementptr inbounds float, float* %4, i64 %3774
  %3780 = bitcast float* %3779 to <4 x float>*
  store <4 x float> %wide.load.1, <4 x float>* %3780, align 4, !tbaa !3167
  %3781 = getelementptr inbounds float, float* %3779, i64 4
  %3782 = bitcast float* %3781 to <4 x float>*
  store <4 x float> %wide.load16.1, <4 x float>* %3782, align 4, !tbaa !3167
  %3783 = add nsw i64 %21, 3344
  %3784 = getelementptr inbounds float, float* %7, i64 %3783
  %3785 = bitcast float* %3784 to <4 x float>*
  %wide.load.2 = load <4 x float>, <4 x float>* %3785, align 4, !tbaa !3164
  %3786 = getelementptr inbounds float, float* %3784, i64 4
  %3787 = bitcast float* %3786 to <4 x float>*
  %wide.load16.2 = load <4 x float>, <4 x float>* %3787, align 4, !tbaa !3164
  %3788 = getelementptr inbounds float, float* %4, i64 %3783
  %3789 = bitcast float* %3788 to <4 x float>*
  store <4 x float> %wide.load.2, <4 x float>* %3789, align 4, !tbaa !3167
  %3790 = getelementptr inbounds float, float* %3788, i64 4
  %3791 = bitcast float* %3790 to <4 x float>*
  store <4 x float> %wide.load16.2, <4 x float>* %3791, align 4, !tbaa !3167
  %3792 = add nsw i64 %21, 3352
  %3793 = getelementptr inbounds float, float* %7, i64 %3792
  %3794 = bitcast float* %3793 to <4 x float>*
  %wide.load.3 = load <4 x float>, <4 x float>* %3794, align 4, !tbaa !3164
  %3795 = getelementptr inbounds float, float* %3793, i64 4
  %3796 = bitcast float* %3795 to <4 x float>*
  %wide.load16.3 = load <4 x float>, <4 x float>* %3796, align 4, !tbaa !3164
  %3797 = getelementptr inbounds float, float* %4, i64 %3792
  %3798 = bitcast float* %3797 to <4 x float>*
  store <4 x float> %wide.load.3, <4 x float>* %3798, align 4, !tbaa !3167
  %3799 = getelementptr inbounds float, float* %3797, i64 4
  %3800 = bitcast float* %3799 to <4 x float>*
  store <4 x float> %wide.load16.3, <4 x float>* %3800, align 4, !tbaa !3167
  %3801 = add nsw i64 %21, 3360
  %3802 = getelementptr inbounds float, float* %7, i64 %3801
  %3803 = bitcast float* %3802 to <4 x float>*
  %wide.load.4 = load <4 x float>, <4 x float>* %3803, align 4, !tbaa !3164
  %3804 = getelementptr inbounds float, float* %3802, i64 4
  %3805 = bitcast float* %3804 to <4 x float>*
  %wide.load16.4 = load <4 x float>, <4 x float>* %3805, align 4, !tbaa !3164
  %3806 = getelementptr inbounds float, float* %4, i64 %3801
  %3807 = bitcast float* %3806 to <4 x float>*
  store <4 x float> %wide.load.4, <4 x float>* %3807, align 4, !tbaa !3167
  %3808 = getelementptr inbounds float, float* %3806, i64 4
  %3809 = bitcast float* %3808 to <4 x float>*
  store <4 x float> %wide.load16.4, <4 x float>* %3809, align 4, !tbaa !3167
  %3810 = add nsw i64 %21, 3368
  %3811 = getelementptr inbounds float, float* %7, i64 %3810
  %3812 = bitcast float* %3811 to <4 x float>*
  %wide.load.5 = load <4 x float>, <4 x float>* %3812, align 4, !tbaa !3164
  %3813 = getelementptr inbounds float, float* %3811, i64 4
  %3814 = bitcast float* %3813 to <4 x float>*
  %wide.load16.5 = load <4 x float>, <4 x float>* %3814, align 4, !tbaa !3164
  %3815 = getelementptr inbounds float, float* %4, i64 %3810
  %3816 = bitcast float* %3815 to <4 x float>*
  store <4 x float> %wide.load.5, <4 x float>* %3816, align 4, !tbaa !3167
  %3817 = getelementptr inbounds float, float* %3815, i64 4
  %3818 = bitcast float* %3817 to <4 x float>*
  store <4 x float> %wide.load16.5, <4 x float>* %3818, align 4, !tbaa !3167
  %3819 = add nsw i64 %21, 3376
  %3820 = getelementptr inbounds float, float* %7, i64 %3819
  %3821 = bitcast float* %3820 to <4 x float>*
  %wide.load.6 = load <4 x float>, <4 x float>* %3821, align 4, !tbaa !3164
  %3822 = getelementptr inbounds float, float* %3820, i64 4
  %3823 = bitcast float* %3822 to <4 x float>*
  %wide.load16.6 = load <4 x float>, <4 x float>* %3823, align 4, !tbaa !3164
  %3824 = getelementptr inbounds float, float* %4, i64 %3819
  %3825 = bitcast float* %3824 to <4 x float>*
  store <4 x float> %wide.load.6, <4 x float>* %3825, align 4, !tbaa !3167
  %3826 = getelementptr inbounds float, float* %3824, i64 4
  %3827 = bitcast float* %3826 to <4 x float>*
  store <4 x float> %wide.load16.6, <4 x float>* %3827, align 4, !tbaa !3167
  %3828 = add nsw i64 %21, 3384
  %3829 = getelementptr inbounds float, float* %7, i64 %3828
  %3830 = bitcast float* %3829 to <4 x float>*
  %wide.load.7 = load <4 x float>, <4 x float>* %3830, align 4, !tbaa !3164
  %3831 = getelementptr inbounds float, float* %3829, i64 4
  %3832 = bitcast float* %3831 to <4 x float>*
  %wide.load16.7 = load <4 x float>, <4 x float>* %3832, align 4, !tbaa !3164
  %3833 = getelementptr inbounds float, float* %4, i64 %3828
  %3834 = bitcast float* %3833 to <4 x float>*
  store <4 x float> %wide.load.7, <4 x float>* %3834, align 4, !tbaa !3167
  %3835 = getelementptr inbounds float, float* %3833, i64 4
  %3836 = bitcast float* %3835 to <4 x float>*
  store <4 x float> %wide.load16.7, <4 x float>* %3836, align 4, !tbaa !3167
  %3837 = add nsw i64 %21, 3392
  %3838 = getelementptr inbounds float, float* %7, i64 %3837
  %3839 = bitcast float* %3838 to <4 x float>*
  %wide.load.8 = load <4 x float>, <4 x float>* %3839, align 4, !tbaa !3164
  %3840 = getelementptr inbounds float, float* %3838, i64 4
  %3841 = bitcast float* %3840 to <4 x float>*
  %wide.load16.8 = load <4 x float>, <4 x float>* %3841, align 4, !tbaa !3164
  %3842 = getelementptr inbounds float, float* %4, i64 %3837
  %3843 = bitcast float* %3842 to <4 x float>*
  store <4 x float> %wide.load.8, <4 x float>* %3843, align 4, !tbaa !3167
  %3844 = getelementptr inbounds float, float* %3842, i64 4
  %3845 = bitcast float* %3844 to <4 x float>*
  store <4 x float> %wide.load16.8, <4 x float>* %3845, align 4, !tbaa !3167
  %3846 = add nsw i64 %21, 3400
  %3847 = getelementptr inbounds float, float* %7, i64 %3846
  %3848 = bitcast float* %3847 to <4 x float>*
  %wide.load.9 = load <4 x float>, <4 x float>* %3848, align 4, !tbaa !3164
  %3849 = getelementptr inbounds float, float* %3847, i64 4
  %3850 = bitcast float* %3849 to <4 x float>*
  %wide.load16.9 = load <4 x float>, <4 x float>* %3850, align 4, !tbaa !3164
  %3851 = getelementptr inbounds float, float* %4, i64 %3846
  %3852 = bitcast float* %3851 to <4 x float>*
  store <4 x float> %wide.load.9, <4 x float>* %3852, align 4, !tbaa !3167
  %3853 = getelementptr inbounds float, float* %3851, i64 4
  %3854 = bitcast float* %3853 to <4 x float>*
  store <4 x float> %wide.load16.9, <4 x float>* %3854, align 4, !tbaa !3167
  %3855 = add nsw i64 %21, 3408
  %3856 = getelementptr inbounds float, float* %7, i64 %3855
  %3857 = bitcast float* %3856 to <4 x float>*
  %wide.load.10 = load <4 x float>, <4 x float>* %3857, align 4, !tbaa !3164
  %3858 = getelementptr inbounds float, float* %3856, i64 4
  %3859 = bitcast float* %3858 to <4 x float>*
  %wide.load16.10 = load <4 x float>, <4 x float>* %3859, align 4, !tbaa !3164
  %3860 = getelementptr inbounds float, float* %4, i64 %3855
  %3861 = bitcast float* %3860 to <4 x float>*
  store <4 x float> %wide.load.10, <4 x float>* %3861, align 4, !tbaa !3167
  %3862 = getelementptr inbounds float, float* %3860, i64 4
  %3863 = bitcast float* %3862 to <4 x float>*
  store <4 x float> %wide.load16.10, <4 x float>* %3863, align 4, !tbaa !3167
  %3864 = add nsw i64 %21, 3416
  %3865 = getelementptr inbounds float, float* %7, i64 %3864
  %3866 = bitcast float* %3865 to <4 x float>*
  %wide.load.11 = load <4 x float>, <4 x float>* %3866, align 4, !tbaa !3164
  %3867 = getelementptr inbounds float, float* %3865, i64 4
  %3868 = bitcast float* %3867 to <4 x float>*
  %wide.load16.11 = load <4 x float>, <4 x float>* %3868, align 4, !tbaa !3164
  %3869 = getelementptr inbounds float, float* %4, i64 %3864
  %3870 = bitcast float* %3869 to <4 x float>*
  store <4 x float> %wide.load.11, <4 x float>* %3870, align 4, !tbaa !3167
  %3871 = getelementptr inbounds float, float* %3869, i64 4
  %3872 = bitcast float* %3871 to <4 x float>*
  store <4 x float> %wide.load16.11, <4 x float>* %3872, align 4, !tbaa !3167
  %3873 = add nsw i64 %21, 3424
  %3874 = getelementptr inbounds float, float* %7, i64 %3873
  %3875 = bitcast float* %3874 to <4 x float>*
  %wide.load.12 = load <4 x float>, <4 x float>* %3875, align 4, !tbaa !3164
  %3876 = getelementptr inbounds float, float* %3874, i64 4
  %3877 = bitcast float* %3876 to <4 x float>*
  %wide.load16.12 = load <4 x float>, <4 x float>* %3877, align 4, !tbaa !3164
  %3878 = getelementptr inbounds float, float* %4, i64 %3873
  %3879 = bitcast float* %3878 to <4 x float>*
  store <4 x float> %wide.load.12, <4 x float>* %3879, align 4, !tbaa !3167
  %3880 = getelementptr inbounds float, float* %3878, i64 4
  %3881 = bitcast float* %3880 to <4 x float>*
  store <4 x float> %wide.load16.12, <4 x float>* %3881, align 4, !tbaa !3167
  %3882 = add nsw i64 %21, 3432
  %3883 = getelementptr inbounds float, float* %7, i64 %3882
  %3884 = bitcast float* %3883 to <4 x float>*
  %wide.load.13 = load <4 x float>, <4 x float>* %3884, align 4, !tbaa !3164
  %3885 = getelementptr inbounds float, float* %3883, i64 4
  %3886 = bitcast float* %3885 to <4 x float>*
  %wide.load16.13 = load <4 x float>, <4 x float>* %3886, align 4, !tbaa !3164
  %3887 = getelementptr inbounds float, float* %4, i64 %3882
  %3888 = bitcast float* %3887 to <4 x float>*
  store <4 x float> %wide.load.13, <4 x float>* %3888, align 4, !tbaa !3167
  %3889 = getelementptr inbounds float, float* %3887, i64 4
  %3890 = bitcast float* %3889 to <4 x float>*
  store <4 x float> %wide.load16.13, <4 x float>* %3890, align 4, !tbaa !3167
  %3891 = add nsw i64 %21, 3440
  %3892 = getelementptr inbounds float, float* %7, i64 %3891
  %3893 = bitcast float* %3892 to <4 x float>*
  %wide.load.14 = load <4 x float>, <4 x float>* %3893, align 4, !tbaa !3164
  %3894 = getelementptr inbounds float, float* %3892, i64 4
  %3895 = bitcast float* %3894 to <4 x float>*
  %wide.load16.14 = load <4 x float>, <4 x float>* %3895, align 4, !tbaa !3164
  %3896 = getelementptr inbounds float, float* %4, i64 %3891
  %3897 = bitcast float* %3896 to <4 x float>*
  store <4 x float> %wide.load.14, <4 x float>* %3897, align 4, !tbaa !3167
  %3898 = getelementptr inbounds float, float* %3896, i64 4
  %3899 = bitcast float* %3898 to <4 x float>*
  store <4 x float> %wide.load16.14, <4 x float>* %3899, align 4, !tbaa !3167
  %3900 = add nsw i64 %21, 3448
  %3901 = getelementptr inbounds float, float* %7, i64 %3900
  %3902 = bitcast float* %3901 to <4 x float>*
  %wide.load.15 = load <4 x float>, <4 x float>* %3902, align 4, !tbaa !3164
  %3903 = getelementptr inbounds float, float* %3901, i64 4
  %3904 = bitcast float* %3903 to <4 x float>*
  %wide.load16.15 = load <4 x float>, <4 x float>* %3904, align 4, !tbaa !3164
  %3905 = getelementptr inbounds float, float* %4, i64 %3900
  %3906 = bitcast float* %3905 to <4 x float>*
  store <4 x float> %wide.load.15, <4 x float>* %3906, align 4, !tbaa !3167
  %3907 = getelementptr inbounds float, float* %3905, i64 4
  %3908 = bitcast float* %3907 to <4 x float>*
  store <4 x float> %wide.load16.15, <4 x float>* %3908, align 4, !tbaa !3167
  %3909 = add nsw i64 %21, 3456
  %3910 = getelementptr inbounds float, float* %7, i64 %3909
  %3911 = bitcast float* %3910 to <4 x float>*
  %wide.load.16 = load <4 x float>, <4 x float>* %3911, align 4, !tbaa !3164
  %3912 = getelementptr inbounds float, float* %3910, i64 4
  %3913 = bitcast float* %3912 to <4 x float>*
  %wide.load16.16 = load <4 x float>, <4 x float>* %3913, align 4, !tbaa !3164
  %3914 = getelementptr inbounds float, float* %4, i64 %3909
  %3915 = bitcast float* %3914 to <4 x float>*
  store <4 x float> %wide.load.16, <4 x float>* %3915, align 4, !tbaa !3167
  %3916 = getelementptr inbounds float, float* %3914, i64 4
  %3917 = bitcast float* %3916 to <4 x float>*
  store <4 x float> %wide.load16.16, <4 x float>* %3917, align 4, !tbaa !3167
  %3918 = add nsw i64 %21, 3464
  %3919 = getelementptr inbounds float, float* %7, i64 %3918
  %3920 = bitcast float* %3919 to <4 x float>*
  %wide.load.17 = load <4 x float>, <4 x float>* %3920, align 4, !tbaa !3164
  %3921 = getelementptr inbounds float, float* %3919, i64 4
  %3922 = bitcast float* %3921 to <4 x float>*
  %wide.load16.17 = load <4 x float>, <4 x float>* %3922, align 4, !tbaa !3164
  %3923 = getelementptr inbounds float, float* %4, i64 %3918
  %3924 = bitcast float* %3923 to <4 x float>*
  store <4 x float> %wide.load.17, <4 x float>* %3924, align 4, !tbaa !3167
  %3925 = getelementptr inbounds float, float* %3923, i64 4
  %3926 = bitcast float* %3925 to <4 x float>*
  store <4 x float> %wide.load16.17, <4 x float>* %3926, align 4, !tbaa !3167
  %3927 = add nsw i64 %21, 3472
  %3928 = getelementptr inbounds float, float* %7, i64 %3927
  %3929 = bitcast float* %3928 to <4 x float>*
  %wide.load.18 = load <4 x float>, <4 x float>* %3929, align 4, !tbaa !3164
  %3930 = getelementptr inbounds float, float* %3928, i64 4
  %3931 = bitcast float* %3930 to <4 x float>*
  %wide.load16.18 = load <4 x float>, <4 x float>* %3931, align 4, !tbaa !3164
  %3932 = getelementptr inbounds float, float* %4, i64 %3927
  %3933 = bitcast float* %3932 to <4 x float>*
  store <4 x float> %wide.load.18, <4 x float>* %3933, align 4, !tbaa !3167
  %3934 = getelementptr inbounds float, float* %3932, i64 4
  %3935 = bitcast float* %3934 to <4 x float>*
  store <4 x float> %wide.load16.18, <4 x float>* %3935, align 4, !tbaa !3167
  %3936 = add nsw i64 %21, 3480
  %3937 = getelementptr inbounds float, float* %7, i64 %3936
  %3938 = bitcast float* %3937 to <4 x float>*
  %wide.load.19 = load <4 x float>, <4 x float>* %3938, align 4, !tbaa !3164
  %3939 = getelementptr inbounds float, float* %3937, i64 4
  %3940 = bitcast float* %3939 to <4 x float>*
  %wide.load16.19 = load <4 x float>, <4 x float>* %3940, align 4, !tbaa !3164
  %3941 = getelementptr inbounds float, float* %4, i64 %3936
  %3942 = bitcast float* %3941 to <4 x float>*
  store <4 x float> %wide.load.19, <4 x float>* %3942, align 4, !tbaa !3167
  %3943 = getelementptr inbounds float, float* %3941, i64 4
  %3944 = bitcast float* %3943 to <4 x float>*
  store <4 x float> %wide.load16.19, <4 x float>* %3944, align 4, !tbaa !3167
  %3945 = add nsw i64 %21, 3488
  %3946 = getelementptr inbounds float, float* %7, i64 %3945
  %3947 = bitcast float* %3946 to <4 x float>*
  %wide.load.20 = load <4 x float>, <4 x float>* %3947, align 4, !tbaa !3164
  %3948 = getelementptr inbounds float, float* %3946, i64 4
  %3949 = bitcast float* %3948 to <4 x float>*
  %wide.load16.20 = load <4 x float>, <4 x float>* %3949, align 4, !tbaa !3164
  %3950 = getelementptr inbounds float, float* %4, i64 %3945
  %3951 = bitcast float* %3950 to <4 x float>*
  store <4 x float> %wide.load.20, <4 x float>* %3951, align 4, !tbaa !3167
  %3952 = getelementptr inbounds float, float* %3950, i64 4
  %3953 = bitcast float* %3952 to <4 x float>*
  store <4 x float> %wide.load16.20, <4 x float>* %3953, align 4, !tbaa !3167
  %3954 = add nsw i64 %21, 3496
  %3955 = getelementptr inbounds float, float* %7, i64 %3954
  %3956 = bitcast float* %3955 to <4 x float>*
  %wide.load.21 = load <4 x float>, <4 x float>* %3956, align 4, !tbaa !3164
  %3957 = getelementptr inbounds float, float* %3955, i64 4
  %3958 = bitcast float* %3957 to <4 x float>*
  %wide.load16.21 = load <4 x float>, <4 x float>* %3958, align 4, !tbaa !3164
  %3959 = getelementptr inbounds float, float* %4, i64 %3954
  %3960 = bitcast float* %3959 to <4 x float>*
  store <4 x float> %wide.load.21, <4 x float>* %3960, align 4, !tbaa !3167
  %3961 = getelementptr inbounds float, float* %3959, i64 4
  %3962 = bitcast float* %3961 to <4 x float>*
  store <4 x float> %wide.load16.21, <4 x float>* %3962, align 4, !tbaa !3167
  %3963 = add nsw i64 %21, 3504
  %3964 = getelementptr inbounds float, float* %7, i64 %3963
  %3965 = bitcast float* %3964 to <4 x float>*
  %wide.load.22 = load <4 x float>, <4 x float>* %3965, align 4, !tbaa !3164
  %3966 = getelementptr inbounds float, float* %3964, i64 4
  %3967 = bitcast float* %3966 to <4 x float>*
  %wide.load16.22 = load <4 x float>, <4 x float>* %3967, align 4, !tbaa !3164
  %3968 = getelementptr inbounds float, float* %4, i64 %3963
  %3969 = bitcast float* %3968 to <4 x float>*
  store <4 x float> %wide.load.22, <4 x float>* %3969, align 4, !tbaa !3167
  %3970 = getelementptr inbounds float, float* %3968, i64 4
  %3971 = bitcast float* %3970 to <4 x float>*
  store <4 x float> %wide.load16.22, <4 x float>* %3971, align 4, !tbaa !3167
  %3972 = add nsw i64 %21, 3512
  %3973 = getelementptr inbounds float, float* %7, i64 %3972
  %3974 = bitcast float* %3973 to <4 x float>*
  %wide.load.23 = load <4 x float>, <4 x float>* %3974, align 4, !tbaa !3164
  %3975 = getelementptr inbounds float, float* %3973, i64 4
  %3976 = bitcast float* %3975 to <4 x float>*
  %wide.load16.23 = load <4 x float>, <4 x float>* %3976, align 4, !tbaa !3164
  %3977 = getelementptr inbounds float, float* %4, i64 %3972
  %3978 = bitcast float* %3977 to <4 x float>*
  store <4 x float> %wide.load.23, <4 x float>* %3978, align 4, !tbaa !3167
  %3979 = getelementptr inbounds float, float* %3977, i64 4
  %3980 = bitcast float* %3979 to <4 x float>*
  store <4 x float> %wide.load16.23, <4 x float>* %3980, align 4, !tbaa !3167
  %3981 = add nsw i64 %21, 3520
  %3982 = getelementptr inbounds float, float* %7, i64 %3981
  %3983 = bitcast float* %3982 to <4 x float>*
  %wide.load.24 = load <4 x float>, <4 x float>* %3983, align 4, !tbaa !3164
  %3984 = getelementptr inbounds float, float* %3982, i64 4
  %3985 = bitcast float* %3984 to <4 x float>*
  %wide.load16.24 = load <4 x float>, <4 x float>* %3985, align 4, !tbaa !3164
  %3986 = getelementptr inbounds float, float* %4, i64 %3981
  %3987 = bitcast float* %3986 to <4 x float>*
  store <4 x float> %wide.load.24, <4 x float>* %3987, align 4, !tbaa !3167
  %3988 = getelementptr inbounds float, float* %3986, i64 4
  %3989 = bitcast float* %3988 to <4 x float>*
  store <4 x float> %wide.load16.24, <4 x float>* %3989, align 4, !tbaa !3167
  %3990 = add nsw i64 %21, 3528
  %3991 = getelementptr inbounds float, float* %7, i64 %3990
  %3992 = bitcast float* %3991 to <4 x float>*
  %wide.load.25 = load <4 x float>, <4 x float>* %3992, align 4, !tbaa !3164
  %3993 = getelementptr inbounds float, float* %3991, i64 4
  %3994 = bitcast float* %3993 to <4 x float>*
  %wide.load16.25 = load <4 x float>, <4 x float>* %3994, align 4, !tbaa !3164
  %3995 = getelementptr inbounds float, float* %4, i64 %3990
  %3996 = bitcast float* %3995 to <4 x float>*
  store <4 x float> %wide.load.25, <4 x float>* %3996, align 4, !tbaa !3167
  %3997 = getelementptr inbounds float, float* %3995, i64 4
  %3998 = bitcast float* %3997 to <4 x float>*
  store <4 x float> %wide.load16.25, <4 x float>* %3998, align 4, !tbaa !3167
  %3999 = add nsw i64 %21, 3536
  %4000 = getelementptr inbounds float, float* %7, i64 %3999
  %4001 = bitcast float* %4000 to <4 x float>*
  %wide.load.26 = load <4 x float>, <4 x float>* %4001, align 4, !tbaa !3164
  %4002 = getelementptr inbounds float, float* %4000, i64 4
  %4003 = bitcast float* %4002 to <4 x float>*
  %wide.load16.26 = load <4 x float>, <4 x float>* %4003, align 4, !tbaa !3164
  %4004 = getelementptr inbounds float, float* %4, i64 %3999
  %4005 = bitcast float* %4004 to <4 x float>*
  store <4 x float> %wide.load.26, <4 x float>* %4005, align 4, !tbaa !3167
  %4006 = getelementptr inbounds float, float* %4004, i64 4
  %4007 = bitcast float* %4006 to <4 x float>*
  store <4 x float> %wide.load16.26, <4 x float>* %4007, align 4, !tbaa !3167
  %4008 = add nsw i64 %21, 3544
  %4009 = getelementptr inbounds float, float* %7, i64 %4008
  %4010 = bitcast float* %4009 to <4 x float>*
  %wide.load.27 = load <4 x float>, <4 x float>* %4010, align 4, !tbaa !3164
  %4011 = getelementptr inbounds float, float* %4009, i64 4
  %4012 = bitcast float* %4011 to <4 x float>*
  %wide.load16.27 = load <4 x float>, <4 x float>* %4012, align 4, !tbaa !3164
  %4013 = getelementptr inbounds float, float* %4, i64 %4008
  %4014 = bitcast float* %4013 to <4 x float>*
  store <4 x float> %wide.load.27, <4 x float>* %4014, align 4, !tbaa !3167
  %4015 = getelementptr inbounds float, float* %4013, i64 4
  %4016 = bitcast float* %4015 to <4 x float>*
  store <4 x float> %wide.load16.27, <4 x float>* %4016, align 4, !tbaa !3167
  %4017 = add nsw i64 %21, 3552
  %4018 = getelementptr inbounds float, float* %7, i64 %4017
  %4019 = bitcast float* %4018 to <4 x float>*
  %wide.load.28 = load <4 x float>, <4 x float>* %4019, align 4, !tbaa !3164
  %4020 = getelementptr inbounds float, float* %4018, i64 4
  %4021 = bitcast float* %4020 to <4 x float>*
  %wide.load16.28 = load <4 x float>, <4 x float>* %4021, align 4, !tbaa !3164
  %4022 = getelementptr inbounds float, float* %4, i64 %4017
  %4023 = bitcast float* %4022 to <4 x float>*
  store <4 x float> %wide.load.28, <4 x float>* %4023, align 4, !tbaa !3167
  %4024 = getelementptr inbounds float, float* %4022, i64 4
  %4025 = bitcast float* %4024 to <4 x float>*
  store <4 x float> %wide.load16.28, <4 x float>* %4025, align 4, !tbaa !3167
  %4026 = add nsw i64 %21, 3560
  %4027 = getelementptr inbounds float, float* %7, i64 %4026
  %4028 = bitcast float* %4027 to <4 x float>*
  %wide.load.29 = load <4 x float>, <4 x float>* %4028, align 4, !tbaa !3164
  %4029 = getelementptr inbounds float, float* %4027, i64 4
  %4030 = bitcast float* %4029 to <4 x float>*
  %wide.load16.29 = load <4 x float>, <4 x float>* %4030, align 4, !tbaa !3164
  %4031 = getelementptr inbounds float, float* %4, i64 %4026
  %4032 = bitcast float* %4031 to <4 x float>*
  store <4 x float> %wide.load.29, <4 x float>* %4032, align 4, !tbaa !3167
  %4033 = getelementptr inbounds float, float* %4031, i64 4
  %4034 = bitcast float* %4033 to <4 x float>*
  store <4 x float> %wide.load16.29, <4 x float>* %4034, align 4, !tbaa !3167
  %4035 = add nsw i64 %21, 3568
  %4036 = getelementptr inbounds float, float* %7, i64 %4035
  %4037 = bitcast float* %4036 to <4 x float>*
  %wide.load.30 = load <4 x float>, <4 x float>* %4037, align 4, !tbaa !3164
  %4038 = getelementptr inbounds float, float* %4036, i64 4
  %4039 = bitcast float* %4038 to <4 x float>*
  %wide.load16.30 = load <4 x float>, <4 x float>* %4039, align 4, !tbaa !3164
  %4040 = getelementptr inbounds float, float* %4, i64 %4035
  %4041 = bitcast float* %4040 to <4 x float>*
  store <4 x float> %wide.load.30, <4 x float>* %4041, align 4, !tbaa !3167
  %4042 = getelementptr inbounds float, float* %4040, i64 4
  %4043 = bitcast float* %4042 to <4 x float>*
  store <4 x float> %wide.load16.30, <4 x float>* %4043, align 4, !tbaa !3167
  %4044 = add nsw i64 %21, 3576
  %4045 = getelementptr inbounds float, float* %7, i64 %4044
  %4046 = bitcast float* %4045 to <4 x float>*
  %wide.load.31 = load <4 x float>, <4 x float>* %4046, align 4, !tbaa !3164
  %4047 = getelementptr inbounds float, float* %4045, i64 4
  %4048 = bitcast float* %4047 to <4 x float>*
  %wide.load16.31 = load <4 x float>, <4 x float>* %4048, align 4, !tbaa !3164
  %4049 = getelementptr inbounds float, float* %4, i64 %4044
  %4050 = bitcast float* %4049 to <4 x float>*
  store <4 x float> %wide.load.31, <4 x float>* %4050, align 4, !tbaa !3167
  %4051 = getelementptr inbounds float, float* %4049, i64 4
  %4052 = bitcast float* %4051 to <4 x float>*
  store <4 x float> %wide.load16.31, <4 x float>* %4052, align 4, !tbaa !3167
  %indvars.iv.next14 = add nsw i64 %indvars.iv13, 1
  %exitcond15.not = icmp eq i64 %indvars.iv.next14, %wide.trip.count
  br i1 %exitcond15.not, label %for_end, label %for_begin1.preheader, !prof !51

for_end:                                          ; preds = %for_begin1.preheader, %entry
  ret i32 0
}

; Function Attrs: nofree nounwind
define private i32 @__tvm_parallel_lambda.239(i32 %0, %0* nocapture readonly %1, i8* nocapture readonly %2) #3 {
entry:
  %3 = bitcast i8* %2 to float**
  %4 = load float*, float** %3, align 8
  %5 = getelementptr inbounds i8, i8* %2, i64 8
  %6 = bitcast i8* %5 to float**
  %7 = load float*, float** %6, align 8
  %8 = getelementptr inbounds i8, i8* %2, i64 16
  %9 = bitcast i8* %8 to float**
  %10 = load float*, float** %9, align 8
  %11 = getelementptr inbounds i8, i8* %2, i64 24
  %12 = bitcast i8* %11 to float**
  %13 = load float*, float** %12, align 8
  %14 = getelementptr inbounds %0, %0* %1, i64 0, i32 1
  %15 = load i32, i32* %14, align 4
  %16 = add nsw i32 %15, 195
  %17 = sdiv i32 %16, %15
  %18 = add nsw i32 %0, 1
  %19 = mul nsw i32 %17, %18
  %20 = icmp slt i32 %19, 196
  %21 = select i1 %20, i32 %19, i32 196
  %22 = mul nsw i32 %17, %0
  %23 = icmp slt i32 %22, 196
  %24 = select i1 %23, i32 %22, i32 196
  %25 = icmp slt i32 %24, %21
  br i1 %25, label %for_begin1.preheader.preheader, label %for_end, !prof !5

for_begin1.preheader.preheader:                   ; preds = %entry
  %26 = sext i32 %24 to i64
  %wide.trip.count = sext i32 %21 to i64
  %27 = bitcast float* %13 to <64 x float>*
  %28 = load <64 x float>, <64 x float>* %27, align 128, !tbaa !3170
  %29 = getelementptr inbounds float, float* %13, i64 64
  %30 = bitcast float* %29 to <64 x float>*
  %31 = load <64 x float>, <64 x float>* %30, align 128, !tbaa !3170
  %32 = getelementptr inbounds float, float* %13, i64 128
  %33 = bitcast float* %32 to <64 x float>*
  %34 = load <64 x float>, <64 x float>* %33, align 128, !tbaa !3170
  %35 = getelementptr inbounds float, float* %13, i64 192
  %36 = bitcast float* %35 to <64 x float>*
  %37 = load <64 x float>, <64 x float>* %36, align 128, !tbaa !3170
  %38 = getelementptr inbounds float, float* %13, i64 256
  %39 = bitcast float* %38 to <64 x float>*
  %40 = load <64 x float>, <64 x float>* %39, align 128, !tbaa !3170
  %41 = getelementptr inbounds float, float* %13, i64 320
  %42 = bitcast float* %41 to <64 x float>*
  %43 = load <64 x float>, <64 x float>* %42, align 128, !tbaa !3170
  %44 = getelementptr inbounds float, float* %13, i64 384
  %45 = bitcast float* %44 to <64 x float>*
  %46 = load <64 x float>, <64 x float>* %45, align 128, !tbaa !3170
  %47 = getelementptr inbounds float, float* %13, i64 448
  %48 = bitcast float* %47 to <64 x float>*
  %49 = load <64 x float>, <64 x float>* %48, align 128, !tbaa !3170
  br label %for_begin1.preheader

for_begin1.preheader:                             ; preds = %for_begin1.preheader.preheader, %for_end6.7
  %indvars.iv14 = phi i64 [ %26, %for_begin1.preheader.preheader ], [ %indvars.iv.next15, %for_end6.7 ]
  %50 = phi i32 [ %24, %for_begin1.preheader.preheader ], [ %293, %for_end6.7 ]
  %51 = shl nsw i32 %50, 8
  %52 = trunc i64 %indvars.iv14 to i32
  %53 = shl nsw i32 %52, 9
  %54 = sext i32 %51 to i64
  %55 = sext i32 %53 to i64
  br label %for_body5

for_end:                                          ; preds = %for_end6.7, %entry
  ret i32 0

for_body5:                                        ; preds = %for_body5, %for_begin1.preheader
  %indvars.iv = phi i64 [ 0, %for_begin1.preheader ], [ %indvars.iv.next.140, %for_body5 ]
  %.010 = phi <64 x float> [ zeroinitializer, %for_begin1.preheader ], [ %75, %for_body5 ]
  %56 = add nsw i64 %indvars.iv, %54
  %57 = getelementptr inbounds float, float* %4, i64 %56
  %58 = load float, float* %57, align 4, !tbaa !3167
  %59 = insertelement <64 x float> undef, float %58, i32 0
  %60 = shufflevector <64 x float> %59, <64 x float> undef, <64 x i32> zeroinitializer
  %61 = shl nuw nsw i64 %indvars.iv, 9
  %62 = getelementptr inbounds float, float* %7, i64 %61
  %63 = bitcast float* %62 to <64 x float>*
  %64 = load <64 x float>, <64 x float>* %63, align 128, !tbaa !3173
  %65 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %60, <64 x float> %64, <64 x float> %.010)
  %indvars.iv.next = or i64 %indvars.iv, 1
  %66 = add nsw i64 %indvars.iv.next, %54
  %67 = getelementptr inbounds float, float* %4, i64 %66
  %68 = load float, float* %67, align 4, !tbaa !3167
  %69 = insertelement <64 x float> undef, float %68, i32 0
  %70 = shufflevector <64 x float> %69, <64 x float> undef, <64 x i32> zeroinitializer
  %71 = shl nuw nsw i64 %indvars.iv.next, 9
  %72 = getelementptr inbounds float, float* %7, i64 %71
  %73 = bitcast float* %72 to <64 x float>*
  %74 = load <64 x float>, <64 x float>* %73, align 128, !tbaa !3173
  %75 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %70, <64 x float> %74, <64 x float> %65)
  %indvars.iv.next.140 = add nuw nsw i64 %indvars.iv, 2
  %exitcond.not.1 = icmp eq i64 %indvars.iv.next.140, 256
  br i1 %exitcond.not.1, label %for_end6, label %for_body5, !prof !51

for_end6:                                         ; preds = %for_body5
  %76 = fadd <64 x float> %75, %28
  %77 = fcmp olt <64 x float> %76, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %78 = select <64 x i1> %77, <64 x float> %76, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %79 = fcmp ogt <64 x float> %78, zeroinitializer
  %80 = select <64 x i1> %79, <64 x float> %78, <64 x float> zeroinitializer
  %81 = getelementptr inbounds float, float* %10, i64 %55
  %82 = bitcast float* %81 to <64 x float>*
  store <64 x float> %80, <64 x float>* %82, align 128, !tbaa !3176
  br label %for_body5.1

for_body5.1:                                      ; preds = %for_body5.1, %for_end6
  %indvars.iv.1 = phi i64 [ 0, %for_end6 ], [ %indvars.iv.next.1.1, %for_body5.1 ]
  %.010.1 = phi <64 x float> [ zeroinitializer, %for_end6 ], [ %104, %for_body5.1 ]
  %83 = add nsw i64 %indvars.iv.1, %54
  %84 = getelementptr inbounds float, float* %4, i64 %83
  %85 = load float, float* %84, align 4, !tbaa !3167
  %86 = insertelement <64 x float> undef, float %85, i32 0
  %87 = shufflevector <64 x float> %86, <64 x float> undef, <64 x i32> zeroinitializer
  %88 = shl nuw nsw i64 %indvars.iv.1, 9
  %89 = or i64 %88, 64
  %90 = getelementptr inbounds float, float* %7, i64 %89
  %91 = bitcast float* %90 to <64 x float>*
  %92 = load <64 x float>, <64 x float>* %91, align 128, !tbaa !3173
  %93 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %87, <64 x float> %92, <64 x float> %.010.1)
  %indvars.iv.next.1 = or i64 %indvars.iv.1, 1
  %94 = add nsw i64 %indvars.iv.next.1, %54
  %95 = getelementptr inbounds float, float* %4, i64 %94
  %96 = load float, float* %95, align 4, !tbaa !3167
  %97 = insertelement <64 x float> undef, float %96, i32 0
  %98 = shufflevector <64 x float> %97, <64 x float> undef, <64 x i32> zeroinitializer
  %99 = shl nuw nsw i64 %indvars.iv.next.1, 9
  %100 = or i64 %99, 64
  %101 = getelementptr inbounds float, float* %7, i64 %100
  %102 = bitcast float* %101 to <64 x float>*
  %103 = load <64 x float>, <64 x float>* %102, align 128, !tbaa !3173
  %104 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %98, <64 x float> %103, <64 x float> %93)
  %indvars.iv.next.1.1 = add nuw nsw i64 %indvars.iv.1, 2
  %exitcond.1.not.1 = icmp eq i64 %indvars.iv.next.1.1, 256
  br i1 %exitcond.1.not.1, label %for_end6.1, label %for_body5.1, !prof !51

for_end6.1:                                       ; preds = %for_body5.1
  %105 = or i64 %55, 64
  %106 = fadd <64 x float> %104, %31
  %107 = fcmp olt <64 x float> %106, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %108 = select <64 x i1> %107, <64 x float> %106, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %109 = fcmp ogt <64 x float> %108, zeroinitializer
  %110 = select <64 x i1> %109, <64 x float> %108, <64 x float> zeroinitializer
  %111 = getelementptr inbounds float, float* %10, i64 %105
  %112 = bitcast float* %111 to <64 x float>*
  store <64 x float> %110, <64 x float>* %112, align 128, !tbaa !3176
  br label %for_body5.2

for_body5.2:                                      ; preds = %for_body5.2, %for_end6.1
  %indvars.iv.2 = phi i64 [ 0, %for_end6.1 ], [ %indvars.iv.next.2.1, %for_body5.2 ]
  %.010.2 = phi <64 x float> [ zeroinitializer, %for_end6.1 ], [ %134, %for_body5.2 ]
  %113 = add nsw i64 %indvars.iv.2, %54
  %114 = getelementptr inbounds float, float* %4, i64 %113
  %115 = load float, float* %114, align 4, !tbaa !3167
  %116 = insertelement <64 x float> undef, float %115, i32 0
  %117 = shufflevector <64 x float> %116, <64 x float> undef, <64 x i32> zeroinitializer
  %118 = shl nuw nsw i64 %indvars.iv.2, 9
  %119 = or i64 %118, 128
  %120 = getelementptr inbounds float, float* %7, i64 %119
  %121 = bitcast float* %120 to <64 x float>*
  %122 = load <64 x float>, <64 x float>* %121, align 128, !tbaa !3173
  %123 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %117, <64 x float> %122, <64 x float> %.010.2)
  %indvars.iv.next.2 = or i64 %indvars.iv.2, 1
  %124 = add nsw i64 %indvars.iv.next.2, %54
  %125 = getelementptr inbounds float, float* %4, i64 %124
  %126 = load float, float* %125, align 4, !tbaa !3167
  %127 = insertelement <64 x float> undef, float %126, i32 0
  %128 = shufflevector <64 x float> %127, <64 x float> undef, <64 x i32> zeroinitializer
  %129 = shl nuw nsw i64 %indvars.iv.next.2, 9
  %130 = or i64 %129, 128
  %131 = getelementptr inbounds float, float* %7, i64 %130
  %132 = bitcast float* %131 to <64 x float>*
  %133 = load <64 x float>, <64 x float>* %132, align 128, !tbaa !3173
  %134 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %128, <64 x float> %133, <64 x float> %123)
  %indvars.iv.next.2.1 = add nuw nsw i64 %indvars.iv.2, 2
  %exitcond.2.not.1 = icmp eq i64 %indvars.iv.next.2.1, 256
  br i1 %exitcond.2.not.1, label %for_end6.2, label %for_body5.2, !prof !51

for_end6.2:                                       ; preds = %for_body5.2
  %135 = or i64 %55, 128
  %136 = fadd <64 x float> %134, %34
  %137 = fcmp olt <64 x float> %136, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %138 = select <64 x i1> %137, <64 x float> %136, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %139 = fcmp ogt <64 x float> %138, zeroinitializer
  %140 = select <64 x i1> %139, <64 x float> %138, <64 x float> zeroinitializer
  %141 = getelementptr inbounds float, float* %10, i64 %135
  %142 = bitcast float* %141 to <64 x float>*
  store <64 x float> %140, <64 x float>* %142, align 128, !tbaa !3176
  br label %for_body5.3

for_body5.3:                                      ; preds = %for_body5.3, %for_end6.2
  %indvars.iv.3 = phi i64 [ 0, %for_end6.2 ], [ %indvars.iv.next.3.1, %for_body5.3 ]
  %.010.3 = phi <64 x float> [ zeroinitializer, %for_end6.2 ], [ %164, %for_body5.3 ]
  %143 = add nsw i64 %indvars.iv.3, %54
  %144 = getelementptr inbounds float, float* %4, i64 %143
  %145 = load float, float* %144, align 4, !tbaa !3167
  %146 = insertelement <64 x float> undef, float %145, i32 0
  %147 = shufflevector <64 x float> %146, <64 x float> undef, <64 x i32> zeroinitializer
  %148 = shl nuw nsw i64 %indvars.iv.3, 9
  %149 = or i64 %148, 192
  %150 = getelementptr inbounds float, float* %7, i64 %149
  %151 = bitcast float* %150 to <64 x float>*
  %152 = load <64 x float>, <64 x float>* %151, align 128, !tbaa !3173
  %153 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %147, <64 x float> %152, <64 x float> %.010.3)
  %indvars.iv.next.3 = or i64 %indvars.iv.3, 1
  %154 = add nsw i64 %indvars.iv.next.3, %54
  %155 = getelementptr inbounds float, float* %4, i64 %154
  %156 = load float, float* %155, align 4, !tbaa !3167
  %157 = insertelement <64 x float> undef, float %156, i32 0
  %158 = shufflevector <64 x float> %157, <64 x float> undef, <64 x i32> zeroinitializer
  %159 = shl nuw nsw i64 %indvars.iv.next.3, 9
  %160 = or i64 %159, 192
  %161 = getelementptr inbounds float, float* %7, i64 %160
  %162 = bitcast float* %161 to <64 x float>*
  %163 = load <64 x float>, <64 x float>* %162, align 128, !tbaa !3173
  %164 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %158, <64 x float> %163, <64 x float> %153)
  %indvars.iv.next.3.1 = add nuw nsw i64 %indvars.iv.3, 2
  %exitcond.3.not.1 = icmp eq i64 %indvars.iv.next.3.1, 256
  br i1 %exitcond.3.not.1, label %for_end6.3, label %for_body5.3, !prof !51

for_end6.3:                                       ; preds = %for_body5.3
  %165 = or i64 %55, 192
  %166 = fadd <64 x float> %164, %37
  %167 = fcmp olt <64 x float> %166, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %168 = select <64 x i1> %167, <64 x float> %166, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %169 = fcmp ogt <64 x float> %168, zeroinitializer
  %170 = select <64 x i1> %169, <64 x float> %168, <64 x float> zeroinitializer
  %171 = getelementptr inbounds float, float* %10, i64 %165
  %172 = bitcast float* %171 to <64 x float>*
  store <64 x float> %170, <64 x float>* %172, align 128, !tbaa !3176
  br label %for_body5.4

for_body5.4:                                      ; preds = %for_body5.4, %for_end6.3
  %indvars.iv.4 = phi i64 [ 0, %for_end6.3 ], [ %indvars.iv.next.4.1, %for_body5.4 ]
  %.010.4 = phi <64 x float> [ zeroinitializer, %for_end6.3 ], [ %194, %for_body5.4 ]
  %173 = add nsw i64 %indvars.iv.4, %54
  %174 = getelementptr inbounds float, float* %4, i64 %173
  %175 = load float, float* %174, align 4, !tbaa !3167
  %176 = insertelement <64 x float> undef, float %175, i32 0
  %177 = shufflevector <64 x float> %176, <64 x float> undef, <64 x i32> zeroinitializer
  %178 = shl nuw nsw i64 %indvars.iv.4, 9
  %179 = or i64 %178, 256
  %180 = getelementptr inbounds float, float* %7, i64 %179
  %181 = bitcast float* %180 to <64 x float>*
  %182 = load <64 x float>, <64 x float>* %181, align 128, !tbaa !3173
  %183 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %177, <64 x float> %182, <64 x float> %.010.4)
  %indvars.iv.next.4 = or i64 %indvars.iv.4, 1
  %184 = add nsw i64 %indvars.iv.next.4, %54
  %185 = getelementptr inbounds float, float* %4, i64 %184
  %186 = load float, float* %185, align 4, !tbaa !3167
  %187 = insertelement <64 x float> undef, float %186, i32 0
  %188 = shufflevector <64 x float> %187, <64 x float> undef, <64 x i32> zeroinitializer
  %189 = shl nuw nsw i64 %indvars.iv.next.4, 9
  %190 = or i64 %189, 256
  %191 = getelementptr inbounds float, float* %7, i64 %190
  %192 = bitcast float* %191 to <64 x float>*
  %193 = load <64 x float>, <64 x float>* %192, align 128, !tbaa !3173
  %194 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %188, <64 x float> %193, <64 x float> %183)
  %indvars.iv.next.4.1 = add nuw nsw i64 %indvars.iv.4, 2
  %exitcond.4.not.1 = icmp eq i64 %indvars.iv.next.4.1, 256
  br i1 %exitcond.4.not.1, label %for_end6.4, label %for_body5.4, !prof !51

for_end6.4:                                       ; preds = %for_body5.4
  %195 = or i64 %55, 256
  %196 = fadd <64 x float> %194, %40
  %197 = fcmp olt <64 x float> %196, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %198 = select <64 x i1> %197, <64 x float> %196, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %199 = fcmp ogt <64 x float> %198, zeroinitializer
  %200 = select <64 x i1> %199, <64 x float> %198, <64 x float> zeroinitializer
  %201 = getelementptr inbounds float, float* %10, i64 %195
  %202 = bitcast float* %201 to <64 x float>*
  store <64 x float> %200, <64 x float>* %202, align 128, !tbaa !3176
  br label %for_body5.5

for_body5.5:                                      ; preds = %for_body5.5, %for_end6.4
  %indvars.iv.5 = phi i64 [ 0, %for_end6.4 ], [ %indvars.iv.next.5.1, %for_body5.5 ]
  %.010.5 = phi <64 x float> [ zeroinitializer, %for_end6.4 ], [ %224, %for_body5.5 ]
  %203 = add nsw i64 %indvars.iv.5, %54
  %204 = getelementptr inbounds float, float* %4, i64 %203
  %205 = load float, float* %204, align 4, !tbaa !3167
  %206 = insertelement <64 x float> undef, float %205, i32 0
  %207 = shufflevector <64 x float> %206, <64 x float> undef, <64 x i32> zeroinitializer
  %208 = shl nuw nsw i64 %indvars.iv.5, 9
  %209 = or i64 %208, 320
  %210 = getelementptr inbounds float, float* %7, i64 %209
  %211 = bitcast float* %210 to <64 x float>*
  %212 = load <64 x float>, <64 x float>* %211, align 128, !tbaa !3173
  %213 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %207, <64 x float> %212, <64 x float> %.010.5)
  %indvars.iv.next.5 = or i64 %indvars.iv.5, 1
  %214 = add nsw i64 %indvars.iv.next.5, %54
  %215 = getelementptr inbounds float, float* %4, i64 %214
  %216 = load float, float* %215, align 4, !tbaa !3167
  %217 = insertelement <64 x float> undef, float %216, i32 0
  %218 = shufflevector <64 x float> %217, <64 x float> undef, <64 x i32> zeroinitializer
  %219 = shl nuw nsw i64 %indvars.iv.next.5, 9
  %220 = or i64 %219, 320
  %221 = getelementptr inbounds float, float* %7, i64 %220
  %222 = bitcast float* %221 to <64 x float>*
  %223 = load <64 x float>, <64 x float>* %222, align 128, !tbaa !3173
  %224 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %218, <64 x float> %223, <64 x float> %213)
  %indvars.iv.next.5.1 = add nuw nsw i64 %indvars.iv.5, 2
  %exitcond.5.not.1 = icmp eq i64 %indvars.iv.next.5.1, 256
  br i1 %exitcond.5.not.1, label %for_end6.5, label %for_body5.5, !prof !51

for_end6.5:                                       ; preds = %for_body5.5
  %225 = or i64 %55, 320
  %226 = fadd <64 x float> %224, %43
  %227 = fcmp olt <64 x float> %226, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %228 = select <64 x i1> %227, <64 x float> %226, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %229 = fcmp ogt <64 x float> %228, zeroinitializer
  %230 = select <64 x i1> %229, <64 x float> %228, <64 x float> zeroinitializer
  %231 = getelementptr inbounds float, float* %10, i64 %225
  %232 = bitcast float* %231 to <64 x float>*
  store <64 x float> %230, <64 x float>* %232, align 128, !tbaa !3176
  br label %for_body5.6

for_body5.6:                                      ; preds = %for_body5.6, %for_end6.5
  %indvars.iv.6 = phi i64 [ 0, %for_end6.5 ], [ %indvars.iv.next.6.1, %for_body5.6 ]
  %.010.6 = phi <64 x float> [ zeroinitializer, %for_end6.5 ], [ %254, %for_body5.6 ]
  %233 = add nsw i64 %indvars.iv.6, %54
  %234 = getelementptr inbounds float, float* %4, i64 %233
  %235 = load float, float* %234, align 4, !tbaa !3167
  %236 = insertelement <64 x float> undef, float %235, i32 0
  %237 = shufflevector <64 x float> %236, <64 x float> undef, <64 x i32> zeroinitializer
  %238 = shl nuw nsw i64 %indvars.iv.6, 9
  %239 = or i64 %238, 384
  %240 = getelementptr inbounds float, float* %7, i64 %239
  %241 = bitcast float* %240 to <64 x float>*
  %242 = load <64 x float>, <64 x float>* %241, align 128, !tbaa !3173
  %243 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %237, <64 x float> %242, <64 x float> %.010.6)
  %indvars.iv.next.6 = or i64 %indvars.iv.6, 1
  %244 = add nsw i64 %indvars.iv.next.6, %54
  %245 = getelementptr inbounds float, float* %4, i64 %244
  %246 = load float, float* %245, align 4, !tbaa !3167
  %247 = insertelement <64 x float> undef, float %246, i32 0
  %248 = shufflevector <64 x float> %247, <64 x float> undef, <64 x i32> zeroinitializer
  %249 = shl nuw nsw i64 %indvars.iv.next.6, 9
  %250 = or i64 %249, 384
  %251 = getelementptr inbounds float, float* %7, i64 %250
  %252 = bitcast float* %251 to <64 x float>*
  %253 = load <64 x float>, <64 x float>* %252, align 128, !tbaa !3173
  %254 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %248, <64 x float> %253, <64 x float> %243)
  %indvars.iv.next.6.1 = add nuw nsw i64 %indvars.iv.6, 2
  %exitcond.6.not.1 = icmp eq i64 %indvars.iv.next.6.1, 256
  br i1 %exitcond.6.not.1, label %for_end6.6, label %for_body5.6, !prof !51

for_end6.6:                                       ; preds = %for_body5.6
  %255 = or i64 %55, 384
  %256 = fadd <64 x float> %254, %46
  %257 = fcmp olt <64 x float> %256, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %258 = select <64 x i1> %257, <64 x float> %256, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %259 = fcmp ogt <64 x float> %258, zeroinitializer
  %260 = select <64 x i1> %259, <64 x float> %258, <64 x float> zeroinitializer
  %261 = getelementptr inbounds float, float* %10, i64 %255
  %262 = bitcast float* %261 to <64 x float>*
  store <64 x float> %260, <64 x float>* %262, align 128, !tbaa !3176
  br label %for_body5.7

for_body5.7:                                      ; preds = %for_body5.7, %for_end6.6
  %indvars.iv.7 = phi i64 [ 0, %for_end6.6 ], [ %indvars.iv.next.7.1, %for_body5.7 ]
  %.010.7 = phi <64 x float> [ zeroinitializer, %for_end6.6 ], [ %284, %for_body5.7 ]
  %263 = add nsw i64 %indvars.iv.7, %54
  %264 = getelementptr inbounds float, float* %4, i64 %263
  %265 = load float, float* %264, align 4, !tbaa !3167
  %266 = insertelement <64 x float> undef, float %265, i32 0
  %267 = shufflevector <64 x float> %266, <64 x float> undef, <64 x i32> zeroinitializer
  %268 = shl nuw nsw i64 %indvars.iv.7, 9
  %269 = or i64 %268, 448
  %270 = getelementptr inbounds float, float* %7, i64 %269
  %271 = bitcast float* %270 to <64 x float>*
  %272 = load <64 x float>, <64 x float>* %271, align 128, !tbaa !3173
  %273 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %267, <64 x float> %272, <64 x float> %.010.7)
  %indvars.iv.next.7 = or i64 %indvars.iv.7, 1
  %274 = add nsw i64 %indvars.iv.next.7, %54
  %275 = getelementptr inbounds float, float* %4, i64 %274
  %276 = load float, float* %275, align 4, !tbaa !3167
  %277 = insertelement <64 x float> undef, float %276, i32 0
  %278 = shufflevector <64 x float> %277, <64 x float> undef, <64 x i32> zeroinitializer
  %279 = shl nuw nsw i64 %indvars.iv.next.7, 9
  %280 = or i64 %279, 448
  %281 = getelementptr inbounds float, float* %7, i64 %280
  %282 = bitcast float* %281 to <64 x float>*
  %283 = load <64 x float>, <64 x float>* %282, align 128, !tbaa !3173
  %284 = tail call <64 x float> @llvm.fmuladd.v64f32(<64 x float> %278, <64 x float> %283, <64 x float> %273)
  %indvars.iv.next.7.1 = add nuw nsw i64 %indvars.iv.7, 2
  %exitcond.7.not.1 = icmp eq i64 %indvars.iv.next.7.1, 256
  br i1 %exitcond.7.not.1, label %for_end6.7, label %for_body5.7, !prof !51

for_end6.7:                                       ; preds = %for_body5.7
  %285 = or i64 %55, 448
  %286 = fadd <64 x float> %284, %49
  %287 = fcmp olt <64 x float> %286, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %288 = select <64 x i1> %287, <64 x float> %286, <64 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %289 = fcmp ogt <64 x float> %288, zeroinitializer
  %290 = select <64 x i1> %289, <64 x float> %288, <64 x float> zeroinitializer
  %291 = getelementptr inbounds float, float* %10, i64 %285
  %292 = bitcast float* %291 to <64 x float>*
  store <64 x float> %290, <64 x float>* %292, align 128, !tbaa !3176
  %indvars.iv.next15 = add nsw i64 %indvars.iv14, 1
  %293 = add nsw i32 %50, 1
  %exitcond16.not = icmp eq i64 %indvars.iv.next15, %wide.trip.count
  br i1 %exitcond16.not, label %for_end, label %for_begin1.preheader, !prof !51
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_9(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.240, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !3179
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !3193
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !3195
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !3198
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.241, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.242, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.243, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([162 x i8], [162 x i8]* @.str.244, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !3200
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !3214
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 28
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.90, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !3216
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 28
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.91, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !3219
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 256
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.92, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !3221
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 200704, i32 7168, i32 256, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.93, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !3233
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 3
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.51, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !3247
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 3
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.52, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !3249
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 256
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.94, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !3252
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 1
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.54, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !3254
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 768, i32 256, i32 1, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.116, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !3266
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 256
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.97, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !3280
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !3294
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !3308
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 28
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.73, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !3310
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 28
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.74, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !3313
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 256
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.98, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !3315
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 200704, i32 7168, i32 256, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.99, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_9_compute_(i8* %31, i8* %39, i8* %45, i8* %51, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_9_compute_(i8* noalias nocapture readonly align 128 %0, i8* noalias nocapture readonly align 128 %1, i8* noalias nocapture readonly align 128 %2, i8* noalias nocapture align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 921600, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %for_end38, %if_end, %entry
  ret i32 -1

if_end:                                           ; preds = %entry
  %8 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %9 = tail call i8* %8(i32 1, i32 %4, i64 802816, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %9, i64 128) ]
  %10 = icmp eq i8* %9, null
  br i1 %10, label %if_then, label %for_begin4.preheader, !prof !5

for_begin4.preheader:                             ; preds = %if_end, %for_end6
  %indvar = phi i64 [ %indvar.next, %for_end6 ], [ 0, %if_end ]
  %11 = phi i32 [ %25, %for_end6 ], [ 0, %if_end ]
  %12 = mul nuw nsw i64 %indvar, 30720
  %13 = mul nuw nsw i64 %indvar, 28672
  %14 = add nsw i64 %13, -29696
  %.off = add nsw i32 %11, -1
  %15 = icmp ult i32 %.off, 28
  br i1 %15, label %for_begin7.preheader.us, label %for_begin7.preheader.preheader

for_begin7.preheader.preheader:                   ; preds = %for_begin4.preheader
  %scevgep104 = getelementptr i8, i8* %6, i64 %12
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(30720) %scevgep104, i8 0, i64 30720, i1 false)
  br label %for_end6

for_begin7.preheader.us:                          ; preds = %for_begin4.preheader, %for_end9.us
  %indvar105 = phi i64 [ %indvar.next106, %for_end9.us ], [ 0, %for_begin4.preheader ]
  %16 = phi i32 [ %21, %for_end9.us ], [ 0, %for_begin4.preheader ]
  %17 = shl nuw nsw i64 %indvar105, 10
  %18 = add nuw nsw i64 %12, %17
  %scevgep109 = getelementptr i8, i8* %6, i64 %18
  %.off58.us = add nsw i32 %16, -1
  %19 = icmp ult i32 %.off58.us, 28
  br i1 %19, label %for_body8.us.us.preheader, label %for_body8.us61.preheader

for_body8.us61.preheader:                         ; preds = %for_begin7.preheader.us
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(1024) %scevgep109, i8 0, i64 1024, i1 false)
  br label %for_end9.us

for_body8.us.us.preheader:                        ; preds = %for_begin7.preheader.us
  %20 = add nsw i64 %14, %17
  %scevgep110 = getelementptr i8, i8* %0, i64 %20
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(1024) %scevgep109, i8* nonnull align 128 dereferenceable(1024) %scevgep110, i64 1024, i1 false)
  br label %for_end9.us

for_end9.us:                                      ; preds = %for_body8.us61.preheader, %for_body8.us.us.preheader
  %21 = add nuw nsw i32 %16, 1
  %indvar.next106 = add nuw nsw i64 %indvar105, 1
  %exitcond113.not = icmp eq i64 %indvar.next106, 30
  br i1 %exitcond113.not, label %for_end6, label %for_begin7.preheader.us, !prof !51

for_begin12.preheader:                            ; preds = %for_end6
  %22 = bitcast i8* %9 to float*
  %23 = bitcast i8* %6 to float*
  %24 = bitcast i8* %1 to float*
  br label %for_begin15.preheader

for_end6:                                         ; preds = %for_end9.us, %for_begin7.preheader.preheader
  %25 = add nuw nsw i32 %11, 1
  %indvar.next = add nuw nsw i64 %indvar, 1
  %exitcond114.not = icmp eq i64 %indvar.next, 30
  br i1 %exitcond114.not, label %for_begin12.preheader, label %for_begin4.preheader, !prof !51

for_begin15.preheader:                            ; preds = %for_begin12.preheader, %for_end17
  %indvars.iv97 = phi i64 [ 0, %for_begin12.preheader ], [ %28, %for_end17 ]
  %26 = mul nuw nsw i64 %indvars.iv97, 7168
  %27 = mul nuw nsw i64 %indvars.iv97, 7680
  %28 = add nuw nsw i64 %indvars.iv97, 1
  %29 = mul nuw nsw i64 %28, 7680
  %30 = mul i64 %indvars.iv97, 7680
  %31 = add i64 %30, 15360
  br label %for_begin18.preheader

for_begin27.preheader:                            ; preds = %for_end17
  %32 = bitcast i8* %2 to float*
  br label %for_begin30.preheader

for_begin18.preheader:                            ; preds = %for_begin15.preheader, %for_end20
  %indvars.iv94 = phi i64 [ 0, %for_begin15.preheader ], [ %indvars.iv.next95, %for_end20 ]
  %33 = shl nsw i64 %indvars.iv94, 8
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_begin18.preheader
  %index = phi i64 [ 0, %for_begin18.preheader ], [ %index.next, %vector.body ]
  %34 = add nuw nsw i64 %index, %33
  %35 = add nuw nsw i64 %34, %26
  %36 = getelementptr inbounds float, float* %22, i64 %35
  %37 = add nuw nsw i64 %34, %27
  %38 = getelementptr inbounds float, float* %23, i64 %37
  %39 = bitcast float* %38 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %39, align 16, !tbaa !3327
  %40 = getelementptr inbounds float, float* %24, i64 %index
  %41 = bitcast float* %40 to <4 x float>*
  %wide.load115 = load <4 x float>, <4 x float>* %41, align 16, !tbaa !3330
  %42 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load, <4 x float> %wide.load115, <4 x float> zeroinitializer)
  %43 = add nuw nsw i64 %37, 256
  %44 = getelementptr inbounds float, float* %23, i64 %43
  %45 = bitcast float* %44 to <4 x float>*
  %wide.load116 = load <4 x float>, <4 x float>* %45, align 16, !tbaa !3327
  %46 = add nuw nsw i64 %index, 256
  %47 = getelementptr inbounds float, float* %24, i64 %46
  %48 = bitcast float* %47 to <4 x float>*
  %wide.load117 = load <4 x float>, <4 x float>* %48, align 16, !tbaa !3330
  %49 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load116, <4 x float> %wide.load117, <4 x float> %42)
  %50 = add nuw nsw i64 %37, 512
  %51 = getelementptr inbounds float, float* %23, i64 %50
  %52 = bitcast float* %51 to <4 x float>*
  %wide.load118 = load <4 x float>, <4 x float>* %52, align 16, !tbaa !3327
  %53 = add nuw nsw i64 %index, 512
  %54 = getelementptr inbounds float, float* %24, i64 %53
  %55 = bitcast float* %54 to <4 x float>*
  %wide.load119 = load <4 x float>, <4 x float>* %55, align 16, !tbaa !3330
  %56 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load118, <4 x float> %wide.load119, <4 x float> %49)
  %57 = add nuw nsw i64 %34, %29
  %58 = getelementptr inbounds float, float* %23, i64 %57
  %59 = bitcast float* %58 to <4 x float>*
  %wide.load120 = load <4 x float>, <4 x float>* %59, align 16, !tbaa !3327
  %60 = add nuw nsw i64 %index, 768
  %61 = getelementptr inbounds float, float* %24, i64 %60
  %62 = bitcast float* %61 to <4 x float>*
  %wide.load121 = load <4 x float>, <4 x float>* %62, align 16, !tbaa !3330
  %63 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load120, <4 x float> %wide.load121, <4 x float> %56)
  %64 = add nuw nsw i64 %57, 256
  %65 = getelementptr inbounds float, float* %23, i64 %64
  %66 = bitcast float* %65 to <4 x float>*
  %wide.load122 = load <4 x float>, <4 x float>* %66, align 16, !tbaa !3327
  %67 = add nuw nsw i64 %index, 1024
  %68 = getelementptr inbounds float, float* %24, i64 %67
  %69 = bitcast float* %68 to <4 x float>*
  %wide.load123 = load <4 x float>, <4 x float>* %69, align 16, !tbaa !3330
  %70 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load122, <4 x float> %wide.load123, <4 x float> %63)
  %71 = add nuw nsw i64 %57, 512
  %72 = getelementptr inbounds float, float* %23, i64 %71
  %73 = bitcast float* %72 to <4 x float>*
  %wide.load124 = load <4 x float>, <4 x float>* %73, align 16, !tbaa !3327
  %74 = add nuw nsw i64 %index, 1280
  %75 = getelementptr inbounds float, float* %24, i64 %74
  %76 = bitcast float* %75 to <4 x float>*
  %wide.load125 = load <4 x float>, <4 x float>* %76, align 16, !tbaa !3330
  %77 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load124, <4 x float> %wide.load125, <4 x float> %70)
  %78 = add nuw nsw i64 %34, %31
  %79 = getelementptr inbounds float, float* %23, i64 %78
  %80 = bitcast float* %79 to <4 x float>*
  %wide.load126 = load <4 x float>, <4 x float>* %80, align 16, !tbaa !3327
  %81 = add nuw nsw i64 %index, 1536
  %82 = getelementptr inbounds float, float* %24, i64 %81
  %83 = bitcast float* %82 to <4 x float>*
  %wide.load127 = load <4 x float>, <4 x float>* %83, align 16, !tbaa !3330
  %84 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load126, <4 x float> %wide.load127, <4 x float> %77)
  %85 = add nuw nsw i64 %78, 256
  %86 = getelementptr inbounds float, float* %23, i64 %85
  %87 = bitcast float* %86 to <4 x float>*
  %wide.load128 = load <4 x float>, <4 x float>* %87, align 16, !tbaa !3327
  %88 = add nuw nsw i64 %index, 1792
  %89 = getelementptr inbounds float, float* %24, i64 %88
  %90 = bitcast float* %89 to <4 x float>*
  %wide.load129 = load <4 x float>, <4 x float>* %90, align 16, !tbaa !3330
  %91 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load128, <4 x float> %wide.load129, <4 x float> %84)
  %92 = add nuw nsw i64 %78, 512
  %93 = getelementptr inbounds float, float* %23, i64 %92
  %94 = bitcast float* %93 to <4 x float>*
  %wide.load130 = load <4 x float>, <4 x float>* %94, align 16, !tbaa !3327
  %95 = add nuw nsw i64 %index, 2048
  %96 = getelementptr inbounds float, float* %24, i64 %95
  %97 = bitcast float* %96 to <4 x float>*
  %wide.load131 = load <4 x float>, <4 x float>* %97, align 16, !tbaa !3330
  %98 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load130, <4 x float> %wide.load131, <4 x float> %91)
  %99 = bitcast float* %36 to <4 x float>*
  store <4 x float> %98, <4 x float>* %99, align 16, !tbaa !3333
  %index.next = add i64 %index, 4
  %100 = icmp eq i64 %index.next, 256
  br i1 %100, label %for_end20, label %vector.body, !prof !335, !llvm.loop !3336

for_end17:                                        ; preds = %for_end20
  %exitcond99.not = icmp eq i64 %28, 28
  br i1 %exitcond99.not, label %for_begin27.preheader, label %for_begin15.preheader, !prof !51

for_end20:                                        ; preds = %vector.body
  %indvars.iv.next95 = add nuw nsw i64 %indvars.iv94, 1
  %exitcond96.not = icmp eq i64 %indvars.iv.next95, 28
  br i1 %exitcond96.not, label %for_end17, label %for_begin18.preheader, !prof !51

for_begin30.preheader:                            ; preds = %for_begin27.preheader, %for_end32
  %indvars.iv80 = phi i64 [ 0, %for_begin27.preheader ], [ %indvars.iv.next81, %for_end32 ]
  %101 = mul nuw nsw i64 %indvars.iv80, 7168
  br label %for_begin33.preheader

for_begin36.preheader:                            ; preds = %for_end32
  %102 = bitcast i8* %3 to float*
  br label %for_begin39.preheader

for_begin33.preheader:                            ; preds = %for_begin30.preheader, %for_end35
  %indvars.iv77 = phi i64 [ 0, %for_begin30.preheader ], [ %indvars.iv.next78, %for_end35 ]
  %103 = shl nsw i64 %indvars.iv77, 8
  %104 = add nuw nsw i64 %103, %101
  br label %vector.body134

vector.body134:                                   ; preds = %vector.body134, %for_begin33.preheader
  %index136 = phi i64 [ 0, %for_begin33.preheader ], [ %index.next137.1, %vector.body134 ]
  %105 = add nuw nsw i64 %104, %index136
  %106 = getelementptr inbounds float, float* %32, i64 %index136
  %107 = bitcast float* %106 to <4 x float>*
  %wide.load140 = load <4 x float>, <4 x float>* %107, align 64, !tbaa !3337
  %108 = getelementptr inbounds float, float* %106, i64 4
  %109 = bitcast float* %108 to <4 x float>*
  %wide.load141 = load <4 x float>, <4 x float>* %109, align 16, !tbaa !3337
  %110 = getelementptr inbounds float, float* %22, i64 %105
  %111 = bitcast float* %110 to <4 x float>*
  %wide.load142 = load <4 x float>, <4 x float>* %111, align 64, !tbaa !3333
  %112 = getelementptr inbounds float, float* %110, i64 4
  %113 = bitcast float* %112 to <4 x float>*
  %wide.load143 = load <4 x float>, <4 x float>* %113, align 16, !tbaa !3333
  %114 = fadd <4 x float> %wide.load140, %wide.load142
  %115 = fadd <4 x float> %wide.load141, %wide.load143
  %116 = bitcast float* %110 to <4 x float>*
  store <4 x float> %114, <4 x float>* %116, align 64, !tbaa !3333
  %117 = bitcast float* %112 to <4 x float>*
  store <4 x float> %115, <4 x float>* %117, align 16, !tbaa !3333
  %index.next137 = or i64 %index136, 8
  %118 = add nuw nsw i64 %104, %index.next137
  %119 = getelementptr inbounds float, float* %32, i64 %index.next137
  %120 = bitcast float* %119 to <4 x float>*
  %wide.load140.1 = load <4 x float>, <4 x float>* %120, align 32, !tbaa !3337
  %121 = getelementptr inbounds float, float* %119, i64 4
  %122 = bitcast float* %121 to <4 x float>*
  %wide.load141.1 = load <4 x float>, <4 x float>* %122, align 16, !tbaa !3337
  %123 = getelementptr inbounds float, float* %22, i64 %118
  %124 = bitcast float* %123 to <4 x float>*
  %wide.load142.1 = load <4 x float>, <4 x float>* %124, align 32, !tbaa !3333
  %125 = getelementptr inbounds float, float* %123, i64 4
  %126 = bitcast float* %125 to <4 x float>*
  %wide.load143.1 = load <4 x float>, <4 x float>* %126, align 16, !tbaa !3333
  %127 = fadd <4 x float> %wide.load140.1, %wide.load142.1
  %128 = fadd <4 x float> %wide.load141.1, %wide.load143.1
  %129 = bitcast float* %123 to <4 x float>*
  store <4 x float> %127, <4 x float>* %129, align 32, !tbaa !3333
  %130 = bitcast float* %125 to <4 x float>*
  store <4 x float> %128, <4 x float>* %130, align 16, !tbaa !3333
  %index.next137.1 = add nuw nsw i64 %index136, 16
  %131 = icmp eq i64 %index.next137.1, 256
  br i1 %131, label %for_end35, label %vector.body134, !prof !341, !llvm.loop !3340

for_end32:                                        ; preds = %for_end35
  %indvars.iv.next81 = add nuw nsw i64 %indvars.iv80, 1
  %exitcond82.not = icmp eq i64 %indvars.iv.next81, 28
  br i1 %exitcond82.not, label %for_begin36.preheader, label %for_begin30.preheader, !prof !51

for_end35:                                        ; preds = %vector.body134
  %indvars.iv.next78 = add nuw nsw i64 %indvars.iv77, 1
  %exitcond79.not = icmp eq i64 %indvars.iv.next78, 28
  br i1 %exitcond79.not, label %for_end32, label %for_begin33.preheader, !prof !51

for_begin39.preheader:                            ; preds = %for_begin36.preheader, %for_end41
  %indvars.iv71 = phi i64 [ 0, %for_begin36.preheader ], [ %indvars.iv.next72, %for_end41 ]
  %132 = mul nuw nsw i64 %indvars.iv71, 7168
  br label %for_begin42.preheader

for_end38:                                        ; preds = %for_end41
  %133 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %134 = tail call i32 %133(i32 1, i32 %4, i8* nonnull %9)
  %.not = icmp eq i32 %134, 0
  br i1 %.not, label %if_end46, label %if_then, !prof !51

for_begin42.preheader:                            ; preds = %for_begin39.preheader, %for_end44
  %indvars.iv68 = phi i64 [ 0, %for_begin39.preheader ], [ %indvars.iv.next69, %for_end44 ]
  %135 = shl nsw i64 %indvars.iv68, 8
  %136 = add nuw nsw i64 %135, %132
  br label %vector.body146

vector.body146:                                   ; preds = %vector.body146, %for_begin42.preheader
  %index148 = phi i64 [ 0, %for_begin42.preheader ], [ %index.next149, %vector.body146 ]
  %137 = add nuw nsw i64 %136, %index148
  %138 = getelementptr inbounds float, float* %22, i64 %137
  %139 = bitcast float* %138 to <4 x float>*
  %wide.load152 = load <4 x float>, <4 x float>* %139, align 32, !tbaa !3333
  %140 = getelementptr inbounds float, float* %138, i64 4
  %141 = bitcast float* %140 to <4 x float>*
  %wide.load153 = load <4 x float>, <4 x float>* %141, align 16, !tbaa !3333
  %142 = fcmp olt <4 x float> %wide.load152, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %143 = fcmp olt <4 x float> %wide.load153, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %144 = select <4 x i1> %142, <4 x float> %wide.load152, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %145 = select <4 x i1> %143, <4 x float> %wide.load153, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %146 = fcmp ogt <4 x float> %144, zeroinitializer
  %147 = fcmp ogt <4 x float> %145, zeroinitializer
  %148 = select <4 x i1> %146, <4 x float> %144, <4 x float> zeroinitializer
  %149 = select <4 x i1> %147, <4 x float> %145, <4 x float> zeroinitializer
  %150 = getelementptr inbounds float, float* %102, i64 %137
  %151 = bitcast float* %150 to <4 x float>*
  store <4 x float> %148, <4 x float>* %151, align 32, !tbaa !3341
  %152 = getelementptr inbounds float, float* %150, i64 4
  %153 = bitcast float* %152 to <4 x float>*
  store <4 x float> %149, <4 x float>* %153, align 16, !tbaa !3341
  %index.next149 = add i64 %index148, 8
  %154 = icmp eq i64 %index.next149, 256
  br i1 %154, label %for_end44, label %vector.body146, !prof !341, !llvm.loop !3344

for_end41:                                        ; preds = %for_end44
  %indvars.iv.next72 = add nuw nsw i64 %indvars.iv71, 1
  %exitcond73.not = icmp eq i64 %indvars.iv.next72, 28
  br i1 %exitcond73.not, label %for_end38, label %for_begin39.preheader, !prof !51

for_end44:                                        ; preds = %vector.body146
  %indvars.iv.next69 = add nuw nsw i64 %indvars.iv68, 1
  %exitcond70.not = icmp eq i64 %indvars.iv.next69, 28
  br i1 %exitcond70.not, label %for_end41, label %for_begin42.preheader, !prof !51

if_end46:                                         ; preds = %for_end38
  %155 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %156 = tail call i32 %155(i32 1, i32 %4, i8* nonnull %6)
  %.not57 = icmp ne i32 %156, 0
  %spec.select = sext i1 %.not57 to i32
  ret i32 %spec.select
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add_clip_15(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([88 x i8], [88 x i8]* @.str.245, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !3345
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !3359
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !3361
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !3364
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.246, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.247, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.248, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.249, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !3366
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !3380
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 112
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.8, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !3382
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 112
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([128 x i8], [128 x i8]* @.str.9, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !3385
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 64
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.201, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !3387
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 802816, i32 7168, i32 64, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.250, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !3399
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 3
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.51, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !3413
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 3
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.52, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !3415
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 64
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.203, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !3418
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 1
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.54, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !3420
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 192, i32 64, i32 1, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([195 x i8], [195 x i8]* @.str.251, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !3432
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 64
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.26, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !3446
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !3460
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !3474
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 56
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.82, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !3476
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 56
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.83, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !3479
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 64
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([126 x i8], [126 x i8]* @.str.36, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !3481
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 200704, i32 3584, i32 64, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.252, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_15_compute_(i8* %31, i8* %39, i8* %45, i8* %51, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_clip_15_compute_(i8* noalias nocapture readonly align 128 %0, i8* noalias nocapture readonly align 128 %1, i8* noalias nocapture readonly align 128 %2, i8* noalias nocapture align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 3268864, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %if_end, !prof !5

if_then:                                          ; preds = %for_end38, %if_end, %entry
  ret i32 -1

if_end:                                           ; preds = %entry
  %8 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %9 = tail call i8* %8(i32 1, i32 %4, i64 802816, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %9, i64 128) ]
  %10 = icmp eq i8* %9, null
  br i1 %10, label %if_then, label %for_begin4.preheader, !prof !5

for_begin4.preheader:                             ; preds = %if_end, %for_end6
  %indvar = phi i64 [ %indvar.next, %for_end6 ], [ 0, %if_end ]
  %11 = phi i32 [ %24, %for_end6 ], [ 0, %if_end ]
  %12 = mul nuw nsw i64 %indvar, 28928
  %13 = mul nuw nsw i64 %indvar, 28672
  %14 = icmp ult i32 %11, 112
  br i1 %14, label %for_begin7.preheader.us, label %for_begin7.preheader.preheader

for_begin7.preheader.preheader:                   ; preds = %for_begin4.preheader
  %scevgep103 = getelementptr i8, i8* %6, i64 %12
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(28928) %scevgep103, i8 0, i64 28928, i1 false)
  br label %for_end6

for_begin7.preheader.us:                          ; preds = %for_begin4.preheader, %for_end9.us
  %indvar104 = phi i64 [ %indvar.next105, %for_end9.us ], [ 0, %for_begin4.preheader ]
  %15 = phi i32 [ %20, %for_end9.us ], [ 0, %for_begin4.preheader ]
  %16 = shl nuw nsw i64 %indvar104, 8
  %17 = add nuw nsw i64 %12, %16
  %scevgep108 = getelementptr i8, i8* %6, i64 %17
  %18 = icmp ult i32 %15, 112
  br i1 %18, label %for_body8.us.us.preheader, label %for_body8.us60.preheader

for_body8.us60.preheader:                         ; preds = %for_begin7.preheader.us
  call void @llvm.memset.p0i8.i64(i8* nonnull align 128 dereferenceable(256) %scevgep108, i8 0, i64 256, i1 false)
  br label %for_end9.us

for_body8.us.us.preheader:                        ; preds = %for_begin7.preheader.us
  %19 = add nuw nsw i64 %13, %16
  %scevgep109 = getelementptr i8, i8* %0, i64 %19
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(256) %scevgep108, i8* nonnull align 128 dereferenceable(256) %scevgep109, i64 256, i1 false)
  br label %for_end9.us

for_end9.us:                                      ; preds = %for_body8.us60.preheader, %for_body8.us.us.preheader
  %20 = add nuw nsw i32 %15, 1
  %indvar.next105 = add nuw nsw i64 %indvar104, 1
  %exitcond112.not = icmp eq i64 %indvar.next105, 113
  br i1 %exitcond112.not, label %for_end6, label %for_begin7.preheader.us, !prof !51

for_begin12.preheader:                            ; preds = %for_end6
  %21 = bitcast i8* %9 to float*
  %22 = bitcast i8* %6 to float*
  %23 = bitcast i8* %1 to float*
  br label %for_begin15.preheader

for_end6:                                         ; preds = %for_end9.us, %for_begin7.preheader.preheader
  %24 = add nuw nsw i32 %11, 1
  %indvar.next = add nuw nsw i64 %indvar, 1
  %exitcond113.not = icmp eq i64 %indvar.next, 113
  br i1 %exitcond113.not, label %for_begin12.preheader, label %for_begin4.preheader, !prof !51

for_begin15.preheader:                            ; preds = %for_begin12.preheader, %for_end17
  %indvars.iv96 = phi i64 [ 0, %for_begin12.preheader ], [ %indvars.iv.next97, %for_end17 ]
  %25 = mul nuw nsw i64 %indvars.iv96, 3584
  %26 = mul nuw nsw i64 %indvars.iv96, 14464
  br label %for_begin18.preheader

for_begin27.preheader:                            ; preds = %for_end17
  %27 = bitcast i8* %2 to <4 x float>*
  %wide.load139 = load <4 x float>, <4 x float>* %27, align 128, !tbaa !3493
  %28 = getelementptr inbounds i8, i8* %2, i64 16
  %29 = bitcast i8* %28 to <4 x float>*
  %wide.load139.1 = load <4 x float>, <4 x float>* %29, align 16, !tbaa !3493
  %30 = getelementptr inbounds i8, i8* %2, i64 32
  %31 = bitcast i8* %30 to <4 x float>*
  %wide.load139.2 = load <4 x float>, <4 x float>* %31, align 32, !tbaa !3493
  %32 = getelementptr inbounds i8, i8* %2, i64 48
  %33 = bitcast i8* %32 to <4 x float>*
  %wide.load139.3 = load <4 x float>, <4 x float>* %33, align 16, !tbaa !3493
  %34 = getelementptr inbounds i8, i8* %2, i64 64
  %35 = bitcast i8* %34 to <4 x float>*
  %wide.load139.4 = load <4 x float>, <4 x float>* %35, align 64, !tbaa !3493
  %36 = getelementptr inbounds i8, i8* %2, i64 80
  %37 = bitcast i8* %36 to <4 x float>*
  %wide.load139.5 = load <4 x float>, <4 x float>* %37, align 16, !tbaa !3493
  %38 = getelementptr inbounds i8, i8* %2, i64 96
  %39 = bitcast i8* %38 to <4 x float>*
  %wide.load139.6 = load <4 x float>, <4 x float>* %39, align 32, !tbaa !3493
  %40 = getelementptr inbounds i8, i8* %2, i64 112
  %41 = bitcast i8* %40 to <4 x float>*
  %wide.load139.7 = load <4 x float>, <4 x float>* %41, align 16, !tbaa !3493
  %42 = getelementptr inbounds i8, i8* %2, i64 128
  %43 = bitcast i8* %42 to <4 x float>*
  %wide.load139.8 = load <4 x float>, <4 x float>* %43, align 128, !tbaa !3493
  %44 = getelementptr inbounds i8, i8* %2, i64 144
  %45 = bitcast i8* %44 to <4 x float>*
  %wide.load139.9 = load <4 x float>, <4 x float>* %45, align 16, !tbaa !3493
  %46 = getelementptr inbounds i8, i8* %2, i64 160
  %47 = bitcast i8* %46 to <4 x float>*
  %wide.load139.10 = load <4 x float>, <4 x float>* %47, align 32, !tbaa !3493
  %48 = getelementptr inbounds i8, i8* %2, i64 176
  %49 = bitcast i8* %48 to <4 x float>*
  %wide.load139.11 = load <4 x float>, <4 x float>* %49, align 16, !tbaa !3493
  %50 = getelementptr inbounds i8, i8* %2, i64 192
  %51 = bitcast i8* %50 to <4 x float>*
  %wide.load139.12 = load <4 x float>, <4 x float>* %51, align 64, !tbaa !3493
  %52 = getelementptr inbounds i8, i8* %2, i64 208
  %53 = bitcast i8* %52 to <4 x float>*
  %wide.load139.13 = load <4 x float>, <4 x float>* %53, align 16, !tbaa !3493
  %54 = getelementptr inbounds i8, i8* %2, i64 224
  %55 = bitcast i8* %54 to <4 x float>*
  %wide.load139.14 = load <4 x float>, <4 x float>* %55, align 32, !tbaa !3493
  %56 = getelementptr inbounds i8, i8* %2, i64 240
  %57 = bitcast i8* %56 to <4 x float>*
  %wide.load139.15 = load <4 x float>, <4 x float>* %57, align 16, !tbaa !3493
  br label %for_begin30.preheader

for_begin18.preheader:                            ; preds = %for_begin15.preheader, %for_end20
  %indvars.iv93 = phi i64 [ 0, %for_begin15.preheader ], [ %indvars.iv.next94, %for_end20 ]
  %58 = shl nsw i64 %indvars.iv93, 6
  %59 = add nuw nsw i64 %58, %25
  %60 = shl nsw i64 %indvars.iv93, 7
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %for_begin18.preheader
  %index = phi i64 [ 0, %for_begin18.preheader ], [ %index.next, %vector.body ]
  %61 = add nuw nsw i64 %59, %index
  %62 = getelementptr inbounds float, float* %21, i64 %61
  %63 = add nuw nsw i64 %index, %26
  %64 = add nuw nsw i64 %63, %60
  %65 = getelementptr inbounds float, float* %22, i64 %64
  %66 = bitcast float* %65 to <4 x float>*
  %wide.load = load <4 x float>, <4 x float>* %66, align 16, !tbaa !3496
  %67 = getelementptr inbounds float, float* %23, i64 %index
  %68 = bitcast float* %67 to <4 x float>*
  %wide.load114 = load <4 x float>, <4 x float>* %68, align 16, !tbaa !3499
  %69 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load, <4 x float> %wide.load114, <4 x float> zeroinitializer)
  %70 = add nuw nsw i64 %index, 64
  %71 = add nuw nsw i64 %70, %26
  %72 = add nuw nsw i64 %71, %60
  %73 = getelementptr inbounds float, float* %22, i64 %72
  %74 = bitcast float* %73 to <4 x float>*
  %wide.load115 = load <4 x float>, <4 x float>* %74, align 16, !tbaa !3496
  %75 = getelementptr inbounds float, float* %23, i64 %70
  %76 = bitcast float* %75 to <4 x float>*
  %wide.load116 = load <4 x float>, <4 x float>* %76, align 16, !tbaa !3499
  %77 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load115, <4 x float> %wide.load116, <4 x float> %69)
  %78 = add nuw nsw i64 %index, 128
  %79 = add nuw nsw i64 %78, %26
  %80 = add nuw nsw i64 %79, %60
  %81 = getelementptr inbounds float, float* %22, i64 %80
  %82 = bitcast float* %81 to <4 x float>*
  %wide.load117 = load <4 x float>, <4 x float>* %82, align 16, !tbaa !3496
  %83 = getelementptr inbounds float, float* %23, i64 %78
  %84 = bitcast float* %83 to <4 x float>*
  %wide.load118 = load <4 x float>, <4 x float>* %84, align 16, !tbaa !3499
  %85 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load117, <4 x float> %wide.load118, <4 x float> %77)
  %86 = add nuw nsw i64 %64, 7232
  %87 = getelementptr inbounds float, float* %22, i64 %86
  %88 = bitcast float* %87 to <4 x float>*
  %wide.load119 = load <4 x float>, <4 x float>* %88, align 16, !tbaa !3496
  %89 = add nuw nsw i64 %index, 192
  %90 = getelementptr inbounds float, float* %23, i64 %89
  %91 = bitcast float* %90 to <4 x float>*
  %wide.load120 = load <4 x float>, <4 x float>* %91, align 16, !tbaa !3499
  %92 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load119, <4 x float> %wide.load120, <4 x float> %85)
  %93 = add nuw nsw i64 %72, 7232
  %94 = getelementptr inbounds float, float* %22, i64 %93
  %95 = bitcast float* %94 to <4 x float>*
  %wide.load121 = load <4 x float>, <4 x float>* %95, align 16, !tbaa !3496
  %96 = add nuw nsw i64 %index, 256
  %97 = getelementptr inbounds float, float* %23, i64 %96
  %98 = bitcast float* %97 to <4 x float>*
  %wide.load122 = load <4 x float>, <4 x float>* %98, align 16, !tbaa !3499
  %99 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load121, <4 x float> %wide.load122, <4 x float> %92)
  %100 = add nuw nsw i64 %80, 7232
  %101 = getelementptr inbounds float, float* %22, i64 %100
  %102 = bitcast float* %101 to <4 x float>*
  %wide.load123 = load <4 x float>, <4 x float>* %102, align 16, !tbaa !3496
  %103 = add nuw nsw i64 %index, 320
  %104 = getelementptr inbounds float, float* %23, i64 %103
  %105 = bitcast float* %104 to <4 x float>*
  %wide.load124 = load <4 x float>, <4 x float>* %105, align 16, !tbaa !3499
  %106 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load123, <4 x float> %wide.load124, <4 x float> %99)
  %107 = add nuw nsw i64 %64, 14464
  %108 = getelementptr inbounds float, float* %22, i64 %107
  %109 = bitcast float* %108 to <4 x float>*
  %wide.load125 = load <4 x float>, <4 x float>* %109, align 16, !tbaa !3496
  %110 = add nuw nsw i64 %index, 384
  %111 = getelementptr inbounds float, float* %23, i64 %110
  %112 = bitcast float* %111 to <4 x float>*
  %wide.load126 = load <4 x float>, <4 x float>* %112, align 16, !tbaa !3499
  %113 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load125, <4 x float> %wide.load126, <4 x float> %106)
  %114 = add nuw nsw i64 %72, 14464
  %115 = getelementptr inbounds float, float* %22, i64 %114
  %116 = bitcast float* %115 to <4 x float>*
  %wide.load127 = load <4 x float>, <4 x float>* %116, align 16, !tbaa !3496
  %117 = add nuw nsw i64 %index, 448
  %118 = getelementptr inbounds float, float* %23, i64 %117
  %119 = bitcast float* %118 to <4 x float>*
  %wide.load128 = load <4 x float>, <4 x float>* %119, align 16, !tbaa !3499
  %120 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load127, <4 x float> %wide.load128, <4 x float> %113)
  %121 = add nuw nsw i64 %80, 14464
  %122 = getelementptr inbounds float, float* %22, i64 %121
  %123 = bitcast float* %122 to <4 x float>*
  %wide.load129 = load <4 x float>, <4 x float>* %123, align 16, !tbaa !3496
  %124 = add nuw nsw i64 %index, 512
  %125 = getelementptr inbounds float, float* %23, i64 %124
  %126 = bitcast float* %125 to <4 x float>*
  %wide.load130 = load <4 x float>, <4 x float>* %126, align 16, !tbaa !3499
  %127 = call <4 x float> @llvm.fmuladd.v4f32(<4 x float> %wide.load129, <4 x float> %wide.load130, <4 x float> %120)
  %128 = bitcast float* %62 to <4 x float>*
  store <4 x float> %127, <4 x float>* %128, align 16, !tbaa !3502
  %index.next = add i64 %index, 4
  %129 = icmp eq i64 %index.next, 64
  br i1 %129, label %for_end20, label %vector.body, !prof !335, !llvm.loop !3505

for_end17:                                        ; preds = %for_end20
  %indvars.iv.next97 = add nuw nsw i64 %indvars.iv96, 1
  %exitcond98.not = icmp eq i64 %indvars.iv.next97, 56
  br i1 %exitcond98.not, label %for_begin27.preheader, label %for_begin15.preheader, !prof !51

for_end20:                                        ; preds = %vector.body
  %indvars.iv.next94 = add nuw nsw i64 %indvars.iv93, 1
  %exitcond95.not = icmp eq i64 %indvars.iv.next94, 56
  br i1 %exitcond95.not, label %for_end17, label %for_begin18.preheader, !prof !51

for_begin30.preheader:                            ; preds = %for_begin27.preheader, %for_end32
  %indvars.iv79 = phi i64 [ 0, %for_begin27.preheader ], [ %indvars.iv.next80, %for_end32 ]
  %130 = mul nuw nsw i64 %indvars.iv79, 3584
  br label %for_begin33.preheader

for_begin36.preheader:                            ; preds = %for_end32
  %131 = bitcast i8* %3 to float*
  br label %for_begin39.preheader

for_begin33.preheader:                            ; preds = %for_begin30.preheader, %for_begin33.preheader
  %indvars.iv76 = phi i64 [ 0, %for_begin30.preheader ], [ %indvars.iv.next77, %for_begin33.preheader ]
  %132 = shl nsw i64 %indvars.iv76, 6
  %133 = add nuw nsw i64 %132, %130
  %134 = getelementptr inbounds float, float* %21, i64 %133
  %135 = bitcast float* %134 to <4 x float>*
  %wide.load140 = load <4 x float>, <4 x float>* %135, align 128, !tbaa !3502
  %136 = fadd <4 x float> %wide.load139, %wide.load140
  %137 = bitcast float* %134 to <4 x float>*
  store <4 x float> %136, <4 x float>* %137, align 128, !tbaa !3502
  %138 = or i64 %133, 4
  %139 = getelementptr inbounds float, float* %21, i64 %138
  %140 = bitcast float* %139 to <4 x float>*
  %wide.load140.1 = load <4 x float>, <4 x float>* %140, align 16, !tbaa !3502
  %141 = fadd <4 x float> %wide.load139.1, %wide.load140.1
  %142 = bitcast float* %139 to <4 x float>*
  store <4 x float> %141, <4 x float>* %142, align 16, !tbaa !3502
  %143 = or i64 %133, 8
  %144 = getelementptr inbounds float, float* %21, i64 %143
  %145 = bitcast float* %144 to <4 x float>*
  %wide.load140.2 = load <4 x float>, <4 x float>* %145, align 32, !tbaa !3502
  %146 = fadd <4 x float> %wide.load139.2, %wide.load140.2
  %147 = bitcast float* %144 to <4 x float>*
  store <4 x float> %146, <4 x float>* %147, align 32, !tbaa !3502
  %148 = or i64 %133, 12
  %149 = getelementptr inbounds float, float* %21, i64 %148
  %150 = bitcast float* %149 to <4 x float>*
  %wide.load140.3 = load <4 x float>, <4 x float>* %150, align 16, !tbaa !3502
  %151 = fadd <4 x float> %wide.load139.3, %wide.load140.3
  %152 = bitcast float* %149 to <4 x float>*
  store <4 x float> %151, <4 x float>* %152, align 16, !tbaa !3502
  %153 = or i64 %133, 16
  %154 = getelementptr inbounds float, float* %21, i64 %153
  %155 = bitcast float* %154 to <4 x float>*
  %wide.load140.4 = load <4 x float>, <4 x float>* %155, align 64, !tbaa !3502
  %156 = fadd <4 x float> %wide.load139.4, %wide.load140.4
  %157 = bitcast float* %154 to <4 x float>*
  store <4 x float> %156, <4 x float>* %157, align 64, !tbaa !3502
  %158 = or i64 %133, 20
  %159 = getelementptr inbounds float, float* %21, i64 %158
  %160 = bitcast float* %159 to <4 x float>*
  %wide.load140.5 = load <4 x float>, <4 x float>* %160, align 16, !tbaa !3502
  %161 = fadd <4 x float> %wide.load139.5, %wide.load140.5
  %162 = bitcast float* %159 to <4 x float>*
  store <4 x float> %161, <4 x float>* %162, align 16, !tbaa !3502
  %163 = or i64 %133, 24
  %164 = getelementptr inbounds float, float* %21, i64 %163
  %165 = bitcast float* %164 to <4 x float>*
  %wide.load140.6 = load <4 x float>, <4 x float>* %165, align 32, !tbaa !3502
  %166 = fadd <4 x float> %wide.load139.6, %wide.load140.6
  %167 = bitcast float* %164 to <4 x float>*
  store <4 x float> %166, <4 x float>* %167, align 32, !tbaa !3502
  %168 = or i64 %133, 28
  %169 = getelementptr inbounds float, float* %21, i64 %168
  %170 = bitcast float* %169 to <4 x float>*
  %wide.load140.7 = load <4 x float>, <4 x float>* %170, align 16, !tbaa !3502
  %171 = fadd <4 x float> %wide.load139.7, %wide.load140.7
  %172 = bitcast float* %169 to <4 x float>*
  store <4 x float> %171, <4 x float>* %172, align 16, !tbaa !3502
  %173 = or i64 %133, 32
  %174 = getelementptr inbounds float, float* %21, i64 %173
  %175 = bitcast float* %174 to <4 x float>*
  %wide.load140.8 = load <4 x float>, <4 x float>* %175, align 128, !tbaa !3502
  %176 = fadd <4 x float> %wide.load139.8, %wide.load140.8
  %177 = bitcast float* %174 to <4 x float>*
  store <4 x float> %176, <4 x float>* %177, align 128, !tbaa !3502
  %178 = or i64 %133, 36
  %179 = getelementptr inbounds float, float* %21, i64 %178
  %180 = bitcast float* %179 to <4 x float>*
  %wide.load140.9 = load <4 x float>, <4 x float>* %180, align 16, !tbaa !3502
  %181 = fadd <4 x float> %wide.load139.9, %wide.load140.9
  %182 = bitcast float* %179 to <4 x float>*
  store <4 x float> %181, <4 x float>* %182, align 16, !tbaa !3502
  %183 = or i64 %133, 40
  %184 = getelementptr inbounds float, float* %21, i64 %183
  %185 = bitcast float* %184 to <4 x float>*
  %wide.load140.10 = load <4 x float>, <4 x float>* %185, align 32, !tbaa !3502
  %186 = fadd <4 x float> %wide.load139.10, %wide.load140.10
  %187 = bitcast float* %184 to <4 x float>*
  store <4 x float> %186, <4 x float>* %187, align 32, !tbaa !3502
  %188 = or i64 %133, 44
  %189 = getelementptr inbounds float, float* %21, i64 %188
  %190 = bitcast float* %189 to <4 x float>*
  %wide.load140.11 = load <4 x float>, <4 x float>* %190, align 16, !tbaa !3502
  %191 = fadd <4 x float> %wide.load139.11, %wide.load140.11
  %192 = bitcast float* %189 to <4 x float>*
  store <4 x float> %191, <4 x float>* %192, align 16, !tbaa !3502
  %193 = or i64 %133, 48
  %194 = getelementptr inbounds float, float* %21, i64 %193
  %195 = bitcast float* %194 to <4 x float>*
  %wide.load140.12 = load <4 x float>, <4 x float>* %195, align 64, !tbaa !3502
  %196 = fadd <4 x float> %wide.load139.12, %wide.load140.12
  %197 = bitcast float* %194 to <4 x float>*
  store <4 x float> %196, <4 x float>* %197, align 64, !tbaa !3502
  %198 = or i64 %133, 52
  %199 = getelementptr inbounds float, float* %21, i64 %198
  %200 = bitcast float* %199 to <4 x float>*
  %wide.load140.13 = load <4 x float>, <4 x float>* %200, align 16, !tbaa !3502
  %201 = fadd <4 x float> %wide.load139.13, %wide.load140.13
  %202 = bitcast float* %199 to <4 x float>*
  store <4 x float> %201, <4 x float>* %202, align 16, !tbaa !3502
  %203 = or i64 %133, 56
  %204 = getelementptr inbounds float, float* %21, i64 %203
  %205 = bitcast float* %204 to <4 x float>*
  %wide.load140.14 = load <4 x float>, <4 x float>* %205, align 32, !tbaa !3502
  %206 = fadd <4 x float> %wide.load139.14, %wide.load140.14
  %207 = bitcast float* %204 to <4 x float>*
  store <4 x float> %206, <4 x float>* %207, align 32, !tbaa !3502
  %208 = or i64 %133, 60
  %209 = getelementptr inbounds float, float* %21, i64 %208
  %210 = bitcast float* %209 to <4 x float>*
  %wide.load140.15 = load <4 x float>, <4 x float>* %210, align 16, !tbaa !3502
  %211 = fadd <4 x float> %wide.load139.15, %wide.load140.15
  %212 = bitcast float* %209 to <4 x float>*
  store <4 x float> %211, <4 x float>* %212, align 16, !tbaa !3502
  %indvars.iv.next77 = add nuw nsw i64 %indvars.iv76, 1
  %exitcond78.not = icmp eq i64 %indvars.iv.next77, 56
  br i1 %exitcond78.not, label %for_end32, label %for_begin33.preheader, !prof !51

for_end32:                                        ; preds = %for_begin33.preheader
  %indvars.iv.next80 = add nuw nsw i64 %indvars.iv79, 1
  %exitcond81.not = icmp eq i64 %indvars.iv.next80, 56
  br i1 %exitcond81.not, label %for_begin36.preheader, label %for_begin30.preheader, !prof !51

for_begin39.preheader:                            ; preds = %for_begin36.preheader, %for_end41
  %indvars.iv70 = phi i64 [ 0, %for_begin36.preheader ], [ %indvars.iv.next71, %for_end41 ]
  %213 = mul nuw nsw i64 %indvars.iv70, 3584
  br label %for_begin42.preheader

for_end38:                                        ; preds = %for_end41
  %214 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %215 = tail call i32 %214(i32 1, i32 %4, i8* nonnull %9)
  %.not = icmp eq i32 %215, 0
  br i1 %.not, label %if_end46, label %if_then, !prof !51

for_begin42.preheader:                            ; preds = %for_begin39.preheader, %for_begin42.preheader
  %indvars.iv67 = phi i64 [ 0, %for_begin39.preheader ], [ %indvars.iv.next68, %for_begin42.preheader ]
  %216 = shl nsw i64 %indvars.iv67, 6
  %217 = add nuw nsw i64 %216, %213
  %218 = getelementptr inbounds float, float* %21, i64 %217
  %219 = bitcast float* %218 to <4 x float>*
  %wide.load149 = load <4 x float>, <4 x float>* %219, align 128, !tbaa !3502
  %220 = fcmp olt <4 x float> %wide.load149, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %221 = select <4 x i1> %220, <4 x float> %wide.load149, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %222 = fcmp ogt <4 x float> %221, zeroinitializer
  %223 = select <4 x i1> %222, <4 x float> %221, <4 x float> zeroinitializer
  %224 = getelementptr inbounds float, float* %131, i64 %217
  %225 = bitcast float* %224 to <4 x float>*
  store <4 x float> %223, <4 x float>* %225, align 128, !tbaa !3506
  %226 = or i64 %217, 4
  %227 = getelementptr inbounds float, float* %21, i64 %226
  %228 = bitcast float* %227 to <4 x float>*
  %wide.load149.1 = load <4 x float>, <4 x float>* %228, align 16, !tbaa !3502
  %229 = fcmp olt <4 x float> %wide.load149.1, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %230 = select <4 x i1> %229, <4 x float> %wide.load149.1, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %231 = fcmp ogt <4 x float> %230, zeroinitializer
  %232 = select <4 x i1> %231, <4 x float> %230, <4 x float> zeroinitializer
  %233 = getelementptr inbounds float, float* %131, i64 %226
  %234 = bitcast float* %233 to <4 x float>*
  store <4 x float> %232, <4 x float>* %234, align 16, !tbaa !3506
  %235 = or i64 %217, 8
  %236 = getelementptr inbounds float, float* %21, i64 %235
  %237 = bitcast float* %236 to <4 x float>*
  %wide.load149.2 = load <4 x float>, <4 x float>* %237, align 32, !tbaa !3502
  %238 = fcmp olt <4 x float> %wide.load149.2, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %239 = select <4 x i1> %238, <4 x float> %wide.load149.2, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %240 = fcmp ogt <4 x float> %239, zeroinitializer
  %241 = select <4 x i1> %240, <4 x float> %239, <4 x float> zeroinitializer
  %242 = getelementptr inbounds float, float* %131, i64 %235
  %243 = bitcast float* %242 to <4 x float>*
  store <4 x float> %241, <4 x float>* %243, align 32, !tbaa !3506
  %244 = or i64 %217, 12
  %245 = getelementptr inbounds float, float* %21, i64 %244
  %246 = bitcast float* %245 to <4 x float>*
  %wide.load149.3 = load <4 x float>, <4 x float>* %246, align 16, !tbaa !3502
  %247 = fcmp olt <4 x float> %wide.load149.3, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %248 = select <4 x i1> %247, <4 x float> %wide.load149.3, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %249 = fcmp ogt <4 x float> %248, zeroinitializer
  %250 = select <4 x i1> %249, <4 x float> %248, <4 x float> zeroinitializer
  %251 = getelementptr inbounds float, float* %131, i64 %244
  %252 = bitcast float* %251 to <4 x float>*
  store <4 x float> %250, <4 x float>* %252, align 16, !tbaa !3506
  %253 = or i64 %217, 16
  %254 = getelementptr inbounds float, float* %21, i64 %253
  %255 = bitcast float* %254 to <4 x float>*
  %wide.load149.4 = load <4 x float>, <4 x float>* %255, align 64, !tbaa !3502
  %256 = fcmp olt <4 x float> %wide.load149.4, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %257 = select <4 x i1> %256, <4 x float> %wide.load149.4, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %258 = fcmp ogt <4 x float> %257, zeroinitializer
  %259 = select <4 x i1> %258, <4 x float> %257, <4 x float> zeroinitializer
  %260 = getelementptr inbounds float, float* %131, i64 %253
  %261 = bitcast float* %260 to <4 x float>*
  store <4 x float> %259, <4 x float>* %261, align 64, !tbaa !3506
  %262 = or i64 %217, 20
  %263 = getelementptr inbounds float, float* %21, i64 %262
  %264 = bitcast float* %263 to <4 x float>*
  %wide.load149.5 = load <4 x float>, <4 x float>* %264, align 16, !tbaa !3502
  %265 = fcmp olt <4 x float> %wide.load149.5, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %266 = select <4 x i1> %265, <4 x float> %wide.load149.5, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %267 = fcmp ogt <4 x float> %266, zeroinitializer
  %268 = select <4 x i1> %267, <4 x float> %266, <4 x float> zeroinitializer
  %269 = getelementptr inbounds float, float* %131, i64 %262
  %270 = bitcast float* %269 to <4 x float>*
  store <4 x float> %268, <4 x float>* %270, align 16, !tbaa !3506
  %271 = or i64 %217, 24
  %272 = getelementptr inbounds float, float* %21, i64 %271
  %273 = bitcast float* %272 to <4 x float>*
  %wide.load149.6 = load <4 x float>, <4 x float>* %273, align 32, !tbaa !3502
  %274 = fcmp olt <4 x float> %wide.load149.6, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %275 = select <4 x i1> %274, <4 x float> %wide.load149.6, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %276 = fcmp ogt <4 x float> %275, zeroinitializer
  %277 = select <4 x i1> %276, <4 x float> %275, <4 x float> zeroinitializer
  %278 = getelementptr inbounds float, float* %131, i64 %271
  %279 = bitcast float* %278 to <4 x float>*
  store <4 x float> %277, <4 x float>* %279, align 32, !tbaa !3506
  %280 = or i64 %217, 28
  %281 = getelementptr inbounds float, float* %21, i64 %280
  %282 = bitcast float* %281 to <4 x float>*
  %wide.load149.7 = load <4 x float>, <4 x float>* %282, align 16, !tbaa !3502
  %283 = fcmp olt <4 x float> %wide.load149.7, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %284 = select <4 x i1> %283, <4 x float> %wide.load149.7, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %285 = fcmp ogt <4 x float> %284, zeroinitializer
  %286 = select <4 x i1> %285, <4 x float> %284, <4 x float> zeroinitializer
  %287 = getelementptr inbounds float, float* %131, i64 %280
  %288 = bitcast float* %287 to <4 x float>*
  store <4 x float> %286, <4 x float>* %288, align 16, !tbaa !3506
  %289 = or i64 %217, 32
  %290 = getelementptr inbounds float, float* %21, i64 %289
  %291 = bitcast float* %290 to <4 x float>*
  %wide.load149.8 = load <4 x float>, <4 x float>* %291, align 128, !tbaa !3502
  %292 = fcmp olt <4 x float> %wide.load149.8, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %293 = select <4 x i1> %292, <4 x float> %wide.load149.8, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %294 = fcmp ogt <4 x float> %293, zeroinitializer
  %295 = select <4 x i1> %294, <4 x float> %293, <4 x float> zeroinitializer
  %296 = getelementptr inbounds float, float* %131, i64 %289
  %297 = bitcast float* %296 to <4 x float>*
  store <4 x float> %295, <4 x float>* %297, align 128, !tbaa !3506
  %298 = or i64 %217, 36
  %299 = getelementptr inbounds float, float* %21, i64 %298
  %300 = bitcast float* %299 to <4 x float>*
  %wide.load149.9 = load <4 x float>, <4 x float>* %300, align 16, !tbaa !3502
  %301 = fcmp olt <4 x float> %wide.load149.9, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %302 = select <4 x i1> %301, <4 x float> %wide.load149.9, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %303 = fcmp ogt <4 x float> %302, zeroinitializer
  %304 = select <4 x i1> %303, <4 x float> %302, <4 x float> zeroinitializer
  %305 = getelementptr inbounds float, float* %131, i64 %298
  %306 = bitcast float* %305 to <4 x float>*
  store <4 x float> %304, <4 x float>* %306, align 16, !tbaa !3506
  %307 = or i64 %217, 40
  %308 = getelementptr inbounds float, float* %21, i64 %307
  %309 = bitcast float* %308 to <4 x float>*
  %wide.load149.10 = load <4 x float>, <4 x float>* %309, align 32, !tbaa !3502
  %310 = fcmp olt <4 x float> %wide.load149.10, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %311 = select <4 x i1> %310, <4 x float> %wide.load149.10, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %312 = fcmp ogt <4 x float> %311, zeroinitializer
  %313 = select <4 x i1> %312, <4 x float> %311, <4 x float> zeroinitializer
  %314 = getelementptr inbounds float, float* %131, i64 %307
  %315 = bitcast float* %314 to <4 x float>*
  store <4 x float> %313, <4 x float>* %315, align 32, !tbaa !3506
  %316 = or i64 %217, 44
  %317 = getelementptr inbounds float, float* %21, i64 %316
  %318 = bitcast float* %317 to <4 x float>*
  %wide.load149.11 = load <4 x float>, <4 x float>* %318, align 16, !tbaa !3502
  %319 = fcmp olt <4 x float> %wide.load149.11, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %320 = select <4 x i1> %319, <4 x float> %wide.load149.11, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %321 = fcmp ogt <4 x float> %320, zeroinitializer
  %322 = select <4 x i1> %321, <4 x float> %320, <4 x float> zeroinitializer
  %323 = getelementptr inbounds float, float* %131, i64 %316
  %324 = bitcast float* %323 to <4 x float>*
  store <4 x float> %322, <4 x float>* %324, align 16, !tbaa !3506
  %325 = or i64 %217, 48
  %326 = getelementptr inbounds float, float* %21, i64 %325
  %327 = bitcast float* %326 to <4 x float>*
  %wide.load149.12 = load <4 x float>, <4 x float>* %327, align 64, !tbaa !3502
  %328 = fcmp olt <4 x float> %wide.load149.12, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %329 = select <4 x i1> %328, <4 x float> %wide.load149.12, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %330 = fcmp ogt <4 x float> %329, zeroinitializer
  %331 = select <4 x i1> %330, <4 x float> %329, <4 x float> zeroinitializer
  %332 = getelementptr inbounds float, float* %131, i64 %325
  %333 = bitcast float* %332 to <4 x float>*
  store <4 x float> %331, <4 x float>* %333, align 64, !tbaa !3506
  %334 = or i64 %217, 52
  %335 = getelementptr inbounds float, float* %21, i64 %334
  %336 = bitcast float* %335 to <4 x float>*
  %wide.load149.13 = load <4 x float>, <4 x float>* %336, align 16, !tbaa !3502
  %337 = fcmp olt <4 x float> %wide.load149.13, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %338 = select <4 x i1> %337, <4 x float> %wide.load149.13, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %339 = fcmp ogt <4 x float> %338, zeroinitializer
  %340 = select <4 x i1> %339, <4 x float> %338, <4 x float> zeroinitializer
  %341 = getelementptr inbounds float, float* %131, i64 %334
  %342 = bitcast float* %341 to <4 x float>*
  store <4 x float> %340, <4 x float>* %342, align 16, !tbaa !3506
  %343 = or i64 %217, 56
  %344 = getelementptr inbounds float, float* %21, i64 %343
  %345 = bitcast float* %344 to <4 x float>*
  %wide.load149.14 = load <4 x float>, <4 x float>* %345, align 32, !tbaa !3502
  %346 = fcmp olt <4 x float> %wide.load149.14, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %347 = select <4 x i1> %346, <4 x float> %wide.load149.14, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %348 = fcmp ogt <4 x float> %347, zeroinitializer
  %349 = select <4 x i1> %348, <4 x float> %347, <4 x float> zeroinitializer
  %350 = getelementptr inbounds float, float* %131, i64 %343
  %351 = bitcast float* %350 to <4 x float>*
  store <4 x float> %349, <4 x float>* %351, align 32, !tbaa !3506
  %352 = or i64 %217, 60
  %353 = getelementptr inbounds float, float* %21, i64 %352
  %354 = bitcast float* %353 to <4 x float>*
  %wide.load149.15 = load <4 x float>, <4 x float>* %354, align 16, !tbaa !3502
  %355 = fcmp olt <4 x float> %wide.load149.15, <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %356 = select <4 x i1> %355, <4 x float> %wide.load149.15, <4 x float> <float 6.000000e+00, float 6.000000e+00, float 6.000000e+00, float 6.000000e+00>
  %357 = fcmp ogt <4 x float> %356, zeroinitializer
  %358 = select <4 x i1> %357, <4 x float> %356, <4 x float> zeroinitializer
  %359 = getelementptr inbounds float, float* %131, i64 %352
  %360 = bitcast float* %359 to <4 x float>*
  store <4 x float> %358, <4 x float>* %360, align 16, !tbaa !3506
  %indvars.iv.next68 = add nuw nsw i64 %indvars.iv67, 1
  %exitcond69.not = icmp eq i64 %indvars.iv.next68, 56
  br i1 %exitcond69.not, label %for_end41, label %for_begin42.preheader, !prof !51

for_end41:                                        ; preds = %for_begin42.preheader
  %indvars.iv.next71 = add nuw nsw i64 %indvars.iv70, 1
  %exitcond72.not = icmp eq i64 %indvars.iv.next71, 56
  br i1 %exitcond72.not, label %for_end38, label %for_begin39.preheader, !prof !51

if_end46:                                         ; preds = %for_end38
  %361 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %362 = tail call i32 %361(i32 1, i32 %4, i8* nonnull %6)
  %.not57 = icmp ne i32 %362, 0
  %spec.select = sext i1 %.not57 to i32
  ret i32 %spec.select
}

define dllexport i32 @fused_nn_conv2d_nn_bias_add(i8* noalias nocapture readonly %0, i8* noalias nocapture readonly %1, i32 %2, i8* noalias nocapture readnone %3, i8* noalias nocapture readnone %4, i8* noalias nocapture readnone %5) local_unnamed_addr {
entry:
  %6 = icmp eq i32 %2, 4
  br i1 %6, label %assert_end, label %assert_fail, !prof !5

assert_fail:                                      ; preds = %entry
  %7 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %7(i8* getelementptr inbounds ([80 x i8], [80 x i8]* @.str.253, i64 0, i64 0))
  ret i32 -1

assert_end:                                       ; preds = %entry
  %8 = bitcast i8* %0 to %1**
  %9 = load %1*, %1** %8, align 8
  %10 = bitcast i8* %1 to i32*
  %11 = load i32, i32* %10, align 4, !tbaa !3509
  %12 = getelementptr inbounds i8, i8* %0, i64 8
  %13 = bitcast i8* %12 to %1**
  %14 = load %1*, %1** %13, align 8
  %15 = getelementptr inbounds i8, i8* %1, i64 4
  %16 = bitcast i8* %15 to i32*
  %17 = load i32, i32* %16, align 4, !tbaa !3523
  %18 = getelementptr inbounds i8, i8* %0, i64 16
  %19 = bitcast i8* %18 to %1**
  %20 = load %1*, %1** %19, align 8
  %21 = getelementptr inbounds i8, i8* %1, i64 8
  %22 = bitcast i8* %21 to i32*
  %23 = load i32, i32* %22, align 4, !tbaa !3525
  %24 = getelementptr inbounds i8, i8* %0, i64 24
  %25 = bitcast i8* %24 to %1**
  %26 = load %1*, %1** %25, align 8
  %27 = getelementptr inbounds i8, i8* %1, i64 12
  %28 = bitcast i8* %27 to i32*
  %29 = load i32, i32* %28, align 4, !tbaa !3528
  %30 = getelementptr inbounds %1, %1* %9, i64 0, i32 0
  %31 = load i8*, i8** %30, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %31, i64 128) ]
  %32 = getelementptr inbounds %1, %1* %9, i64 0, i32 4
  %33 = load i64*, i64** %32, align 8
  %34 = getelementptr inbounds %1, %1* %9, i64 0, i32 5
  %35 = load i64*, i64** %34, align 8
  %36 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 1
  %37 = load i32, i32* %36, align 4
  %38 = getelementptr inbounds %1, %1* %14, i64 0, i32 0
  %39 = load i8*, i8** %38, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %39, i64 128) ]
  %40 = getelementptr inbounds %1, %1* %14, i64 0, i32 4
  %41 = load i64*, i64** %40, align 8
  %42 = getelementptr inbounds %1, %1* %14, i64 0, i32 5
  %43 = load i64*, i64** %42, align 8
  %44 = getelementptr inbounds %1, %1* %20, i64 0, i32 0
  %45 = load i8*, i8** %44, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %45, i64 128) ]
  %46 = getelementptr inbounds %1, %1* %20, i64 0, i32 4
  %47 = load i64*, i64** %46, align 8
  %48 = getelementptr inbounds %1, %1* %20, i64 0, i32 5
  %49 = load i64*, i64** %48, align 8
  %50 = getelementptr inbounds %1, %1* %26, i64 0, i32 0
  %51 = load i8*, i8** %50, align 8
  call void @llvm.assume(i1 true) [ "align"(i8* %51, i64 128) ]
  %52 = getelementptr inbounds %1, %1* %26, i64 0, i32 4
  %53 = load i64*, i64** %52, align 8
  %54 = getelementptr inbounds %1, %1* %26, i64 0, i32 5
  %55 = load i64*, i64** %54, align 8
  switch i32 %11, label %assert_fail1 [
    i32 13, label %assert_end2
    i32 7, label %assert_end2
    i32 4, label %assert_end2
    i32 3, label %assert_end2
  ]

assert_fail1:                                     ; preds = %assert_end
  %56 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %56(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.254, i64 0, i64 0))
  ret i32 -1

assert_end2:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end
  switch i32 %17, label %assert_fail3 [
    i32 13, label %assert_end4
    i32 7, label %assert_end4
    i32 4, label %assert_end4
    i32 3, label %assert_end4
  ]

assert_fail3:                                     ; preds = %assert_end2
  %57 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %57(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.255, i64 0, i64 0))
  ret i32 -1

assert_end4:                                      ; preds = %assert_end2, %assert_end2, %assert_end2, %assert_end2
  switch i32 %23, label %assert_fail5 [
    i32 13, label %assert_end6
    i32 7, label %assert_end6
    i32 4, label %assert_end6
    i32 3, label %assert_end6
  ]

assert_fail5:                                     ; preds = %assert_end4
  %58 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %58(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.256, i64 0, i64 0))
  ret i32 -1

assert_end6:                                      ; preds = %assert_end4, %assert_end4, %assert_end4, %assert_end4
  switch i32 %29, label %assert_fail7 [
    i32 13, label %assert_end8
    i32 7, label %assert_end8
    i32 4, label %assert_end8
    i32 3, label %assert_end8
  ]

assert_fail7:                                     ; preds = %assert_end6
  %59 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %59(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.257, i64 0, i64 0))
  ret i32 -1

assert_end8:                                      ; preds = %assert_end6, %assert_end6, %assert_end6, %assert_end6
  %60 = getelementptr inbounds %1, %1* %9, i64 0, i32 2
  %61 = load i32, i32* %60, align 4
  %62 = icmp eq i32 %61, 4
  br i1 %62, label %assert_end12, label %assert_fail9, !prof !5

assert_fail9:                                     ; preds = %assert_end8
  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %63(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.5, i64 0, i64 0))
  ret i32 -1

assert_end12:                                     ; preds = %assert_end8
  %64 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 2
  %65 = load i16, i16* %64, align 2
  %66 = icmp eq i16 %65, 1
  %67 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 1
  %68 = load i8, i8* %67, align 1
  %69 = icmp eq i8 %68, 32
  %70 = getelementptr inbounds %1, %1* %9, i64 0, i32 3, i32 0
  %71 = load i8, i8* %70, align 1
  %72 = icmp eq i8 %71, 2
  %73 = and i1 %69, %72
  %74 = and i1 %66, %73
  br i1 %74, label %assert_end14, label %assert_fail13, !prof !5

assert_fail13:                                    ; preds = %assert_end12
  %75 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %75(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.6, i64 0, i64 0))
  ret i32 -1

assert_end14:                                     ; preds = %assert_end12
  %76 = load i64, i64* %33, align 8, !tbaa !3530
  %77 = trunc i64 %76 to i32
  %78 = icmp eq i32 %77, 1
  br i1 %78, label %assert_end16, label %assert_fail15, !prof !5

assert_fail15:                                    ; preds = %assert_end14
  %79 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %79(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.7, i64 0, i64 0))
  ret i32 -1

assert_end16:                                     ; preds = %assert_end14
  %80 = getelementptr inbounds i64, i64* %33, i64 1
  %81 = load i64, i64* %80, align 8, !tbaa !3544
  %82 = trunc i64 %81 to i32
  %83 = icmp eq i32 %82, 1
  br i1 %83, label %assert_end18, label %assert_fail17, !prof !5

assert_fail17:                                    ; preds = %assert_end16
  %84 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %84(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.166, i64 0, i64 0))
  ret i32 -1

assert_end18:                                     ; preds = %assert_end16
  %85 = getelementptr inbounds i64, i64* %33, i64 2
  %86 = load i64, i64* %85, align 8, !tbaa !3546
  %87 = trunc i64 %86 to i32
  %88 = icmp eq i32 %87, 1
  br i1 %88, label %assert_end20, label %assert_fail19, !prof !5

assert_fail19:                                    ; preds = %assert_end18
  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %89(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.167, i64 0, i64 0))
  ret i32 -1

assert_end20:                                     ; preds = %assert_end18
  %90 = getelementptr inbounds i64, i64* %33, i64 3
  %91 = load i64, i64* %90, align 8, !tbaa !3549
  %92 = trunc i64 %91 to i32
  %93 = icmp eq i32 %92, 1024
  br i1 %93, label %assert_end22, label %assert_fail21, !prof !5

assert_fail21:                                    ; preds = %assert_end20
  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %94(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.125, i64 0, i64 0))
  ret i32 -1

assert_end22:                                     ; preds = %assert_end20
  %.not = icmp eq i64* %35, null
  br i1 %.not, label %if_end, label %if_then, !prof !51

if_then:                                          ; preds = %assert_end22
  %95 = bitcast i64* %35 to <4 x i64>*
  %96 = load <4 x i64>, <4 x i64>* %95, align 8, !tbaa !3551
  %97 = trunc <4 x i64> %96 to <4 x i32>
  %98 = icmp eq <4 x i32> %97, <i32 1024, i32 1024, i32 1024, i32 1>
  %99 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %98)
  br i1 %99, label %if_end, label %assert_fail23, !prof !5

if_end:                                           ; preds = %if_then, %assert_end22
  %100 = getelementptr inbounds %1, %1* %9, i64 0, i32 6
  %101 = load i64, i64* %100, align 8
  %102 = icmp eq i64 %101, 0
  br i1 %102, label %assert_end26, label %assert_fail25, !prof !5

assert_fail23:                                    ; preds = %if_then
  %103 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %103(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.258, i64 0, i64 0))
  ret i32 -1

assert_fail25:                                    ; preds = %if_end
  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %104(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.12, i64 0, i64 0))
  ret i32 -1

assert_end26:                                     ; preds = %if_end
  %105 = getelementptr inbounds %1, %1* %9, i64 0, i32 1, i32 0
  %106 = load i32, i32* %105, align 4
  %107 = icmp eq i32 %106, 1
  br i1 %107, label %assert_end28, label %assert_fail27, !prof !5

assert_fail27:                                    ; preds = %assert_end26
  %108 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %108(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.13, i64 0, i64 0))
  ret i32 -1

assert_end28:                                     ; preds = %assert_end26
  %109 = getelementptr inbounds %1, %1* %14, i64 0, i32 2
  %110 = load i32, i32* %109, align 4
  %111 = icmp eq i32 %110, 4
  br i1 %111, label %assert_end32, label %assert_fail29, !prof !5

assert_fail29:                                    ; preds = %assert_end28
  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %112(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.14, i64 0, i64 0))
  ret i32 -1

assert_end32:                                     ; preds = %assert_end28
  %113 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 2
  %114 = load i16, i16* %113, align 2
  %115 = icmp eq i16 %114, 1
  %116 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 1
  %117 = load i8, i8* %116, align 1
  %118 = icmp eq i8 %117, 32
  %119 = getelementptr inbounds %1, %1* %14, i64 0, i32 3, i32 0
  %120 = load i8, i8* %119, align 1
  %121 = icmp eq i8 %120, 2
  %122 = and i1 %118, %121
  %123 = and i1 %115, %122
  br i1 %123, label %assert_end34, label %assert_fail33, !prof !5

assert_fail33:                                    ; preds = %assert_end32
  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %124(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.15, i64 0, i64 0))
  ret i32 -1

assert_end34:                                     ; preds = %assert_end32
  %125 = load i64, i64* %41, align 8, !tbaa !3563
  %126 = trunc i64 %125 to i32
  %127 = icmp eq i32 %126, 1
  br i1 %127, label %assert_end36, label %assert_fail35, !prof !5

assert_fail35:                                    ; preds = %assert_end34
  %128 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %128(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.16, i64 0, i64 0))
  ret i32 -1

assert_end36:                                     ; preds = %assert_end34
  %129 = getelementptr inbounds i64, i64* %41, i64 1
  %130 = load i64, i64* %129, align 8, !tbaa !3577
  %131 = trunc i64 %130 to i32
  %132 = icmp eq i32 %131, 1
  br i1 %132, label %assert_end38, label %assert_fail37, !prof !5

assert_fail37:                                    ; preds = %assert_end36
  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %133(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.17, i64 0, i64 0))
  ret i32 -1

assert_end38:                                     ; preds = %assert_end36
  %134 = getelementptr inbounds i64, i64* %41, i64 2
  %135 = load i64, i64* %134, align 8, !tbaa !3579
  %136 = trunc i64 %135 to i32
  %137 = icmp eq i32 %136, 1024
  br i1 %137, label %assert_end40, label %assert_fail39, !prof !5

assert_fail39:                                    ; preds = %assert_end38
  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %138(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.127, i64 0, i64 0))
  ret i32 -1

assert_end40:                                     ; preds = %assert_end38
  %139 = getelementptr inbounds i64, i64* %41, i64 3
  %140 = load i64, i64* %139, align 8, !tbaa !3582
  %141 = trunc i64 %140 to i32
  %142 = icmp eq i32 %141, 1001
  br i1 %142, label %assert_end42, label %assert_fail41, !prof !5

assert_fail41:                                    ; preds = %assert_end40
  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %143(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.259, i64 0, i64 0))
  ret i32 -1

assert_end42:                                     ; preds = %assert_end40
  %.not95 = icmp eq i64* %43, null
  br i1 %.not95, label %if_end44, label %if_then43, !prof !51

if_then43:                                        ; preds = %assert_end42
  %144 = bitcast i64* %43 to <4 x i64>*
  %145 = load <4 x i64>, <4 x i64>* %144, align 8, !tbaa !3584
  %146 = trunc <4 x i64> %145 to <4 x i32>
  %147 = icmp eq <4 x i32> %146, <i32 1025024, i32 1025024, i32 1001, i32 1>
  %148 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %147)
  br i1 %148, label %if_end44, label %assert_fail45, !prof !5

if_end44:                                         ; preds = %if_then43, %assert_end42
  %149 = getelementptr inbounds %1, %1* %14, i64 0, i32 6
  %150 = load i64, i64* %149, align 8
  %151 = icmp eq i64 %150, 0
  br i1 %151, label %assert_end48, label %assert_fail47, !prof !5

assert_fail45:                                    ; preds = %if_then43
  %152 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %152(i8* getelementptr inbounds ([207 x i8], [207 x i8]* @.str.260, i64 0, i64 0))
  ret i32 -1

assert_fail47:                                    ; preds = %if_end44
  %153 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %153(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.21, i64 0, i64 0))
  ret i32 -1

assert_end48:                                     ; preds = %if_end44
  %154 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 0
  %155 = load i32, i32* %154, align 4
  %156 = icmp eq i32 %155, 1
  br i1 %156, label %assert_end50, label %assert_fail49, !prof !5

assert_fail49:                                    ; preds = %assert_end48
  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %157(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.22, i64 0, i64 0))
  ret i32 -1

assert_end50:                                     ; preds = %assert_end48
  %158 = getelementptr inbounds %1, %1* %14, i64 0, i32 1, i32 1
  %159 = load i32, i32* %158, align 4
  %160 = icmp eq i32 %37, %159
  br i1 %160, label %assert_end52, label %assert_fail51, !prof !5

assert_fail51:                                    ; preds = %assert_end50
  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %161(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.23, i64 0, i64 0))
  ret i32 -1

assert_end52:                                     ; preds = %assert_end50
  %162 = getelementptr inbounds %1, %1* %20, i64 0, i32 2
  %163 = load i32, i32* %162, align 4
  %164 = icmp eq i32 %163, 1
  br i1 %164, label %assert_end56, label %assert_fail53, !prof !5

assert_fail53:                                    ; preds = %assert_end52
  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %165(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.24, i64 0, i64 0))
  ret i32 -1

assert_end56:                                     ; preds = %assert_end52
  %166 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 2
  %167 = load i16, i16* %166, align 2
  %168 = icmp eq i16 %167, 1
  %169 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 1
  %170 = load i8, i8* %169, align 1
  %171 = icmp eq i8 %170, 32
  %172 = getelementptr inbounds %1, %1* %20, i64 0, i32 3, i32 0
  %173 = load i8, i8* %172, align 1
  %174 = icmp eq i8 %173, 2
  %175 = and i1 %171, %174
  %176 = and i1 %168, %175
  br i1 %176, label %assert_end58, label %assert_fail57, !prof !5

assert_fail57:                                    ; preds = %assert_end56
  %177 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %177(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.25, i64 0, i64 0))
  ret i32 -1

assert_end58:                                     ; preds = %assert_end56
  %178 = load i64, i64* %47, align 8, !tbaa !3596
  %179 = trunc i64 %178 to i32
  %180 = icmp eq i32 %179, 1001
  br i1 %180, label %assert_end60, label %assert_fail59, !prof !5

assert_fail59:                                    ; preds = %assert_end58
  %181 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %181(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.261, i64 0, i64 0))
  ret i32 -1

assert_end60:                                     ; preds = %assert_end58
  %.not96 = icmp eq i64* %49, null
  br i1 %.not96, label %if_end62, label %if_then61, !prof !51

if_then61:                                        ; preds = %assert_end60
  %182 = load i64, i64* %49, align 8, !tbaa !3610
  %183 = trunc i64 %182 to i32
  %184 = icmp eq i32 %183, 1
  br i1 %184, label %if_end62, label %assert_fail63, !prof !5

if_end62:                                         ; preds = %if_then61, %assert_end60
  %185 = getelementptr inbounds %1, %1* %20, i64 0, i32 6
  %186 = load i64, i64* %185, align 8
  %187 = icmp eq i64 %186, 0
  br i1 %187, label %assert_end66, label %assert_fail65, !prof !5

assert_fail63:                                    ; preds = %if_then61
  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %188(i8* getelementptr inbounds ([87 x i8], [87 x i8]* @.str.27, i64 0, i64 0))
  ret i32 -1

assert_fail65:                                    ; preds = %if_end62
  %189 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %189(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.28, i64 0, i64 0))
  ret i32 -1

assert_end66:                                     ; preds = %if_end62
  %190 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 0
  %191 = load i32, i32* %190, align 4
  %192 = icmp eq i32 %191, 1
  br i1 %192, label %assert_end68, label %assert_fail67, !prof !5

assert_fail67:                                    ; preds = %assert_end66
  %193 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %193(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.29, i64 0, i64 0))
  ret i32 -1

assert_end68:                                     ; preds = %assert_end66
  %194 = getelementptr inbounds %1, %1* %20, i64 0, i32 1, i32 1
  %195 = load i32, i32* %194, align 4
  %196 = icmp eq i32 %37, %195
  br i1 %196, label %assert_end70, label %assert_fail69, !prof !5

assert_fail69:                                    ; preds = %assert_end68
  %197 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %197(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.30, i64 0, i64 0))
  ret i32 -1

assert_end70:                                     ; preds = %assert_end68
  %198 = getelementptr inbounds %1, %1* %26, i64 0, i32 2
  %199 = load i32, i32* %198, align 4
  %200 = icmp eq i32 %199, 4
  br i1 %200, label %assert_end74, label %assert_fail71, !prof !5

assert_fail71:                                    ; preds = %assert_end70
  %201 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %201(i8* getelementptr inbounds ([85 x i8], [85 x i8]* @.str.31, i64 0, i64 0))
  ret i32 -1

assert_end74:                                     ; preds = %assert_end70
  %202 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 2
  %203 = load i16, i16* %202, align 2
  %204 = icmp eq i16 %203, 1
  %205 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 1
  %206 = load i8, i8* %205, align 1
  %207 = icmp eq i8 %206, 32
  %208 = getelementptr inbounds %1, %1* %26, i64 0, i32 3, i32 0
  %209 = load i8, i8* %208, align 1
  %210 = icmp eq i8 %209, 2
  %211 = and i1 %207, %210
  %212 = and i1 %204, %211
  br i1 %212, label %assert_end76, label %assert_fail75, !prof !5

assert_fail75:                                    ; preds = %assert_end74
  %213 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %213(i8* getelementptr inbounds ([198 x i8], [198 x i8]* @.str.32, i64 0, i64 0))
  ret i32 -1

assert_end76:                                     ; preds = %assert_end74
  %214 = load i64, i64* %53, align 8, !tbaa !3624
  %215 = trunc i64 %214 to i32
  %216 = icmp eq i32 %215, 1
  br i1 %216, label %assert_end78, label %assert_fail77, !prof !5

assert_fail77:                                    ; preds = %assert_end76
  %217 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %217(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.33, i64 0, i64 0))
  ret i32 -1

assert_end78:                                     ; preds = %assert_end76
  %218 = getelementptr inbounds i64, i64* %53, i64 1
  %219 = load i64, i64* %218, align 8, !tbaa !3638
  %220 = trunc i64 %219 to i32
  %221 = icmp eq i32 %220, 1
  br i1 %221, label %assert_end80, label %assert_fail79, !prof !5

assert_fail79:                                    ; preds = %assert_end78
  %222 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %222(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.262, i64 0, i64 0))
  ret i32 -1

assert_end80:                                     ; preds = %assert_end78
  %223 = getelementptr inbounds i64, i64* %53, i64 2
  %224 = load i64, i64* %223, align 8, !tbaa !3640
  %225 = trunc i64 %224 to i32
  %226 = icmp eq i32 %225, 1
  br i1 %226, label %assert_end82, label %assert_fail81, !prof !5

assert_fail81:                                    ; preds = %assert_end80
  %227 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %227(i8* getelementptr inbounds ([124 x i8], [124 x i8]* @.str.263, i64 0, i64 0))
  ret i32 -1

assert_end82:                                     ; preds = %assert_end80
  %228 = getelementptr inbounds i64, i64* %53, i64 3
  %229 = load i64, i64* %228, align 8, !tbaa !3643
  %230 = trunc i64 %229 to i32
  %231 = icmp eq i32 %230, 1001
  br i1 %231, label %assert_end84, label %assert_fail83, !prof !5

assert_fail83:                                    ; preds = %assert_end82
  %232 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %232(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.264, i64 0, i64 0))
  ret i32 -1

assert_end84:                                     ; preds = %assert_end82
  %.not97 = icmp eq i64* %55, null
  br i1 %.not97, label %if_end86, label %if_then85, !prof !51

if_then85:                                        ; preds = %assert_end84
  %233 = bitcast i64* %55 to <4 x i64>*
  %234 = load <4 x i64>, <4 x i64>* %233, align 8, !tbaa !3645
  %235 = trunc <4 x i64> %234 to <4 x i32>
  %236 = icmp eq <4 x i32> %235, <i32 1001, i32 1001, i32 1001, i32 1>
  %237 = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %236)
  br i1 %237, label %if_end86, label %assert_fail87, !prof !5

if_end86:                                         ; preds = %if_then85, %assert_end84
  %238 = getelementptr inbounds %1, %1* %26, i64 0, i32 6
  %239 = load i64, i64* %238, align 8
  %240 = icmp eq i64 %239, 0
  br i1 %240, label %assert_end90, label %assert_fail89, !prof !5

assert_fail87:                                    ; preds = %if_then85
  %241 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %241(i8* getelementptr inbounds ([201 x i8], [201 x i8]* @.str.265, i64 0, i64 0))
  ret i32 -1

assert_fail89:                                    ; preds = %if_end86
  %242 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %242(i8* getelementptr inbounds ([163 x i8], [163 x i8]* @.str.38, i64 0, i64 0))
  ret i32 -1

assert_end90:                                     ; preds = %if_end86
  %243 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 0
  %244 = load i32, i32* %243, align 4
  %245 = icmp eq i32 %244, 1
  br i1 %245, label %assert_end92, label %assert_fail91, !prof !5

assert_fail91:                                    ; preds = %assert_end90
  %246 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %246(i8* getelementptr inbounds ([149 x i8], [149 x i8]* @.str.39, i64 0, i64 0))
  ret i32 -1

assert_end92:                                     ; preds = %assert_end90
  %247 = getelementptr inbounds %1, %1* %26, i64 0, i32 1, i32 1
  %248 = load i32, i32* %247, align 4
  %249 = icmp eq i32 %37, %248
  br i1 %249, label %assert_end94, label %assert_fail93, !prof !5

assert_fail93:                                    ; preds = %assert_end92
  %250 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !6
  tail call void %250(i8* getelementptr inbounds ([155 x i8], [155 x i8]* @.str.40, i64 0, i64 0))
  ret i32 -1

assert_end94:                                     ; preds = %assert_end92
  %251 = tail call fastcc i32 @fused_nn_conv2d_nn_bias_add_compute_(i8* %31, i8* %39, i8* %51, i8* %45, i32 %37)
  ret i32 %251
}

; Function Attrs: noinline
define private fastcc i32 @fused_nn_conv2d_nn_bias_add_compute_(i8* noalias nocapture readonly align 128 %0, i8* noalias nocapture readonly align 128 %1, i8* noalias nocapture align 128 %2, i8* noalias nocapture readonly align 128 %3, i32 %4) unnamed_addr #1 {
entry:
  %5 = load i8* (i32, i32, i64, i32, i32)*, i8* (i32, i32, i64, i32, i32)** @__TVMBackendAllocWorkspace, align 8, !tbaa !6
  %6 = tail call i8* %5(i32 1, i32 %4, i64 4096, i32 2, i32 32)
  call void @llvm.assume(i1 true) [ "align"(i8* %6, i64 128) ]
  %7 = icmp eq i8* %6, null
  br i1 %7, label %if_then, label %for_begin.preheader, !prof !5

for_begin.preheader:                              ; preds = %entry
  call void @llvm.memcpy.p0i8.p0i8.i64(i8* nonnull align 128 dereferenceable(4096) %6, i8* nonnull align 128 dereferenceable(4096) %0, i64 4096, i1 false)
  %8 = bitcast i8* %6 to float*
  %9 = bitcast i8* %1 to float*
  %10 = bitcast i8* %3 to float*
  %11 = bitcast i8* %2 to float*
  br label %for_begin4.preheader

if_then:                                          ; preds = %entry
  ret i32 -1

for_begin4.preheader:                             ; preds = %for_begin.preheader, %for_end6
  %indvars.iv15 = phi i64 [ 0, %for_begin.preheader ], [ %indvars.iv.next16, %for_end6 ]
  br label %for_body5

for_end3:                                         ; preds = %for_end6
  %12 = load i32 (i32, i32, i8*)*, i32 (i32, i32, i8*)** @__TVMBackendFreeWorkspace, align 8, !tbaa !6
  %13 = tail call i32 %12(i32 1, i32 %4, i8* nonnull %6)
  %.not = icmp ne i32 %13, 0
  %spec.select = sext i1 %.not to i32
  ret i32 %spec.select

for_body5:                                        ; preds = %for_body5, %for_begin4.preheader
  %indvars.iv = phi i64 [ 0, %for_begin4.preheader ], [ %indvars.iv.next.1, %for_body5 ]
  %.014 = phi float [ 0.000000e+00, %for_begin4.preheader ], [ %27, %for_body5 ]
  %14 = getelementptr inbounds float, float* %8, i64 %indvars.iv
  %15 = load float, float* %14, align 8, !tbaa !3657
  %16 = mul nuw nsw i64 %indvars.iv, 1001
  %17 = add nuw nsw i64 %16, %indvars.iv15
  %18 = getelementptr inbounds float, float* %9, i64 %17
  %19 = load float, float* %18, align 4, !tbaa !3660
  %20 = tail call float @llvm.fmuladd.f32(float %15, float %19, float %.014)
  %indvars.iv.next = or i64 %indvars.iv, 1
  %21 = getelementptr inbounds float, float* %8, i64 %indvars.iv.next
  %22 = load float, float* %21, align 4, !tbaa !3657
  %23 = mul nuw nsw i64 %indvars.iv.next, 1001
  %24 = add nuw nsw i64 %23, %indvars.iv15
  %25 = getelementptr inbounds float, float* %9, i64 %24
  %26 = load float, float* %25, align 4, !tbaa !3660
  %27 = tail call float @llvm.fmuladd.f32(float %22, float %26, float %20)
  %indvars.iv.next.1 = add nuw nsw i64 %indvars.iv, 2
  %exitcond.not.1 = icmp eq i64 %indvars.iv.next.1, 1024
  br i1 %exitcond.not.1, label %for_end6, label %for_body5, !prof !51

for_end6:                                         ; preds = %for_body5
  %28 = getelementptr inbounds float, float* %10, i64 %indvars.iv15
  %29 = load float, float* %28, align 4, !tbaa !3663
  %30 = fadd float %27, %29
  %31 = getelementptr inbounds float, float* %11, i64 %indvars.iv15
  store float %30, float* %31, align 4, !tbaa !3666
  %indvars.iv.next16 = add nuw nsw i64 %indvars.iv15, 1
  %exitcond17.not = icmp eq i64 %indvars.iv.next16, 1001
  br i1 %exitcond17.not, label %for_end3, label %for_begin4.preheader, !prof !51
}

; Function Attrs: argmemonly nofree nosync nounwind willreturn writeonly
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #6

; Function Attrs: argmemonly nofree nosync nounwind willreturn
declare void @llvm.memcpy.p0i8.p0i8.i64(i8* noalias nocapture writeonly, i8* noalias nocapture readonly, i64, i1 immarg) #7

; Function Attrs: nofree nosync nounwind readnone willreturn
declare i1 @llvm.vector.reduce.and.v4i1(<4 x i1>) #8

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare <4 x float> @llvm.fmuladd.v4f32(<4 x float>, <4 x float>, <4 x float>) #4

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare <4 x float> @llvm.exp.v4f32(<4 x float>) #4

attributes #0 = { nofree nosync nounwind willreturn }
attributes #1 = { noinline }
attributes #2 = { nofree norecurse nounwind }
attributes #3 = { nofree nounwind }
attributes #4 = { nofree nosync nounwind readnone speculatable willreturn }
attributes #5 = { nofree noinline norecurse nounwind }
attributes #6 = { argmemonly nofree nosync nounwind willreturn writeonly }
attributes #7 = { argmemonly nofree nosync nounwind willreturn }
attributes #8 = { nofree nosync nounwind readnone willreturn }

!llvm.dbg.cu = !{!0}
!llvm.module.flags = !{!3, !4}

!0 = distinct !DICompileUnit(language: DW_LANG_C, file: !1, producer: "TVM", isOptimized: false, runtimeVersion: 0, emissionKind: FullDebug, enums: !2, dwoId: 1)
!1 = !DIFile(filename: "model.tvm", directory: "/tmp/")
!2 = !{}
!3 = !{i32 2, !"tvm_target", !"llvm"}
!4 = !{i32 4, !"Debug Info Version", i32 3}
!5 = !{!"branch_weights", i32 1048576, i32 1}
!6 = !{!7, !7, i64 0}
!7 = !{!"ctx_ptr", !8, i64 0}
!8 = !{!"tvm-tbaa"}
!9 = !{!10, !10, i64 0}
!10 = !{!"0x55c7bf546fd0.w1.b0", !11, i64 0}
!11 = !{!"0x55c7bf546fd0.w2.b0", !12, i64 0}
!12 = !{!"0x55c7bf546fd0.w4.b0", !13, i64 0}
!13 = !{!"0x55c7bf546fd0.w8.b0", !14, i64 0}
!14 = !{!"0x55c7bf546fd0.w16.b0", !15, i64 0}
!15 = !{!"0x55c7bf546fd0.w32.b0", !16, i64 0}
!16 = !{!"0x55c7bf546fd0.w64.b0", !17, i64 0}
!17 = !{!"0x55c7bf546fd0.w128.b0", !18, i64 0}
!18 = !{!"0x55c7bf546fd0.w256.b0", !19, i64 0}
!19 = !{!"0x55c7bf546fd0.w512.b0", !20, i64 0}
!20 = !{!"0x55c7bf546fd0.w1024.b0", !21, i64 0}
!21 = !{!"i8", !22, i64 0}
!22 = !{!"0x55c7bf546fd0", !8, i64 0}
!23 = !{!24, !24, i64 0}
!24 = !{!"0x55c7bf546fd0.w1.b1", !11, i64 0}
!25 = !{!26, !26, i64 0}
!26 = !{!"0x55c7bf546fd0.w1.b2", !27, i64 0}
!27 = !{!"0x55c7bf546fd0.w2.b2", !12, i64 0}
!28 = !{!29, !29, i64 0}
!29 = !{!"0x55c7bf546fd0.w1.b3", !27, i64 0}
!30 = !{!31, !31, i64 0}
!31 = !{!"0x55c7bf549db0.w1.b0", !32, i64 0}
!32 = !{!"0x55c7bf549db0.w2.b0", !33, i64 0}
!33 = !{!"0x55c7bf549db0.w4.b0", !34, i64 0}
!34 = !{!"0x55c7bf549db0.w8.b0", !35, i64 0}
!35 = !{!"0x55c7bf549db0.w16.b0", !36, i64 0}
!36 = !{!"0x55c7bf549db0.w32.b0", !37, i64 0}
!37 = !{!"0x55c7bf549db0.w64.b0", !38, i64 0}
!38 = !{!"0x55c7bf549db0.w128.b0", !39, i64 0}
!39 = !{!"0x55c7bf549db0.w256.b0", !40, i64 0}
!40 = !{!"0x55c7bf549db0.w512.b0", !41, i64 0}
!41 = !{!"0x55c7bf549db0.w1024.b0", !42, i64 0}
!42 = !{!"i64", !43, i64 0}
!43 = !{!"0x55c7bf549db0", !8, i64 0}
!44 = !{!45, !45, i64 0}
!45 = !{!"0x55c7bf549db0.w1.b1", !32, i64 0}
!46 = !{!47, !47, i64 0}
!47 = !{!"0x55c7bf549db0.w1.b2", !48, i64 0}
!48 = !{!"0x55c7bf549db0.w2.b2", !33, i64 0}
!49 = !{!50, !50, i64 0}
!50 = !{!"0x55c7bf549db0.w1.b3", !48, i64 0}
!51 = !{!"branch_weights", i32 1, i32 1048576}
!52 = !{!53, !53, i64 0}
!53 = !{!"0x55c7bf54a690.w4.b0", !54, i64 0}
!54 = !{!"0x55c7bf54a690.w8.b0", !55, i64 0}
!55 = !{!"0x55c7bf54a690.w16.b0", !56, i64 0}
!56 = !{!"0x55c7bf54a690.w32.b0", !57, i64 0}
!57 = !{!"0x55c7bf54a690.w64.b0", !58, i64 0}
!58 = !{!"0x55c7bf54a690.w128.b0", !59, i64 0}
!59 = !{!"0x55c7bf54a690.w256.b0", !60, i64 0}
!60 = !{!"0x55c7bf54a690.w512.b0", !61, i64 0}
!61 = !{!"0x55c7bf54a690.w1024.b0", !62, i64 0}
!62 = !{!"i64", !63, i64 0}
!63 = !{!"0x55c7bf54a690", !8, i64 0}
!64 = !{!65, !65, i64 0}
!65 = !{!"0x55c7bf54c870.w1.b0", !66, i64 0}
!66 = !{!"0x55c7bf54c870.w2.b0", !67, i64 0}
!67 = !{!"0x55c7bf54c870.w4.b0", !68, i64 0}
!68 = !{!"0x55c7bf54c870.w8.b0", !69, i64 0}
!69 = !{!"0x55c7bf54c870.w16.b0", !70, i64 0}
!70 = !{!"0x55c7bf54c870.w32.b0", !71, i64 0}
!71 = !{!"0x55c7bf54c870.w64.b0", !72, i64 0}
!72 = !{!"0x55c7bf54c870.w128.b0", !73, i64 0}
!73 = !{!"0x55c7bf54c870.w256.b0", !74, i64 0}
!74 = !{!"0x55c7bf54c870.w512.b0", !75, i64 0}
!75 = !{!"0x55c7bf54c870.w1024.b0", !76, i64 0}
!76 = !{!"i64", !77, i64 0}
!77 = !{!"0x55c7bf54c870", !8, i64 0}
!78 = !{!79, !79, i64 0}
!79 = !{!"0x55c7bf54c870.w1.b1", !66, i64 0}
!80 = !{!81, !81, i64 0}
!81 = !{!"0x55c7bf54c870.w1.b2", !82, i64 0}
!82 = !{!"0x55c7bf54c870.w2.b2", !67, i64 0}
!83 = !{!84, !84, i64 0}
!84 = !{!"0x55c7bf54c870.w1.b3", !82, i64 0}
!85 = !{!86, !86, i64 0}
!86 = !{!"0x55c7bf54d200.w4.b0", !87, i64 0}
!87 = !{!"0x55c7bf54d200.w8.b0", !88, i64 0}
!88 = !{!"0x55c7bf54d200.w16.b0", !89, i64 0}
!89 = !{!"0x55c7bf54d200.w32.b0", !90, i64 0}
!90 = !{!"0x55c7bf54d200.w64.b0", !91, i64 0}
!91 = !{!"0x55c7bf54d200.w128.b0", !92, i64 0}
!92 = !{!"0x55c7bf54d200.w256.b0", !93, i64 0}
!93 = !{!"0x55c7bf54d200.w512.b0", !94, i64 0}
!94 = !{!"0x55c7bf54d200.w1024.b0", !95, i64 0}
!95 = !{!"i64", !96, i64 0}
!96 = !{!"0x55c7bf54d200", !8, i64 0}
!97 = !{!98, !98, i64 0}
!98 = !{!"0x55c7bf555580.w1.b0", !99, i64 0}
!99 = !{!"0x55c7bf555580.w2.b0", !100, i64 0}
!100 = !{!"0x55c7bf555580.w4.b0", !101, i64 0}
!101 = !{!"0x55c7bf555580.w8.b0", !102, i64 0}
!102 = !{!"0x55c7bf555580.w16.b0", !103, i64 0}
!103 = !{!"0x55c7bf555580.w32.b0", !104, i64 0}
!104 = !{!"0x55c7bf555580.w64.b0", !105, i64 0}
!105 = !{!"0x55c7bf555580.w128.b0", !106, i64 0}
!106 = !{!"0x55c7bf555580.w256.b0", !107, i64 0}
!107 = !{!"0x55c7bf555580.w512.b0", !108, i64 0}
!108 = !{!"0x55c7bf555580.w1024.b0", !109, i64 0}
!109 = !{!"i64", !110, i64 0}
!110 = !{!"0x55c7bf555580", !8, i64 0}
!111 = !{!112, !112, i64 0}
!112 = !{!"0x55c7bf555860.w1.b0", !113, i64 0}
!113 = !{!"0x55c7bf555860.w2.b0", !114, i64 0}
!114 = !{!"0x55c7bf555860.w4.b0", !115, i64 0}
!115 = !{!"0x55c7bf555860.w8.b0", !116, i64 0}
!116 = !{!"0x55c7bf555860.w16.b0", !117, i64 0}
!117 = !{!"0x55c7bf555860.w32.b0", !118, i64 0}
!118 = !{!"0x55c7bf555860.w64.b0", !119, i64 0}
!119 = !{!"0x55c7bf555860.w128.b0", !120, i64 0}
!120 = !{!"0x55c7bf555860.w256.b0", !121, i64 0}
!121 = !{!"0x55c7bf555860.w512.b0", !122, i64 0}
!122 = !{!"0x55c7bf555860.w1024.b0", !123, i64 0}
!123 = !{!"i64", !124, i64 0}
!124 = !{!"0x55c7bf555860", !8, i64 0}
!125 = !{!126, !126, i64 0}
!126 = !{!"0x55c7bf557560.w1.b0", !127, i64 0}
!127 = !{!"0x55c7bf557560.w2.b0", !128, i64 0}
!128 = !{!"0x55c7bf557560.w4.b0", !129, i64 0}
!129 = !{!"0x55c7bf557560.w8.b0", !130, i64 0}
!130 = !{!"0x55c7bf557560.w16.b0", !131, i64 0}
!131 = !{!"0x55c7bf557560.w32.b0", !132, i64 0}
!132 = !{!"0x55c7bf557560.w64.b0", !133, i64 0}
!133 = !{!"0x55c7bf557560.w128.b0", !134, i64 0}
!134 = !{!"0x55c7bf557560.w256.b0", !135, i64 0}
!135 = !{!"0x55c7bf557560.w512.b0", !136, i64 0}
!136 = !{!"0x55c7bf557560.w1024.b0", !137, i64 0}
!137 = !{!"i64", !138, i64 0}
!138 = !{!"0x55c7bf557560", !8, i64 0}
!139 = !{!140, !140, i64 0}
!140 = !{!"0x55c7bf557560.w1.b1", !127, i64 0}
!141 = !{!142, !142, i64 0}
!142 = !{!"0x55c7bf557560.w1.b2", !143, i64 0}
!143 = !{!"0x55c7bf557560.w2.b2", !128, i64 0}
!144 = !{!145, !145, i64 0}
!145 = !{!"0x55c7bf557560.w1.b3", !143, i64 0}
!146 = !{!147, !147, i64 0}
!147 = !{!"0x55c7bf557e50.w4.b0", !148, i64 0}
!148 = !{!"0x55c7bf557e50.w8.b0", !149, i64 0}
!149 = !{!"0x55c7bf557e50.w16.b0", !150, i64 0}
!150 = !{!"0x55c7bf557e50.w32.b0", !151, i64 0}
!151 = !{!"0x55c7bf557e50.w64.b0", !152, i64 0}
!152 = !{!"0x55c7bf557e50.w128.b0", !153, i64 0}
!153 = !{!"0x55c7bf557e50.w256.b0", !154, i64 0}
!154 = !{!"0x55c7bf557e50.w512.b0", !155, i64 0}
!155 = !{!"0x55c7bf557e50.w1024.b0", !156, i64 0}
!156 = !{!"i64", !157, i64 0}
!157 = !{!"0x55c7bf557e50", !8, i64 0}
!158 = !{!159, !159, i64 0}
!159 = !{!"i8", !160, i64 0}
!160 = !{!"0x55c7bf5156d0", !8, i64 0}
!161 = !{!162, !162, i64 0}
!162 = !{!"i8", !163, i64 0}
!163 = !{!"0x55c7bf5155d0", !8, i64 0}
!164 = !{!165, !165, i64 0}
!165 = !{!"0x55c7bf515650.w64.b0", !166, i64 0}
!166 = !{!"0x55c7bf515650.w128.b0", !167, i64 0}
!167 = !{!"0x55c7bf515650.w256.b0", !168, i64 0}
!168 = !{!"0x55c7bf515650.w512.b0", !169, i64 0}
!169 = !{!"0x55c7bf515650.w1024.b0", !170, i64 0}
!170 = !{!"i8", !171, i64 0}
!171 = !{!"0x55c7bf515650", !8, i64 0}
!172 = !{!173, !173, i64 0}
!173 = !{!"i8", !174, i64 0}
!174 = !{!"0x55c7bf515690", !8, i64 0}
!175 = !{!176, !176, i64 0}
!176 = !{!"i8", !177, i64 0}
!177 = !{!"0x55c7bf515610", !8, i64 0}
!178 = !{!179, !179, i64 0}
!179 = !{!"0x55c7bf54a3d0.w1.b0", !180, i64 0}
!180 = !{!"0x55c7bf54a3d0.w2.b0", !181, i64 0}
!181 = !{!"0x55c7bf54a3d0.w4.b0", !182, i64 0}
!182 = !{!"0x55c7bf54a3d0.w8.b0", !183, i64 0}
!183 = !{!"0x55c7bf54a3d0.w16.b0", !184, i64 0}
!184 = !{!"0x55c7bf54a3d0.w32.b0", !185, i64 0}
!185 = !{!"0x55c7bf54a3d0.w64.b0", !186, i64 0}
!186 = !{!"0x55c7bf54a3d0.w128.b0", !187, i64 0}
!187 = !{!"0x55c7bf54a3d0.w256.b0", !188, i64 0}
!188 = !{!"0x55c7bf54a3d0.w512.b0", !189, i64 0}
!189 = !{!"0x55c7bf54a3d0.w1024.b0", !190, i64 0}
!190 = !{!"i8", !191, i64 0}
!191 = !{!"0x55c7bf54a3d0", !8, i64 0}
!192 = !{!193, !193, i64 0}
!193 = !{!"0x55c7bf54a3d0.w1.b1", !180, i64 0}
!194 = !{!195, !195, i64 0}
!195 = !{!"0x55c7bf54a3d0.w1.b2", !196, i64 0}
!196 = !{!"0x55c7bf54a3d0.w2.b2", !181, i64 0}
!197 = !{!198, !198, i64 0}
!198 = !{!"0x55c7bf54a3d0.w1.b3", !196, i64 0}
!199 = !{!200, !200, i64 0}
!200 = !{!"0x55c7bf55b900.w1.b0", !201, i64 0}
!201 = !{!"0x55c7bf55b900.w2.b0", !202, i64 0}
!202 = !{!"0x55c7bf55b900.w4.b0", !203, i64 0}
!203 = !{!"0x55c7bf55b900.w8.b0", !204, i64 0}
!204 = !{!"0x55c7bf55b900.w16.b0", !205, i64 0}
!205 = !{!"0x55c7bf55b900.w32.b0", !206, i64 0}
!206 = !{!"0x55c7bf55b900.w64.b0", !207, i64 0}
!207 = !{!"0x55c7bf55b900.w128.b0", !208, i64 0}
!208 = !{!"0x55c7bf55b900.w256.b0", !209, i64 0}
!209 = !{!"0x55c7bf55b900.w512.b0", !210, i64 0}
!210 = !{!"0x55c7bf55b900.w1024.b0", !211, i64 0}
!211 = !{!"i64", !212, i64 0}
!212 = !{!"0x55c7bf55b900", !8, i64 0}
!213 = !{!214, !214, i64 0}
!214 = !{!"0x55c7bf55b900.w1.b1", !201, i64 0}
!215 = !{!216, !216, i64 0}
!216 = !{!"0x55c7bf55b900.w1.b2", !217, i64 0}
!217 = !{!"0x55c7bf55b900.w2.b2", !202, i64 0}
!218 = !{!219, !219, i64 0}
!219 = !{!"0x55c7bf55b900.w1.b3", !217, i64 0}
!220 = !{!221, !221, i64 0}
!221 = !{!"0x55c7bf55bee0.w4.b0", !222, i64 0}
!222 = !{!"0x55c7bf55bee0.w8.b0", !223, i64 0}
!223 = !{!"0x55c7bf55bee0.w16.b0", !224, i64 0}
!224 = !{!"0x55c7bf55bee0.w32.b0", !225, i64 0}
!225 = !{!"0x55c7bf55bee0.w64.b0", !226, i64 0}
!226 = !{!"0x55c7bf55bee0.w128.b0", !227, i64 0}
!227 = !{!"0x55c7bf55bee0.w256.b0", !228, i64 0}
!228 = !{!"0x55c7bf55bee0.w512.b0", !229, i64 0}
!229 = !{!"0x55c7bf55bee0.w1024.b0", !230, i64 0}
!230 = !{!"i64", !231, i64 0}
!231 = !{!"0x55c7bf55bee0", !8, i64 0}
!232 = !{!233, !233, i64 0}
!233 = !{!"0x55c7bf55e0c0.w1.b0", !234, i64 0}
!234 = !{!"0x55c7bf55e0c0.w2.b0", !235, i64 0}
!235 = !{!"0x55c7bf55e0c0.w4.b0", !236, i64 0}
!236 = !{!"0x55c7bf55e0c0.w8.b0", !237, i64 0}
!237 = !{!"0x55c7bf55e0c0.w16.b0", !238, i64 0}
!238 = !{!"0x55c7bf55e0c0.w32.b0", !239, i64 0}
!239 = !{!"0x55c7bf55e0c0.w64.b0", !240, i64 0}
!240 = !{!"0x55c7bf55e0c0.w128.b0", !241, i64 0}
!241 = !{!"0x55c7bf55e0c0.w256.b0", !242, i64 0}
!242 = !{!"0x55c7bf55e0c0.w512.b0", !243, i64 0}
!243 = !{!"0x55c7bf55e0c0.w1024.b0", !244, i64 0}
!244 = !{!"i64", !245, i64 0}
!245 = !{!"0x55c7bf55e0c0", !8, i64 0}
!246 = !{!247, !247, i64 0}
!247 = !{!"0x55c7bf55e0c0.w1.b1", !234, i64 0}
!248 = !{!249, !249, i64 0}
!249 = !{!"0x55c7bf55e0c0.w1.b2", !250, i64 0}
!250 = !{!"0x55c7bf55e0c0.w2.b2", !235, i64 0}
!251 = !{!252, !252, i64 0}
!252 = !{!"0x55c7bf55e0c0.w1.b3", !250, i64 0}
!253 = !{!254, !254, i64 0}
!254 = !{!"0x55c7bf55ea50.w4.b0", !255, i64 0}
!255 = !{!"0x55c7bf55ea50.w8.b0", !256, i64 0}
!256 = !{!"0x55c7bf55ea50.w16.b0", !257, i64 0}
!257 = !{!"0x55c7bf55ea50.w32.b0", !258, i64 0}
!258 = !{!"0x55c7bf55ea50.w64.b0", !259, i64 0}
!259 = !{!"0x55c7bf55ea50.w128.b0", !260, i64 0}
!260 = !{!"0x55c7bf55ea50.w256.b0", !261, i64 0}
!261 = !{!"0x55c7bf55ea50.w512.b0", !262, i64 0}
!262 = !{!"0x55c7bf55ea50.w1024.b0", !263, i64 0}
!263 = !{!"i64", !264, i64 0}
!264 = !{!"0x55c7bf55ea50", !8, i64 0}
!265 = !{!266, !266, i64 0}
!266 = !{!"0x55c7bf560c30.w1.b0", !267, i64 0}
!267 = !{!"0x55c7bf560c30.w2.b0", !268, i64 0}
!268 = !{!"0x55c7bf560c30.w4.b0", !269, i64 0}
!269 = !{!"0x55c7bf560c30.w8.b0", !270, i64 0}
!270 = !{!"0x55c7bf560c30.w16.b0", !271, i64 0}
!271 = !{!"0x55c7bf560c30.w32.b0", !272, i64 0}
!272 = !{!"0x55c7bf560c30.w64.b0", !273, i64 0}
!273 = !{!"0x55c7bf560c30.w128.b0", !274, i64 0}
!274 = !{!"0x55c7bf560c30.w256.b0", !275, i64 0}
!275 = !{!"0x55c7bf560c30.w512.b0", !276, i64 0}
!276 = !{!"0x55c7bf560c30.w1024.b0", !277, i64 0}
!277 = !{!"i64", !278, i64 0}
!278 = !{!"0x55c7bf560c30", !8, i64 0}
!279 = !{!280, !280, i64 0}
!280 = !{!"0x55c7bf560f10.w1.b0", !281, i64 0}
!281 = !{!"0x55c7bf560f10.w2.b0", !282, i64 0}
!282 = !{!"0x55c7bf560f10.w4.b0", !283, i64 0}
!283 = !{!"0x55c7bf560f10.w8.b0", !284, i64 0}
!284 = !{!"0x55c7bf560f10.w16.b0", !285, i64 0}
!285 = !{!"0x55c7bf560f10.w32.b0", !286, i64 0}
!286 = !{!"0x55c7bf560f10.w64.b0", !287, i64 0}
!287 = !{!"0x55c7bf560f10.w128.b0", !288, i64 0}
!288 = !{!"0x55c7bf560f10.w256.b0", !289, i64 0}
!289 = !{!"0x55c7bf560f10.w512.b0", !290, i64 0}
!290 = !{!"0x55c7bf560f10.w1024.b0", !291, i64 0}
!291 = !{!"i64", !292, i64 0}
!292 = !{!"0x55c7bf560f10", !8, i64 0}
!293 = !{!294, !294, i64 0}
!294 = !{!"0x55c7bf562bb0.w1.b0", !295, i64 0}
!295 = !{!"0x55c7bf562bb0.w2.b0", !296, i64 0}
!296 = !{!"0x55c7bf562bb0.w4.b0", !297, i64 0}
!297 = !{!"0x55c7bf562bb0.w8.b0", !298, i64 0}
!298 = !{!"0x55c7bf562bb0.w16.b0", !299, i64 0}
!299 = !{!"0x55c7bf562bb0.w32.b0", !300, i64 0}
!300 = !{!"0x55c7bf562bb0.w64.b0", !301, i64 0}
!301 = !{!"0x55c7bf562bb0.w128.b0", !302, i64 0}
!302 = !{!"0x55c7bf562bb0.w256.b0", !303, i64 0}
!303 = !{!"0x55c7bf562bb0.w512.b0", !304, i64 0}
!304 = !{!"0x55c7bf562bb0.w1024.b0", !305, i64 0}
!305 = !{!"i64", !306, i64 0}
!306 = !{!"0x55c7bf562bb0", !8, i64 0}
!307 = !{!308, !308, i64 0}
!308 = !{!"0x55c7bf562bb0.w1.b1", !295, i64 0}
!309 = !{!310, !310, i64 0}
!310 = !{!"0x55c7bf562bb0.w1.b2", !311, i64 0}
!311 = !{!"0x55c7bf562bb0.w2.b2", !296, i64 0}
!312 = !{!313, !313, i64 0}
!313 = !{!"0x55c7bf562bb0.w1.b3", !311, i64 0}
!314 = !{!315, !315, i64 0}
!315 = !{!"0x55c7bf5634a0.w4.b0", !316, i64 0}
!316 = !{!"0x55c7bf5634a0.w8.b0", !317, i64 0}
!317 = !{!"0x55c7bf5634a0.w16.b0", !318, i64 0}
!318 = !{!"0x55c7bf5634a0.w32.b0", !319, i64 0}
!319 = !{!"0x55c7bf5634a0.w64.b0", !320, i64 0}
!320 = !{!"0x55c7bf5634a0.w128.b0", !321, i64 0}
!321 = !{!"0x55c7bf5634a0.w256.b0", !322, i64 0}
!322 = !{!"0x55c7bf5634a0.w512.b0", !323, i64 0}
!323 = !{!"0x55c7bf5634a0.w1024.b0", !324, i64 0}
!324 = !{!"i64", !325, i64 0}
!325 = !{!"0x55c7bf5634a0", !8, i64 0}
!326 = !{!327, !327, i64 0}
!327 = !{!"i8", !328, i64 0}
!328 = !{!"0x55c7bf42b3e0", !8, i64 0}
!329 = !{!330, !330, i64 0}
!330 = !{!"i8", !331, i64 0}
!331 = !{!"0x55c7bf42c490", !8, i64 0}
!332 = !{!333, !333, i64 0}
!333 = !{!"i8", !334, i64 0}
!334 = !{!"0x55c7bf42db10", !8, i64 0}
!335 = !{!"branch_weights", i32 1, i32 262143}
!336 = distinct !{!336, !337}
!337 = !{!"llvm.loop.isvectorized", i32 1}
!338 = !{!339, !339, i64 0}
!339 = !{!"i8", !340, i64 0}
!340 = !{!"0x55c7bf42c5c0", !8, i64 0}
!341 = !{!"branch_weights", i32 1, i32 131071}
!342 = distinct !{!342, !337}
!343 = distinct !{!343, !337}
!344 = !{!345, !345, i64 0}
!345 = !{!"i8", !346, i64 0}
!346 = !{!"0x55c7bf42bcf0", !8, i64 0}
!347 = distinct !{!347, !337}
!348 = distinct !{!348, !337}
!349 = distinct !{!349, !337}
!350 = distinct !{!350, !337}
!351 = distinct !{!351, !337}
!352 = distinct !{!352, !337}
!353 = distinct !{!353, !337}
!354 = distinct !{!354, !337}
!355 = distinct !{!355, !337}
!356 = distinct !{!356, !337}
!357 = distinct !{!357, !337}
!358 = distinct !{!358, !337}
!359 = distinct !{!359, !337}
!360 = distinct !{!360, !337}
!361 = distinct !{!361, !337}
!362 = distinct !{!362, !337}
!363 = distinct !{!363, !337}
!364 = distinct !{!364, !337}
!365 = distinct !{!365, !337}
!366 = distinct !{!366, !337}
!367 = distinct !{!367, !337}
!368 = distinct !{!368, !337}
!369 = distinct !{!369, !337}
!370 = distinct !{!370, !337}
!371 = distinct !{!371, !337}
!372 = distinct !{!372, !337}
!373 = !{!374, !374, i64 0}
!374 = !{!"0x55c7bf57e240.w1.b0", !375, i64 0}
!375 = !{!"0x55c7bf57e240.w2.b0", !376, i64 0}
!376 = !{!"0x55c7bf57e240.w4.b0", !377, i64 0}
!377 = !{!"0x55c7bf57e240.w8.b0", !378, i64 0}
!378 = !{!"0x55c7bf57e240.w16.b0", !379, i64 0}
!379 = !{!"0x55c7bf57e240.w32.b0", !380, i64 0}
!380 = !{!"0x55c7bf57e240.w64.b0", !381, i64 0}
!381 = !{!"0x55c7bf57e240.w128.b0", !382, i64 0}
!382 = !{!"0x55c7bf57e240.w256.b0", !383, i64 0}
!383 = !{!"0x55c7bf57e240.w512.b0", !384, i64 0}
!384 = !{!"0x55c7bf57e240.w1024.b0", !385, i64 0}
!385 = !{!"i8", !386, i64 0}
!386 = !{!"0x55c7bf57e240", !8, i64 0}
!387 = !{!388, !388, i64 0}
!388 = !{!"0x55c7bf57e240.w1.b1", !375, i64 0}
!389 = !{!390, !390, i64 0}
!390 = !{!"0x55c7bf57e240.w1.b2", !391, i64 0}
!391 = !{!"0x55c7bf57e240.w2.b2", !376, i64 0}
!392 = !{!393, !393, i64 0}
!393 = !{!"0x55c7bf57e240.w1.b3", !391, i64 0}
!394 = !{!395, !395, i64 0}
!395 = !{!"0x55c7bf5894c0.w1.b0", !396, i64 0}
!396 = !{!"0x55c7bf5894c0.w2.b0", !397, i64 0}
!397 = !{!"0x55c7bf5894c0.w4.b0", !398, i64 0}
!398 = !{!"0x55c7bf5894c0.w8.b0", !399, i64 0}
!399 = !{!"0x55c7bf5894c0.w16.b0", !400, i64 0}
!400 = !{!"0x55c7bf5894c0.w32.b0", !401, i64 0}
!401 = !{!"0x55c7bf5894c0.w64.b0", !402, i64 0}
!402 = !{!"0x55c7bf5894c0.w128.b0", !403, i64 0}
!403 = !{!"0x55c7bf5894c0.w256.b0", !404, i64 0}
!404 = !{!"0x55c7bf5894c0.w512.b0", !405, i64 0}
!405 = !{!"0x55c7bf5894c0.w1024.b0", !406, i64 0}
!406 = !{!"i64", !407, i64 0}
!407 = !{!"0x55c7bf5894c0", !8, i64 0}
!408 = !{!409, !409, i64 0}
!409 = !{!"0x55c7bf5894c0.w1.b1", !396, i64 0}
!410 = !{!411, !411, i64 0}
!411 = !{!"0x55c7bf5894c0.w1.b2", !412, i64 0}
!412 = !{!"0x55c7bf5894c0.w2.b2", !397, i64 0}
!413 = !{!414, !414, i64 0}
!414 = !{!"0x55c7bf5894c0.w1.b3", !412, i64 0}
!415 = !{!416, !416, i64 0}
!416 = !{!"0x55c7bf589aa0.w4.b0", !417, i64 0}
!417 = !{!"0x55c7bf589aa0.w8.b0", !418, i64 0}
!418 = !{!"0x55c7bf589aa0.w16.b0", !419, i64 0}
!419 = !{!"0x55c7bf589aa0.w32.b0", !420, i64 0}
!420 = !{!"0x55c7bf589aa0.w64.b0", !421, i64 0}
!421 = !{!"0x55c7bf589aa0.w128.b0", !422, i64 0}
!422 = !{!"0x55c7bf589aa0.w256.b0", !423, i64 0}
!423 = !{!"0x55c7bf589aa0.w512.b0", !424, i64 0}
!424 = !{!"0x55c7bf589aa0.w1024.b0", !425, i64 0}
!425 = !{!"i64", !426, i64 0}
!426 = !{!"0x55c7bf589aa0", !8, i64 0}
!427 = !{!428, !428, i64 0}
!428 = !{!"0x55c7bf58bc80.w1.b0", !429, i64 0}
!429 = !{!"0x55c7bf58bc80.w2.b0", !430, i64 0}
!430 = !{!"0x55c7bf58bc80.w4.b0", !431, i64 0}
!431 = !{!"0x55c7bf58bc80.w8.b0", !432, i64 0}
!432 = !{!"0x55c7bf58bc80.w16.b0", !433, i64 0}
!433 = !{!"0x55c7bf58bc80.w32.b0", !434, i64 0}
!434 = !{!"0x55c7bf58bc80.w64.b0", !435, i64 0}
!435 = !{!"0x55c7bf58bc80.w128.b0", !436, i64 0}
!436 = !{!"0x55c7bf58bc80.w256.b0", !437, i64 0}
!437 = !{!"0x55c7bf58bc80.w512.b0", !438, i64 0}
!438 = !{!"0x55c7bf58bc80.w1024.b0", !439, i64 0}
!439 = !{!"i64", !440, i64 0}
!440 = !{!"0x55c7bf58bc80", !8, i64 0}
!441 = !{!442, !442, i64 0}
!442 = !{!"0x55c7bf58bc80.w1.b1", !429, i64 0}
!443 = !{!444, !444, i64 0}
!444 = !{!"0x55c7bf58bc80.w1.b2", !445, i64 0}
!445 = !{!"0x55c7bf58bc80.w2.b2", !430, i64 0}
!446 = !{!447, !447, i64 0}
!447 = !{!"0x55c7bf58bc80.w1.b3", !445, i64 0}
!448 = !{!449, !449, i64 0}
!449 = !{!"0x55c7bf58c610.w4.b0", !450, i64 0}
!450 = !{!"0x55c7bf58c610.w8.b0", !451, i64 0}
!451 = !{!"0x55c7bf58c610.w16.b0", !452, i64 0}
!452 = !{!"0x55c7bf58c610.w32.b0", !453, i64 0}
!453 = !{!"0x55c7bf58c610.w64.b0", !454, i64 0}
!454 = !{!"0x55c7bf58c610.w128.b0", !455, i64 0}
!455 = !{!"0x55c7bf58c610.w256.b0", !456, i64 0}
!456 = !{!"0x55c7bf58c610.w512.b0", !457, i64 0}
!457 = !{!"0x55c7bf58c610.w1024.b0", !458, i64 0}
!458 = !{!"i64", !459, i64 0}
!459 = !{!"0x55c7bf58c610", !8, i64 0}
!460 = !{!461, !461, i64 0}
!461 = !{!"0x55c7bf58e7f0.w1.b0", !462, i64 0}
!462 = !{!"0x55c7bf58e7f0.w2.b0", !463, i64 0}
!463 = !{!"0x55c7bf58e7f0.w4.b0", !464, i64 0}
!464 = !{!"0x55c7bf58e7f0.w8.b0", !465, i64 0}
!465 = !{!"0x55c7bf58e7f0.w16.b0", !466, i64 0}
!466 = !{!"0x55c7bf58e7f0.w32.b0", !467, i64 0}
!467 = !{!"0x55c7bf58e7f0.w64.b0", !468, i64 0}
!468 = !{!"0x55c7bf58e7f0.w128.b0", !469, i64 0}
!469 = !{!"0x55c7bf58e7f0.w256.b0", !470, i64 0}
!470 = !{!"0x55c7bf58e7f0.w512.b0", !471, i64 0}
!471 = !{!"0x55c7bf58e7f0.w1024.b0", !472, i64 0}
!472 = !{!"i64", !473, i64 0}
!473 = !{!"0x55c7bf58e7f0", !8, i64 0}
!474 = !{!475, !475, i64 0}
!475 = !{!"0x55c7bf58ead0.w1.b0", !476, i64 0}
!476 = !{!"0x55c7bf58ead0.w2.b0", !477, i64 0}
!477 = !{!"0x55c7bf58ead0.w4.b0", !478, i64 0}
!478 = !{!"0x55c7bf58ead0.w8.b0", !479, i64 0}
!479 = !{!"0x55c7bf58ead0.w16.b0", !480, i64 0}
!480 = !{!"0x55c7bf58ead0.w32.b0", !481, i64 0}
!481 = !{!"0x55c7bf58ead0.w64.b0", !482, i64 0}
!482 = !{!"0x55c7bf58ead0.w128.b0", !483, i64 0}
!483 = !{!"0x55c7bf58ead0.w256.b0", !484, i64 0}
!484 = !{!"0x55c7bf58ead0.w512.b0", !485, i64 0}
!485 = !{!"0x55c7bf58ead0.w1024.b0", !486, i64 0}
!486 = !{!"i64", !487, i64 0}
!487 = !{!"0x55c7bf58ead0", !8, i64 0}
!488 = !{!489, !489, i64 0}
!489 = !{!"0x55c7bf590770.w1.b0", !490, i64 0}
!490 = !{!"0x55c7bf590770.w2.b0", !491, i64 0}
!491 = !{!"0x55c7bf590770.w4.b0", !492, i64 0}
!492 = !{!"0x55c7bf590770.w8.b0", !493, i64 0}
!493 = !{!"0x55c7bf590770.w16.b0", !494, i64 0}
!494 = !{!"0x55c7bf590770.w32.b0", !495, i64 0}
!495 = !{!"0x55c7bf590770.w64.b0", !496, i64 0}
!496 = !{!"0x55c7bf590770.w128.b0", !497, i64 0}
!497 = !{!"0x55c7bf590770.w256.b0", !498, i64 0}
!498 = !{!"0x55c7bf590770.w512.b0", !499, i64 0}
!499 = !{!"0x55c7bf590770.w1024.b0", !500, i64 0}
!500 = !{!"i64", !501, i64 0}
!501 = !{!"0x55c7bf590770", !8, i64 0}
!502 = !{!503, !503, i64 0}
!503 = !{!"0x55c7bf590770.w1.b1", !490, i64 0}
!504 = !{!505, !505, i64 0}
!505 = !{!"0x55c7bf590770.w1.b2", !506, i64 0}
!506 = !{!"0x55c7bf590770.w2.b2", !491, i64 0}
!507 = !{!508, !508, i64 0}
!508 = !{!"0x55c7bf590770.w1.b3", !506, i64 0}
!509 = !{!510, !510, i64 0}
!510 = !{!"0x55c7bf591060.w4.b0", !511, i64 0}
!511 = !{!"0x55c7bf591060.w8.b0", !512, i64 0}
!512 = !{!"0x55c7bf591060.w16.b0", !513, i64 0}
!513 = !{!"0x55c7bf591060.w32.b0", !514, i64 0}
!514 = !{!"0x55c7bf591060.w64.b0", !515, i64 0}
!515 = !{!"0x55c7bf591060.w128.b0", !516, i64 0}
!516 = !{!"0x55c7bf591060.w256.b0", !517, i64 0}
!517 = !{!"0x55c7bf591060.w512.b0", !518, i64 0}
!518 = !{!"0x55c7bf591060.w1024.b0", !519, i64 0}
!519 = !{!"i64", !520, i64 0}
!520 = !{!"0x55c7bf591060", !8, i64 0}
!521 = !{!522, !522, i64 0}
!522 = !{!"i8", !523, i64 0}
!523 = !{!"0x55c7bf4e57a0", !8, i64 0}
!524 = !{!525, !525, i64 0}
!525 = !{!"i8", !526, i64 0}
!526 = !{!"0x55c7bf4e46c0", !8, i64 0}
!527 = !{!528, !528, i64 0}
!528 = !{!"i8", !529, i64 0}
!529 = !{!"0x55c7bf4e8390", !8, i64 0}
!530 = !{!531, !531, i64 0}
!531 = !{!"i8", !532, i64 0}
!532 = !{!"0x55c7bf4e3250", !8, i64 0}
!533 = distinct !{!533, !337}
!534 = !{!535, !535, i64 0}
!535 = !{!"i8", !536, i64 0}
!536 = !{!"0x55c7bf4e4fd0", !8, i64 0}
!537 = !{!538, !538, i64 0}
!538 = !{!"0x55c7bf567550.w1.b0", !539, i64 0}
!539 = !{!"0x55c7bf567550.w2.b0", !540, i64 0}
!540 = !{!"0x55c7bf567550.w4.b0", !541, i64 0}
!541 = !{!"0x55c7bf567550.w8.b0", !542, i64 0}
!542 = !{!"0x55c7bf567550.w16.b0", !543, i64 0}
!543 = !{!"0x55c7bf567550.w32.b0", !544, i64 0}
!544 = !{!"0x55c7bf567550.w64.b0", !545, i64 0}
!545 = !{!"0x55c7bf567550.w128.b0", !546, i64 0}
!546 = !{!"0x55c7bf567550.w256.b0", !547, i64 0}
!547 = !{!"0x55c7bf567550.w512.b0", !548, i64 0}
!548 = !{!"0x55c7bf567550.w1024.b0", !549, i64 0}
!549 = !{!"i8", !550, i64 0}
!550 = !{!"0x55c7bf567550", !8, i64 0}
!551 = !{!552, !552, i64 0}
!552 = !{!"0x55c7bf567550.w1.b1", !539, i64 0}
!553 = !{!554, !554, i64 0}
!554 = !{!"0x55c7bf567550.w1.b2", !555, i64 0}
!555 = !{!"0x55c7bf567550.w2.b2", !540, i64 0}
!556 = !{!557, !557, i64 0}
!557 = !{!"0x55c7bf567550.w1.b3", !555, i64 0}
!558 = !{!559, !559, i64 0}
!559 = !{!"0x55c7bf5728a0.w1.b0", !560, i64 0}
!560 = !{!"0x55c7bf5728a0.w2.b0", !561, i64 0}
!561 = !{!"0x55c7bf5728a0.w4.b0", !562, i64 0}
!562 = !{!"0x55c7bf5728a0.w8.b0", !563, i64 0}
!563 = !{!"0x55c7bf5728a0.w16.b0", !564, i64 0}
!564 = !{!"0x55c7bf5728a0.w32.b0", !565, i64 0}
!565 = !{!"0x55c7bf5728a0.w64.b0", !566, i64 0}
!566 = !{!"0x55c7bf5728a0.w128.b0", !567, i64 0}
!567 = !{!"0x55c7bf5728a0.w256.b0", !568, i64 0}
!568 = !{!"0x55c7bf5728a0.w512.b0", !569, i64 0}
!569 = !{!"0x55c7bf5728a0.w1024.b0", !570, i64 0}
!570 = !{!"i64", !571, i64 0}
!571 = !{!"0x55c7bf5728a0", !8, i64 0}
!572 = !{!573, !573, i64 0}
!573 = !{!"0x55c7bf5728a0.w1.b1", !560, i64 0}
!574 = !{!575, !575, i64 0}
!575 = !{!"0x55c7bf5728a0.w1.b2", !576, i64 0}
!576 = !{!"0x55c7bf5728a0.w2.b2", !561, i64 0}
!577 = !{!578, !578, i64 0}
!578 = !{!"0x55c7bf5728a0.w1.b3", !576, i64 0}
!579 = !{!580, !580, i64 0}
!580 = !{!"0x55c7bf572e50.w4.b0", !581, i64 0}
!581 = !{!"0x55c7bf572e50.w8.b0", !582, i64 0}
!582 = !{!"0x55c7bf572e50.w16.b0", !583, i64 0}
!583 = !{!"0x55c7bf572e50.w32.b0", !584, i64 0}
!584 = !{!"0x55c7bf572e50.w64.b0", !585, i64 0}
!585 = !{!"0x55c7bf572e50.w128.b0", !586, i64 0}
!586 = !{!"0x55c7bf572e50.w256.b0", !587, i64 0}
!587 = !{!"0x55c7bf572e50.w512.b0", !588, i64 0}
!588 = !{!"0x55c7bf572e50.w1024.b0", !589, i64 0}
!589 = !{!"i64", !590, i64 0}
!590 = !{!"0x55c7bf572e50", !8, i64 0}
!591 = !{!592, !592, i64 0}
!592 = !{!"0x55c7bf575030.w1.b0", !593, i64 0}
!593 = !{!"0x55c7bf575030.w2.b0", !594, i64 0}
!594 = !{!"0x55c7bf575030.w4.b0", !595, i64 0}
!595 = !{!"0x55c7bf575030.w8.b0", !596, i64 0}
!596 = !{!"0x55c7bf575030.w16.b0", !597, i64 0}
!597 = !{!"0x55c7bf575030.w32.b0", !598, i64 0}
!598 = !{!"0x55c7bf575030.w64.b0", !599, i64 0}
!599 = !{!"0x55c7bf575030.w128.b0", !600, i64 0}
!600 = !{!"0x55c7bf575030.w256.b0", !601, i64 0}
!601 = !{!"0x55c7bf575030.w512.b0", !602, i64 0}
!602 = !{!"0x55c7bf575030.w1024.b0", !603, i64 0}
!603 = !{!"i64", !604, i64 0}
!604 = !{!"0x55c7bf575030", !8, i64 0}
!605 = !{!606, !606, i64 0}
!606 = !{!"0x55c7bf575030.w1.b1", !593, i64 0}
!607 = !{!608, !608, i64 0}
!608 = !{!"0x55c7bf575030.w1.b2", !609, i64 0}
!609 = !{!"0x55c7bf575030.w2.b2", !594, i64 0}
!610 = !{!611, !611, i64 0}
!611 = !{!"0x55c7bf575030.w1.b3", !609, i64 0}
!612 = !{!613, !613, i64 0}
!613 = !{!"0x55c7bf5759c0.w4.b0", !614, i64 0}
!614 = !{!"0x55c7bf5759c0.w8.b0", !615, i64 0}
!615 = !{!"0x55c7bf5759c0.w16.b0", !616, i64 0}
!616 = !{!"0x55c7bf5759c0.w32.b0", !617, i64 0}
!617 = !{!"0x55c7bf5759c0.w64.b0", !618, i64 0}
!618 = !{!"0x55c7bf5759c0.w128.b0", !619, i64 0}
!619 = !{!"0x55c7bf5759c0.w256.b0", !620, i64 0}
!620 = !{!"0x55c7bf5759c0.w512.b0", !621, i64 0}
!621 = !{!"0x55c7bf5759c0.w1024.b0", !622, i64 0}
!622 = !{!"i64", !623, i64 0}
!623 = !{!"0x55c7bf5759c0", !8, i64 0}
!624 = !{!625, !625, i64 0}
!625 = !{!"0x55c7bf577ba0.w1.b0", !626, i64 0}
!626 = !{!"0x55c7bf577ba0.w2.b0", !627, i64 0}
!627 = !{!"0x55c7bf577ba0.w4.b0", !628, i64 0}
!628 = !{!"0x55c7bf577ba0.w8.b0", !629, i64 0}
!629 = !{!"0x55c7bf577ba0.w16.b0", !630, i64 0}
!630 = !{!"0x55c7bf577ba0.w32.b0", !631, i64 0}
!631 = !{!"0x55c7bf577ba0.w64.b0", !632, i64 0}
!632 = !{!"0x55c7bf577ba0.w128.b0", !633, i64 0}
!633 = !{!"0x55c7bf577ba0.w256.b0", !634, i64 0}
!634 = !{!"0x55c7bf577ba0.w512.b0", !635, i64 0}
!635 = !{!"0x55c7bf577ba0.w1024.b0", !636, i64 0}
!636 = !{!"i64", !637, i64 0}
!637 = !{!"0x55c7bf577ba0", !8, i64 0}
!638 = !{!639, !639, i64 0}
!639 = !{!"0x55c7bf577e80.w1.b0", !640, i64 0}
!640 = !{!"0x55c7bf577e80.w2.b0", !641, i64 0}
!641 = !{!"0x55c7bf577e80.w4.b0", !642, i64 0}
!642 = !{!"0x55c7bf577e80.w8.b0", !643, i64 0}
!643 = !{!"0x55c7bf577e80.w16.b0", !644, i64 0}
!644 = !{!"0x55c7bf577e80.w32.b0", !645, i64 0}
!645 = !{!"0x55c7bf577e80.w64.b0", !646, i64 0}
!646 = !{!"0x55c7bf577e80.w128.b0", !647, i64 0}
!647 = !{!"0x55c7bf577e80.w256.b0", !648, i64 0}
!648 = !{!"0x55c7bf577e80.w512.b0", !649, i64 0}
!649 = !{!"0x55c7bf577e80.w1024.b0", !650, i64 0}
!650 = !{!"i64", !651, i64 0}
!651 = !{!"0x55c7bf577e80", !8, i64 0}
!652 = !{!653, !653, i64 0}
!653 = !{!"0x55c7bf579b20.w1.b0", !654, i64 0}
!654 = !{!"0x55c7bf579b20.w2.b0", !655, i64 0}
!655 = !{!"0x55c7bf579b20.w4.b0", !656, i64 0}
!656 = !{!"0x55c7bf579b20.w8.b0", !657, i64 0}
!657 = !{!"0x55c7bf579b20.w16.b0", !658, i64 0}
!658 = !{!"0x55c7bf579b20.w32.b0", !659, i64 0}
!659 = !{!"0x55c7bf579b20.w64.b0", !660, i64 0}
!660 = !{!"0x55c7bf579b20.w128.b0", !661, i64 0}
!661 = !{!"0x55c7bf579b20.w256.b0", !662, i64 0}
!662 = !{!"0x55c7bf579b20.w512.b0", !663, i64 0}
!663 = !{!"0x55c7bf579b20.w1024.b0", !664, i64 0}
!664 = !{!"i64", !665, i64 0}
!665 = !{!"0x55c7bf579b20", !8, i64 0}
!666 = !{!667, !667, i64 0}
!667 = !{!"0x55c7bf579b20.w1.b1", !654, i64 0}
!668 = !{!669, !669, i64 0}
!669 = !{!"0x55c7bf579b20.w1.b2", !670, i64 0}
!670 = !{!"0x55c7bf579b20.w2.b2", !655, i64 0}
!671 = !{!672, !672, i64 0}
!672 = !{!"0x55c7bf579b20.w1.b3", !670, i64 0}
!673 = !{!674, !674, i64 0}
!674 = !{!"0x55c7bf57a410.w4.b0", !675, i64 0}
!675 = !{!"0x55c7bf57a410.w8.b0", !676, i64 0}
!676 = !{!"0x55c7bf57a410.w16.b0", !677, i64 0}
!677 = !{!"0x55c7bf57a410.w32.b0", !678, i64 0}
!678 = !{!"0x55c7bf57a410.w64.b0", !679, i64 0}
!679 = !{!"0x55c7bf57a410.w128.b0", !680, i64 0}
!680 = !{!"0x55c7bf57a410.w256.b0", !681, i64 0}
!681 = !{!"0x55c7bf57a410.w512.b0", !682, i64 0}
!682 = !{!"0x55c7bf57a410.w1024.b0", !683, i64 0}
!683 = !{!"i64", !684, i64 0}
!684 = !{!"0x55c7bf57a410", !8, i64 0}
!685 = !{!686, !686, i64 0}
!686 = !{!"i8", !687, i64 0}
!687 = !{!"0x55c7bf4f9e10", !8, i64 0}
!688 = !{!689, !689, i64 0}
!689 = !{!"i8", !690, i64 0}
!690 = !{!"0x55c7bf4f6ce0", !8, i64 0}
!691 = !{!692, !692, i64 0}
!692 = !{!"i8", !693, i64 0}
!693 = !{!"0x55c7bf4fa960", !8, i64 0}
!694 = !{!695, !695, i64 0}
!695 = !{!"i8", !696, i64 0}
!696 = !{!"0x55c7bf4f92b0", !8, i64 0}
!697 = distinct !{!697, !337}
!698 = !{!699, !699, i64 0}
!699 = !{!"i8", !700, i64 0}
!700 = !{!"0x55c7bf4f5550", !8, i64 0}
!701 = !{!702, !702, i64 0}
!702 = !{!"0x55c7bf572c20.w1.b0", !703, i64 0}
!703 = !{!"0x55c7bf572c20.w2.b0", !704, i64 0}
!704 = !{!"0x55c7bf572c20.w4.b0", !705, i64 0}
!705 = !{!"0x55c7bf572c20.w8.b0", !706, i64 0}
!706 = !{!"0x55c7bf572c20.w16.b0", !707, i64 0}
!707 = !{!"0x55c7bf572c20.w32.b0", !708, i64 0}
!708 = !{!"0x55c7bf572c20.w64.b0", !709, i64 0}
!709 = !{!"0x55c7bf572c20.w128.b0", !710, i64 0}
!710 = !{!"0x55c7bf572c20.w256.b0", !711, i64 0}
!711 = !{!"0x55c7bf572c20.w512.b0", !712, i64 0}
!712 = !{!"0x55c7bf572c20.w1024.b0", !713, i64 0}
!713 = !{!"i8", !714, i64 0}
!714 = !{!"0x55c7bf572c20", !8, i64 0}
!715 = !{!716, !716, i64 0}
!716 = !{!"0x55c7bf572c20.w1.b1", !703, i64 0}
!717 = !{!718, !718, i64 0}
!718 = !{!"0x55c7bf572c20.w1.b2", !719, i64 0}
!719 = !{!"0x55c7bf572c20.w2.b2", !704, i64 0}
!720 = !{!721, !721, i64 0}
!721 = !{!"0x55c7bf572c20.w1.b3", !719, i64 0}
!722 = !{!723, !723, i64 0}
!723 = !{!"0x55c7bf57dec0.w1.b0", !724, i64 0}
!724 = !{!"0x55c7bf57dec0.w2.b0", !725, i64 0}
!725 = !{!"0x55c7bf57dec0.w4.b0", !726, i64 0}
!726 = !{!"0x55c7bf57dec0.w8.b0", !727, i64 0}
!727 = !{!"0x55c7bf57dec0.w16.b0", !728, i64 0}
!728 = !{!"0x55c7bf57dec0.w32.b0", !729, i64 0}
!729 = !{!"0x55c7bf57dec0.w64.b0", !730, i64 0}
!730 = !{!"0x55c7bf57dec0.w128.b0", !731, i64 0}
!731 = !{!"0x55c7bf57dec0.w256.b0", !732, i64 0}
!732 = !{!"0x55c7bf57dec0.w512.b0", !733, i64 0}
!733 = !{!"0x55c7bf57dec0.w1024.b0", !734, i64 0}
!734 = !{!"i64", !735, i64 0}
!735 = !{!"0x55c7bf57dec0", !8, i64 0}
!736 = !{!737, !737, i64 0}
!737 = !{!"0x55c7bf57dec0.w1.b1", !724, i64 0}
!738 = !{!739, !739, i64 0}
!739 = !{!"0x55c7bf57dec0.w1.b2", !740, i64 0}
!740 = !{!"0x55c7bf57dec0.w2.b2", !725, i64 0}
!741 = !{!742, !742, i64 0}
!742 = !{!"0x55c7bf57dec0.w1.b3", !740, i64 0}
!743 = !{!744, !744, i64 0}
!744 = !{!"0x55c7bf57e4a0.w4.b0", !745, i64 0}
!745 = !{!"0x55c7bf57e4a0.w8.b0", !746, i64 0}
!746 = !{!"0x55c7bf57e4a0.w16.b0", !747, i64 0}
!747 = !{!"0x55c7bf57e4a0.w32.b0", !748, i64 0}
!748 = !{!"0x55c7bf57e4a0.w64.b0", !749, i64 0}
!749 = !{!"0x55c7bf57e4a0.w128.b0", !750, i64 0}
!750 = !{!"0x55c7bf57e4a0.w256.b0", !751, i64 0}
!751 = !{!"0x55c7bf57e4a0.w512.b0", !752, i64 0}
!752 = !{!"0x55c7bf57e4a0.w1024.b0", !753, i64 0}
!753 = !{!"i64", !754, i64 0}
!754 = !{!"0x55c7bf57e4a0", !8, i64 0}
!755 = !{!756, !756, i64 0}
!756 = !{!"0x55c7bf580680.w1.b0", !757, i64 0}
!757 = !{!"0x55c7bf580680.w2.b0", !758, i64 0}
!758 = !{!"0x55c7bf580680.w4.b0", !759, i64 0}
!759 = !{!"0x55c7bf580680.w8.b0", !760, i64 0}
!760 = !{!"0x55c7bf580680.w16.b0", !761, i64 0}
!761 = !{!"0x55c7bf580680.w32.b0", !762, i64 0}
!762 = !{!"0x55c7bf580680.w64.b0", !763, i64 0}
!763 = !{!"0x55c7bf580680.w128.b0", !764, i64 0}
!764 = !{!"0x55c7bf580680.w256.b0", !765, i64 0}
!765 = !{!"0x55c7bf580680.w512.b0", !766, i64 0}
!766 = !{!"0x55c7bf580680.w1024.b0", !767, i64 0}
!767 = !{!"i64", !768, i64 0}
!768 = !{!"0x55c7bf580680", !8, i64 0}
!769 = !{!770, !770, i64 0}
!770 = !{!"0x55c7bf580680.w1.b1", !757, i64 0}
!771 = !{!772, !772, i64 0}
!772 = !{!"0x55c7bf580680.w1.b2", !773, i64 0}
!773 = !{!"0x55c7bf580680.w2.b2", !758, i64 0}
!774 = !{!775, !775, i64 0}
!775 = !{!"0x55c7bf580680.w1.b3", !773, i64 0}
!776 = !{!777, !777, i64 0}
!777 = !{!"0x55c7bf581010.w4.b0", !778, i64 0}
!778 = !{!"0x55c7bf581010.w8.b0", !779, i64 0}
!779 = !{!"0x55c7bf581010.w16.b0", !780, i64 0}
!780 = !{!"0x55c7bf581010.w32.b0", !781, i64 0}
!781 = !{!"0x55c7bf581010.w64.b0", !782, i64 0}
!782 = !{!"0x55c7bf581010.w128.b0", !783, i64 0}
!783 = !{!"0x55c7bf581010.w256.b0", !784, i64 0}
!784 = !{!"0x55c7bf581010.w512.b0", !785, i64 0}
!785 = !{!"0x55c7bf581010.w1024.b0", !786, i64 0}
!786 = !{!"i64", !787, i64 0}
!787 = !{!"0x55c7bf581010", !8, i64 0}
!788 = !{!789, !789, i64 0}
!789 = !{!"0x55c7bf5831f0.w1.b0", !790, i64 0}
!790 = !{!"0x55c7bf5831f0.w2.b0", !791, i64 0}
!791 = !{!"0x55c7bf5831f0.w4.b0", !792, i64 0}
!792 = !{!"0x55c7bf5831f0.w8.b0", !793, i64 0}
!793 = !{!"0x55c7bf5831f0.w16.b0", !794, i64 0}
!794 = !{!"0x55c7bf5831f0.w32.b0", !795, i64 0}
!795 = !{!"0x55c7bf5831f0.w64.b0", !796, i64 0}
!796 = !{!"0x55c7bf5831f0.w128.b0", !797, i64 0}
!797 = !{!"0x55c7bf5831f0.w256.b0", !798, i64 0}
!798 = !{!"0x55c7bf5831f0.w512.b0", !799, i64 0}
!799 = !{!"0x55c7bf5831f0.w1024.b0", !800, i64 0}
!800 = !{!"i64", !801, i64 0}
!801 = !{!"0x55c7bf5831f0", !8, i64 0}
!802 = !{!803, !803, i64 0}
!803 = !{!"0x55c7bf5834d0.w1.b0", !804, i64 0}
!804 = !{!"0x55c7bf5834d0.w2.b0", !805, i64 0}
!805 = !{!"0x55c7bf5834d0.w4.b0", !806, i64 0}
!806 = !{!"0x55c7bf5834d0.w8.b0", !807, i64 0}
!807 = !{!"0x55c7bf5834d0.w16.b0", !808, i64 0}
!808 = !{!"0x55c7bf5834d0.w32.b0", !809, i64 0}
!809 = !{!"0x55c7bf5834d0.w64.b0", !810, i64 0}
!810 = !{!"0x55c7bf5834d0.w128.b0", !811, i64 0}
!811 = !{!"0x55c7bf5834d0.w256.b0", !812, i64 0}
!812 = !{!"0x55c7bf5834d0.w512.b0", !813, i64 0}
!813 = !{!"0x55c7bf5834d0.w1024.b0", !814, i64 0}
!814 = !{!"i64", !815, i64 0}
!815 = !{!"0x55c7bf5834d0", !8, i64 0}
!816 = !{!817, !817, i64 0}
!817 = !{!"0x55c7bf585170.w1.b0", !818, i64 0}
!818 = !{!"0x55c7bf585170.w2.b0", !819, i64 0}
!819 = !{!"0x55c7bf585170.w4.b0", !820, i64 0}
!820 = !{!"0x55c7bf585170.w8.b0", !821, i64 0}
!821 = !{!"0x55c7bf585170.w16.b0", !822, i64 0}
!822 = !{!"0x55c7bf585170.w32.b0", !823, i64 0}
!823 = !{!"0x55c7bf585170.w64.b0", !824, i64 0}
!824 = !{!"0x55c7bf585170.w128.b0", !825, i64 0}
!825 = !{!"0x55c7bf585170.w256.b0", !826, i64 0}
!826 = !{!"0x55c7bf585170.w512.b0", !827, i64 0}
!827 = !{!"0x55c7bf585170.w1024.b0", !828, i64 0}
!828 = !{!"i64", !829, i64 0}
!829 = !{!"0x55c7bf585170", !8, i64 0}
!830 = !{!831, !831, i64 0}
!831 = !{!"0x55c7bf585170.w1.b1", !818, i64 0}
!832 = !{!833, !833, i64 0}
!833 = !{!"0x55c7bf585170.w1.b2", !834, i64 0}
!834 = !{!"0x55c7bf585170.w2.b2", !819, i64 0}
!835 = !{!836, !836, i64 0}
!836 = !{!"0x55c7bf585170.w1.b3", !834, i64 0}
!837 = !{!838, !838, i64 0}
!838 = !{!"0x55c7bf585a60.w4.b0", !839, i64 0}
!839 = !{!"0x55c7bf585a60.w8.b0", !840, i64 0}
!840 = !{!"0x55c7bf585a60.w16.b0", !841, i64 0}
!841 = !{!"0x55c7bf585a60.w32.b0", !842, i64 0}
!842 = !{!"0x55c7bf585a60.w64.b0", !843, i64 0}
!843 = !{!"0x55c7bf585a60.w128.b0", !844, i64 0}
!844 = !{!"0x55c7bf585a60.w256.b0", !845, i64 0}
!845 = !{!"0x55c7bf585a60.w512.b0", !846, i64 0}
!846 = !{!"0x55c7bf585a60.w1024.b0", !847, i64 0}
!847 = !{!"i64", !848, i64 0}
!848 = !{!"0x55c7bf585a60", !8, i64 0}
!849 = !{!850, !850, i64 0}
!850 = !{!"i8", !851, i64 0}
!851 = !{!"0x55c7bf4ccd40", !8, i64 0}
!852 = !{!853, !853, i64 0}
!853 = !{!"i8", !854, i64 0}
!854 = !{!"0x55c7bf4ccc40", !8, i64 0}
!855 = !{!856, !856, i64 0}
!856 = !{!"i8", !857, i64 0}
!857 = !{!"0x55c7bf4cccc0", !8, i64 0}
!858 = !{!859, !859, i64 0}
!859 = !{!"i8", !860, i64 0}
!860 = !{!"0x55c7bf4ccd00", !8, i64 0}
!861 = !{!862, !862, i64 0}
!862 = !{!"i8", !863, i64 0}
!863 = !{!"0x55c7bf4ccc80", !8, i64 0}
!864 = !{!865, !865, i64 0}
!865 = !{!"0x55c7bfa2a1d0.w1.b0", !866, i64 0}
!866 = !{!"0x55c7bfa2a1d0.w2.b0", !867, i64 0}
!867 = !{!"0x55c7bfa2a1d0.w4.b0", !868, i64 0}
!868 = !{!"0x55c7bfa2a1d0.w8.b0", !869, i64 0}
!869 = !{!"0x55c7bfa2a1d0.w16.b0", !870, i64 0}
!870 = !{!"0x55c7bfa2a1d0.w32.b0", !871, i64 0}
!871 = !{!"0x55c7bfa2a1d0.w64.b0", !872, i64 0}
!872 = !{!"0x55c7bfa2a1d0.w128.b0", !873, i64 0}
!873 = !{!"0x55c7bfa2a1d0.w256.b0", !874, i64 0}
!874 = !{!"0x55c7bfa2a1d0.w512.b0", !875, i64 0}
!875 = !{!"0x55c7bfa2a1d0.w1024.b0", !876, i64 0}
!876 = !{!"i8", !877, i64 0}
!877 = !{!"0x55c7bfa2a1d0", !8, i64 0}
!878 = !{!879, !879, i64 0}
!879 = !{!"0x55c7bfa2a1d0.w1.b1", !866, i64 0}
!880 = !{!881, !881, i64 0}
!881 = !{!"0x55c7bfa30d80.w1.b0", !882, i64 0}
!882 = !{!"0x55c7bfa30d80.w2.b0", !883, i64 0}
!883 = !{!"0x55c7bfa30d80.w4.b0", !884, i64 0}
!884 = !{!"0x55c7bfa30d80.w8.b0", !885, i64 0}
!885 = !{!"0x55c7bfa30d80.w16.b0", !886, i64 0}
!886 = !{!"0x55c7bfa30d80.w32.b0", !887, i64 0}
!887 = !{!"0x55c7bfa30d80.w64.b0", !888, i64 0}
!888 = !{!"0x55c7bfa30d80.w128.b0", !889, i64 0}
!889 = !{!"0x55c7bfa30d80.w256.b0", !890, i64 0}
!890 = !{!"0x55c7bfa30d80.w512.b0", !891, i64 0}
!891 = !{!"0x55c7bfa30d80.w1024.b0", !892, i64 0}
!892 = !{!"i64", !893, i64 0}
!893 = !{!"0x55c7bfa30d80", !8, i64 0}
!894 = !{!895, !895, i64 0}
!895 = !{!"0x55c7bfa30d80.w1.b1", !882, i64 0}
!896 = !{!897, !897, i64 0}
!897 = !{!"0x55c7bfa2e850.w1.b0", !898, i64 0}
!898 = !{!"0x55c7bfa2e850.w2.b0", !899, i64 0}
!899 = !{!"0x55c7bfa2e850.w4.b0", !900, i64 0}
!900 = !{!"0x55c7bfa2e850.w8.b0", !901, i64 0}
!901 = !{!"0x55c7bfa2e850.w16.b0", !902, i64 0}
!902 = !{!"0x55c7bfa2e850.w32.b0", !903, i64 0}
!903 = !{!"0x55c7bfa2e850.w64.b0", !904, i64 0}
!904 = !{!"0x55c7bfa2e850.w128.b0", !905, i64 0}
!905 = !{!"0x55c7bfa2e850.w256.b0", !906, i64 0}
!906 = !{!"0x55c7bfa2e850.w512.b0", !907, i64 0}
!907 = !{!"0x55c7bfa2e850.w1024.b0", !908, i64 0}
!908 = !{!"i64", !909, i64 0}
!909 = !{!"0x55c7bfa2e850", !8, i64 0}
!910 = !{!911, !911, i64 0}
!911 = !{!"0x55c7bfa2e850.w1.b1", !898, i64 0}
!912 = !{!913, !913, i64 0}
!913 = !{!"0x55c7bfa36650.w1.b0", !914, i64 0}
!914 = !{!"0x55c7bfa36650.w2.b0", !915, i64 0}
!915 = !{!"0x55c7bfa36650.w4.b0", !916, i64 0}
!916 = !{!"0x55c7bfa36650.w8.b0", !917, i64 0}
!917 = !{!"0x55c7bfa36650.w16.b0", !918, i64 0}
!918 = !{!"0x55c7bfa36650.w32.b0", !919, i64 0}
!919 = !{!"0x55c7bfa36650.w64.b0", !920, i64 0}
!920 = !{!"0x55c7bfa36650.w128.b0", !921, i64 0}
!921 = !{!"0x55c7bfa36650.w256.b0", !922, i64 0}
!922 = !{!"0x55c7bfa36650.w512.b0", !923, i64 0}
!923 = !{!"0x55c7bfa36650.w1024.b0", !924, i64 0}
!924 = !{!"i64", !925, i64 0}
!925 = !{!"0x55c7bfa36650", !8, i64 0}
!926 = !{!927, !927, i64 0}
!927 = !{!"0x55c7bfa36650.w1.b1", !914, i64 0}
!928 = !{!929, !929, i64 0}
!929 = !{!"0x55c7bfa36ac0.w1.b0", !930, i64 0}
!930 = !{!"0x55c7bfa36ac0.w2.b0", !931, i64 0}
!931 = !{!"0x55c7bfa36ac0.w4.b0", !932, i64 0}
!932 = !{!"0x55c7bfa36ac0.w8.b0", !933, i64 0}
!933 = !{!"0x55c7bfa36ac0.w16.b0", !934, i64 0}
!934 = !{!"0x55c7bfa36ac0.w32.b0", !935, i64 0}
!935 = !{!"0x55c7bfa36ac0.w64.b0", !936, i64 0}
!936 = !{!"0x55c7bfa36ac0.w128.b0", !937, i64 0}
!937 = !{!"0x55c7bfa36ac0.w256.b0", !938, i64 0}
!938 = !{!"0x55c7bfa36ac0.w512.b0", !939, i64 0}
!939 = !{!"0x55c7bfa36ac0.w1024.b0", !940, i64 0}
!940 = !{!"i64", !941, i64 0}
!941 = !{!"0x55c7bfa36ac0", !8, i64 0}
!942 = !{!943, !943, i64 0}
!943 = !{!"0x55c7bfa36ac0.w1.b1", !930, i64 0}
!944 = !{!945, !945, i64 0}
!945 = !{!"i8", !946, i64 0}
!946 = !{!"0x55c7bf3745a0", !8, i64 0}
!947 = !{!948, !948, i64 0}
!948 = !{!"i8", !949, i64 0}
!949 = !{!"0x55c7bf3df8a0", !8, i64 0}
!950 = distinct !{!950, !337}
!951 = !{!952, !952, i64 0}
!952 = !{!"i8", !953, i64 0}
!953 = !{!"0x55c7bf3de3e0", !8, i64 0}
!954 = distinct !{!954, !337}
!955 = !{!956, !956, i64 0}
!956 = !{!"0x55c7bf589840.w1.b0", !957, i64 0}
!957 = !{!"0x55c7bf589840.w2.b0", !958, i64 0}
!958 = !{!"0x55c7bf589840.w4.b0", !959, i64 0}
!959 = !{!"0x55c7bf589840.w8.b0", !960, i64 0}
!960 = !{!"0x55c7bf589840.w16.b0", !961, i64 0}
!961 = !{!"0x55c7bf589840.w32.b0", !962, i64 0}
!962 = !{!"0x55c7bf589840.w64.b0", !963, i64 0}
!963 = !{!"0x55c7bf589840.w128.b0", !964, i64 0}
!964 = !{!"0x55c7bf589840.w256.b0", !965, i64 0}
!965 = !{!"0x55c7bf589840.w512.b0", !966, i64 0}
!966 = !{!"0x55c7bf589840.w1024.b0", !967, i64 0}
!967 = !{!"i8", !968, i64 0}
!968 = !{!"0x55c7bf589840", !8, i64 0}
!969 = !{!970, !970, i64 0}
!970 = !{!"0x55c7bf589840.w1.b1", !957, i64 0}
!971 = !{!972, !972, i64 0}
!972 = !{!"0x55c7bf589840.w1.b2", !973, i64 0}
!973 = !{!"0x55c7bf589840.w2.b2", !958, i64 0}
!974 = !{!975, !975, i64 0}
!975 = !{!"0x55c7bf589840.w1.b3", !973, i64 0}
!976 = !{!977, !977, i64 0}
!977 = !{!"0x55c7bf594b10.w1.b0", !978, i64 0}
!978 = !{!"0x55c7bf594b10.w2.b0", !979, i64 0}
!979 = !{!"0x55c7bf594b10.w4.b0", !980, i64 0}
!980 = !{!"0x55c7bf594b10.w8.b0", !981, i64 0}
!981 = !{!"0x55c7bf594b10.w16.b0", !982, i64 0}
!982 = !{!"0x55c7bf594b10.w32.b0", !983, i64 0}
!983 = !{!"0x55c7bf594b10.w64.b0", !984, i64 0}
!984 = !{!"0x55c7bf594b10.w128.b0", !985, i64 0}
!985 = !{!"0x55c7bf594b10.w256.b0", !986, i64 0}
!986 = !{!"0x55c7bf594b10.w512.b0", !987, i64 0}
!987 = !{!"0x55c7bf594b10.w1024.b0", !988, i64 0}
!988 = !{!"i64", !989, i64 0}
!989 = !{!"0x55c7bf594b10", !8, i64 0}
!990 = !{!991, !991, i64 0}
!991 = !{!"0x55c7bf594b10.w1.b1", !978, i64 0}
!992 = !{!993, !993, i64 0}
!993 = !{!"0x55c7bf594b10.w1.b2", !994, i64 0}
!994 = !{!"0x55c7bf594b10.w2.b2", !979, i64 0}
!995 = !{!996, !996, i64 0}
!996 = !{!"0x55c7bf594b10.w1.b3", !994, i64 0}
!997 = !{!998, !998, i64 0}
!998 = !{!"0x55c7bf5950f0.w4.b0", !999, i64 0}
!999 = !{!"0x55c7bf5950f0.w8.b0", !1000, i64 0}
!1000 = !{!"0x55c7bf5950f0.w16.b0", !1001, i64 0}
!1001 = !{!"0x55c7bf5950f0.w32.b0", !1002, i64 0}
!1002 = !{!"0x55c7bf5950f0.w64.b0", !1003, i64 0}
!1003 = !{!"0x55c7bf5950f0.w128.b0", !1004, i64 0}
!1004 = !{!"0x55c7bf5950f0.w256.b0", !1005, i64 0}
!1005 = !{!"0x55c7bf5950f0.w512.b0", !1006, i64 0}
!1006 = !{!"0x55c7bf5950f0.w1024.b0", !1007, i64 0}
!1007 = !{!"i64", !1008, i64 0}
!1008 = !{!"0x55c7bf5950f0", !8, i64 0}
!1009 = !{!1010, !1010, i64 0}
!1010 = !{!"0x55c7bf5972d0.w1.b0", !1011, i64 0}
!1011 = !{!"0x55c7bf5972d0.w2.b0", !1012, i64 0}
!1012 = !{!"0x55c7bf5972d0.w4.b0", !1013, i64 0}
!1013 = !{!"0x55c7bf5972d0.w8.b0", !1014, i64 0}
!1014 = !{!"0x55c7bf5972d0.w16.b0", !1015, i64 0}
!1015 = !{!"0x55c7bf5972d0.w32.b0", !1016, i64 0}
!1016 = !{!"0x55c7bf5972d0.w64.b0", !1017, i64 0}
!1017 = !{!"0x55c7bf5972d0.w128.b0", !1018, i64 0}
!1018 = !{!"0x55c7bf5972d0.w256.b0", !1019, i64 0}
!1019 = !{!"0x55c7bf5972d0.w512.b0", !1020, i64 0}
!1020 = !{!"0x55c7bf5972d0.w1024.b0", !1021, i64 0}
!1021 = !{!"i64", !1022, i64 0}
!1022 = !{!"0x55c7bf5972d0", !8, i64 0}
!1023 = !{!1024, !1024, i64 0}
!1024 = !{!"0x55c7bf5972d0.w1.b1", !1011, i64 0}
!1025 = !{!1026, !1026, i64 0}
!1026 = !{!"0x55c7bf5972d0.w1.b2", !1027, i64 0}
!1027 = !{!"0x55c7bf5972d0.w2.b2", !1012, i64 0}
!1028 = !{!1029, !1029, i64 0}
!1029 = !{!"0x55c7bf5972d0.w1.b3", !1027, i64 0}
!1030 = !{!1031, !1031, i64 0}
!1031 = !{!"0x55c7bf597c60.w4.b0", !1032, i64 0}
!1032 = !{!"0x55c7bf597c60.w8.b0", !1033, i64 0}
!1033 = !{!"0x55c7bf597c60.w16.b0", !1034, i64 0}
!1034 = !{!"0x55c7bf597c60.w32.b0", !1035, i64 0}
!1035 = !{!"0x55c7bf597c60.w64.b0", !1036, i64 0}
!1036 = !{!"0x55c7bf597c60.w128.b0", !1037, i64 0}
!1037 = !{!"0x55c7bf597c60.w256.b0", !1038, i64 0}
!1038 = !{!"0x55c7bf597c60.w512.b0", !1039, i64 0}
!1039 = !{!"0x55c7bf597c60.w1024.b0", !1040, i64 0}
!1040 = !{!"i64", !1041, i64 0}
!1041 = !{!"0x55c7bf597c60", !8, i64 0}
!1042 = !{!1043, !1043, i64 0}
!1043 = !{!"0x55c7bf599e40.w1.b0", !1044, i64 0}
!1044 = !{!"0x55c7bf599e40.w2.b0", !1045, i64 0}
!1045 = !{!"0x55c7bf599e40.w4.b0", !1046, i64 0}
!1046 = !{!"0x55c7bf599e40.w8.b0", !1047, i64 0}
!1047 = !{!"0x55c7bf599e40.w16.b0", !1048, i64 0}
!1048 = !{!"0x55c7bf599e40.w32.b0", !1049, i64 0}
!1049 = !{!"0x55c7bf599e40.w64.b0", !1050, i64 0}
!1050 = !{!"0x55c7bf599e40.w128.b0", !1051, i64 0}
!1051 = !{!"0x55c7bf599e40.w256.b0", !1052, i64 0}
!1052 = !{!"0x55c7bf599e40.w512.b0", !1053, i64 0}
!1053 = !{!"0x55c7bf599e40.w1024.b0", !1054, i64 0}
!1054 = !{!"i64", !1055, i64 0}
!1055 = !{!"0x55c7bf599e40", !8, i64 0}
!1056 = !{!1057, !1057, i64 0}
!1057 = !{!"0x55c7bf59a120.w1.b0", !1058, i64 0}
!1058 = !{!"0x55c7bf59a120.w2.b0", !1059, i64 0}
!1059 = !{!"0x55c7bf59a120.w4.b0", !1060, i64 0}
!1060 = !{!"0x55c7bf59a120.w8.b0", !1061, i64 0}
!1061 = !{!"0x55c7bf59a120.w16.b0", !1062, i64 0}
!1062 = !{!"0x55c7bf59a120.w32.b0", !1063, i64 0}
!1063 = !{!"0x55c7bf59a120.w64.b0", !1064, i64 0}
!1064 = !{!"0x55c7bf59a120.w128.b0", !1065, i64 0}
!1065 = !{!"0x55c7bf59a120.w256.b0", !1066, i64 0}
!1066 = !{!"0x55c7bf59a120.w512.b0", !1067, i64 0}
!1067 = !{!"0x55c7bf59a120.w1024.b0", !1068, i64 0}
!1068 = !{!"i64", !1069, i64 0}
!1069 = !{!"0x55c7bf59a120", !8, i64 0}
!1070 = !{!1071, !1071, i64 0}
!1071 = !{!"0x55c7bf59bdc0.w1.b0", !1072, i64 0}
!1072 = !{!"0x55c7bf59bdc0.w2.b0", !1073, i64 0}
!1073 = !{!"0x55c7bf59bdc0.w4.b0", !1074, i64 0}
!1074 = !{!"0x55c7bf59bdc0.w8.b0", !1075, i64 0}
!1075 = !{!"0x55c7bf59bdc0.w16.b0", !1076, i64 0}
!1076 = !{!"0x55c7bf59bdc0.w32.b0", !1077, i64 0}
!1077 = !{!"0x55c7bf59bdc0.w64.b0", !1078, i64 0}
!1078 = !{!"0x55c7bf59bdc0.w128.b0", !1079, i64 0}
!1079 = !{!"0x55c7bf59bdc0.w256.b0", !1080, i64 0}
!1080 = !{!"0x55c7bf59bdc0.w512.b0", !1081, i64 0}
!1081 = !{!"0x55c7bf59bdc0.w1024.b0", !1082, i64 0}
!1082 = !{!"i64", !1083, i64 0}
!1083 = !{!"0x55c7bf59bdc0", !8, i64 0}
!1084 = !{!1085, !1085, i64 0}
!1085 = !{!"0x55c7bf59bdc0.w1.b1", !1072, i64 0}
!1086 = !{!1087, !1087, i64 0}
!1087 = !{!"0x55c7bf59bdc0.w1.b2", !1088, i64 0}
!1088 = !{!"0x55c7bf59bdc0.w2.b2", !1073, i64 0}
!1089 = !{!1090, !1090, i64 0}
!1090 = !{!"0x55c7bf59bdc0.w1.b3", !1088, i64 0}
!1091 = !{!1092, !1092, i64 0}
!1092 = !{!"0x55c7bf59c6b0.w4.b0", !1093, i64 0}
!1093 = !{!"0x55c7bf59c6b0.w8.b0", !1094, i64 0}
!1094 = !{!"0x55c7bf59c6b0.w16.b0", !1095, i64 0}
!1095 = !{!"0x55c7bf59c6b0.w32.b0", !1096, i64 0}
!1096 = !{!"0x55c7bf59c6b0.w64.b0", !1097, i64 0}
!1097 = !{!"0x55c7bf59c6b0.w128.b0", !1098, i64 0}
!1098 = !{!"0x55c7bf59c6b0.w256.b0", !1099, i64 0}
!1099 = !{!"0x55c7bf59c6b0.w512.b0", !1100, i64 0}
!1100 = !{!"0x55c7bf59c6b0.w1024.b0", !1101, i64 0}
!1101 = !{!"i64", !1102, i64 0}
!1102 = !{!"0x55c7bf59c6b0", !8, i64 0}
!1103 = !{!1104, !1104, i64 0}
!1104 = !{!"i8", !1105, i64 0}
!1105 = !{!"0x55c7bf437a00", !8, i64 0}
!1106 = !{!1107, !1107, i64 0}
!1107 = !{!"i8", !1108, i64 0}
!1108 = !{!"0x55c7bf442910", !8, i64 0}
!1109 = !{!1110, !1110, i64 0}
!1110 = !{!"i8", !1111, i64 0}
!1111 = !{!"0x55c7bf43a520", !8, i64 0}
!1112 = distinct !{!1112, !337}
!1113 = !{!1114, !1114, i64 0}
!1114 = !{!"i8", !1115, i64 0}
!1115 = !{!"0x55c7bf4428d0", !8, i64 0}
!1116 = distinct !{!1116, !337}
!1117 = distinct !{!1117, !337}
!1118 = !{!1119, !1119, i64 0}
!1119 = !{!"i8", !1120, i64 0}
!1120 = !{!"0x55c7bf437210", !8, i64 0}
!1121 = distinct !{!1121, !337}
!1122 = distinct !{!1122, !337}
!1123 = distinct !{!1123, !337}
!1124 = distinct !{!1124, !337}
!1125 = distinct !{!1125, !337}
!1126 = distinct !{!1126, !337}
!1127 = distinct !{!1127, !337}
!1128 = distinct !{!1128, !337}
!1129 = distinct !{!1129, !337}
!1130 = distinct !{!1130, !337}
!1131 = distinct !{!1131, !337}
!1132 = distinct !{!1132, !337}
!1133 = distinct !{!1133, !337}
!1134 = distinct !{!1134, !337}
!1135 = distinct !{!1135, !337}
!1136 = distinct !{!1136, !337}
!1137 = distinct !{!1137, !337}
!1138 = distinct !{!1138, !337}
!1139 = distinct !{!1139, !337}
!1140 = distinct !{!1140, !337}
!1141 = distinct !{!1141, !337}
!1142 = distinct !{!1142, !337}
!1143 = distinct !{!1143, !337}
!1144 = distinct !{!1144, !337}
!1145 = distinct !{!1145, !337}
!1146 = distinct !{!1146, !337}
!1147 = !{!1148, !1148, i64 0}
!1148 = !{!"0x55c7bf594e90.w1.b0", !1149, i64 0}
!1149 = !{!"0x55c7bf594e90.w2.b0", !1150, i64 0}
!1150 = !{!"0x55c7bf594e90.w4.b0", !1151, i64 0}
!1151 = !{!"0x55c7bf594e90.w8.b0", !1152, i64 0}
!1152 = !{!"0x55c7bf594e90.w16.b0", !1153, i64 0}
!1153 = !{!"0x55c7bf594e90.w32.b0", !1154, i64 0}
!1154 = !{!"0x55c7bf594e90.w64.b0", !1155, i64 0}
!1155 = !{!"0x55c7bf594e90.w128.b0", !1156, i64 0}
!1156 = !{!"0x55c7bf594e90.w256.b0", !1157, i64 0}
!1157 = !{!"0x55c7bf594e90.w512.b0", !1158, i64 0}
!1158 = !{!"0x55c7bf594e90.w1024.b0", !1159, i64 0}
!1159 = !{!"i8", !1160, i64 0}
!1160 = !{!"0x55c7bf594e90", !8, i64 0}
!1161 = !{!1162, !1162, i64 0}
!1162 = !{!"0x55c7bf594e90.w1.b1", !1149, i64 0}
!1163 = !{!1164, !1164, i64 0}
!1164 = !{!"0x55c7bf594e90.w1.b2", !1165, i64 0}
!1165 = !{!"0x55c7bf594e90.w2.b2", !1150, i64 0}
!1166 = !{!1167, !1167, i64 0}
!1167 = !{!"0x55c7bf594e90.w1.b3", !1165, i64 0}
!1168 = !{!1169, !1169, i64 0}
!1169 = !{!"0x55c7bf5a0150.w1.b0", !1170, i64 0}
!1170 = !{!"0x55c7bf5a0150.w2.b0", !1171, i64 0}
!1171 = !{!"0x55c7bf5a0150.w4.b0", !1172, i64 0}
!1172 = !{!"0x55c7bf5a0150.w8.b0", !1173, i64 0}
!1173 = !{!"0x55c7bf5a0150.w16.b0", !1174, i64 0}
!1174 = !{!"0x55c7bf5a0150.w32.b0", !1175, i64 0}
!1175 = !{!"0x55c7bf5a0150.w64.b0", !1176, i64 0}
!1176 = !{!"0x55c7bf5a0150.w128.b0", !1177, i64 0}
!1177 = !{!"0x55c7bf5a0150.w256.b0", !1178, i64 0}
!1178 = !{!"0x55c7bf5a0150.w512.b0", !1179, i64 0}
!1179 = !{!"0x55c7bf5a0150.w1024.b0", !1180, i64 0}
!1180 = !{!"i64", !1181, i64 0}
!1181 = !{!"0x55c7bf5a0150", !8, i64 0}
!1182 = !{!1183, !1183, i64 0}
!1183 = !{!"0x55c7bf5a0150.w1.b1", !1170, i64 0}
!1184 = !{!1185, !1185, i64 0}
!1185 = !{!"0x55c7bf5a0150.w1.b2", !1186, i64 0}
!1186 = !{!"0x55c7bf5a0150.w2.b2", !1171, i64 0}
!1187 = !{!1188, !1188, i64 0}
!1188 = !{!"0x55c7bf5a0150.w1.b3", !1186, i64 0}
!1189 = !{!1190, !1190, i64 0}
!1190 = !{!"0x55c7bf5a0730.w4.b0", !1191, i64 0}
!1191 = !{!"0x55c7bf5a0730.w8.b0", !1192, i64 0}
!1192 = !{!"0x55c7bf5a0730.w16.b0", !1193, i64 0}
!1193 = !{!"0x55c7bf5a0730.w32.b0", !1194, i64 0}
!1194 = !{!"0x55c7bf5a0730.w64.b0", !1195, i64 0}
!1195 = !{!"0x55c7bf5a0730.w128.b0", !1196, i64 0}
!1196 = !{!"0x55c7bf5a0730.w256.b0", !1197, i64 0}
!1197 = !{!"0x55c7bf5a0730.w512.b0", !1198, i64 0}
!1198 = !{!"0x55c7bf5a0730.w1024.b0", !1199, i64 0}
!1199 = !{!"i64", !1200, i64 0}
!1200 = !{!"0x55c7bf5a0730", !8, i64 0}
!1201 = !{!1202, !1202, i64 0}
!1202 = !{!"0x55c7bf5a2910.w1.b0", !1203, i64 0}
!1203 = !{!"0x55c7bf5a2910.w2.b0", !1204, i64 0}
!1204 = !{!"0x55c7bf5a2910.w4.b0", !1205, i64 0}
!1205 = !{!"0x55c7bf5a2910.w8.b0", !1206, i64 0}
!1206 = !{!"0x55c7bf5a2910.w16.b0", !1207, i64 0}
!1207 = !{!"0x55c7bf5a2910.w32.b0", !1208, i64 0}
!1208 = !{!"0x55c7bf5a2910.w64.b0", !1209, i64 0}
!1209 = !{!"0x55c7bf5a2910.w128.b0", !1210, i64 0}
!1210 = !{!"0x55c7bf5a2910.w256.b0", !1211, i64 0}
!1211 = !{!"0x55c7bf5a2910.w512.b0", !1212, i64 0}
!1212 = !{!"0x55c7bf5a2910.w1024.b0", !1213, i64 0}
!1213 = !{!"i64", !1214, i64 0}
!1214 = !{!"0x55c7bf5a2910", !8, i64 0}
!1215 = !{!1216, !1216, i64 0}
!1216 = !{!"0x55c7bf5a2910.w1.b1", !1203, i64 0}
!1217 = !{!1218, !1218, i64 0}
!1218 = !{!"0x55c7bf5a2910.w1.b2", !1219, i64 0}
!1219 = !{!"0x55c7bf5a2910.w2.b2", !1204, i64 0}
!1220 = !{!1221, !1221, i64 0}
!1221 = !{!"0x55c7bf5a2910.w1.b3", !1219, i64 0}
!1222 = !{!1223, !1223, i64 0}
!1223 = !{!"0x55c7bf5a32a0.w4.b0", !1224, i64 0}
!1224 = !{!"0x55c7bf5a32a0.w8.b0", !1225, i64 0}
!1225 = !{!"0x55c7bf5a32a0.w16.b0", !1226, i64 0}
!1226 = !{!"0x55c7bf5a32a0.w32.b0", !1227, i64 0}
!1227 = !{!"0x55c7bf5a32a0.w64.b0", !1228, i64 0}
!1228 = !{!"0x55c7bf5a32a0.w128.b0", !1229, i64 0}
!1229 = !{!"0x55c7bf5a32a0.w256.b0", !1230, i64 0}
!1230 = !{!"0x55c7bf5a32a0.w512.b0", !1231, i64 0}
!1231 = !{!"0x55c7bf5a32a0.w1024.b0", !1232, i64 0}
!1232 = !{!"i64", !1233, i64 0}
!1233 = !{!"0x55c7bf5a32a0", !8, i64 0}
!1234 = !{!1235, !1235, i64 0}
!1235 = !{!"0x55c7bf5a5480.w1.b0", !1236, i64 0}
!1236 = !{!"0x55c7bf5a5480.w2.b0", !1237, i64 0}
!1237 = !{!"0x55c7bf5a5480.w4.b0", !1238, i64 0}
!1238 = !{!"0x55c7bf5a5480.w8.b0", !1239, i64 0}
!1239 = !{!"0x55c7bf5a5480.w16.b0", !1240, i64 0}
!1240 = !{!"0x55c7bf5a5480.w32.b0", !1241, i64 0}
!1241 = !{!"0x55c7bf5a5480.w64.b0", !1242, i64 0}
!1242 = !{!"0x55c7bf5a5480.w128.b0", !1243, i64 0}
!1243 = !{!"0x55c7bf5a5480.w256.b0", !1244, i64 0}
!1244 = !{!"0x55c7bf5a5480.w512.b0", !1245, i64 0}
!1245 = !{!"0x55c7bf5a5480.w1024.b0", !1246, i64 0}
!1246 = !{!"i64", !1247, i64 0}
!1247 = !{!"0x55c7bf5a5480", !8, i64 0}
!1248 = !{!1249, !1249, i64 0}
!1249 = !{!"0x55c7bf5a5760.w1.b0", !1250, i64 0}
!1250 = !{!"0x55c7bf5a5760.w2.b0", !1251, i64 0}
!1251 = !{!"0x55c7bf5a5760.w4.b0", !1252, i64 0}
!1252 = !{!"0x55c7bf5a5760.w8.b0", !1253, i64 0}
!1253 = !{!"0x55c7bf5a5760.w16.b0", !1254, i64 0}
!1254 = !{!"0x55c7bf5a5760.w32.b0", !1255, i64 0}
!1255 = !{!"0x55c7bf5a5760.w64.b0", !1256, i64 0}
!1256 = !{!"0x55c7bf5a5760.w128.b0", !1257, i64 0}
!1257 = !{!"0x55c7bf5a5760.w256.b0", !1258, i64 0}
!1258 = !{!"0x55c7bf5a5760.w512.b0", !1259, i64 0}
!1259 = !{!"0x55c7bf5a5760.w1024.b0", !1260, i64 0}
!1260 = !{!"i64", !1261, i64 0}
!1261 = !{!"0x55c7bf5a5760", !8, i64 0}
!1262 = !{!1263, !1263, i64 0}
!1263 = !{!"0x55c7bf5a7400.w1.b0", !1264, i64 0}
!1264 = !{!"0x55c7bf5a7400.w2.b0", !1265, i64 0}
!1265 = !{!"0x55c7bf5a7400.w4.b0", !1266, i64 0}
!1266 = !{!"0x55c7bf5a7400.w8.b0", !1267, i64 0}
!1267 = !{!"0x55c7bf5a7400.w16.b0", !1268, i64 0}
!1268 = !{!"0x55c7bf5a7400.w32.b0", !1269, i64 0}
!1269 = !{!"0x55c7bf5a7400.w64.b0", !1270, i64 0}
!1270 = !{!"0x55c7bf5a7400.w128.b0", !1271, i64 0}
!1271 = !{!"0x55c7bf5a7400.w256.b0", !1272, i64 0}
!1272 = !{!"0x55c7bf5a7400.w512.b0", !1273, i64 0}
!1273 = !{!"0x55c7bf5a7400.w1024.b0", !1274, i64 0}
!1274 = !{!"i64", !1275, i64 0}
!1275 = !{!"0x55c7bf5a7400", !8, i64 0}
!1276 = !{!1277, !1277, i64 0}
!1277 = !{!"0x55c7bf5a7400.w1.b1", !1264, i64 0}
!1278 = !{!1279, !1279, i64 0}
!1279 = !{!"0x55c7bf5a7400.w1.b2", !1280, i64 0}
!1280 = !{!"0x55c7bf5a7400.w2.b2", !1265, i64 0}
!1281 = !{!1282, !1282, i64 0}
!1282 = !{!"0x55c7bf5a7400.w1.b3", !1280, i64 0}
!1283 = !{!1284, !1284, i64 0}
!1284 = !{!"0x55c7bf5a7cf0.w4.b0", !1285, i64 0}
!1285 = !{!"0x55c7bf5a7cf0.w8.b0", !1286, i64 0}
!1286 = !{!"0x55c7bf5a7cf0.w16.b0", !1287, i64 0}
!1287 = !{!"0x55c7bf5a7cf0.w32.b0", !1288, i64 0}
!1288 = !{!"0x55c7bf5a7cf0.w64.b0", !1289, i64 0}
!1289 = !{!"0x55c7bf5a7cf0.w128.b0", !1290, i64 0}
!1290 = !{!"0x55c7bf5a7cf0.w256.b0", !1291, i64 0}
!1291 = !{!"0x55c7bf5a7cf0.w512.b0", !1292, i64 0}
!1292 = !{!"0x55c7bf5a7cf0.w1024.b0", !1293, i64 0}
!1293 = !{!"i64", !1294, i64 0}
!1294 = !{!"0x55c7bf5a7cf0", !8, i64 0}
!1295 = !{!1296, !1296, i64 0}
!1296 = !{!"i8", !1297, i64 0}
!1297 = !{!"0x55c7bf3f1720", !8, i64 0}
!1298 = !{!1299, !1299, i64 0}
!1299 = !{!"i8", !1300, i64 0}
!1300 = !{!"0x55c7bf3f1620", !8, i64 0}
!1301 = distinct !{!1301, !337}
!1302 = distinct !{!1302, !337}
!1303 = distinct !{!1303, !337}
!1304 = distinct !{!1304, !337}
!1305 = distinct !{!1305, !337}
!1306 = distinct !{!1306, !337}
!1307 = distinct !{!1307, !337}
!1308 = !{!1309, !1309, i64 0}
!1309 = !{!"i8", !1310, i64 0}
!1310 = !{!"0x55c7bf3f16e0", !8, i64 0}
!1311 = !{!1312, !1312, i64 0}
!1312 = !{!"i8", !1313, i64 0}
!1313 = !{!"0x55c7bf3f16a0", !8, i64 0}
!1314 = !{!1315, !1315, i64 0}
!1315 = !{!"i8", !1316, i64 0}
!1316 = !{!"0x55c7bf3f1660", !8, i64 0}
!1317 = !{!1318, !1318, i64 0}
!1318 = !{!"0x55c7bf5a04d0.w1.b0", !1319, i64 0}
!1319 = !{!"0x55c7bf5a04d0.w2.b0", !1320, i64 0}
!1320 = !{!"0x55c7bf5a04d0.w4.b0", !1321, i64 0}
!1321 = !{!"0x55c7bf5a04d0.w8.b0", !1322, i64 0}
!1322 = !{!"0x55c7bf5a04d0.w16.b0", !1323, i64 0}
!1323 = !{!"0x55c7bf5a04d0.w32.b0", !1324, i64 0}
!1324 = !{!"0x55c7bf5a04d0.w64.b0", !1325, i64 0}
!1325 = !{!"0x55c7bf5a04d0.w128.b0", !1326, i64 0}
!1326 = !{!"0x55c7bf5a04d0.w256.b0", !1327, i64 0}
!1327 = !{!"0x55c7bf5a04d0.w512.b0", !1328, i64 0}
!1328 = !{!"0x55c7bf5a04d0.w1024.b0", !1329, i64 0}
!1329 = !{!"i8", !1330, i64 0}
!1330 = !{!"0x55c7bf5a04d0", !8, i64 0}
!1331 = !{!1332, !1332, i64 0}
!1332 = !{!"0x55c7bf5a04d0.w1.b1", !1319, i64 0}
!1333 = !{!1334, !1334, i64 0}
!1334 = !{!"0x55c7bf5a04d0.w1.b2", !1335, i64 0}
!1335 = !{!"0x55c7bf5a04d0.w2.b2", !1320, i64 0}
!1336 = !{!1337, !1337, i64 0}
!1337 = !{!"0x55c7bf5a04d0.w1.b3", !1335, i64 0}
!1338 = !{!1339, !1339, i64 0}
!1339 = !{!"0x55c7bf5ab7a0.w1.b0", !1340, i64 0}
!1340 = !{!"0x55c7bf5ab7a0.w2.b0", !1341, i64 0}
!1341 = !{!"0x55c7bf5ab7a0.w4.b0", !1342, i64 0}
!1342 = !{!"0x55c7bf5ab7a0.w8.b0", !1343, i64 0}
!1343 = !{!"0x55c7bf5ab7a0.w16.b0", !1344, i64 0}
!1344 = !{!"0x55c7bf5ab7a0.w32.b0", !1345, i64 0}
!1345 = !{!"0x55c7bf5ab7a0.w64.b0", !1346, i64 0}
!1346 = !{!"0x55c7bf5ab7a0.w128.b0", !1347, i64 0}
!1347 = !{!"0x55c7bf5ab7a0.w256.b0", !1348, i64 0}
!1348 = !{!"0x55c7bf5ab7a0.w512.b0", !1349, i64 0}
!1349 = !{!"0x55c7bf5ab7a0.w1024.b0", !1350, i64 0}
!1350 = !{!"i64", !1351, i64 0}
!1351 = !{!"0x55c7bf5ab7a0", !8, i64 0}
!1352 = !{!1353, !1353, i64 0}
!1353 = !{!"0x55c7bf5ab7a0.w1.b1", !1340, i64 0}
!1354 = !{!1355, !1355, i64 0}
!1355 = !{!"0x55c7bf5ab7a0.w1.b2", !1356, i64 0}
!1356 = !{!"0x55c7bf5ab7a0.w2.b2", !1341, i64 0}
!1357 = !{!1358, !1358, i64 0}
!1358 = !{!"0x55c7bf5ab7a0.w1.b3", !1356, i64 0}
!1359 = !{!1360, !1360, i64 0}
!1360 = !{!"0x55c7bf5abd80.w4.b0", !1361, i64 0}
!1361 = !{!"0x55c7bf5abd80.w8.b0", !1362, i64 0}
!1362 = !{!"0x55c7bf5abd80.w16.b0", !1363, i64 0}
!1363 = !{!"0x55c7bf5abd80.w32.b0", !1364, i64 0}
!1364 = !{!"0x55c7bf5abd80.w64.b0", !1365, i64 0}
!1365 = !{!"0x55c7bf5abd80.w128.b0", !1366, i64 0}
!1366 = !{!"0x55c7bf5abd80.w256.b0", !1367, i64 0}
!1367 = !{!"0x55c7bf5abd80.w512.b0", !1368, i64 0}
!1368 = !{!"0x55c7bf5abd80.w1024.b0", !1369, i64 0}
!1369 = !{!"i64", !1370, i64 0}
!1370 = !{!"0x55c7bf5abd80", !8, i64 0}
!1371 = !{!1372, !1372, i64 0}
!1372 = !{!"0x55c7bf5adf60.w1.b0", !1373, i64 0}
!1373 = !{!"0x55c7bf5adf60.w2.b0", !1374, i64 0}
!1374 = !{!"0x55c7bf5adf60.w4.b0", !1375, i64 0}
!1375 = !{!"0x55c7bf5adf60.w8.b0", !1376, i64 0}
!1376 = !{!"0x55c7bf5adf60.w16.b0", !1377, i64 0}
!1377 = !{!"0x55c7bf5adf60.w32.b0", !1378, i64 0}
!1378 = !{!"0x55c7bf5adf60.w64.b0", !1379, i64 0}
!1379 = !{!"0x55c7bf5adf60.w128.b0", !1380, i64 0}
!1380 = !{!"0x55c7bf5adf60.w256.b0", !1381, i64 0}
!1381 = !{!"0x55c7bf5adf60.w512.b0", !1382, i64 0}
!1382 = !{!"0x55c7bf5adf60.w1024.b0", !1383, i64 0}
!1383 = !{!"i64", !1384, i64 0}
!1384 = !{!"0x55c7bf5adf60", !8, i64 0}
!1385 = !{!1386, !1386, i64 0}
!1386 = !{!"0x55c7bf5adf60.w1.b1", !1373, i64 0}
!1387 = !{!1388, !1388, i64 0}
!1388 = !{!"0x55c7bf5adf60.w1.b2", !1389, i64 0}
!1389 = !{!"0x55c7bf5adf60.w2.b2", !1374, i64 0}
!1390 = !{!1391, !1391, i64 0}
!1391 = !{!"0x55c7bf5adf60.w1.b3", !1389, i64 0}
!1392 = !{!1393, !1393, i64 0}
!1393 = !{!"0x55c7bf5ae8f0.w4.b0", !1394, i64 0}
!1394 = !{!"0x55c7bf5ae8f0.w8.b0", !1395, i64 0}
!1395 = !{!"0x55c7bf5ae8f0.w16.b0", !1396, i64 0}
!1396 = !{!"0x55c7bf5ae8f0.w32.b0", !1397, i64 0}
!1397 = !{!"0x55c7bf5ae8f0.w64.b0", !1398, i64 0}
!1398 = !{!"0x55c7bf5ae8f0.w128.b0", !1399, i64 0}
!1399 = !{!"0x55c7bf5ae8f0.w256.b0", !1400, i64 0}
!1400 = !{!"0x55c7bf5ae8f0.w512.b0", !1401, i64 0}
!1401 = !{!"0x55c7bf5ae8f0.w1024.b0", !1402, i64 0}
!1402 = !{!"i64", !1403, i64 0}
!1403 = !{!"0x55c7bf5ae8f0", !8, i64 0}
!1404 = !{!1405, !1405, i64 0}
!1405 = !{!"0x55c7bf5b0ad0.w1.b0", !1406, i64 0}
!1406 = !{!"0x55c7bf5b0ad0.w2.b0", !1407, i64 0}
!1407 = !{!"0x55c7bf5b0ad0.w4.b0", !1408, i64 0}
!1408 = !{!"0x55c7bf5b0ad0.w8.b0", !1409, i64 0}
!1409 = !{!"0x55c7bf5b0ad0.w16.b0", !1410, i64 0}
!1410 = !{!"0x55c7bf5b0ad0.w32.b0", !1411, i64 0}
!1411 = !{!"0x55c7bf5b0ad0.w64.b0", !1412, i64 0}
!1412 = !{!"0x55c7bf5b0ad0.w128.b0", !1413, i64 0}
!1413 = !{!"0x55c7bf5b0ad0.w256.b0", !1414, i64 0}
!1414 = !{!"0x55c7bf5b0ad0.w512.b0", !1415, i64 0}
!1415 = !{!"0x55c7bf5b0ad0.w1024.b0", !1416, i64 0}
!1416 = !{!"i64", !1417, i64 0}
!1417 = !{!"0x55c7bf5b0ad0", !8, i64 0}
!1418 = !{!1419, !1419, i64 0}
!1419 = !{!"0x55c7bf5b0db0.w1.b0", !1420, i64 0}
!1420 = !{!"0x55c7bf5b0db0.w2.b0", !1421, i64 0}
!1421 = !{!"0x55c7bf5b0db0.w4.b0", !1422, i64 0}
!1422 = !{!"0x55c7bf5b0db0.w8.b0", !1423, i64 0}
!1423 = !{!"0x55c7bf5b0db0.w16.b0", !1424, i64 0}
!1424 = !{!"0x55c7bf5b0db0.w32.b0", !1425, i64 0}
!1425 = !{!"0x55c7bf5b0db0.w64.b0", !1426, i64 0}
!1426 = !{!"0x55c7bf5b0db0.w128.b0", !1427, i64 0}
!1427 = !{!"0x55c7bf5b0db0.w256.b0", !1428, i64 0}
!1428 = !{!"0x55c7bf5b0db0.w512.b0", !1429, i64 0}
!1429 = !{!"0x55c7bf5b0db0.w1024.b0", !1430, i64 0}
!1430 = !{!"i64", !1431, i64 0}
!1431 = !{!"0x55c7bf5b0db0", !8, i64 0}
!1432 = !{!1433, !1433, i64 0}
!1433 = !{!"0x55c7bf5b2a50.w1.b0", !1434, i64 0}
!1434 = !{!"0x55c7bf5b2a50.w2.b0", !1435, i64 0}
!1435 = !{!"0x55c7bf5b2a50.w4.b0", !1436, i64 0}
!1436 = !{!"0x55c7bf5b2a50.w8.b0", !1437, i64 0}
!1437 = !{!"0x55c7bf5b2a50.w16.b0", !1438, i64 0}
!1438 = !{!"0x55c7bf5b2a50.w32.b0", !1439, i64 0}
!1439 = !{!"0x55c7bf5b2a50.w64.b0", !1440, i64 0}
!1440 = !{!"0x55c7bf5b2a50.w128.b0", !1441, i64 0}
!1441 = !{!"0x55c7bf5b2a50.w256.b0", !1442, i64 0}
!1442 = !{!"0x55c7bf5b2a50.w512.b0", !1443, i64 0}
!1443 = !{!"0x55c7bf5b2a50.w1024.b0", !1444, i64 0}
!1444 = !{!"i64", !1445, i64 0}
!1445 = !{!"0x55c7bf5b2a50", !8, i64 0}
!1446 = !{!1447, !1447, i64 0}
!1447 = !{!"0x55c7bf5b2a50.w1.b1", !1434, i64 0}
!1448 = !{!1449, !1449, i64 0}
!1449 = !{!"0x55c7bf5b2a50.w1.b2", !1450, i64 0}
!1450 = !{!"0x55c7bf5b2a50.w2.b2", !1435, i64 0}
!1451 = !{!1452, !1452, i64 0}
!1452 = !{!"0x55c7bf5b2a50.w1.b3", !1450, i64 0}
!1453 = !{!1454, !1454, i64 0}
!1454 = !{!"0x55c7bf5b3340.w4.b0", !1455, i64 0}
!1455 = !{!"0x55c7bf5b3340.w8.b0", !1456, i64 0}
!1456 = !{!"0x55c7bf5b3340.w16.b0", !1457, i64 0}
!1457 = !{!"0x55c7bf5b3340.w32.b0", !1458, i64 0}
!1458 = !{!"0x55c7bf5b3340.w64.b0", !1459, i64 0}
!1459 = !{!"0x55c7bf5b3340.w128.b0", !1460, i64 0}
!1460 = !{!"0x55c7bf5b3340.w256.b0", !1461, i64 0}
!1461 = !{!"0x55c7bf5b3340.w512.b0", !1462, i64 0}
!1462 = !{!"0x55c7bf5b3340.w1024.b0", !1463, i64 0}
!1463 = !{!"i64", !1464, i64 0}
!1464 = !{!"0x55c7bf5b3340", !8, i64 0}
!1465 = !{!1466, !1466, i64 0}
!1466 = !{!"i8", !1467, i64 0}
!1467 = !{!"0x55c7bf5288e0", !8, i64 0}
!1468 = !{!1469, !1469, i64 0}
!1469 = !{!"i8", !1470, i64 0}
!1470 = !{!"0x55c7bf528aa0", !8, i64 0}
!1471 = !{!1472, !1472, i64 0}
!1472 = !{!"i8", !1473, i64 0}
!1473 = !{!"0x55c7bf528a60", !8, i64 0}
!1474 = !{!1475, !1475, i64 0}
!1475 = !{!"i8", !1476, i64 0}
!1476 = !{!"0x55c7bf528a20", !8, i64 0}
!1477 = !{!1478, !1478, i64 0}
!1478 = !{!"i8", !1479, i64 0}
!1479 = !{!"0x55c7bf528980", !8, i64 0}
!1480 = distinct !{!1480, !337}
!1481 = !{!1482, !1482, i64 0}
!1482 = !{!"0x55c7bf5abb20.w1.b0", !1483, i64 0}
!1483 = !{!"0x55c7bf5abb20.w2.b0", !1484, i64 0}
!1484 = !{!"0x55c7bf5abb20.w4.b0", !1485, i64 0}
!1485 = !{!"0x55c7bf5abb20.w8.b0", !1486, i64 0}
!1486 = !{!"0x55c7bf5abb20.w16.b0", !1487, i64 0}
!1487 = !{!"0x55c7bf5abb20.w32.b0", !1488, i64 0}
!1488 = !{!"0x55c7bf5abb20.w64.b0", !1489, i64 0}
!1489 = !{!"0x55c7bf5abb20.w128.b0", !1490, i64 0}
!1490 = !{!"0x55c7bf5abb20.w256.b0", !1491, i64 0}
!1491 = !{!"0x55c7bf5abb20.w512.b0", !1492, i64 0}
!1492 = !{!"0x55c7bf5abb20.w1024.b0", !1493, i64 0}
!1493 = !{!"i8", !1494, i64 0}
!1494 = !{!"0x55c7bf5abb20", !8, i64 0}
!1495 = !{!1496, !1496, i64 0}
!1496 = !{!"0x55c7bf5abb20.w1.b1", !1483, i64 0}
!1497 = !{!1498, !1498, i64 0}
!1498 = !{!"0x55c7bf5abb20.w1.b2", !1499, i64 0}
!1499 = !{!"0x55c7bf5abb20.w2.b2", !1484, i64 0}
!1500 = !{!1501, !1501, i64 0}
!1501 = !{!"0x55c7bf5abb20.w1.b3", !1499, i64 0}
!1502 = !{!1503, !1503, i64 0}
!1503 = !{!"0x55c7bf5b6df0.w1.b0", !1504, i64 0}
!1504 = !{!"0x55c7bf5b6df0.w2.b0", !1505, i64 0}
!1505 = !{!"0x55c7bf5b6df0.w4.b0", !1506, i64 0}
!1506 = !{!"0x55c7bf5b6df0.w8.b0", !1507, i64 0}
!1507 = !{!"0x55c7bf5b6df0.w16.b0", !1508, i64 0}
!1508 = !{!"0x55c7bf5b6df0.w32.b0", !1509, i64 0}
!1509 = !{!"0x55c7bf5b6df0.w64.b0", !1510, i64 0}
!1510 = !{!"0x55c7bf5b6df0.w128.b0", !1511, i64 0}
!1511 = !{!"0x55c7bf5b6df0.w256.b0", !1512, i64 0}
!1512 = !{!"0x55c7bf5b6df0.w512.b0", !1513, i64 0}
!1513 = !{!"0x55c7bf5b6df0.w1024.b0", !1514, i64 0}
!1514 = !{!"i64", !1515, i64 0}
!1515 = !{!"0x55c7bf5b6df0", !8, i64 0}
!1516 = !{!1517, !1517, i64 0}
!1517 = !{!"0x55c7bf5b6df0.w1.b1", !1504, i64 0}
!1518 = !{!1519, !1519, i64 0}
!1519 = !{!"0x55c7bf5b6df0.w1.b2", !1520, i64 0}
!1520 = !{!"0x55c7bf5b6df0.w2.b2", !1505, i64 0}
!1521 = !{!1522, !1522, i64 0}
!1522 = !{!"0x55c7bf5b6df0.w1.b3", !1520, i64 0}
!1523 = !{!1524, !1524, i64 0}
!1524 = !{!"0x55c7bf5b73d0.w4.b0", !1525, i64 0}
!1525 = !{!"0x55c7bf5b73d0.w8.b0", !1526, i64 0}
!1526 = !{!"0x55c7bf5b73d0.w16.b0", !1527, i64 0}
!1527 = !{!"0x55c7bf5b73d0.w32.b0", !1528, i64 0}
!1528 = !{!"0x55c7bf5b73d0.w64.b0", !1529, i64 0}
!1529 = !{!"0x55c7bf5b73d0.w128.b0", !1530, i64 0}
!1530 = !{!"0x55c7bf5b73d0.w256.b0", !1531, i64 0}
!1531 = !{!"0x55c7bf5b73d0.w512.b0", !1532, i64 0}
!1532 = !{!"0x55c7bf5b73d0.w1024.b0", !1533, i64 0}
!1533 = !{!"i64", !1534, i64 0}
!1534 = !{!"0x55c7bf5b73d0", !8, i64 0}
!1535 = !{!1536, !1536, i64 0}
!1536 = !{!"0x55c7bf5b95b0.w1.b0", !1537, i64 0}
!1537 = !{!"0x55c7bf5b95b0.w2.b0", !1538, i64 0}
!1538 = !{!"0x55c7bf5b95b0.w4.b0", !1539, i64 0}
!1539 = !{!"0x55c7bf5b95b0.w8.b0", !1540, i64 0}
!1540 = !{!"0x55c7bf5b95b0.w16.b0", !1541, i64 0}
!1541 = !{!"0x55c7bf5b95b0.w32.b0", !1542, i64 0}
!1542 = !{!"0x55c7bf5b95b0.w64.b0", !1543, i64 0}
!1543 = !{!"0x55c7bf5b95b0.w128.b0", !1544, i64 0}
!1544 = !{!"0x55c7bf5b95b0.w256.b0", !1545, i64 0}
!1545 = !{!"0x55c7bf5b95b0.w512.b0", !1546, i64 0}
!1546 = !{!"0x55c7bf5b95b0.w1024.b0", !1547, i64 0}
!1547 = !{!"i64", !1548, i64 0}
!1548 = !{!"0x55c7bf5b95b0", !8, i64 0}
!1549 = !{!1550, !1550, i64 0}
!1550 = !{!"0x55c7bf5b95b0.w1.b1", !1537, i64 0}
!1551 = !{!1552, !1552, i64 0}
!1552 = !{!"0x55c7bf5b95b0.w1.b2", !1553, i64 0}
!1553 = !{!"0x55c7bf5b95b0.w2.b2", !1538, i64 0}
!1554 = !{!1555, !1555, i64 0}
!1555 = !{!"0x55c7bf5b95b0.w1.b3", !1553, i64 0}
!1556 = !{!1557, !1557, i64 0}
!1557 = !{!"0x55c7bf5b9f40.w4.b0", !1558, i64 0}
!1558 = !{!"0x55c7bf5b9f40.w8.b0", !1559, i64 0}
!1559 = !{!"0x55c7bf5b9f40.w16.b0", !1560, i64 0}
!1560 = !{!"0x55c7bf5b9f40.w32.b0", !1561, i64 0}
!1561 = !{!"0x55c7bf5b9f40.w64.b0", !1562, i64 0}
!1562 = !{!"0x55c7bf5b9f40.w128.b0", !1563, i64 0}
!1563 = !{!"0x55c7bf5b9f40.w256.b0", !1564, i64 0}
!1564 = !{!"0x55c7bf5b9f40.w512.b0", !1565, i64 0}
!1565 = !{!"0x55c7bf5b9f40.w1024.b0", !1566, i64 0}
!1566 = !{!"i64", !1567, i64 0}
!1567 = !{!"0x55c7bf5b9f40", !8, i64 0}
!1568 = !{!1569, !1569, i64 0}
!1569 = !{!"0x55c7bf5bc120.w1.b0", !1570, i64 0}
!1570 = !{!"0x55c7bf5bc120.w2.b0", !1571, i64 0}
!1571 = !{!"0x55c7bf5bc120.w4.b0", !1572, i64 0}
!1572 = !{!"0x55c7bf5bc120.w8.b0", !1573, i64 0}
!1573 = !{!"0x55c7bf5bc120.w16.b0", !1574, i64 0}
!1574 = !{!"0x55c7bf5bc120.w32.b0", !1575, i64 0}
!1575 = !{!"0x55c7bf5bc120.w64.b0", !1576, i64 0}
!1576 = !{!"0x55c7bf5bc120.w128.b0", !1577, i64 0}
!1577 = !{!"0x55c7bf5bc120.w256.b0", !1578, i64 0}
!1578 = !{!"0x55c7bf5bc120.w512.b0", !1579, i64 0}
!1579 = !{!"0x55c7bf5bc120.w1024.b0", !1580, i64 0}
!1580 = !{!"i64", !1581, i64 0}
!1581 = !{!"0x55c7bf5bc120", !8, i64 0}
!1582 = !{!1583, !1583, i64 0}
!1583 = !{!"0x55c7bf5bc400.w1.b0", !1584, i64 0}
!1584 = !{!"0x55c7bf5bc400.w2.b0", !1585, i64 0}
!1585 = !{!"0x55c7bf5bc400.w4.b0", !1586, i64 0}
!1586 = !{!"0x55c7bf5bc400.w8.b0", !1587, i64 0}
!1587 = !{!"0x55c7bf5bc400.w16.b0", !1588, i64 0}
!1588 = !{!"0x55c7bf5bc400.w32.b0", !1589, i64 0}
!1589 = !{!"0x55c7bf5bc400.w64.b0", !1590, i64 0}
!1590 = !{!"0x55c7bf5bc400.w128.b0", !1591, i64 0}
!1591 = !{!"0x55c7bf5bc400.w256.b0", !1592, i64 0}
!1592 = !{!"0x55c7bf5bc400.w512.b0", !1593, i64 0}
!1593 = !{!"0x55c7bf5bc400.w1024.b0", !1594, i64 0}
!1594 = !{!"i64", !1595, i64 0}
!1595 = !{!"0x55c7bf5bc400", !8, i64 0}
!1596 = !{!1597, !1597, i64 0}
!1597 = !{!"0x55c7bf5be0a0.w1.b0", !1598, i64 0}
!1598 = !{!"0x55c7bf5be0a0.w2.b0", !1599, i64 0}
!1599 = !{!"0x55c7bf5be0a0.w4.b0", !1600, i64 0}
!1600 = !{!"0x55c7bf5be0a0.w8.b0", !1601, i64 0}
!1601 = !{!"0x55c7bf5be0a0.w16.b0", !1602, i64 0}
!1602 = !{!"0x55c7bf5be0a0.w32.b0", !1603, i64 0}
!1603 = !{!"0x55c7bf5be0a0.w64.b0", !1604, i64 0}
!1604 = !{!"0x55c7bf5be0a0.w128.b0", !1605, i64 0}
!1605 = !{!"0x55c7bf5be0a0.w256.b0", !1606, i64 0}
!1606 = !{!"0x55c7bf5be0a0.w512.b0", !1607, i64 0}
!1607 = !{!"0x55c7bf5be0a0.w1024.b0", !1608, i64 0}
!1608 = !{!"i64", !1609, i64 0}
!1609 = !{!"0x55c7bf5be0a0", !8, i64 0}
!1610 = !{!1611, !1611, i64 0}
!1611 = !{!"0x55c7bf5be0a0.w1.b1", !1598, i64 0}
!1612 = !{!1613, !1613, i64 0}
!1613 = !{!"0x55c7bf5be0a0.w1.b2", !1614, i64 0}
!1614 = !{!"0x55c7bf5be0a0.w2.b2", !1599, i64 0}
!1615 = !{!1616, !1616, i64 0}
!1616 = !{!"0x55c7bf5be0a0.w1.b3", !1614, i64 0}
!1617 = !{!1618, !1618, i64 0}
!1618 = !{!"0x55c7bf5be990.w4.b0", !1619, i64 0}
!1619 = !{!"0x55c7bf5be990.w8.b0", !1620, i64 0}
!1620 = !{!"0x55c7bf5be990.w16.b0", !1621, i64 0}
!1621 = !{!"0x55c7bf5be990.w32.b0", !1622, i64 0}
!1622 = !{!"0x55c7bf5be990.w64.b0", !1623, i64 0}
!1623 = !{!"0x55c7bf5be990.w128.b0", !1624, i64 0}
!1624 = !{!"0x55c7bf5be990.w256.b0", !1625, i64 0}
!1625 = !{!"0x55c7bf5be990.w512.b0", !1626, i64 0}
!1626 = !{!"0x55c7bf5be990.w1024.b0", !1627, i64 0}
!1627 = !{!"i64", !1628, i64 0}
!1628 = !{!"0x55c7bf5be990", !8, i64 0}
!1629 = !{!1630, !1630, i64 0}
!1630 = !{!"i8", !1631, i64 0}
!1631 = !{!"0x55c7bf4f1be0", !8, i64 0}
!1632 = !{!1633, !1633, i64 0}
!1633 = !{!"i8", !1634, i64 0}
!1634 = !{!"0x55c7bf4eccc0", !8, i64 0}
!1635 = !{!1636, !1636, i64 0}
!1636 = !{!"i8", !1637, i64 0}
!1637 = !{!"0x55c7bf4f1b60", !8, i64 0}
!1638 = !{!1639, !1639, i64 0}
!1639 = !{!"i8", !1640, i64 0}
!1640 = !{!"0x55c7bf4f1ba0", !8, i64 0}
!1641 = !{!1642, !1642, i64 0}
!1642 = !{!"i8", !1643, i64 0}
!1643 = !{!"0x55c7bf4f1b20", !8, i64 0}
!1644 = !{!1645, !1645, i64 0}
!1645 = !{!"0x55c7bf5b7170.w1.b0", !1646, i64 0}
!1646 = !{!"0x55c7bf5b7170.w2.b0", !1647, i64 0}
!1647 = !{!"0x55c7bf5b7170.w4.b0", !1648, i64 0}
!1648 = !{!"0x55c7bf5b7170.w8.b0", !1649, i64 0}
!1649 = !{!"0x55c7bf5b7170.w16.b0", !1650, i64 0}
!1650 = !{!"0x55c7bf5b7170.w32.b0", !1651, i64 0}
!1651 = !{!"0x55c7bf5b7170.w64.b0", !1652, i64 0}
!1652 = !{!"0x55c7bf5b7170.w128.b0", !1653, i64 0}
!1653 = !{!"0x55c7bf5b7170.w256.b0", !1654, i64 0}
!1654 = !{!"0x55c7bf5b7170.w512.b0", !1655, i64 0}
!1655 = !{!"0x55c7bf5b7170.w1024.b0", !1656, i64 0}
!1656 = !{!"i8", !1657, i64 0}
!1657 = !{!"0x55c7bf5b7170", !8, i64 0}
!1658 = !{!1659, !1659, i64 0}
!1659 = !{!"0x55c7bf5b7170.w1.b1", !1646, i64 0}
!1660 = !{!1661, !1661, i64 0}
!1661 = !{!"0x55c7bf5bdd20.w1.b0", !1662, i64 0}
!1662 = !{!"0x55c7bf5bdd20.w2.b0", !1663, i64 0}
!1663 = !{!"0x55c7bf5bdd20.w4.b0", !1664, i64 0}
!1664 = !{!"0x55c7bf5bdd20.w8.b0", !1665, i64 0}
!1665 = !{!"0x55c7bf5bdd20.w16.b0", !1666, i64 0}
!1666 = !{!"0x55c7bf5bdd20.w32.b0", !1667, i64 0}
!1667 = !{!"0x55c7bf5bdd20.w64.b0", !1668, i64 0}
!1668 = !{!"0x55c7bf5bdd20.w128.b0", !1669, i64 0}
!1669 = !{!"0x55c7bf5bdd20.w256.b0", !1670, i64 0}
!1670 = !{!"0x55c7bf5bdd20.w512.b0", !1671, i64 0}
!1671 = !{!"0x55c7bf5bdd20.w1024.b0", !1672, i64 0}
!1672 = !{!"i64", !1673, i64 0}
!1673 = !{!"0x55c7bf5bdd20", !8, i64 0}
!1674 = !{!1675, !1675, i64 0}
!1675 = !{!"0x55c7bf5bdd20.w1.b1", !1662, i64 0}
!1676 = !{!1677, !1677, i64 0}
!1677 = !{!"0x55c7bf5bdd20.w1.b2", !1678, i64 0}
!1678 = !{!"0x55c7bf5bdd20.w2.b2", !1663, i64 0}
!1679 = !{!1680, !1680, i64 0}
!1680 = !{!"0x55c7bf5bdd20.w1.b3", !1678, i64 0}
!1681 = !{!1682, !1682, i64 0}
!1682 = !{!"0x55c7bf5b9c40.w4.b0", !1683, i64 0}
!1683 = !{!"0x55c7bf5b9c40.w8.b0", !1684, i64 0}
!1684 = !{!"0x55c7bf5b9c40.w16.b0", !1685, i64 0}
!1685 = !{!"0x55c7bf5b9c40.w32.b0", !1686, i64 0}
!1686 = !{!"0x55c7bf5b9c40.w64.b0", !1687, i64 0}
!1687 = !{!"0x55c7bf5b9c40.w128.b0", !1688, i64 0}
!1688 = !{!"0x55c7bf5b9c40.w256.b0", !1689, i64 0}
!1689 = !{!"0x55c7bf5b9c40.w512.b0", !1690, i64 0}
!1690 = !{!"0x55c7bf5b9c40.w1024.b0", !1691, i64 0}
!1691 = !{!"i64", !1692, i64 0}
!1692 = !{!"0x55c7bf5b9c40", !8, i64 0}
!1693 = !{!1694, !1694, i64 0}
!1694 = !{!"0x55c7bf5c3e10.w1.b0", !1695, i64 0}
!1695 = !{!"0x55c7bf5c3e10.w2.b0", !1696, i64 0}
!1696 = !{!"0x55c7bf5c3e10.w4.b0", !1697, i64 0}
!1697 = !{!"0x55c7bf5c3e10.w8.b0", !1698, i64 0}
!1698 = !{!"0x55c7bf5c3e10.w16.b0", !1699, i64 0}
!1699 = !{!"0x55c7bf5c3e10.w32.b0", !1700, i64 0}
!1700 = !{!"0x55c7bf5c3e10.w64.b0", !1701, i64 0}
!1701 = !{!"0x55c7bf5c3e10.w128.b0", !1702, i64 0}
!1702 = !{!"0x55c7bf5c3e10.w256.b0", !1703, i64 0}
!1703 = !{!"0x55c7bf5c3e10.w512.b0", !1704, i64 0}
!1704 = !{!"0x55c7bf5c3e10.w1024.b0", !1705, i64 0}
!1705 = !{!"i64", !1706, i64 0}
!1706 = !{!"0x55c7bf5c3e10", !8, i64 0}
!1707 = !{!1708, !1708, i64 0}
!1708 = !{!"0x55c7bf5c3e10.w1.b1", !1695, i64 0}
!1709 = !{!1710, !1710, i64 0}
!1710 = !{!"0x55c7bf5c4280.w1.b0", !1711, i64 0}
!1711 = !{!"0x55c7bf5c4280.w2.b0", !1712, i64 0}
!1712 = !{!"0x55c7bf5c4280.w4.b0", !1713, i64 0}
!1713 = !{!"0x55c7bf5c4280.w8.b0", !1714, i64 0}
!1714 = !{!"0x55c7bf5c4280.w16.b0", !1715, i64 0}
!1715 = !{!"0x55c7bf5c4280.w32.b0", !1716, i64 0}
!1716 = !{!"0x55c7bf5c4280.w64.b0", !1717, i64 0}
!1717 = !{!"0x55c7bf5c4280.w128.b0", !1718, i64 0}
!1718 = !{!"0x55c7bf5c4280.w256.b0", !1719, i64 0}
!1719 = !{!"0x55c7bf5c4280.w512.b0", !1720, i64 0}
!1720 = !{!"0x55c7bf5c4280.w1024.b0", !1721, i64 0}
!1721 = !{!"i64", !1722, i64 0}
!1722 = !{!"0x55c7bf5c4280", !8, i64 0}
!1723 = !{!1724, !1724, i64 0}
!1724 = !{!"0x55c7bf5c4280.w1.b1", !1711, i64 0}
!1725 = !{!1726, !1726, i64 0}
!1726 = !{!"i8", !1727, i64 0}
!1727 = !{!"0x55c7bf3629b0", !8, i64 0}
!1728 = !{!1729, !1729, i64 0}
!1729 = !{!"i8", !1730, i64 0}
!1730 = !{!"0x55c7bf368780", !8, i64 0}
!1731 = !{!1732, !1732, i64 0}
!1732 = !{!"0x55c7bf5b99b0.w1.b0", !1733, i64 0}
!1733 = !{!"0x55c7bf5b99b0.w2.b0", !1734, i64 0}
!1734 = !{!"0x55c7bf5b99b0.w4.b0", !1735, i64 0}
!1735 = !{!"0x55c7bf5b99b0.w8.b0", !1736, i64 0}
!1736 = !{!"0x55c7bf5b99b0.w16.b0", !1737, i64 0}
!1737 = !{!"0x55c7bf5b99b0.w32.b0", !1738, i64 0}
!1738 = !{!"0x55c7bf5b99b0.w64.b0", !1739, i64 0}
!1739 = !{!"0x55c7bf5b99b0.w128.b0", !1740, i64 0}
!1740 = !{!"0x55c7bf5b99b0.w256.b0", !1741, i64 0}
!1741 = !{!"0x55c7bf5b99b0.w512.b0", !1742, i64 0}
!1742 = !{!"0x55c7bf5b99b0.w1024.b0", !1743, i64 0}
!1743 = !{!"i8", !1744, i64 0}
!1744 = !{!"0x55c7bf5b99b0", !8, i64 0}
!1745 = !{!1746, !1746, i64 0}
!1746 = !{!"0x55c7bf5b99b0.w1.b1", !1733, i64 0}
!1747 = !{!1748, !1748, i64 0}
!1748 = !{!"0x55c7bf5b99b0.w1.b2", !1749, i64 0}
!1749 = !{!"0x55c7bf5b99b0.w2.b2", !1734, i64 0}
!1750 = !{!1751, !1751, i64 0}
!1751 = !{!"0x55c7bf5b99b0.w1.b3", !1749, i64 0}
!1752 = !{!1753, !1753, i64 0}
!1753 = !{!"0x55c7bf5c7fe0.w1.b0", !1754, i64 0}
!1754 = !{!"0x55c7bf5c7fe0.w2.b0", !1755, i64 0}
!1755 = !{!"0x55c7bf5c7fe0.w4.b0", !1756, i64 0}
!1756 = !{!"0x55c7bf5c7fe0.w8.b0", !1757, i64 0}
!1757 = !{!"0x55c7bf5c7fe0.w16.b0", !1758, i64 0}
!1758 = !{!"0x55c7bf5c7fe0.w32.b0", !1759, i64 0}
!1759 = !{!"0x55c7bf5c7fe0.w64.b0", !1760, i64 0}
!1760 = !{!"0x55c7bf5c7fe0.w128.b0", !1761, i64 0}
!1761 = !{!"0x55c7bf5c7fe0.w256.b0", !1762, i64 0}
!1762 = !{!"0x55c7bf5c7fe0.w512.b0", !1763, i64 0}
!1763 = !{!"0x55c7bf5c7fe0.w1024.b0", !1764, i64 0}
!1764 = !{!"i64", !1765, i64 0}
!1765 = !{!"0x55c7bf5c7fe0", !8, i64 0}
!1766 = !{!1767, !1767, i64 0}
!1767 = !{!"0x55c7bf5c7fe0.w1.b1", !1754, i64 0}
!1768 = !{!1769, !1769, i64 0}
!1769 = !{!"0x55c7bf5c7fe0.w1.b2", !1770, i64 0}
!1770 = !{!"0x55c7bf5c7fe0.w2.b2", !1755, i64 0}
!1771 = !{!1772, !1772, i64 0}
!1772 = !{!"0x55c7bf5c7fe0.w1.b3", !1770, i64 0}
!1773 = !{!1774, !1774, i64 0}
!1774 = !{!"0x55c7bf5c8920.w4.b0", !1775, i64 0}
!1775 = !{!"0x55c7bf5c8920.w8.b0", !1776, i64 0}
!1776 = !{!"0x55c7bf5c8920.w16.b0", !1777, i64 0}
!1777 = !{!"0x55c7bf5c8920.w32.b0", !1778, i64 0}
!1778 = !{!"0x55c7bf5c8920.w64.b0", !1779, i64 0}
!1779 = !{!"0x55c7bf5c8920.w128.b0", !1780, i64 0}
!1780 = !{!"0x55c7bf5c8920.w256.b0", !1781, i64 0}
!1781 = !{!"0x55c7bf5c8920.w512.b0", !1782, i64 0}
!1782 = !{!"0x55c7bf5c8920.w1024.b0", !1783, i64 0}
!1783 = !{!"i64", !1784, i64 0}
!1784 = !{!"0x55c7bf5c8920", !8, i64 0}
!1785 = !{!1786, !1786, i64 0}
!1786 = !{!"0x55c7bf9cacf0.w1.b0", !1787, i64 0}
!1787 = !{!"0x55c7bf9cacf0.w2.b0", !1788, i64 0}
!1788 = !{!"0x55c7bf9cacf0.w4.b0", !1789, i64 0}
!1789 = !{!"0x55c7bf9cacf0.w8.b0", !1790, i64 0}
!1790 = !{!"0x55c7bf9cacf0.w16.b0", !1791, i64 0}
!1791 = !{!"0x55c7bf9cacf0.w32.b0", !1792, i64 0}
!1792 = !{!"0x55c7bf9cacf0.w64.b0", !1793, i64 0}
!1793 = !{!"0x55c7bf9cacf0.w128.b0", !1794, i64 0}
!1794 = !{!"0x55c7bf9cacf0.w256.b0", !1795, i64 0}
!1795 = !{!"0x55c7bf9cacf0.w512.b0", !1796, i64 0}
!1796 = !{!"0x55c7bf9cacf0.w1024.b0", !1797, i64 0}
!1797 = !{!"i64", !1798, i64 0}
!1798 = !{!"0x55c7bf9cacf0", !8, i64 0}
!1799 = !{!1800, !1800, i64 0}
!1800 = !{!"0x55c7bf9cacf0.w1.b1", !1787, i64 0}
!1801 = !{!1802, !1802, i64 0}
!1802 = !{!"0x55c7bf9cacf0.w1.b2", !1803, i64 0}
!1803 = !{!"0x55c7bf9cacf0.w2.b2", !1788, i64 0}
!1804 = !{!1805, !1805, i64 0}
!1805 = !{!"0x55c7bf9cacf0.w1.b3", !1803, i64 0}
!1806 = !{!1807, !1807, i64 0}
!1807 = !{!"0x55c7bf9cb680.w4.b0", !1808, i64 0}
!1808 = !{!"0x55c7bf9cb680.w8.b0", !1809, i64 0}
!1809 = !{!"0x55c7bf9cb680.w16.b0", !1810, i64 0}
!1810 = !{!"0x55c7bf9cb680.w32.b0", !1811, i64 0}
!1811 = !{!"0x55c7bf9cb680.w64.b0", !1812, i64 0}
!1812 = !{!"0x55c7bf9cb680.w128.b0", !1813, i64 0}
!1813 = !{!"0x55c7bf9cb680.w256.b0", !1814, i64 0}
!1814 = !{!"0x55c7bf9cb680.w512.b0", !1815, i64 0}
!1815 = !{!"0x55c7bf9cb680.w1024.b0", !1816, i64 0}
!1816 = !{!"i64", !1817, i64 0}
!1817 = !{!"0x55c7bf9cb680", !8, i64 0}
!1818 = !{!1819, !1819, i64 0}
!1819 = !{!"0x55c7bf9cd860.w1.b0", !1820, i64 0}
!1820 = !{!"0x55c7bf9cd860.w2.b0", !1821, i64 0}
!1821 = !{!"0x55c7bf9cd860.w4.b0", !1822, i64 0}
!1822 = !{!"0x55c7bf9cd860.w8.b0", !1823, i64 0}
!1823 = !{!"0x55c7bf9cd860.w16.b0", !1824, i64 0}
!1824 = !{!"0x55c7bf9cd860.w32.b0", !1825, i64 0}
!1825 = !{!"0x55c7bf9cd860.w64.b0", !1826, i64 0}
!1826 = !{!"0x55c7bf9cd860.w128.b0", !1827, i64 0}
!1827 = !{!"0x55c7bf9cd860.w256.b0", !1828, i64 0}
!1828 = !{!"0x55c7bf9cd860.w512.b0", !1829, i64 0}
!1829 = !{!"0x55c7bf9cd860.w1024.b0", !1830, i64 0}
!1830 = !{!"i64", !1831, i64 0}
!1831 = !{!"0x55c7bf9cd860", !8, i64 0}
!1832 = !{!1833, !1833, i64 0}
!1833 = !{!"0x55c7bf9cdb40.w1.b0", !1834, i64 0}
!1834 = !{!"0x55c7bf9cdb40.w2.b0", !1835, i64 0}
!1835 = !{!"0x55c7bf9cdb40.w4.b0", !1836, i64 0}
!1836 = !{!"0x55c7bf9cdb40.w8.b0", !1837, i64 0}
!1837 = !{!"0x55c7bf9cdb40.w16.b0", !1838, i64 0}
!1838 = !{!"0x55c7bf9cdb40.w32.b0", !1839, i64 0}
!1839 = !{!"0x55c7bf9cdb40.w64.b0", !1840, i64 0}
!1840 = !{!"0x55c7bf9cdb40.w128.b0", !1841, i64 0}
!1841 = !{!"0x55c7bf9cdb40.w256.b0", !1842, i64 0}
!1842 = !{!"0x55c7bf9cdb40.w512.b0", !1843, i64 0}
!1843 = !{!"0x55c7bf9cdb40.w1024.b0", !1844, i64 0}
!1844 = !{!"i64", !1845, i64 0}
!1845 = !{!"0x55c7bf9cdb40", !8, i64 0}
!1846 = !{!1847, !1847, i64 0}
!1847 = !{!"0x55c7bf9cf7e0.w1.b0", !1848, i64 0}
!1848 = !{!"0x55c7bf9cf7e0.w2.b0", !1849, i64 0}
!1849 = !{!"0x55c7bf9cf7e0.w4.b0", !1850, i64 0}
!1850 = !{!"0x55c7bf9cf7e0.w8.b0", !1851, i64 0}
!1851 = !{!"0x55c7bf9cf7e0.w16.b0", !1852, i64 0}
!1852 = !{!"0x55c7bf9cf7e0.w32.b0", !1853, i64 0}
!1853 = !{!"0x55c7bf9cf7e0.w64.b0", !1854, i64 0}
!1854 = !{!"0x55c7bf9cf7e0.w128.b0", !1855, i64 0}
!1855 = !{!"0x55c7bf9cf7e0.w256.b0", !1856, i64 0}
!1856 = !{!"0x55c7bf9cf7e0.w512.b0", !1857, i64 0}
!1857 = !{!"0x55c7bf9cf7e0.w1024.b0", !1858, i64 0}
!1858 = !{!"i64", !1859, i64 0}
!1859 = !{!"0x55c7bf9cf7e0", !8, i64 0}
!1860 = !{!1861, !1861, i64 0}
!1861 = !{!"0x55c7bf9cf7e0.w1.b1", !1848, i64 0}
!1862 = !{!1863, !1863, i64 0}
!1863 = !{!"0x55c7bf9cf7e0.w1.b2", !1864, i64 0}
!1864 = !{!"0x55c7bf9cf7e0.w2.b2", !1849, i64 0}
!1865 = !{!1866, !1866, i64 0}
!1866 = !{!"0x55c7bf9cf7e0.w1.b3", !1864, i64 0}
!1867 = !{!1868, !1868, i64 0}
!1868 = !{!"0x55c7bf9d00d0.w4.b0", !1869, i64 0}
!1869 = !{!"0x55c7bf9d00d0.w8.b0", !1870, i64 0}
!1870 = !{!"0x55c7bf9d00d0.w16.b0", !1871, i64 0}
!1871 = !{!"0x55c7bf9d00d0.w32.b0", !1872, i64 0}
!1872 = !{!"0x55c7bf9d00d0.w64.b0", !1873, i64 0}
!1873 = !{!"0x55c7bf9d00d0.w128.b0", !1874, i64 0}
!1874 = !{!"0x55c7bf9d00d0.w256.b0", !1875, i64 0}
!1875 = !{!"0x55c7bf9d00d0.w512.b0", !1876, i64 0}
!1876 = !{!"0x55c7bf9d00d0.w1024.b0", !1877, i64 0}
!1877 = !{!"i64", !1878, i64 0}
!1878 = !{!"0x55c7bf9d00d0", !8, i64 0}
!1879 = !{!1880, !1880, i64 0}
!1880 = !{!"i8", !1881, i64 0}
!1881 = !{!"0x55c7bf51cc60", !8, i64 0}
!1882 = !{!1883, !1883, i64 0}
!1883 = !{!"i8", !1884, i64 0}
!1884 = !{!"0x55c7bf51b540", !8, i64 0}
!1885 = !{!1886, !1886, i64 0}
!1886 = !{!"i8", !1887, i64 0}
!1887 = !{!"0x55c7bf51cca0", !8, i64 0}
!1888 = !{!1889, !1889, i64 0}
!1889 = !{!"i8", !1890, i64 0}
!1890 = !{!"0x55c7bf51dbc0", !8, i64 0}
!1891 = distinct !{!1891, !337}
!1892 = !{!1893, !1893, i64 0}
!1893 = !{!"i8", !1894, i64 0}
!1894 = !{!"0x55c7bf51ce70", !8, i64 0}
!1895 = !{!1896, !1896, i64 0}
!1896 = !{!"0x55c7bf5c8600.w1.b0", !1897, i64 0}
!1897 = !{!"0x55c7bf5c8600.w2.b0", !1898, i64 0}
!1898 = !{!"0x55c7bf5c8600.w4.b0", !1899, i64 0}
!1899 = !{!"0x55c7bf5c8600.w8.b0", !1900, i64 0}
!1900 = !{!"0x55c7bf5c8600.w16.b0", !1901, i64 0}
!1901 = !{!"0x55c7bf5c8600.w32.b0", !1902, i64 0}
!1902 = !{!"0x55c7bf5c8600.w64.b0", !1903, i64 0}
!1903 = !{!"0x55c7bf5c8600.w128.b0", !1904, i64 0}
!1904 = !{!"0x55c7bf5c8600.w256.b0", !1905, i64 0}
!1905 = !{!"0x55c7bf5c8600.w512.b0", !1906, i64 0}
!1906 = !{!"0x55c7bf5c8600.w1024.b0", !1907, i64 0}
!1907 = !{!"i8", !1908, i64 0}
!1908 = !{!"0x55c7bf5c8600", !8, i64 0}
!1909 = !{!1910, !1910, i64 0}
!1910 = !{!"0x55c7bf5c8600.w1.b1", !1897, i64 0}
!1911 = !{!1912, !1912, i64 0}
!1912 = !{!"0x55c7bf5c8600.w1.b2", !1913, i64 0}
!1913 = !{!"0x55c7bf5c8600.w2.b2", !1898, i64 0}
!1914 = !{!1915, !1915, i64 0}
!1915 = !{!"0x55c7bf5c8600.w1.b3", !1913, i64 0}
!1916 = !{!1917, !1917, i64 0}
!1917 = !{!"0x55c7bf9d3b80.w1.b0", !1918, i64 0}
!1918 = !{!"0x55c7bf9d3b80.w2.b0", !1919, i64 0}
!1919 = !{!"0x55c7bf9d3b80.w4.b0", !1920, i64 0}
!1920 = !{!"0x55c7bf9d3b80.w8.b0", !1921, i64 0}
!1921 = !{!"0x55c7bf9d3b80.w16.b0", !1922, i64 0}
!1922 = !{!"0x55c7bf9d3b80.w32.b0", !1923, i64 0}
!1923 = !{!"0x55c7bf9d3b80.w64.b0", !1924, i64 0}
!1924 = !{!"0x55c7bf9d3b80.w128.b0", !1925, i64 0}
!1925 = !{!"0x55c7bf9d3b80.w256.b0", !1926, i64 0}
!1926 = !{!"0x55c7bf9d3b80.w512.b0", !1927, i64 0}
!1927 = !{!"0x55c7bf9d3b80.w1024.b0", !1928, i64 0}
!1928 = !{!"i64", !1929, i64 0}
!1929 = !{!"0x55c7bf9d3b80", !8, i64 0}
!1930 = !{!1931, !1931, i64 0}
!1931 = !{!"0x55c7bf9d3b80.w1.b1", !1918, i64 0}
!1932 = !{!1933, !1933, i64 0}
!1933 = !{!"0x55c7bf9d3b80.w1.b2", !1934, i64 0}
!1934 = !{!"0x55c7bf9d3b80.w2.b2", !1919, i64 0}
!1935 = !{!1936, !1936, i64 0}
!1936 = !{!"0x55c7bf9d3b80.w1.b3", !1934, i64 0}
!1937 = !{!1938, !1938, i64 0}
!1938 = !{!"0x55c7bf9d4160.w4.b0", !1939, i64 0}
!1939 = !{!"0x55c7bf9d4160.w8.b0", !1940, i64 0}
!1940 = !{!"0x55c7bf9d4160.w16.b0", !1941, i64 0}
!1941 = !{!"0x55c7bf9d4160.w32.b0", !1942, i64 0}
!1942 = !{!"0x55c7bf9d4160.w64.b0", !1943, i64 0}
!1943 = !{!"0x55c7bf9d4160.w128.b0", !1944, i64 0}
!1944 = !{!"0x55c7bf9d4160.w256.b0", !1945, i64 0}
!1945 = !{!"0x55c7bf9d4160.w512.b0", !1946, i64 0}
!1946 = !{!"0x55c7bf9d4160.w1024.b0", !1947, i64 0}
!1947 = !{!"i64", !1948, i64 0}
!1948 = !{!"0x55c7bf9d4160", !8, i64 0}
!1949 = !{!1950, !1950, i64 0}
!1950 = !{!"0x55c7bf9d6340.w1.b0", !1951, i64 0}
!1951 = !{!"0x55c7bf9d6340.w2.b0", !1952, i64 0}
!1952 = !{!"0x55c7bf9d6340.w4.b0", !1953, i64 0}
!1953 = !{!"0x55c7bf9d6340.w8.b0", !1954, i64 0}
!1954 = !{!"0x55c7bf9d6340.w16.b0", !1955, i64 0}
!1955 = !{!"0x55c7bf9d6340.w32.b0", !1956, i64 0}
!1956 = !{!"0x55c7bf9d6340.w64.b0", !1957, i64 0}
!1957 = !{!"0x55c7bf9d6340.w128.b0", !1958, i64 0}
!1958 = !{!"0x55c7bf9d6340.w256.b0", !1959, i64 0}
!1959 = !{!"0x55c7bf9d6340.w512.b0", !1960, i64 0}
!1960 = !{!"0x55c7bf9d6340.w1024.b0", !1961, i64 0}
!1961 = !{!"i64", !1962, i64 0}
!1962 = !{!"0x55c7bf9d6340", !8, i64 0}
!1963 = !{!1964, !1964, i64 0}
!1964 = !{!"0x55c7bf9d6340.w1.b1", !1951, i64 0}
!1965 = !{!1966, !1966, i64 0}
!1966 = !{!"0x55c7bf9d6340.w1.b2", !1967, i64 0}
!1967 = !{!"0x55c7bf9d6340.w2.b2", !1952, i64 0}
!1968 = !{!1969, !1969, i64 0}
!1969 = !{!"0x55c7bf9d6340.w1.b3", !1967, i64 0}
!1970 = !{!1971, !1971, i64 0}
!1971 = !{!"0x55c7bf9d6cd0.w4.b0", !1972, i64 0}
!1972 = !{!"0x55c7bf9d6cd0.w8.b0", !1973, i64 0}
!1973 = !{!"0x55c7bf9d6cd0.w16.b0", !1974, i64 0}
!1974 = !{!"0x55c7bf9d6cd0.w32.b0", !1975, i64 0}
!1975 = !{!"0x55c7bf9d6cd0.w64.b0", !1976, i64 0}
!1976 = !{!"0x55c7bf9d6cd0.w128.b0", !1977, i64 0}
!1977 = !{!"0x55c7bf9d6cd0.w256.b0", !1978, i64 0}
!1978 = !{!"0x55c7bf9d6cd0.w512.b0", !1979, i64 0}
!1979 = !{!"0x55c7bf9d6cd0.w1024.b0", !1980, i64 0}
!1980 = !{!"i64", !1981, i64 0}
!1981 = !{!"0x55c7bf9d6cd0", !8, i64 0}
!1982 = !{!1983, !1983, i64 0}
!1983 = !{!"0x55c7bf9d8eb0.w1.b0", !1984, i64 0}
!1984 = !{!"0x55c7bf9d8eb0.w2.b0", !1985, i64 0}
!1985 = !{!"0x55c7bf9d8eb0.w4.b0", !1986, i64 0}
!1986 = !{!"0x55c7bf9d8eb0.w8.b0", !1987, i64 0}
!1987 = !{!"0x55c7bf9d8eb0.w16.b0", !1988, i64 0}
!1988 = !{!"0x55c7bf9d8eb0.w32.b0", !1989, i64 0}
!1989 = !{!"0x55c7bf9d8eb0.w64.b0", !1990, i64 0}
!1990 = !{!"0x55c7bf9d8eb0.w128.b0", !1991, i64 0}
!1991 = !{!"0x55c7bf9d8eb0.w256.b0", !1992, i64 0}
!1992 = !{!"0x55c7bf9d8eb0.w512.b0", !1993, i64 0}
!1993 = !{!"0x55c7bf9d8eb0.w1024.b0", !1994, i64 0}
!1994 = !{!"i64", !1995, i64 0}
!1995 = !{!"0x55c7bf9d8eb0", !8, i64 0}
!1996 = !{!1997, !1997, i64 0}
!1997 = !{!"0x55c7bf9d9190.w1.b0", !1998, i64 0}
!1998 = !{!"0x55c7bf9d9190.w2.b0", !1999, i64 0}
!1999 = !{!"0x55c7bf9d9190.w4.b0", !2000, i64 0}
!2000 = !{!"0x55c7bf9d9190.w8.b0", !2001, i64 0}
!2001 = !{!"0x55c7bf9d9190.w16.b0", !2002, i64 0}
!2002 = !{!"0x55c7bf9d9190.w32.b0", !2003, i64 0}
!2003 = !{!"0x55c7bf9d9190.w64.b0", !2004, i64 0}
!2004 = !{!"0x55c7bf9d9190.w128.b0", !2005, i64 0}
!2005 = !{!"0x55c7bf9d9190.w256.b0", !2006, i64 0}
!2006 = !{!"0x55c7bf9d9190.w512.b0", !2007, i64 0}
!2007 = !{!"0x55c7bf9d9190.w1024.b0", !2008, i64 0}
!2008 = !{!"i64", !2009, i64 0}
!2009 = !{!"0x55c7bf9d9190", !8, i64 0}
!2010 = !{!2011, !2011, i64 0}
!2011 = !{!"0x55c7bf9dae30.w1.b0", !2012, i64 0}
!2012 = !{!"0x55c7bf9dae30.w2.b0", !2013, i64 0}
!2013 = !{!"0x55c7bf9dae30.w4.b0", !2014, i64 0}
!2014 = !{!"0x55c7bf9dae30.w8.b0", !2015, i64 0}
!2015 = !{!"0x55c7bf9dae30.w16.b0", !2016, i64 0}
!2016 = !{!"0x55c7bf9dae30.w32.b0", !2017, i64 0}
!2017 = !{!"0x55c7bf9dae30.w64.b0", !2018, i64 0}
!2018 = !{!"0x55c7bf9dae30.w128.b0", !2019, i64 0}
!2019 = !{!"0x55c7bf9dae30.w256.b0", !2020, i64 0}
!2020 = !{!"0x55c7bf9dae30.w512.b0", !2021, i64 0}
!2021 = !{!"0x55c7bf9dae30.w1024.b0", !2022, i64 0}
!2022 = !{!"i64", !2023, i64 0}
!2023 = !{!"0x55c7bf9dae30", !8, i64 0}
!2024 = !{!2025, !2025, i64 0}
!2025 = !{!"0x55c7bf9dae30.w1.b1", !2012, i64 0}
!2026 = !{!2027, !2027, i64 0}
!2027 = !{!"0x55c7bf9dae30.w1.b2", !2028, i64 0}
!2028 = !{!"0x55c7bf9dae30.w2.b2", !2013, i64 0}
!2029 = !{!2030, !2030, i64 0}
!2030 = !{!"0x55c7bf9dae30.w1.b3", !2028, i64 0}
!2031 = !{!2032, !2032, i64 0}
!2032 = !{!"0x55c7bf9db720.w4.b0", !2033, i64 0}
!2033 = !{!"0x55c7bf9db720.w8.b0", !2034, i64 0}
!2034 = !{!"0x55c7bf9db720.w16.b0", !2035, i64 0}
!2035 = !{!"0x55c7bf9db720.w32.b0", !2036, i64 0}
!2036 = !{!"0x55c7bf9db720.w64.b0", !2037, i64 0}
!2037 = !{!"0x55c7bf9db720.w128.b0", !2038, i64 0}
!2038 = !{!"0x55c7bf9db720.w256.b0", !2039, i64 0}
!2039 = !{!"0x55c7bf9db720.w512.b0", !2040, i64 0}
!2040 = !{!"0x55c7bf9db720.w1024.b0", !2041, i64 0}
!2041 = !{!"i64", !2042, i64 0}
!2042 = !{!"0x55c7bf9db720", !8, i64 0}
!2043 = !{!2044, !2044, i64 0}
!2044 = !{!"i8", !2045, i64 0}
!2045 = !{!"0x55c7bf4273b0", !8, i64 0}
!2046 = !{!2047, !2047, i64 0}
!2047 = !{!"i8", !2048, i64 0}
!2048 = !{!"0x55c7bf4272b0", !8, i64 0}
!2049 = distinct !{!2049, !337}
!2050 = distinct !{!2050, !337}
!2051 = distinct !{!2051, !337}
!2052 = distinct !{!2052, !337}
!2053 = distinct !{!2053, !337}
!2054 = distinct !{!2054, !337}
!2055 = distinct !{!2055, !337}
!2056 = distinct !{!2056, !337}
!2057 = distinct !{!2057, !337}
!2058 = distinct !{!2058, !337}
!2059 = distinct !{!2059, !337}
!2060 = distinct !{!2060, !337}
!2061 = distinct !{!2061, !337}
!2062 = distinct !{!2062, !337}
!2063 = !{!2064, !2064, i64 0}
!2064 = !{!"i8", !2065, i64 0}
!2065 = !{!"0x55c7bf427330", !8, i64 0}
!2066 = !{!2067, !2067, i64 0}
!2067 = !{!"i8", !2068, i64 0}
!2068 = !{!"0x55c7bf427370", !8, i64 0}
!2069 = !{!2070, !2070, i64 0}
!2070 = !{!"i8", !2071, i64 0}
!2071 = !{!"0x55c7bf4272f0", !8, i64 0}
!2072 = !{!2073, !2073, i64 0}
!2073 = !{!"0x55c7bf9d3f00.w1.b0", !2074, i64 0}
!2074 = !{!"0x55c7bf9d3f00.w2.b0", !2075, i64 0}
!2075 = !{!"0x55c7bf9d3f00.w4.b0", !2076, i64 0}
!2076 = !{!"0x55c7bf9d3f00.w8.b0", !2077, i64 0}
!2077 = !{!"0x55c7bf9d3f00.w16.b0", !2078, i64 0}
!2078 = !{!"0x55c7bf9d3f00.w32.b0", !2079, i64 0}
!2079 = !{!"0x55c7bf9d3f00.w64.b0", !2080, i64 0}
!2080 = !{!"0x55c7bf9d3f00.w128.b0", !2081, i64 0}
!2081 = !{!"0x55c7bf9d3f00.w256.b0", !2082, i64 0}
!2082 = !{!"0x55c7bf9d3f00.w512.b0", !2083, i64 0}
!2083 = !{!"0x55c7bf9d3f00.w1024.b0", !2084, i64 0}
!2084 = !{!"i8", !2085, i64 0}
!2085 = !{!"0x55c7bf9d3f00", !8, i64 0}
!2086 = !{!2087, !2087, i64 0}
!2087 = !{!"0x55c7bf9d3f00.w1.b1", !2074, i64 0}
!2088 = !{!2089, !2089, i64 0}
!2089 = !{!"0x55c7bf9d3f00.w1.b2", !2090, i64 0}
!2090 = !{!"0x55c7bf9d3f00.w2.b2", !2075, i64 0}
!2091 = !{!2092, !2092, i64 0}
!2092 = !{!"0x55c7bf9d3f00.w1.b3", !2090, i64 0}
!2093 = !{!2094, !2094, i64 0}
!2094 = !{!"0x55c7bf9df1d0.w1.b0", !2095, i64 0}
!2095 = !{!"0x55c7bf9df1d0.w2.b0", !2096, i64 0}
!2096 = !{!"0x55c7bf9df1d0.w4.b0", !2097, i64 0}
!2097 = !{!"0x55c7bf9df1d0.w8.b0", !2098, i64 0}
!2098 = !{!"0x55c7bf9df1d0.w16.b0", !2099, i64 0}
!2099 = !{!"0x55c7bf9df1d0.w32.b0", !2100, i64 0}
!2100 = !{!"0x55c7bf9df1d0.w64.b0", !2101, i64 0}
!2101 = !{!"0x55c7bf9df1d0.w128.b0", !2102, i64 0}
!2102 = !{!"0x55c7bf9df1d0.w256.b0", !2103, i64 0}
!2103 = !{!"0x55c7bf9df1d0.w512.b0", !2104, i64 0}
!2104 = !{!"0x55c7bf9df1d0.w1024.b0", !2105, i64 0}
!2105 = !{!"i64", !2106, i64 0}
!2106 = !{!"0x55c7bf9df1d0", !8, i64 0}
!2107 = !{!2108, !2108, i64 0}
!2108 = !{!"0x55c7bf9df1d0.w1.b1", !2095, i64 0}
!2109 = !{!2110, !2110, i64 0}
!2110 = !{!"0x55c7bf9df1d0.w1.b2", !2111, i64 0}
!2111 = !{!"0x55c7bf9df1d0.w2.b2", !2096, i64 0}
!2112 = !{!2113, !2113, i64 0}
!2113 = !{!"0x55c7bf9df1d0.w1.b3", !2111, i64 0}
!2114 = !{!2115, !2115, i64 0}
!2115 = !{!"0x55c7bf9df7b0.w4.b0", !2116, i64 0}
!2116 = !{!"0x55c7bf9df7b0.w8.b0", !2117, i64 0}
!2117 = !{!"0x55c7bf9df7b0.w16.b0", !2118, i64 0}
!2118 = !{!"0x55c7bf9df7b0.w32.b0", !2119, i64 0}
!2119 = !{!"0x55c7bf9df7b0.w64.b0", !2120, i64 0}
!2120 = !{!"0x55c7bf9df7b0.w128.b0", !2121, i64 0}
!2121 = !{!"0x55c7bf9df7b0.w256.b0", !2122, i64 0}
!2122 = !{!"0x55c7bf9df7b0.w512.b0", !2123, i64 0}
!2123 = !{!"0x55c7bf9df7b0.w1024.b0", !2124, i64 0}
!2124 = !{!"i64", !2125, i64 0}
!2125 = !{!"0x55c7bf9df7b0", !8, i64 0}
!2126 = !{!2127, !2127, i64 0}
!2127 = !{!"0x55c7bf9e1990.w1.b0", !2128, i64 0}
!2128 = !{!"0x55c7bf9e1990.w2.b0", !2129, i64 0}
!2129 = !{!"0x55c7bf9e1990.w4.b0", !2130, i64 0}
!2130 = !{!"0x55c7bf9e1990.w8.b0", !2131, i64 0}
!2131 = !{!"0x55c7bf9e1990.w16.b0", !2132, i64 0}
!2132 = !{!"0x55c7bf9e1990.w32.b0", !2133, i64 0}
!2133 = !{!"0x55c7bf9e1990.w64.b0", !2134, i64 0}
!2134 = !{!"0x55c7bf9e1990.w128.b0", !2135, i64 0}
!2135 = !{!"0x55c7bf9e1990.w256.b0", !2136, i64 0}
!2136 = !{!"0x55c7bf9e1990.w512.b0", !2137, i64 0}
!2137 = !{!"0x55c7bf9e1990.w1024.b0", !2138, i64 0}
!2138 = !{!"i64", !2139, i64 0}
!2139 = !{!"0x55c7bf9e1990", !8, i64 0}
!2140 = !{!2141, !2141, i64 0}
!2141 = !{!"0x55c7bf9e1990.w1.b1", !2128, i64 0}
!2142 = !{!2143, !2143, i64 0}
!2143 = !{!"0x55c7bf9e1990.w1.b2", !2144, i64 0}
!2144 = !{!"0x55c7bf9e1990.w2.b2", !2129, i64 0}
!2145 = !{!2146, !2146, i64 0}
!2146 = !{!"0x55c7bf9e1990.w1.b3", !2144, i64 0}
!2147 = !{!2148, !2148, i64 0}
!2148 = !{!"0x55c7bf9e2320.w4.b0", !2149, i64 0}
!2149 = !{!"0x55c7bf9e2320.w8.b0", !2150, i64 0}
!2150 = !{!"0x55c7bf9e2320.w16.b0", !2151, i64 0}
!2151 = !{!"0x55c7bf9e2320.w32.b0", !2152, i64 0}
!2152 = !{!"0x55c7bf9e2320.w64.b0", !2153, i64 0}
!2153 = !{!"0x55c7bf9e2320.w128.b0", !2154, i64 0}
!2154 = !{!"0x55c7bf9e2320.w256.b0", !2155, i64 0}
!2155 = !{!"0x55c7bf9e2320.w512.b0", !2156, i64 0}
!2156 = !{!"0x55c7bf9e2320.w1024.b0", !2157, i64 0}
!2157 = !{!"i64", !2158, i64 0}
!2158 = !{!"0x55c7bf9e2320", !8, i64 0}
!2159 = !{!2160, !2160, i64 0}
!2160 = !{!"0x55c7bf9e4500.w1.b0", !2161, i64 0}
!2161 = !{!"0x55c7bf9e4500.w2.b0", !2162, i64 0}
!2162 = !{!"0x55c7bf9e4500.w4.b0", !2163, i64 0}
!2163 = !{!"0x55c7bf9e4500.w8.b0", !2164, i64 0}
!2164 = !{!"0x55c7bf9e4500.w16.b0", !2165, i64 0}
!2165 = !{!"0x55c7bf9e4500.w32.b0", !2166, i64 0}
!2166 = !{!"0x55c7bf9e4500.w64.b0", !2167, i64 0}
!2167 = !{!"0x55c7bf9e4500.w128.b0", !2168, i64 0}
!2168 = !{!"0x55c7bf9e4500.w256.b0", !2169, i64 0}
!2169 = !{!"0x55c7bf9e4500.w512.b0", !2170, i64 0}
!2170 = !{!"0x55c7bf9e4500.w1024.b0", !2171, i64 0}
!2171 = !{!"i64", !2172, i64 0}
!2172 = !{!"0x55c7bf9e4500", !8, i64 0}
!2173 = !{!2174, !2174, i64 0}
!2174 = !{!"0x55c7bf9e47e0.w1.b0", !2175, i64 0}
!2175 = !{!"0x55c7bf9e47e0.w2.b0", !2176, i64 0}
!2176 = !{!"0x55c7bf9e47e0.w4.b0", !2177, i64 0}
!2177 = !{!"0x55c7bf9e47e0.w8.b0", !2178, i64 0}
!2178 = !{!"0x55c7bf9e47e0.w16.b0", !2179, i64 0}
!2179 = !{!"0x55c7bf9e47e0.w32.b0", !2180, i64 0}
!2180 = !{!"0x55c7bf9e47e0.w64.b0", !2181, i64 0}
!2181 = !{!"0x55c7bf9e47e0.w128.b0", !2182, i64 0}
!2182 = !{!"0x55c7bf9e47e0.w256.b0", !2183, i64 0}
!2183 = !{!"0x55c7bf9e47e0.w512.b0", !2184, i64 0}
!2184 = !{!"0x55c7bf9e47e0.w1024.b0", !2185, i64 0}
!2185 = !{!"i64", !2186, i64 0}
!2186 = !{!"0x55c7bf9e47e0", !8, i64 0}
!2187 = !{!2188, !2188, i64 0}
!2188 = !{!"0x55c7bf9e6480.w1.b0", !2189, i64 0}
!2189 = !{!"0x55c7bf9e6480.w2.b0", !2190, i64 0}
!2190 = !{!"0x55c7bf9e6480.w4.b0", !2191, i64 0}
!2191 = !{!"0x55c7bf9e6480.w8.b0", !2192, i64 0}
!2192 = !{!"0x55c7bf9e6480.w16.b0", !2193, i64 0}
!2193 = !{!"0x55c7bf9e6480.w32.b0", !2194, i64 0}
!2194 = !{!"0x55c7bf9e6480.w64.b0", !2195, i64 0}
!2195 = !{!"0x55c7bf9e6480.w128.b0", !2196, i64 0}
!2196 = !{!"0x55c7bf9e6480.w256.b0", !2197, i64 0}
!2197 = !{!"0x55c7bf9e6480.w512.b0", !2198, i64 0}
!2198 = !{!"0x55c7bf9e6480.w1024.b0", !2199, i64 0}
!2199 = !{!"i64", !2200, i64 0}
!2200 = !{!"0x55c7bf9e6480", !8, i64 0}
!2201 = !{!2202, !2202, i64 0}
!2202 = !{!"0x55c7bf9e6480.w1.b1", !2189, i64 0}
!2203 = !{!2204, !2204, i64 0}
!2204 = !{!"0x55c7bf9e6480.w1.b2", !2205, i64 0}
!2205 = !{!"0x55c7bf9e6480.w2.b2", !2190, i64 0}
!2206 = !{!2207, !2207, i64 0}
!2207 = !{!"0x55c7bf9e6480.w1.b3", !2205, i64 0}
!2208 = !{!2209, !2209, i64 0}
!2209 = !{!"0x55c7bf9e6d70.w4.b0", !2210, i64 0}
!2210 = !{!"0x55c7bf9e6d70.w8.b0", !2211, i64 0}
!2211 = !{!"0x55c7bf9e6d70.w16.b0", !2212, i64 0}
!2212 = !{!"0x55c7bf9e6d70.w32.b0", !2213, i64 0}
!2213 = !{!"0x55c7bf9e6d70.w64.b0", !2214, i64 0}
!2214 = !{!"0x55c7bf9e6d70.w128.b0", !2215, i64 0}
!2215 = !{!"0x55c7bf9e6d70.w256.b0", !2216, i64 0}
!2216 = !{!"0x55c7bf9e6d70.w512.b0", !2217, i64 0}
!2217 = !{!"0x55c7bf9e6d70.w1024.b0", !2218, i64 0}
!2218 = !{!"i64", !2219, i64 0}
!2219 = !{!"0x55c7bf9e6d70", !8, i64 0}
!2220 = !{!2221, !2221, i64 0}
!2221 = !{!"i8", !2222, i64 0}
!2222 = !{!"0x55c7bf405df0", !8, i64 0}
!2223 = !{!2224, !2224, i64 0}
!2224 = !{!"i8", !2225, i64 0}
!2225 = !{!"0x55c7bf409010", !8, i64 0}
!2226 = !{!2227, !2227, i64 0}
!2227 = !{!"i8", !2228, i64 0}
!2228 = !{!"0x55c7bf4084a0", !8, i64 0}
!2229 = distinct !{!2229, !337}
!2230 = !{!2231, !2231, i64 0}
!2231 = !{!"i8", !2232, i64 0}
!2232 = !{!"0x55c7bf40a260", !8, i64 0}
!2233 = distinct !{!2233, !337}
!2234 = distinct !{!2234, !337}
!2235 = !{!2236, !2236, i64 0}
!2236 = !{!"i8", !2237, i64 0}
!2237 = !{!"0x55c7bf404090", !8, i64 0}
!2238 = distinct !{!2238, !337}
!2239 = distinct !{!2239, !337}
!2240 = distinct !{!2240, !337}
!2241 = distinct !{!2241, !337}
!2242 = distinct !{!2242, !337}
!2243 = distinct !{!2243, !337}
!2244 = distinct !{!2244, !337}
!2245 = distinct !{!2245, !337}
!2246 = distinct !{!2246, !337}
!2247 = distinct !{!2247, !337}
!2248 = distinct !{!2248, !337}
!2249 = distinct !{!2249, !337}
!2250 = !{!2251, !2251, i64 0}
!2251 = !{!"0x55c7bf9df550.w1.b0", !2252, i64 0}
!2252 = !{!"0x55c7bf9df550.w2.b0", !2253, i64 0}
!2253 = !{!"0x55c7bf9df550.w4.b0", !2254, i64 0}
!2254 = !{!"0x55c7bf9df550.w8.b0", !2255, i64 0}
!2255 = !{!"0x55c7bf9df550.w16.b0", !2256, i64 0}
!2256 = !{!"0x55c7bf9df550.w32.b0", !2257, i64 0}
!2257 = !{!"0x55c7bf9df550.w64.b0", !2258, i64 0}
!2258 = !{!"0x55c7bf9df550.w128.b0", !2259, i64 0}
!2259 = !{!"0x55c7bf9df550.w256.b0", !2260, i64 0}
!2260 = !{!"0x55c7bf9df550.w512.b0", !2261, i64 0}
!2261 = !{!"0x55c7bf9df550.w1024.b0", !2262, i64 0}
!2262 = !{!"i8", !2263, i64 0}
!2263 = !{!"0x55c7bf9df550", !8, i64 0}
!2264 = !{!2265, !2265, i64 0}
!2265 = !{!"0x55c7bf9df550.w1.b1", !2252, i64 0}
!2266 = !{!2267, !2267, i64 0}
!2267 = !{!"0x55c7bf9e6140.w1.b0", !2268, i64 0}
!2268 = !{!"0x55c7bf9e6140.w2.b0", !2269, i64 0}
!2269 = !{!"0x55c7bf9e6140.w4.b0", !2270, i64 0}
!2270 = !{!"0x55c7bf9e6140.w8.b0", !2271, i64 0}
!2271 = !{!"0x55c7bf9e6140.w16.b0", !2272, i64 0}
!2272 = !{!"0x55c7bf9e6140.w32.b0", !2273, i64 0}
!2273 = !{!"0x55c7bf9e6140.w64.b0", !2274, i64 0}
!2274 = !{!"0x55c7bf9e6140.w128.b0", !2275, i64 0}
!2275 = !{!"0x55c7bf9e6140.w256.b0", !2276, i64 0}
!2276 = !{!"0x55c7bf9e6140.w512.b0", !2277, i64 0}
!2277 = !{!"0x55c7bf9e6140.w1024.b0", !2278, i64 0}
!2278 = !{!"i64", !2279, i64 0}
!2279 = !{!"0x55c7bf9e6140", !8, i64 0}
!2280 = !{!2281, !2281, i64 0}
!2281 = !{!"0x55c7bf9e6140.w1.b1", !2268, i64 0}
!2282 = !{!2283, !2283, i64 0}
!2283 = !{!"0x55c7bf9e6140.w1.b2", !2284, i64 0}
!2284 = !{!"0x55c7bf9e6140.w2.b2", !2269, i64 0}
!2285 = !{!2286, !2286, i64 0}
!2286 = !{!"0x55c7bf9e6140.w1.b3", !2284, i64 0}
!2287 = !{!2288, !2288, i64 0}
!2288 = !{!"0x55c7bf9e2020.w4.b0", !2289, i64 0}
!2289 = !{!"0x55c7bf9e2020.w8.b0", !2290, i64 0}
!2290 = !{!"0x55c7bf9e2020.w16.b0", !2291, i64 0}
!2291 = !{!"0x55c7bf9e2020.w32.b0", !2292, i64 0}
!2292 = !{!"0x55c7bf9e2020.w64.b0", !2293, i64 0}
!2293 = !{!"0x55c7bf9e2020.w128.b0", !2294, i64 0}
!2294 = !{!"0x55c7bf9e2020.w256.b0", !2295, i64 0}
!2295 = !{!"0x55c7bf9e2020.w512.b0", !2296, i64 0}
!2296 = !{!"0x55c7bf9e2020.w1024.b0", !2297, i64 0}
!2297 = !{!"i64", !2298, i64 0}
!2298 = !{!"0x55c7bf9e2020", !8, i64 0}
!2299 = !{!2300, !2300, i64 0}
!2300 = !{!"0x55c7bf9ec200.w1.b0", !2301, i64 0}
!2301 = !{!"0x55c7bf9ec200.w2.b0", !2302, i64 0}
!2302 = !{!"0x55c7bf9ec200.w4.b0", !2303, i64 0}
!2303 = !{!"0x55c7bf9ec200.w8.b0", !2304, i64 0}
!2304 = !{!"0x55c7bf9ec200.w16.b0", !2305, i64 0}
!2305 = !{!"0x55c7bf9ec200.w32.b0", !2306, i64 0}
!2306 = !{!"0x55c7bf9ec200.w64.b0", !2307, i64 0}
!2307 = !{!"0x55c7bf9ec200.w128.b0", !2308, i64 0}
!2308 = !{!"0x55c7bf9ec200.w256.b0", !2309, i64 0}
!2309 = !{!"0x55c7bf9ec200.w512.b0", !2310, i64 0}
!2310 = !{!"0x55c7bf9ec200.w1024.b0", !2311, i64 0}
!2311 = !{!"i64", !2312, i64 0}
!2312 = !{!"0x55c7bf9ec200", !8, i64 0}
!2313 = !{!2314, !2314, i64 0}
!2314 = !{!"0x55c7bf9ec200.w1.b1", !2301, i64 0}
!2315 = !{!2316, !2316, i64 0}
!2316 = !{!"0x55c7bf9ec200.w1.b2", !2317, i64 0}
!2317 = !{!"0x55c7bf9ec200.w2.b2", !2302, i64 0}
!2318 = !{!2319, !2319, i64 0}
!2319 = !{!"0x55c7bf9ec200.w1.b3", !2317, i64 0}
!2320 = !{!2321, !2321, i64 0}
!2321 = !{!"0x55c7bf9ecb90.w4.b0", !2322, i64 0}
!2322 = !{!"0x55c7bf9ecb90.w8.b0", !2323, i64 0}
!2323 = !{!"0x55c7bf9ecb90.w16.b0", !2324, i64 0}
!2324 = !{!"0x55c7bf9ecb90.w32.b0", !2325, i64 0}
!2325 = !{!"0x55c7bf9ecb90.w64.b0", !2326, i64 0}
!2326 = !{!"0x55c7bf9ecb90.w128.b0", !2327, i64 0}
!2327 = !{!"0x55c7bf9ecb90.w256.b0", !2328, i64 0}
!2328 = !{!"0x55c7bf9ecb90.w512.b0", !2329, i64 0}
!2329 = !{!"0x55c7bf9ecb90.w1024.b0", !2330, i64 0}
!2330 = !{!"i64", !2331, i64 0}
!2331 = !{!"0x55c7bf9ecb90", !8, i64 0}
!2332 = !{!2333, !2333, i64 0}
!2333 = !{!"i8", !2334, i64 0}
!2334 = !{!"0x55c7bf369140", !8, i64 0}
!2335 = !{!2336, !2336, i64 0}
!2336 = !{!"i8", !2337, i64 0}
!2337 = !{!"0x55c7bf381ac0", !8, i64 0}
!2338 = !{!2339, !2339, i64 0}
!2339 = !{!"i8", !2340, i64 0}
!2340 = !{!"0x55c7bf3d9d30", !8, i64 0}
!2341 = distinct !{!2341, !337}
!2342 = !{!2343, !2343, i64 0}
!2343 = !{!"0x55c7bfa1eb80.w1.b0", !2344, i64 0}
!2344 = !{!"0x55c7bfa1eb80.w2.b0", !2345, i64 0}
!2345 = !{!"0x55c7bfa1eb80.w4.b0", !2346, i64 0}
!2346 = !{!"0x55c7bfa1eb80.w8.b0", !2347, i64 0}
!2347 = !{!"0x55c7bfa1eb80.w16.b0", !2348, i64 0}
!2348 = !{!"0x55c7bfa1eb80.w32.b0", !2349, i64 0}
!2349 = !{!"0x55c7bfa1eb80.w64.b0", !2350, i64 0}
!2350 = !{!"0x55c7bfa1eb80.w128.b0", !2351, i64 0}
!2351 = !{!"0x55c7bfa1eb80.w256.b0", !2352, i64 0}
!2352 = !{!"0x55c7bfa1eb80.w512.b0", !2353, i64 0}
!2353 = !{!"0x55c7bfa1eb80.w1024.b0", !2354, i64 0}
!2354 = !{!"i8", !2355, i64 0}
!2355 = !{!"0x55c7bfa1eb80", !8, i64 0}
!2356 = !{!2357, !2357, i64 0}
!2357 = !{!"0x55c7bfa1eb80.w1.b1", !2344, i64 0}
!2358 = !{!2359, !2359, i64 0}
!2359 = !{!"0x55c7bfa1eb80.w1.b2", !2360, i64 0}
!2360 = !{!"0x55c7bfa1eb80.w2.b2", !2345, i64 0}
!2361 = !{!2362, !2362, i64 0}
!2362 = !{!"0x55c7bfa1eb80.w1.b3", !2360, i64 0}
!2363 = !{!2364, !2364, i64 0}
!2364 = !{!"0x55c7bfa29e50.w1.b0", !2365, i64 0}
!2365 = !{!"0x55c7bfa29e50.w2.b0", !2366, i64 0}
!2366 = !{!"0x55c7bfa29e50.w4.b0", !2367, i64 0}
!2367 = !{!"0x55c7bfa29e50.w8.b0", !2368, i64 0}
!2368 = !{!"0x55c7bfa29e50.w16.b0", !2369, i64 0}
!2369 = !{!"0x55c7bfa29e50.w32.b0", !2370, i64 0}
!2370 = !{!"0x55c7bfa29e50.w64.b0", !2371, i64 0}
!2371 = !{!"0x55c7bfa29e50.w128.b0", !2372, i64 0}
!2372 = !{!"0x55c7bfa29e50.w256.b0", !2373, i64 0}
!2373 = !{!"0x55c7bfa29e50.w512.b0", !2374, i64 0}
!2374 = !{!"0x55c7bfa29e50.w1024.b0", !2375, i64 0}
!2375 = !{!"i64", !2376, i64 0}
!2376 = !{!"0x55c7bfa29e50", !8, i64 0}
!2377 = !{!2378, !2378, i64 0}
!2378 = !{!"0x55c7bfa29e50.w1.b1", !2365, i64 0}
!2379 = !{!2380, !2380, i64 0}
!2380 = !{!"0x55c7bfa29e50.w1.b2", !2381, i64 0}
!2381 = !{!"0x55c7bfa29e50.w2.b2", !2366, i64 0}
!2382 = !{!2383, !2383, i64 0}
!2383 = !{!"0x55c7bfa29e50.w1.b3", !2381, i64 0}
!2384 = !{!2385, !2385, i64 0}
!2385 = !{!"0x55c7bfa2a430.w4.b0", !2386, i64 0}
!2386 = !{!"0x55c7bfa2a430.w8.b0", !2387, i64 0}
!2387 = !{!"0x55c7bfa2a430.w16.b0", !2388, i64 0}
!2388 = !{!"0x55c7bfa2a430.w32.b0", !2389, i64 0}
!2389 = !{!"0x55c7bfa2a430.w64.b0", !2390, i64 0}
!2390 = !{!"0x55c7bfa2a430.w128.b0", !2391, i64 0}
!2391 = !{!"0x55c7bfa2a430.w256.b0", !2392, i64 0}
!2392 = !{!"0x55c7bfa2a430.w512.b0", !2393, i64 0}
!2393 = !{!"0x55c7bfa2a430.w1024.b0", !2394, i64 0}
!2394 = !{!"i64", !2395, i64 0}
!2395 = !{!"0x55c7bfa2a430", !8, i64 0}
!2396 = !{!2397, !2397, i64 0}
!2397 = !{!"0x55c7bfa2c610.w1.b0", !2398, i64 0}
!2398 = !{!"0x55c7bfa2c610.w2.b0", !2399, i64 0}
!2399 = !{!"0x55c7bfa2c610.w4.b0", !2400, i64 0}
!2400 = !{!"0x55c7bfa2c610.w8.b0", !2401, i64 0}
!2401 = !{!"0x55c7bfa2c610.w16.b0", !2402, i64 0}
!2402 = !{!"0x55c7bfa2c610.w32.b0", !2403, i64 0}
!2403 = !{!"0x55c7bfa2c610.w64.b0", !2404, i64 0}
!2404 = !{!"0x55c7bfa2c610.w128.b0", !2405, i64 0}
!2405 = !{!"0x55c7bfa2c610.w256.b0", !2406, i64 0}
!2406 = !{!"0x55c7bfa2c610.w512.b0", !2407, i64 0}
!2407 = !{!"0x55c7bfa2c610.w1024.b0", !2408, i64 0}
!2408 = !{!"i64", !2409, i64 0}
!2409 = !{!"0x55c7bfa2c610", !8, i64 0}
!2410 = !{!2411, !2411, i64 0}
!2411 = !{!"0x55c7bfa2c610.w1.b1", !2398, i64 0}
!2412 = !{!2413, !2413, i64 0}
!2413 = !{!"0x55c7bfa2c610.w1.b2", !2414, i64 0}
!2414 = !{!"0x55c7bfa2c610.w2.b2", !2399, i64 0}
!2415 = !{!2416, !2416, i64 0}
!2416 = !{!"0x55c7bfa2c610.w1.b3", !2414, i64 0}
!2417 = !{!2418, !2418, i64 0}
!2418 = !{!"0x55c7bfa2cfa0.w4.b0", !2419, i64 0}
!2419 = !{!"0x55c7bfa2cfa0.w8.b0", !2420, i64 0}
!2420 = !{!"0x55c7bfa2cfa0.w16.b0", !2421, i64 0}
!2421 = !{!"0x55c7bfa2cfa0.w32.b0", !2422, i64 0}
!2422 = !{!"0x55c7bfa2cfa0.w64.b0", !2423, i64 0}
!2423 = !{!"0x55c7bfa2cfa0.w128.b0", !2424, i64 0}
!2424 = !{!"0x55c7bfa2cfa0.w256.b0", !2425, i64 0}
!2425 = !{!"0x55c7bfa2cfa0.w512.b0", !2426, i64 0}
!2426 = !{!"0x55c7bfa2cfa0.w1024.b0", !2427, i64 0}
!2427 = !{!"i64", !2428, i64 0}
!2428 = !{!"0x55c7bfa2cfa0", !8, i64 0}
!2429 = !{!2430, !2430, i64 0}
!2430 = !{!"0x55c7bfa2f180.w1.b0", !2431, i64 0}
!2431 = !{!"0x55c7bfa2f180.w2.b0", !2432, i64 0}
!2432 = !{!"0x55c7bfa2f180.w4.b0", !2433, i64 0}
!2433 = !{!"0x55c7bfa2f180.w8.b0", !2434, i64 0}
!2434 = !{!"0x55c7bfa2f180.w16.b0", !2435, i64 0}
!2435 = !{!"0x55c7bfa2f180.w32.b0", !2436, i64 0}
!2436 = !{!"0x55c7bfa2f180.w64.b0", !2437, i64 0}
!2437 = !{!"0x55c7bfa2f180.w128.b0", !2438, i64 0}
!2438 = !{!"0x55c7bfa2f180.w256.b0", !2439, i64 0}
!2439 = !{!"0x55c7bfa2f180.w512.b0", !2440, i64 0}
!2440 = !{!"0x55c7bfa2f180.w1024.b0", !2441, i64 0}
!2441 = !{!"i64", !2442, i64 0}
!2442 = !{!"0x55c7bfa2f180", !8, i64 0}
!2443 = !{!2444, !2444, i64 0}
!2444 = !{!"0x55c7bfa2f460.w1.b0", !2445, i64 0}
!2445 = !{!"0x55c7bfa2f460.w2.b0", !2446, i64 0}
!2446 = !{!"0x55c7bfa2f460.w4.b0", !2447, i64 0}
!2447 = !{!"0x55c7bfa2f460.w8.b0", !2448, i64 0}
!2448 = !{!"0x55c7bfa2f460.w16.b0", !2449, i64 0}
!2449 = !{!"0x55c7bfa2f460.w32.b0", !2450, i64 0}
!2450 = !{!"0x55c7bfa2f460.w64.b0", !2451, i64 0}
!2451 = !{!"0x55c7bfa2f460.w128.b0", !2452, i64 0}
!2452 = !{!"0x55c7bfa2f460.w256.b0", !2453, i64 0}
!2453 = !{!"0x55c7bfa2f460.w512.b0", !2454, i64 0}
!2454 = !{!"0x55c7bfa2f460.w1024.b0", !2455, i64 0}
!2455 = !{!"i64", !2456, i64 0}
!2456 = !{!"0x55c7bfa2f460", !8, i64 0}
!2457 = !{!2458, !2458, i64 0}
!2458 = !{!"0x55c7bfa31100.w1.b0", !2459, i64 0}
!2459 = !{!"0x55c7bfa31100.w2.b0", !2460, i64 0}
!2460 = !{!"0x55c7bfa31100.w4.b0", !2461, i64 0}
!2461 = !{!"0x55c7bfa31100.w8.b0", !2462, i64 0}
!2462 = !{!"0x55c7bfa31100.w16.b0", !2463, i64 0}
!2463 = !{!"0x55c7bfa31100.w32.b0", !2464, i64 0}
!2464 = !{!"0x55c7bfa31100.w64.b0", !2465, i64 0}
!2465 = !{!"0x55c7bfa31100.w128.b0", !2466, i64 0}
!2466 = !{!"0x55c7bfa31100.w256.b0", !2467, i64 0}
!2467 = !{!"0x55c7bfa31100.w512.b0", !2468, i64 0}
!2468 = !{!"0x55c7bfa31100.w1024.b0", !2469, i64 0}
!2469 = !{!"i64", !2470, i64 0}
!2470 = !{!"0x55c7bfa31100", !8, i64 0}
!2471 = !{!2472, !2472, i64 0}
!2472 = !{!"0x55c7bfa31100.w1.b1", !2459, i64 0}
!2473 = !{!2474, !2474, i64 0}
!2474 = !{!"0x55c7bfa31100.w1.b2", !2475, i64 0}
!2475 = !{!"0x55c7bfa31100.w2.b2", !2460, i64 0}
!2476 = !{!2477, !2477, i64 0}
!2477 = !{!"0x55c7bfa31100.w1.b3", !2475, i64 0}
!2478 = !{!2479, !2479, i64 0}
!2479 = !{!"0x55c7bfa319f0.w4.b0", !2480, i64 0}
!2480 = !{!"0x55c7bfa319f0.w8.b0", !2481, i64 0}
!2481 = !{!"0x55c7bfa319f0.w16.b0", !2482, i64 0}
!2482 = !{!"0x55c7bfa319f0.w32.b0", !2483, i64 0}
!2483 = !{!"0x55c7bfa319f0.w64.b0", !2484, i64 0}
!2484 = !{!"0x55c7bfa319f0.w128.b0", !2485, i64 0}
!2485 = !{!"0x55c7bfa319f0.w256.b0", !2486, i64 0}
!2486 = !{!"0x55c7bfa319f0.w512.b0", !2487, i64 0}
!2487 = !{!"0x55c7bfa319f0.w1024.b0", !2488, i64 0}
!2488 = !{!"i64", !2489, i64 0}
!2489 = !{!"0x55c7bfa319f0", !8, i64 0}
!2490 = !{!2491, !2491, i64 0}
!2491 = !{!"i8", !2492, i64 0}
!2492 = !{!"0x55c7bf5046e0", !8, i64 0}
!2493 = !{!2494, !2494, i64 0}
!2494 = !{!"i8", !2495, i64 0}
!2495 = !{!"0x55c7bf5045e0", !8, i64 0}
!2496 = !{!2497, !2497, i64 0}
!2497 = !{!"i8", !2498, i64 0}
!2498 = !{!"0x55c7bf504660", !8, i64 0}
!2499 = !{!2500, !2500, i64 0}
!2500 = !{!"i8", !2501, i64 0}
!2501 = !{!"0x55c7bf5046a0", !8, i64 0}
!2502 = !{!2503, !2503, i64 0}
!2503 = !{!"i8", !2504, i64 0}
!2504 = !{!"0x55c7bf504620", !8, i64 0}
!2505 = !{!2506, !2506, i64 0}
!2506 = !{!"0x55c7bf9f1180.w1.b0", !2507, i64 0}
!2507 = !{!"0x55c7bf9f1180.w2.b0", !2508, i64 0}
!2508 = !{!"0x55c7bf9f1180.w4.b0", !2509, i64 0}
!2509 = !{!"0x55c7bf9f1180.w8.b0", !2510, i64 0}
!2510 = !{!"0x55c7bf9f1180.w16.b0", !2511, i64 0}
!2511 = !{!"0x55c7bf9f1180.w32.b0", !2512, i64 0}
!2512 = !{!"0x55c7bf9f1180.w64.b0", !2513, i64 0}
!2513 = !{!"0x55c7bf9f1180.w128.b0", !2514, i64 0}
!2514 = !{!"0x55c7bf9f1180.w256.b0", !2515, i64 0}
!2515 = !{!"0x55c7bf9f1180.w512.b0", !2516, i64 0}
!2516 = !{!"0x55c7bf9f1180.w1024.b0", !2517, i64 0}
!2517 = !{!"i8", !2518, i64 0}
!2518 = !{!"0x55c7bf9f1180", !8, i64 0}
!2519 = !{!2520, !2520, i64 0}
!2520 = !{!"0x55c7bf9f1180.w1.b1", !2507, i64 0}
!2521 = !{!2522, !2522, i64 0}
!2522 = !{!"0x55c7bf9f1180.w1.b2", !2523, i64 0}
!2523 = !{!"0x55c7bf9f1180.w2.b2", !2508, i64 0}
!2524 = !{!2525, !2525, i64 0}
!2525 = !{!"0x55c7bf9f1180.w1.b3", !2523, i64 0}
!2526 = !{!2527, !2527, i64 0}
!2527 = !{!"0x55c7bf9fc510.w1.b0", !2528, i64 0}
!2528 = !{!"0x55c7bf9fc510.w2.b0", !2529, i64 0}
!2529 = !{!"0x55c7bf9fc510.w4.b0", !2530, i64 0}
!2530 = !{!"0x55c7bf9fc510.w8.b0", !2531, i64 0}
!2531 = !{!"0x55c7bf9fc510.w16.b0", !2532, i64 0}
!2532 = !{!"0x55c7bf9fc510.w32.b0", !2533, i64 0}
!2533 = !{!"0x55c7bf9fc510.w64.b0", !2534, i64 0}
!2534 = !{!"0x55c7bf9fc510.w128.b0", !2535, i64 0}
!2535 = !{!"0x55c7bf9fc510.w256.b0", !2536, i64 0}
!2536 = !{!"0x55c7bf9fc510.w512.b0", !2537, i64 0}
!2537 = !{!"0x55c7bf9fc510.w1024.b0", !2538, i64 0}
!2538 = !{!"i64", !2539, i64 0}
!2539 = !{!"0x55c7bf9fc510", !8, i64 0}
!2540 = !{!2541, !2541, i64 0}
!2541 = !{!"0x55c7bf9fc510.w1.b1", !2528, i64 0}
!2542 = !{!2543, !2543, i64 0}
!2543 = !{!"0x55c7bf9fc510.w1.b2", !2544, i64 0}
!2544 = !{!"0x55c7bf9fc510.w2.b2", !2529, i64 0}
!2545 = !{!2546, !2546, i64 0}
!2546 = !{!"0x55c7bf9fc510.w1.b3", !2544, i64 0}
!2547 = !{!2548, !2548, i64 0}
!2548 = !{!"0x55c7bf9fcaf0.w4.b0", !2549, i64 0}
!2549 = !{!"0x55c7bf9fcaf0.w8.b0", !2550, i64 0}
!2550 = !{!"0x55c7bf9fcaf0.w16.b0", !2551, i64 0}
!2551 = !{!"0x55c7bf9fcaf0.w32.b0", !2552, i64 0}
!2552 = !{!"0x55c7bf9fcaf0.w64.b0", !2553, i64 0}
!2553 = !{!"0x55c7bf9fcaf0.w128.b0", !2554, i64 0}
!2554 = !{!"0x55c7bf9fcaf0.w256.b0", !2555, i64 0}
!2555 = !{!"0x55c7bf9fcaf0.w512.b0", !2556, i64 0}
!2556 = !{!"0x55c7bf9fcaf0.w1024.b0", !2557, i64 0}
!2557 = !{!"i64", !2558, i64 0}
!2558 = !{!"0x55c7bf9fcaf0", !8, i64 0}
!2559 = !{!2560, !2560, i64 0}
!2560 = !{!"0x55c7bf9fecd0.w1.b0", !2561, i64 0}
!2561 = !{!"0x55c7bf9fecd0.w2.b0", !2562, i64 0}
!2562 = !{!"0x55c7bf9fecd0.w4.b0", !2563, i64 0}
!2563 = !{!"0x55c7bf9fecd0.w8.b0", !2564, i64 0}
!2564 = !{!"0x55c7bf9fecd0.w16.b0", !2565, i64 0}
!2565 = !{!"0x55c7bf9fecd0.w32.b0", !2566, i64 0}
!2566 = !{!"0x55c7bf9fecd0.w64.b0", !2567, i64 0}
!2567 = !{!"0x55c7bf9fecd0.w128.b0", !2568, i64 0}
!2568 = !{!"0x55c7bf9fecd0.w256.b0", !2569, i64 0}
!2569 = !{!"0x55c7bf9fecd0.w512.b0", !2570, i64 0}
!2570 = !{!"0x55c7bf9fecd0.w1024.b0", !2571, i64 0}
!2571 = !{!"i64", !2572, i64 0}
!2572 = !{!"0x55c7bf9fecd0", !8, i64 0}
!2573 = !{!2574, !2574, i64 0}
!2574 = !{!"0x55c7bf9fecd0.w1.b1", !2561, i64 0}
!2575 = !{!2576, !2576, i64 0}
!2576 = !{!"0x55c7bf9fecd0.w1.b2", !2577, i64 0}
!2577 = !{!"0x55c7bf9fecd0.w2.b2", !2562, i64 0}
!2578 = !{!2579, !2579, i64 0}
!2579 = !{!"0x55c7bf9fecd0.w1.b3", !2577, i64 0}
!2580 = !{!2581, !2581, i64 0}
!2581 = !{!"0x55c7bf9ff660.w4.b0", !2582, i64 0}
!2582 = !{!"0x55c7bf9ff660.w8.b0", !2583, i64 0}
!2583 = !{!"0x55c7bf9ff660.w16.b0", !2584, i64 0}
!2584 = !{!"0x55c7bf9ff660.w32.b0", !2585, i64 0}
!2585 = !{!"0x55c7bf9ff660.w64.b0", !2586, i64 0}
!2586 = !{!"0x55c7bf9ff660.w128.b0", !2587, i64 0}
!2587 = !{!"0x55c7bf9ff660.w256.b0", !2588, i64 0}
!2588 = !{!"0x55c7bf9ff660.w512.b0", !2589, i64 0}
!2589 = !{!"0x55c7bf9ff660.w1024.b0", !2590, i64 0}
!2590 = !{!"i64", !2591, i64 0}
!2591 = !{!"0x55c7bf9ff660", !8, i64 0}
!2592 = !{!2593, !2593, i64 0}
!2593 = !{!"0x55c7bfa01840.w1.b0", !2594, i64 0}
!2594 = !{!"0x55c7bfa01840.w2.b0", !2595, i64 0}
!2595 = !{!"0x55c7bfa01840.w4.b0", !2596, i64 0}
!2596 = !{!"0x55c7bfa01840.w8.b0", !2597, i64 0}
!2597 = !{!"0x55c7bfa01840.w16.b0", !2598, i64 0}
!2598 = !{!"0x55c7bfa01840.w32.b0", !2599, i64 0}
!2599 = !{!"0x55c7bfa01840.w64.b0", !2600, i64 0}
!2600 = !{!"0x55c7bfa01840.w128.b0", !2601, i64 0}
!2601 = !{!"0x55c7bfa01840.w256.b0", !2602, i64 0}
!2602 = !{!"0x55c7bfa01840.w512.b0", !2603, i64 0}
!2603 = !{!"0x55c7bfa01840.w1024.b0", !2604, i64 0}
!2604 = !{!"i64", !2605, i64 0}
!2605 = !{!"0x55c7bfa01840", !8, i64 0}
!2606 = !{!2607, !2607, i64 0}
!2607 = !{!"0x55c7bfa01b20.w1.b0", !2608, i64 0}
!2608 = !{!"0x55c7bfa01b20.w2.b0", !2609, i64 0}
!2609 = !{!"0x55c7bfa01b20.w4.b0", !2610, i64 0}
!2610 = !{!"0x55c7bfa01b20.w8.b0", !2611, i64 0}
!2611 = !{!"0x55c7bfa01b20.w16.b0", !2612, i64 0}
!2612 = !{!"0x55c7bfa01b20.w32.b0", !2613, i64 0}
!2613 = !{!"0x55c7bfa01b20.w64.b0", !2614, i64 0}
!2614 = !{!"0x55c7bfa01b20.w128.b0", !2615, i64 0}
!2615 = !{!"0x55c7bfa01b20.w256.b0", !2616, i64 0}
!2616 = !{!"0x55c7bfa01b20.w512.b0", !2617, i64 0}
!2617 = !{!"0x55c7bfa01b20.w1024.b0", !2618, i64 0}
!2618 = !{!"i64", !2619, i64 0}
!2619 = !{!"0x55c7bfa01b20", !8, i64 0}
!2620 = !{!2621, !2621, i64 0}
!2621 = !{!"0x55c7bfa037c0.w1.b0", !2622, i64 0}
!2622 = !{!"0x55c7bfa037c0.w2.b0", !2623, i64 0}
!2623 = !{!"0x55c7bfa037c0.w4.b0", !2624, i64 0}
!2624 = !{!"0x55c7bfa037c0.w8.b0", !2625, i64 0}
!2625 = !{!"0x55c7bfa037c0.w16.b0", !2626, i64 0}
!2626 = !{!"0x55c7bfa037c0.w32.b0", !2627, i64 0}
!2627 = !{!"0x55c7bfa037c0.w64.b0", !2628, i64 0}
!2628 = !{!"0x55c7bfa037c0.w128.b0", !2629, i64 0}
!2629 = !{!"0x55c7bfa037c0.w256.b0", !2630, i64 0}
!2630 = !{!"0x55c7bfa037c0.w512.b0", !2631, i64 0}
!2631 = !{!"0x55c7bfa037c0.w1024.b0", !2632, i64 0}
!2632 = !{!"i64", !2633, i64 0}
!2633 = !{!"0x55c7bfa037c0", !8, i64 0}
!2634 = !{!2635, !2635, i64 0}
!2635 = !{!"0x55c7bfa037c0.w1.b1", !2622, i64 0}
!2636 = !{!2637, !2637, i64 0}
!2637 = !{!"0x55c7bfa037c0.w1.b2", !2638, i64 0}
!2638 = !{!"0x55c7bfa037c0.w2.b2", !2623, i64 0}
!2639 = !{!2640, !2640, i64 0}
!2640 = !{!"0x55c7bfa037c0.w1.b3", !2638, i64 0}
!2641 = !{!2642, !2642, i64 0}
!2642 = !{!"0x55c7bfa040b0.w4.b0", !2643, i64 0}
!2643 = !{!"0x55c7bfa040b0.w8.b0", !2644, i64 0}
!2644 = !{!"0x55c7bfa040b0.w16.b0", !2645, i64 0}
!2645 = !{!"0x55c7bfa040b0.w32.b0", !2646, i64 0}
!2646 = !{!"0x55c7bfa040b0.w64.b0", !2647, i64 0}
!2647 = !{!"0x55c7bfa040b0.w128.b0", !2648, i64 0}
!2648 = !{!"0x55c7bfa040b0.w256.b0", !2649, i64 0}
!2649 = !{!"0x55c7bfa040b0.w512.b0", !2650, i64 0}
!2650 = !{!"0x55c7bfa040b0.w1024.b0", !2651, i64 0}
!2651 = !{!"i64", !2652, i64 0}
!2652 = !{!"0x55c7bfa040b0", !8, i64 0}
!2653 = !{!2654, !2654, i64 0}
!2654 = !{!"i8", !2655, i64 0}
!2655 = !{!"0x55c7bf41ad00", !8, i64 0}
!2656 = !{!2657, !2657, i64 0}
!2657 = !{!"i8", !2658, i64 0}
!2658 = !{!"0x55c7bf41be80", !8, i64 0}
!2659 = !{!2660, !2660, i64 0}
!2660 = !{!"i8", !2661, i64 0}
!2661 = !{!"0x55c7bf419890", !8, i64 0}
!2662 = distinct !{!2662, !337}
!2663 = !{!2664, !2664, i64 0}
!2664 = !{!"i8", !2665, i64 0}
!2665 = !{!"0x55c7bf41be40", !8, i64 0}
!2666 = distinct !{!2666, !337}
!2667 = distinct !{!2667, !337}
!2668 = !{!2669, !2669, i64 0}
!2669 = !{!"i8", !2670, i64 0}
!2670 = !{!"0x55c7bf4145c0", !8, i64 0}
!2671 = distinct !{!2671, !337}
!2672 = distinct !{!2672, !337}
!2673 = distinct !{!2673, !337}
!2674 = distinct !{!2674, !337}
!2675 = distinct !{!2675, !337}
!2676 = distinct !{!2676, !337}
!2677 = distinct !{!2677, !337}
!2678 = distinct !{!2678, !337}
!2679 = distinct !{!2679, !337}
!2680 = distinct !{!2680, !337}
!2681 = distinct !{!2681, !337}
!2682 = distinct !{!2682, !337}
!2683 = !{!2684, !2684, i64 0}
!2684 = !{!"0x55c7bf9fc890.w1.b0", !2685, i64 0}
!2685 = !{!"0x55c7bf9fc890.w2.b0", !2686, i64 0}
!2686 = !{!"0x55c7bf9fc890.w4.b0", !2687, i64 0}
!2687 = !{!"0x55c7bf9fc890.w8.b0", !2688, i64 0}
!2688 = !{!"0x55c7bf9fc890.w16.b0", !2689, i64 0}
!2689 = !{!"0x55c7bf9fc890.w32.b0", !2690, i64 0}
!2690 = !{!"0x55c7bf9fc890.w64.b0", !2691, i64 0}
!2691 = !{!"0x55c7bf9fc890.w128.b0", !2692, i64 0}
!2692 = !{!"0x55c7bf9fc890.w256.b0", !2693, i64 0}
!2693 = !{!"0x55c7bf9fc890.w512.b0", !2694, i64 0}
!2694 = !{!"0x55c7bf9fc890.w1024.b0", !2695, i64 0}
!2695 = !{!"i8", !2696, i64 0}
!2696 = !{!"0x55c7bf9fc890", !8, i64 0}
!2697 = !{!2698, !2698, i64 0}
!2698 = !{!"0x55c7bf9fc890.w1.b1", !2685, i64 0}
!2699 = !{!2700, !2700, i64 0}
!2700 = !{!"0x55c7bf9fc890.w1.b2", !2701, i64 0}
!2701 = !{!"0x55c7bf9fc890.w2.b2", !2686, i64 0}
!2702 = !{!2703, !2703, i64 0}
!2703 = !{!"0x55c7bf9fc890.w1.b3", !2701, i64 0}
!2704 = !{!2705, !2705, i64 0}
!2705 = !{!"0x55c7bfa07b60.w1.b0", !2706, i64 0}
!2706 = !{!"0x55c7bfa07b60.w2.b0", !2707, i64 0}
!2707 = !{!"0x55c7bfa07b60.w4.b0", !2708, i64 0}
!2708 = !{!"0x55c7bfa07b60.w8.b0", !2709, i64 0}
!2709 = !{!"0x55c7bfa07b60.w16.b0", !2710, i64 0}
!2710 = !{!"0x55c7bfa07b60.w32.b0", !2711, i64 0}
!2711 = !{!"0x55c7bfa07b60.w64.b0", !2712, i64 0}
!2712 = !{!"0x55c7bfa07b60.w128.b0", !2713, i64 0}
!2713 = !{!"0x55c7bfa07b60.w256.b0", !2714, i64 0}
!2714 = !{!"0x55c7bfa07b60.w512.b0", !2715, i64 0}
!2715 = !{!"0x55c7bfa07b60.w1024.b0", !2716, i64 0}
!2716 = !{!"i64", !2717, i64 0}
!2717 = !{!"0x55c7bfa07b60", !8, i64 0}
!2718 = !{!2719, !2719, i64 0}
!2719 = !{!"0x55c7bfa07b60.w1.b1", !2706, i64 0}
!2720 = !{!2721, !2721, i64 0}
!2721 = !{!"0x55c7bfa07b60.w1.b2", !2722, i64 0}
!2722 = !{!"0x55c7bfa07b60.w2.b2", !2707, i64 0}
!2723 = !{!2724, !2724, i64 0}
!2724 = !{!"0x55c7bfa07b60.w1.b3", !2722, i64 0}
!2725 = !{!2726, !2726, i64 0}
!2726 = !{!"0x55c7bfa08140.w4.b0", !2727, i64 0}
!2727 = !{!"0x55c7bfa08140.w8.b0", !2728, i64 0}
!2728 = !{!"0x55c7bfa08140.w16.b0", !2729, i64 0}
!2729 = !{!"0x55c7bfa08140.w32.b0", !2730, i64 0}
!2730 = !{!"0x55c7bfa08140.w64.b0", !2731, i64 0}
!2731 = !{!"0x55c7bfa08140.w128.b0", !2732, i64 0}
!2732 = !{!"0x55c7bfa08140.w256.b0", !2733, i64 0}
!2733 = !{!"0x55c7bfa08140.w512.b0", !2734, i64 0}
!2734 = !{!"0x55c7bfa08140.w1024.b0", !2735, i64 0}
!2735 = !{!"i64", !2736, i64 0}
!2736 = !{!"0x55c7bfa08140", !8, i64 0}
!2737 = !{!2738, !2738, i64 0}
!2738 = !{!"0x55c7bfa0a320.w1.b0", !2739, i64 0}
!2739 = !{!"0x55c7bfa0a320.w2.b0", !2740, i64 0}
!2740 = !{!"0x55c7bfa0a320.w4.b0", !2741, i64 0}
!2741 = !{!"0x55c7bfa0a320.w8.b0", !2742, i64 0}
!2742 = !{!"0x55c7bfa0a320.w16.b0", !2743, i64 0}
!2743 = !{!"0x55c7bfa0a320.w32.b0", !2744, i64 0}
!2744 = !{!"0x55c7bfa0a320.w64.b0", !2745, i64 0}
!2745 = !{!"0x55c7bfa0a320.w128.b0", !2746, i64 0}
!2746 = !{!"0x55c7bfa0a320.w256.b0", !2747, i64 0}
!2747 = !{!"0x55c7bfa0a320.w512.b0", !2748, i64 0}
!2748 = !{!"0x55c7bfa0a320.w1024.b0", !2749, i64 0}
!2749 = !{!"i64", !2750, i64 0}
!2750 = !{!"0x55c7bfa0a320", !8, i64 0}
!2751 = !{!2752, !2752, i64 0}
!2752 = !{!"0x55c7bfa0a320.w1.b1", !2739, i64 0}
!2753 = !{!2754, !2754, i64 0}
!2754 = !{!"0x55c7bfa0a320.w1.b2", !2755, i64 0}
!2755 = !{!"0x55c7bfa0a320.w2.b2", !2740, i64 0}
!2756 = !{!2757, !2757, i64 0}
!2757 = !{!"0x55c7bfa0a320.w1.b3", !2755, i64 0}
!2758 = !{!2759, !2759, i64 0}
!2759 = !{!"0x55c7bfa0acb0.w4.b0", !2760, i64 0}
!2760 = !{!"0x55c7bfa0acb0.w8.b0", !2761, i64 0}
!2761 = !{!"0x55c7bfa0acb0.w16.b0", !2762, i64 0}
!2762 = !{!"0x55c7bfa0acb0.w32.b0", !2763, i64 0}
!2763 = !{!"0x55c7bfa0acb0.w64.b0", !2764, i64 0}
!2764 = !{!"0x55c7bfa0acb0.w128.b0", !2765, i64 0}
!2765 = !{!"0x55c7bfa0acb0.w256.b0", !2766, i64 0}
!2766 = !{!"0x55c7bfa0acb0.w512.b0", !2767, i64 0}
!2767 = !{!"0x55c7bfa0acb0.w1024.b0", !2768, i64 0}
!2768 = !{!"i64", !2769, i64 0}
!2769 = !{!"0x55c7bfa0acb0", !8, i64 0}
!2770 = !{!2771, !2771, i64 0}
!2771 = !{!"0x55c7bfa0ce90.w1.b0", !2772, i64 0}
!2772 = !{!"0x55c7bfa0ce90.w2.b0", !2773, i64 0}
!2773 = !{!"0x55c7bfa0ce90.w4.b0", !2774, i64 0}
!2774 = !{!"0x55c7bfa0ce90.w8.b0", !2775, i64 0}
!2775 = !{!"0x55c7bfa0ce90.w16.b0", !2776, i64 0}
!2776 = !{!"0x55c7bfa0ce90.w32.b0", !2777, i64 0}
!2777 = !{!"0x55c7bfa0ce90.w64.b0", !2778, i64 0}
!2778 = !{!"0x55c7bfa0ce90.w128.b0", !2779, i64 0}
!2779 = !{!"0x55c7bfa0ce90.w256.b0", !2780, i64 0}
!2780 = !{!"0x55c7bfa0ce90.w512.b0", !2781, i64 0}
!2781 = !{!"0x55c7bfa0ce90.w1024.b0", !2782, i64 0}
!2782 = !{!"i64", !2783, i64 0}
!2783 = !{!"0x55c7bfa0ce90", !8, i64 0}
!2784 = !{!2785, !2785, i64 0}
!2785 = !{!"0x55c7bfa0d170.w1.b0", !2786, i64 0}
!2786 = !{!"0x55c7bfa0d170.w2.b0", !2787, i64 0}
!2787 = !{!"0x55c7bfa0d170.w4.b0", !2788, i64 0}
!2788 = !{!"0x55c7bfa0d170.w8.b0", !2789, i64 0}
!2789 = !{!"0x55c7bfa0d170.w16.b0", !2790, i64 0}
!2790 = !{!"0x55c7bfa0d170.w32.b0", !2791, i64 0}
!2791 = !{!"0x55c7bfa0d170.w64.b0", !2792, i64 0}
!2792 = !{!"0x55c7bfa0d170.w128.b0", !2793, i64 0}
!2793 = !{!"0x55c7bfa0d170.w256.b0", !2794, i64 0}
!2794 = !{!"0x55c7bfa0d170.w512.b0", !2795, i64 0}
!2795 = !{!"0x55c7bfa0d170.w1024.b0", !2796, i64 0}
!2796 = !{!"i64", !2797, i64 0}
!2797 = !{!"0x55c7bfa0d170", !8, i64 0}
!2798 = !{!2799, !2799, i64 0}
!2799 = !{!"0x55c7bfa0ee10.w1.b0", !2800, i64 0}
!2800 = !{!"0x55c7bfa0ee10.w2.b0", !2801, i64 0}
!2801 = !{!"0x55c7bfa0ee10.w4.b0", !2802, i64 0}
!2802 = !{!"0x55c7bfa0ee10.w8.b0", !2803, i64 0}
!2803 = !{!"0x55c7bfa0ee10.w16.b0", !2804, i64 0}
!2804 = !{!"0x55c7bfa0ee10.w32.b0", !2805, i64 0}
!2805 = !{!"0x55c7bfa0ee10.w64.b0", !2806, i64 0}
!2806 = !{!"0x55c7bfa0ee10.w128.b0", !2807, i64 0}
!2807 = !{!"0x55c7bfa0ee10.w256.b0", !2808, i64 0}
!2808 = !{!"0x55c7bfa0ee10.w512.b0", !2809, i64 0}
!2809 = !{!"0x55c7bfa0ee10.w1024.b0", !2810, i64 0}
!2810 = !{!"i64", !2811, i64 0}
!2811 = !{!"0x55c7bfa0ee10", !8, i64 0}
!2812 = !{!2813, !2813, i64 0}
!2813 = !{!"0x55c7bfa0ee10.w1.b1", !2800, i64 0}
!2814 = !{!2815, !2815, i64 0}
!2815 = !{!"0x55c7bfa0ee10.w1.b2", !2816, i64 0}
!2816 = !{!"0x55c7bfa0ee10.w2.b2", !2801, i64 0}
!2817 = !{!2818, !2818, i64 0}
!2818 = !{!"0x55c7bfa0ee10.w1.b3", !2816, i64 0}
!2819 = !{!2820, !2820, i64 0}
!2820 = !{!"0x55c7bfa0f700.w4.b0", !2821, i64 0}
!2821 = !{!"0x55c7bfa0f700.w8.b0", !2822, i64 0}
!2822 = !{!"0x55c7bfa0f700.w16.b0", !2823, i64 0}
!2823 = !{!"0x55c7bfa0f700.w32.b0", !2824, i64 0}
!2824 = !{!"0x55c7bfa0f700.w64.b0", !2825, i64 0}
!2825 = !{!"0x55c7bfa0f700.w128.b0", !2826, i64 0}
!2826 = !{!"0x55c7bfa0f700.w256.b0", !2827, i64 0}
!2827 = !{!"0x55c7bfa0f700.w512.b0", !2828, i64 0}
!2828 = !{!"0x55c7bfa0f700.w1024.b0", !2829, i64 0}
!2829 = !{!"i64", !2830, i64 0}
!2830 = !{!"0x55c7bfa0f700", !8, i64 0}
!2831 = !{!2832, !2832, i64 0}
!2832 = !{!"i8", !2833, i64 0}
!2833 = !{!"0x55c7bf4df780", !8, i64 0}
!2834 = !{!2835, !2835, i64 0}
!2835 = !{!"i8", !2836, i64 0}
!2836 = !{!"0x55c7bf4da3b0", !8, i64 0}
!2837 = !{!2838, !2838, i64 0}
!2838 = !{!"i8", !2839, i64 0}
!2839 = !{!"0x55c7bf4df700", !8, i64 0}
!2840 = !{!2841, !2841, i64 0}
!2841 = !{!"i8", !2842, i64 0}
!2842 = !{!"0x55c7bf4df740", !8, i64 0}
!2843 = !{!2844, !2844, i64 0}
!2844 = !{!"i8", !2845, i64 0}
!2845 = !{!"0x55c7bf4cae80", !8, i64 0}
!2846 = !{!2847, !2847, i64 0}
!2847 = !{!"0x55c7bfa07ee0.w1.b0", !2848, i64 0}
!2848 = !{!"0x55c7bfa07ee0.w2.b0", !2849, i64 0}
!2849 = !{!"0x55c7bfa07ee0.w4.b0", !2850, i64 0}
!2850 = !{!"0x55c7bfa07ee0.w8.b0", !2851, i64 0}
!2851 = !{!"0x55c7bfa07ee0.w16.b0", !2852, i64 0}
!2852 = !{!"0x55c7bfa07ee0.w32.b0", !2853, i64 0}
!2853 = !{!"0x55c7bfa07ee0.w64.b0", !2854, i64 0}
!2854 = !{!"0x55c7bfa07ee0.w128.b0", !2855, i64 0}
!2855 = !{!"0x55c7bfa07ee0.w256.b0", !2856, i64 0}
!2856 = !{!"0x55c7bfa07ee0.w512.b0", !2857, i64 0}
!2857 = !{!"0x55c7bfa07ee0.w1024.b0", !2858, i64 0}
!2858 = !{!"i8", !2859, i64 0}
!2859 = !{!"0x55c7bfa07ee0", !8, i64 0}
!2860 = !{!2861, !2861, i64 0}
!2861 = !{!"0x55c7bfa07ee0.w1.b1", !2848, i64 0}
!2862 = !{!2863, !2863, i64 0}
!2863 = !{!"0x55c7bfa07ee0.w1.b2", !2864, i64 0}
!2864 = !{!"0x55c7bfa07ee0.w2.b2", !2849, i64 0}
!2865 = !{!2866, !2866, i64 0}
!2866 = !{!"0x55c7bfa07ee0.w1.b3", !2864, i64 0}
!2867 = !{!2868, !2868, i64 0}
!2868 = !{!"0x55c7bfa131b0.w1.b0", !2869, i64 0}
!2869 = !{!"0x55c7bfa131b0.w2.b0", !2870, i64 0}
!2870 = !{!"0x55c7bfa131b0.w4.b0", !2871, i64 0}
!2871 = !{!"0x55c7bfa131b0.w8.b0", !2872, i64 0}
!2872 = !{!"0x55c7bfa131b0.w16.b0", !2873, i64 0}
!2873 = !{!"0x55c7bfa131b0.w32.b0", !2874, i64 0}
!2874 = !{!"0x55c7bfa131b0.w64.b0", !2875, i64 0}
!2875 = !{!"0x55c7bfa131b0.w128.b0", !2876, i64 0}
!2876 = !{!"0x55c7bfa131b0.w256.b0", !2877, i64 0}
!2877 = !{!"0x55c7bfa131b0.w512.b0", !2878, i64 0}
!2878 = !{!"0x55c7bfa131b0.w1024.b0", !2879, i64 0}
!2879 = !{!"i64", !2880, i64 0}
!2880 = !{!"0x55c7bfa131b0", !8, i64 0}
!2881 = !{!2882, !2882, i64 0}
!2882 = !{!"0x55c7bfa131b0.w1.b1", !2869, i64 0}
!2883 = !{!2884, !2884, i64 0}
!2884 = !{!"0x55c7bfa131b0.w1.b2", !2885, i64 0}
!2885 = !{!"0x55c7bfa131b0.w2.b2", !2870, i64 0}
!2886 = !{!2887, !2887, i64 0}
!2887 = !{!"0x55c7bfa131b0.w1.b3", !2885, i64 0}
!2888 = !{!2889, !2889, i64 0}
!2889 = !{!"0x55c7bfa13790.w4.b0", !2890, i64 0}
!2890 = !{!"0x55c7bfa13790.w8.b0", !2891, i64 0}
!2891 = !{!"0x55c7bfa13790.w16.b0", !2892, i64 0}
!2892 = !{!"0x55c7bfa13790.w32.b0", !2893, i64 0}
!2893 = !{!"0x55c7bfa13790.w64.b0", !2894, i64 0}
!2894 = !{!"0x55c7bfa13790.w128.b0", !2895, i64 0}
!2895 = !{!"0x55c7bfa13790.w256.b0", !2896, i64 0}
!2896 = !{!"0x55c7bfa13790.w512.b0", !2897, i64 0}
!2897 = !{!"0x55c7bfa13790.w1024.b0", !2898, i64 0}
!2898 = !{!"i64", !2899, i64 0}
!2899 = !{!"0x55c7bfa13790", !8, i64 0}
!2900 = !{!2901, !2901, i64 0}
!2901 = !{!"0x55c7bfa15970.w1.b0", !2902, i64 0}
!2902 = !{!"0x55c7bfa15970.w2.b0", !2903, i64 0}
!2903 = !{!"0x55c7bfa15970.w4.b0", !2904, i64 0}
!2904 = !{!"0x55c7bfa15970.w8.b0", !2905, i64 0}
!2905 = !{!"0x55c7bfa15970.w16.b0", !2906, i64 0}
!2906 = !{!"0x55c7bfa15970.w32.b0", !2907, i64 0}
!2907 = !{!"0x55c7bfa15970.w64.b0", !2908, i64 0}
!2908 = !{!"0x55c7bfa15970.w128.b0", !2909, i64 0}
!2909 = !{!"0x55c7bfa15970.w256.b0", !2910, i64 0}
!2910 = !{!"0x55c7bfa15970.w512.b0", !2911, i64 0}
!2911 = !{!"0x55c7bfa15970.w1024.b0", !2912, i64 0}
!2912 = !{!"i64", !2913, i64 0}
!2913 = !{!"0x55c7bfa15970", !8, i64 0}
!2914 = !{!2915, !2915, i64 0}
!2915 = !{!"0x55c7bfa15970.w1.b1", !2902, i64 0}
!2916 = !{!2917, !2917, i64 0}
!2917 = !{!"0x55c7bfa15970.w1.b2", !2918, i64 0}
!2918 = !{!"0x55c7bfa15970.w2.b2", !2903, i64 0}
!2919 = !{!2920, !2920, i64 0}
!2920 = !{!"0x55c7bfa15970.w1.b3", !2918, i64 0}
!2921 = !{!2922, !2922, i64 0}
!2922 = !{!"0x55c7bfa16300.w4.b0", !2923, i64 0}
!2923 = !{!"0x55c7bfa16300.w8.b0", !2924, i64 0}
!2924 = !{!"0x55c7bfa16300.w16.b0", !2925, i64 0}
!2925 = !{!"0x55c7bfa16300.w32.b0", !2926, i64 0}
!2926 = !{!"0x55c7bfa16300.w64.b0", !2927, i64 0}
!2927 = !{!"0x55c7bfa16300.w128.b0", !2928, i64 0}
!2928 = !{!"0x55c7bfa16300.w256.b0", !2929, i64 0}
!2929 = !{!"0x55c7bfa16300.w512.b0", !2930, i64 0}
!2930 = !{!"0x55c7bfa16300.w1024.b0", !2931, i64 0}
!2931 = !{!"i64", !2932, i64 0}
!2932 = !{!"0x55c7bfa16300", !8, i64 0}
!2933 = !{!2934, !2934, i64 0}
!2934 = !{!"0x55c7bfa184e0.w1.b0", !2935, i64 0}
!2935 = !{!"0x55c7bfa184e0.w2.b0", !2936, i64 0}
!2936 = !{!"0x55c7bfa184e0.w4.b0", !2937, i64 0}
!2937 = !{!"0x55c7bfa184e0.w8.b0", !2938, i64 0}
!2938 = !{!"0x55c7bfa184e0.w16.b0", !2939, i64 0}
!2939 = !{!"0x55c7bfa184e0.w32.b0", !2940, i64 0}
!2940 = !{!"0x55c7bfa184e0.w64.b0", !2941, i64 0}
!2941 = !{!"0x55c7bfa184e0.w128.b0", !2942, i64 0}
!2942 = !{!"0x55c7bfa184e0.w256.b0", !2943, i64 0}
!2943 = !{!"0x55c7bfa184e0.w512.b0", !2944, i64 0}
!2944 = !{!"0x55c7bfa184e0.w1024.b0", !2945, i64 0}
!2945 = !{!"i64", !2946, i64 0}
!2946 = !{!"0x55c7bfa184e0", !8, i64 0}
!2947 = !{!2948, !2948, i64 0}
!2948 = !{!"0x55c7bfa187c0.w1.b0", !2949, i64 0}
!2949 = !{!"0x55c7bfa187c0.w2.b0", !2950, i64 0}
!2950 = !{!"0x55c7bfa187c0.w4.b0", !2951, i64 0}
!2951 = !{!"0x55c7bfa187c0.w8.b0", !2952, i64 0}
!2952 = !{!"0x55c7bfa187c0.w16.b0", !2953, i64 0}
!2953 = !{!"0x55c7bfa187c0.w32.b0", !2954, i64 0}
!2954 = !{!"0x55c7bfa187c0.w64.b0", !2955, i64 0}
!2955 = !{!"0x55c7bfa187c0.w128.b0", !2956, i64 0}
!2956 = !{!"0x55c7bfa187c0.w256.b0", !2957, i64 0}
!2957 = !{!"0x55c7bfa187c0.w512.b0", !2958, i64 0}
!2958 = !{!"0x55c7bfa187c0.w1024.b0", !2959, i64 0}
!2959 = !{!"i64", !2960, i64 0}
!2960 = !{!"0x55c7bfa187c0", !8, i64 0}
!2961 = !{!2962, !2962, i64 0}
!2962 = !{!"0x55c7bfa1a460.w1.b0", !2963, i64 0}
!2963 = !{!"0x55c7bfa1a460.w2.b0", !2964, i64 0}
!2964 = !{!"0x55c7bfa1a460.w4.b0", !2965, i64 0}
!2965 = !{!"0x55c7bfa1a460.w8.b0", !2966, i64 0}
!2966 = !{!"0x55c7bfa1a460.w16.b0", !2967, i64 0}
!2967 = !{!"0x55c7bfa1a460.w32.b0", !2968, i64 0}
!2968 = !{!"0x55c7bfa1a460.w64.b0", !2969, i64 0}
!2969 = !{!"0x55c7bfa1a460.w128.b0", !2970, i64 0}
!2970 = !{!"0x55c7bfa1a460.w256.b0", !2971, i64 0}
!2971 = !{!"0x55c7bfa1a460.w512.b0", !2972, i64 0}
!2972 = !{!"0x55c7bfa1a460.w1024.b0", !2973, i64 0}
!2973 = !{!"i64", !2974, i64 0}
!2974 = !{!"0x55c7bfa1a460", !8, i64 0}
!2975 = !{!2976, !2976, i64 0}
!2976 = !{!"0x55c7bfa1a460.w1.b1", !2963, i64 0}
!2977 = !{!2978, !2978, i64 0}
!2978 = !{!"0x55c7bfa1a460.w1.b2", !2979, i64 0}
!2979 = !{!"0x55c7bfa1a460.w2.b2", !2964, i64 0}
!2980 = !{!2981, !2981, i64 0}
!2981 = !{!"0x55c7bfa1a460.w1.b3", !2979, i64 0}
!2982 = !{!2983, !2983, i64 0}
!2983 = !{!"0x55c7bfa1ad50.w4.b0", !2984, i64 0}
!2984 = !{!"0x55c7bfa1ad50.w8.b0", !2985, i64 0}
!2985 = !{!"0x55c7bfa1ad50.w16.b0", !2986, i64 0}
!2986 = !{!"0x55c7bfa1ad50.w32.b0", !2987, i64 0}
!2987 = !{!"0x55c7bfa1ad50.w64.b0", !2988, i64 0}
!2988 = !{!"0x55c7bfa1ad50.w128.b0", !2989, i64 0}
!2989 = !{!"0x55c7bfa1ad50.w256.b0", !2990, i64 0}
!2990 = !{!"0x55c7bfa1ad50.w512.b0", !2991, i64 0}
!2991 = !{!"0x55c7bfa1ad50.w1024.b0", !2992, i64 0}
!2992 = !{!"i64", !2993, i64 0}
!2993 = !{!"0x55c7bfa1ad50", !8, i64 0}
!2994 = !{!2995, !2995, i64 0}
!2995 = !{!"i8", !2996, i64 0}
!2996 = !{!"0x55c7bf4141c0", !8, i64 0}
!2997 = !{!2998, !2998, i64 0}
!2998 = !{!"i8", !2999, i64 0}
!2999 = !{!"0x55c7bf4140c0", !8, i64 0}
!3000 = distinct !{!3000, !337}
!3001 = distinct !{!3001, !337}
!3002 = distinct !{!3002, !337}
!3003 = distinct !{!3003, !337}
!3004 = distinct !{!3004, !337}
!3005 = distinct !{!3005, !337}
!3006 = distinct !{!3006, !337}
!3007 = !{!3008, !3008, i64 0}
!3008 = !{!"i8", !3009, i64 0}
!3009 = !{!"0x55c7bf414180", !8, i64 0}
!3010 = !{!3011, !3011, i64 0}
!3011 = !{!"i8", !3012, i64 0}
!3012 = !{!"0x55c7bf414140", !8, i64 0}
!3013 = !{!3014, !3014, i64 0}
!3014 = !{!"i8", !3015, i64 0}
!3015 = !{!"0x55c7bf414100", !8, i64 0}
!3016 = !{!3017, !3017, i64 0}
!3017 = !{!"0x55c7bfa13530.w1.b0", !3018, i64 0}
!3018 = !{!"0x55c7bfa13530.w2.b0", !3019, i64 0}
!3019 = !{!"0x55c7bfa13530.w4.b0", !3020, i64 0}
!3020 = !{!"0x55c7bfa13530.w8.b0", !3021, i64 0}
!3021 = !{!"0x55c7bfa13530.w16.b0", !3022, i64 0}
!3022 = !{!"0x55c7bfa13530.w32.b0", !3023, i64 0}
!3023 = !{!"0x55c7bfa13530.w64.b0", !3024, i64 0}
!3024 = !{!"0x55c7bfa13530.w128.b0", !3025, i64 0}
!3025 = !{!"0x55c7bfa13530.w256.b0", !3026, i64 0}
!3026 = !{!"0x55c7bfa13530.w512.b0", !3027, i64 0}
!3027 = !{!"0x55c7bfa13530.w1024.b0", !3028, i64 0}
!3028 = !{!"i8", !3029, i64 0}
!3029 = !{!"0x55c7bfa13530", !8, i64 0}
!3030 = !{!3031, !3031, i64 0}
!3031 = !{!"0x55c7bfa13530.w1.b1", !3018, i64 0}
!3032 = !{!3033, !3033, i64 0}
!3033 = !{!"0x55c7bfa13530.w1.b2", !3034, i64 0}
!3034 = !{!"0x55c7bfa13530.w2.b2", !3019, i64 0}
!3035 = !{!3036, !3036, i64 0}
!3036 = !{!"0x55c7bfa13530.w1.b3", !3034, i64 0}
!3037 = !{!3038, !3038, i64 0}
!3038 = !{!"0x55c7bfa1e800.w1.b0", !3039, i64 0}
!3039 = !{!"0x55c7bfa1e800.w2.b0", !3040, i64 0}
!3040 = !{!"0x55c7bfa1e800.w4.b0", !3041, i64 0}
!3041 = !{!"0x55c7bfa1e800.w8.b0", !3042, i64 0}
!3042 = !{!"0x55c7bfa1e800.w16.b0", !3043, i64 0}
!3043 = !{!"0x55c7bfa1e800.w32.b0", !3044, i64 0}
!3044 = !{!"0x55c7bfa1e800.w64.b0", !3045, i64 0}
!3045 = !{!"0x55c7bfa1e800.w128.b0", !3046, i64 0}
!3046 = !{!"0x55c7bfa1e800.w256.b0", !3047, i64 0}
!3047 = !{!"0x55c7bfa1e800.w512.b0", !3048, i64 0}
!3048 = !{!"0x55c7bfa1e800.w1024.b0", !3049, i64 0}
!3049 = !{!"i64", !3050, i64 0}
!3050 = !{!"0x55c7bfa1e800", !8, i64 0}
!3051 = !{!3052, !3052, i64 0}
!3052 = !{!"0x55c7bfa1e800.w1.b1", !3039, i64 0}
!3053 = !{!3054, !3054, i64 0}
!3054 = !{!"0x55c7bfa1e800.w1.b2", !3055, i64 0}
!3055 = !{!"0x55c7bfa1e800.w2.b2", !3040, i64 0}
!3056 = !{!3057, !3057, i64 0}
!3057 = !{!"0x55c7bfa1e800.w1.b3", !3055, i64 0}
!3058 = !{!3059, !3059, i64 0}
!3059 = !{!"0x55c7bfa1ede0.w4.b0", !3060, i64 0}
!3060 = !{!"0x55c7bfa1ede0.w8.b0", !3061, i64 0}
!3061 = !{!"0x55c7bfa1ede0.w16.b0", !3062, i64 0}
!3062 = !{!"0x55c7bfa1ede0.w32.b0", !3063, i64 0}
!3063 = !{!"0x55c7bfa1ede0.w64.b0", !3064, i64 0}
!3064 = !{!"0x55c7bfa1ede0.w128.b0", !3065, i64 0}
!3065 = !{!"0x55c7bfa1ede0.w256.b0", !3066, i64 0}
!3066 = !{!"0x55c7bfa1ede0.w512.b0", !3067, i64 0}
!3067 = !{!"0x55c7bfa1ede0.w1024.b0", !3068, i64 0}
!3068 = !{!"i64", !3069, i64 0}
!3069 = !{!"0x55c7bfa1ede0", !8, i64 0}
!3070 = !{!3071, !3071, i64 0}
!3071 = !{!"0x55c7bfa20fc0.w1.b0", !3072, i64 0}
!3072 = !{!"0x55c7bfa20fc0.w2.b0", !3073, i64 0}
!3073 = !{!"0x55c7bfa20fc0.w4.b0", !3074, i64 0}
!3074 = !{!"0x55c7bfa20fc0.w8.b0", !3075, i64 0}
!3075 = !{!"0x55c7bfa20fc0.w16.b0", !3076, i64 0}
!3076 = !{!"0x55c7bfa20fc0.w32.b0", !3077, i64 0}
!3077 = !{!"0x55c7bfa20fc0.w64.b0", !3078, i64 0}
!3078 = !{!"0x55c7bfa20fc0.w128.b0", !3079, i64 0}
!3079 = !{!"0x55c7bfa20fc0.w256.b0", !3080, i64 0}
!3080 = !{!"0x55c7bfa20fc0.w512.b0", !3081, i64 0}
!3081 = !{!"0x55c7bfa20fc0.w1024.b0", !3082, i64 0}
!3082 = !{!"i64", !3083, i64 0}
!3083 = !{!"0x55c7bfa20fc0", !8, i64 0}
!3084 = !{!3085, !3085, i64 0}
!3085 = !{!"0x55c7bfa20fc0.w1.b1", !3072, i64 0}
!3086 = !{!3087, !3087, i64 0}
!3087 = !{!"0x55c7bfa20fc0.w1.b2", !3088, i64 0}
!3088 = !{!"0x55c7bfa20fc0.w2.b2", !3073, i64 0}
!3089 = !{!3090, !3090, i64 0}
!3090 = !{!"0x55c7bfa20fc0.w1.b3", !3088, i64 0}
!3091 = !{!3092, !3092, i64 0}
!3092 = !{!"0x55c7bfa21950.w4.b0", !3093, i64 0}
!3093 = !{!"0x55c7bfa21950.w8.b0", !3094, i64 0}
!3094 = !{!"0x55c7bfa21950.w16.b0", !3095, i64 0}
!3095 = !{!"0x55c7bfa21950.w32.b0", !3096, i64 0}
!3096 = !{!"0x55c7bfa21950.w64.b0", !3097, i64 0}
!3097 = !{!"0x55c7bfa21950.w128.b0", !3098, i64 0}
!3098 = !{!"0x55c7bfa21950.w256.b0", !3099, i64 0}
!3099 = !{!"0x55c7bfa21950.w512.b0", !3100, i64 0}
!3100 = !{!"0x55c7bfa21950.w1024.b0", !3101, i64 0}
!3101 = !{!"i64", !3102, i64 0}
!3102 = !{!"0x55c7bfa21950", !8, i64 0}
!3103 = !{!3104, !3104, i64 0}
!3104 = !{!"0x55c7bfa23b30.w1.b0", !3105, i64 0}
!3105 = !{!"0x55c7bfa23b30.w2.b0", !3106, i64 0}
!3106 = !{!"0x55c7bfa23b30.w4.b0", !3107, i64 0}
!3107 = !{!"0x55c7bfa23b30.w8.b0", !3108, i64 0}
!3108 = !{!"0x55c7bfa23b30.w16.b0", !3109, i64 0}
!3109 = !{!"0x55c7bfa23b30.w32.b0", !3110, i64 0}
!3110 = !{!"0x55c7bfa23b30.w64.b0", !3111, i64 0}
!3111 = !{!"0x55c7bfa23b30.w128.b0", !3112, i64 0}
!3112 = !{!"0x55c7bfa23b30.w256.b0", !3113, i64 0}
!3113 = !{!"0x55c7bfa23b30.w512.b0", !3114, i64 0}
!3114 = !{!"0x55c7bfa23b30.w1024.b0", !3115, i64 0}
!3115 = !{!"i64", !3116, i64 0}
!3116 = !{!"0x55c7bfa23b30", !8, i64 0}
!3117 = !{!3118, !3118, i64 0}
!3118 = !{!"0x55c7bfa23e10.w1.b0", !3119, i64 0}
!3119 = !{!"0x55c7bfa23e10.w2.b0", !3120, i64 0}
!3120 = !{!"0x55c7bfa23e10.w4.b0", !3121, i64 0}
!3121 = !{!"0x55c7bfa23e10.w8.b0", !3122, i64 0}
!3122 = !{!"0x55c7bfa23e10.w16.b0", !3123, i64 0}
!3123 = !{!"0x55c7bfa23e10.w32.b0", !3124, i64 0}
!3124 = !{!"0x55c7bfa23e10.w64.b0", !3125, i64 0}
!3125 = !{!"0x55c7bfa23e10.w128.b0", !3126, i64 0}
!3126 = !{!"0x55c7bfa23e10.w256.b0", !3127, i64 0}
!3127 = !{!"0x55c7bfa23e10.w512.b0", !3128, i64 0}
!3128 = !{!"0x55c7bfa23e10.w1024.b0", !3129, i64 0}
!3129 = !{!"i64", !3130, i64 0}
!3130 = !{!"0x55c7bfa23e10", !8, i64 0}
!3131 = !{!3132, !3132, i64 0}
!3132 = !{!"0x55c7bfa25ab0.w1.b0", !3133, i64 0}
!3133 = !{!"0x55c7bfa25ab0.w2.b0", !3134, i64 0}
!3134 = !{!"0x55c7bfa25ab0.w4.b0", !3135, i64 0}
!3135 = !{!"0x55c7bfa25ab0.w8.b0", !3136, i64 0}
!3136 = !{!"0x55c7bfa25ab0.w16.b0", !3137, i64 0}
!3137 = !{!"0x55c7bfa25ab0.w32.b0", !3138, i64 0}
!3138 = !{!"0x55c7bfa25ab0.w64.b0", !3139, i64 0}
!3139 = !{!"0x55c7bfa25ab0.w128.b0", !3140, i64 0}
!3140 = !{!"0x55c7bfa25ab0.w256.b0", !3141, i64 0}
!3141 = !{!"0x55c7bfa25ab0.w512.b0", !3142, i64 0}
!3142 = !{!"0x55c7bfa25ab0.w1024.b0", !3143, i64 0}
!3143 = !{!"i64", !3144, i64 0}
!3144 = !{!"0x55c7bfa25ab0", !8, i64 0}
!3145 = !{!3146, !3146, i64 0}
!3146 = !{!"0x55c7bfa25ab0.w1.b1", !3133, i64 0}
!3147 = !{!3148, !3148, i64 0}
!3148 = !{!"0x55c7bfa25ab0.w1.b2", !3149, i64 0}
!3149 = !{!"0x55c7bfa25ab0.w2.b2", !3134, i64 0}
!3150 = !{!3151, !3151, i64 0}
!3151 = !{!"0x55c7bfa25ab0.w1.b3", !3149, i64 0}
!3152 = !{!3153, !3153, i64 0}
!3153 = !{!"0x55c7bfa263a0.w4.b0", !3154, i64 0}
!3154 = !{!"0x55c7bfa263a0.w8.b0", !3155, i64 0}
!3155 = !{!"0x55c7bfa263a0.w16.b0", !3156, i64 0}
!3156 = !{!"0x55c7bfa263a0.w32.b0", !3157, i64 0}
!3157 = !{!"0x55c7bfa263a0.w64.b0", !3158, i64 0}
!3158 = !{!"0x55c7bfa263a0.w128.b0", !3159, i64 0}
!3159 = !{!"0x55c7bfa263a0.w256.b0", !3160, i64 0}
!3160 = !{!"0x55c7bfa263a0.w512.b0", !3161, i64 0}
!3161 = !{!"0x55c7bfa263a0.w1024.b0", !3162, i64 0}
!3162 = !{!"i64", !3163, i64 0}
!3163 = !{!"0x55c7bfa263a0", !8, i64 0}
!3164 = !{!3165, !3165, i64 0}
!3165 = !{!"i8", !3166, i64 0}
!3166 = !{!"0x55c7bf43a910", !8, i64 0}
!3167 = !{!3168, !3168, i64 0}
!3168 = !{!"i8", !3169, i64 0}
!3169 = !{!"0x55c7bf43a810", !8, i64 0}
!3170 = !{!3171, !3171, i64 0}
!3171 = !{!"i8", !3172, i64 0}
!3172 = !{!"0x55c7bf43a890", !8, i64 0}
!3173 = !{!3174, !3174, i64 0}
!3174 = !{!"i8", !3175, i64 0}
!3175 = !{!"0x55c7bf43a8d0", !8, i64 0}
!3176 = !{!3177, !3177, i64 0}
!3177 = !{!"i8", !3178, i64 0}
!3178 = !{!"0x55c7bf43a850", !8, i64 0}
!3179 = !{!3180, !3180, i64 0}
!3180 = !{!"0x55c7bf9e1d90.w1.b0", !3181, i64 0}
!3181 = !{!"0x55c7bf9e1d90.w2.b0", !3182, i64 0}
!3182 = !{!"0x55c7bf9e1d90.w4.b0", !3183, i64 0}
!3183 = !{!"0x55c7bf9e1d90.w8.b0", !3184, i64 0}
!3184 = !{!"0x55c7bf9e1d90.w16.b0", !3185, i64 0}
!3185 = !{!"0x55c7bf9e1d90.w32.b0", !3186, i64 0}
!3186 = !{!"0x55c7bf9e1d90.w64.b0", !3187, i64 0}
!3187 = !{!"0x55c7bf9e1d90.w128.b0", !3188, i64 0}
!3188 = !{!"0x55c7bf9e1d90.w256.b0", !3189, i64 0}
!3189 = !{!"0x55c7bf9e1d90.w512.b0", !3190, i64 0}
!3190 = !{!"0x55c7bf9e1d90.w1024.b0", !3191, i64 0}
!3191 = !{!"i8", !3192, i64 0}
!3192 = !{!"0x55c7bf9e1d90", !8, i64 0}
!3193 = !{!3194, !3194, i64 0}
!3194 = !{!"0x55c7bf9e1d90.w1.b1", !3181, i64 0}
!3195 = !{!3196, !3196, i64 0}
!3196 = !{!"0x55c7bf9e1d90.w1.b2", !3197, i64 0}
!3197 = !{!"0x55c7bf9e1d90.w2.b2", !3182, i64 0}
!3198 = !{!3199, !3199, i64 0}
!3199 = !{!"0x55c7bf9e1d90.w1.b3", !3197, i64 0}
!3200 = !{!3201, !3201, i64 0}
!3201 = !{!"0x55c7bf9f0b60.w1.b0", !3202, i64 0}
!3202 = !{!"0x55c7bf9f0b60.w2.b0", !3203, i64 0}
!3203 = !{!"0x55c7bf9f0b60.w4.b0", !3204, i64 0}
!3204 = !{!"0x55c7bf9f0b60.w8.b0", !3205, i64 0}
!3205 = !{!"0x55c7bf9f0b60.w16.b0", !3206, i64 0}
!3206 = !{!"0x55c7bf9f0b60.w32.b0", !3207, i64 0}
!3207 = !{!"0x55c7bf9f0b60.w64.b0", !3208, i64 0}
!3208 = !{!"0x55c7bf9f0b60.w128.b0", !3209, i64 0}
!3209 = !{!"0x55c7bf9f0b60.w256.b0", !3210, i64 0}
!3210 = !{!"0x55c7bf9f0b60.w512.b0", !3211, i64 0}
!3211 = !{!"0x55c7bf9f0b60.w1024.b0", !3212, i64 0}
!3212 = !{!"i64", !3213, i64 0}
!3213 = !{!"0x55c7bf9f0b60", !8, i64 0}
!3214 = !{!3215, !3215, i64 0}
!3215 = !{!"0x55c7bf9f0b60.w1.b1", !3202, i64 0}
!3216 = !{!3217, !3217, i64 0}
!3217 = !{!"0x55c7bf9f0b60.w1.b2", !3218, i64 0}
!3218 = !{!"0x55c7bf9f0b60.w2.b2", !3203, i64 0}
!3219 = !{!3220, !3220, i64 0}
!3220 = !{!"0x55c7bf9f0b60.w1.b3", !3218, i64 0}
!3221 = !{!3222, !3222, i64 0}
!3222 = !{!"0x55c7bf9f14a0.w4.b0", !3223, i64 0}
!3223 = !{!"0x55c7bf9f14a0.w8.b0", !3224, i64 0}
!3224 = !{!"0x55c7bf9f14a0.w16.b0", !3225, i64 0}
!3225 = !{!"0x55c7bf9f14a0.w32.b0", !3226, i64 0}
!3226 = !{!"0x55c7bf9f14a0.w64.b0", !3227, i64 0}
!3227 = !{!"0x55c7bf9f14a0.w128.b0", !3228, i64 0}
!3228 = !{!"0x55c7bf9f14a0.w256.b0", !3229, i64 0}
!3229 = !{!"0x55c7bf9f14a0.w512.b0", !3230, i64 0}
!3230 = !{!"0x55c7bf9f14a0.w1024.b0", !3231, i64 0}
!3231 = !{!"i64", !3232, i64 0}
!3232 = !{!"0x55c7bf9f14a0", !8, i64 0}
!3233 = !{!3234, !3234, i64 0}
!3234 = !{!"0x55c7bf9f3680.w1.b0", !3235, i64 0}
!3235 = !{!"0x55c7bf9f3680.w2.b0", !3236, i64 0}
!3236 = !{!"0x55c7bf9f3680.w4.b0", !3237, i64 0}
!3237 = !{!"0x55c7bf9f3680.w8.b0", !3238, i64 0}
!3238 = !{!"0x55c7bf9f3680.w16.b0", !3239, i64 0}
!3239 = !{!"0x55c7bf9f3680.w32.b0", !3240, i64 0}
!3240 = !{!"0x55c7bf9f3680.w64.b0", !3241, i64 0}
!3241 = !{!"0x55c7bf9f3680.w128.b0", !3242, i64 0}
!3242 = !{!"0x55c7bf9f3680.w256.b0", !3243, i64 0}
!3243 = !{!"0x55c7bf9f3680.w512.b0", !3244, i64 0}
!3244 = !{!"0x55c7bf9f3680.w1024.b0", !3245, i64 0}
!3245 = !{!"i64", !3246, i64 0}
!3246 = !{!"0x55c7bf9f3680", !8, i64 0}
!3247 = !{!3248, !3248, i64 0}
!3248 = !{!"0x55c7bf9f3680.w1.b1", !3235, i64 0}
!3249 = !{!3250, !3250, i64 0}
!3250 = !{!"0x55c7bf9f3680.w1.b2", !3251, i64 0}
!3251 = !{!"0x55c7bf9f3680.w2.b2", !3236, i64 0}
!3252 = !{!3253, !3253, i64 0}
!3253 = !{!"0x55c7bf9f3680.w1.b3", !3251, i64 0}
!3254 = !{!3255, !3255, i64 0}
!3255 = !{!"0x55c7bf9f4010.w4.b0", !3256, i64 0}
!3256 = !{!"0x55c7bf9f4010.w8.b0", !3257, i64 0}
!3257 = !{!"0x55c7bf9f4010.w16.b0", !3258, i64 0}
!3258 = !{!"0x55c7bf9f4010.w32.b0", !3259, i64 0}
!3259 = !{!"0x55c7bf9f4010.w64.b0", !3260, i64 0}
!3260 = !{!"0x55c7bf9f4010.w128.b0", !3261, i64 0}
!3261 = !{!"0x55c7bf9f4010.w256.b0", !3262, i64 0}
!3262 = !{!"0x55c7bf9f4010.w512.b0", !3263, i64 0}
!3263 = !{!"0x55c7bf9f4010.w1024.b0", !3264, i64 0}
!3264 = !{!"i64", !3265, i64 0}
!3265 = !{!"0x55c7bf9f4010", !8, i64 0}
!3266 = !{!3267, !3267, i64 0}
!3267 = !{!"0x55c7bf9f61f0.w1.b0", !3268, i64 0}
!3268 = !{!"0x55c7bf9f61f0.w2.b0", !3269, i64 0}
!3269 = !{!"0x55c7bf9f61f0.w4.b0", !3270, i64 0}
!3270 = !{!"0x55c7bf9f61f0.w8.b0", !3271, i64 0}
!3271 = !{!"0x55c7bf9f61f0.w16.b0", !3272, i64 0}
!3272 = !{!"0x55c7bf9f61f0.w32.b0", !3273, i64 0}
!3273 = !{!"0x55c7bf9f61f0.w64.b0", !3274, i64 0}
!3274 = !{!"0x55c7bf9f61f0.w128.b0", !3275, i64 0}
!3275 = !{!"0x55c7bf9f61f0.w256.b0", !3276, i64 0}
!3276 = !{!"0x55c7bf9f61f0.w512.b0", !3277, i64 0}
!3277 = !{!"0x55c7bf9f61f0.w1024.b0", !3278, i64 0}
!3278 = !{!"i64", !3279, i64 0}
!3279 = !{!"0x55c7bf9f61f0", !8, i64 0}
!3280 = !{!3281, !3281, i64 0}
!3281 = !{!"0x55c7bf9f64d0.w1.b0", !3282, i64 0}
!3282 = !{!"0x55c7bf9f64d0.w2.b0", !3283, i64 0}
!3283 = !{!"0x55c7bf9f64d0.w4.b0", !3284, i64 0}
!3284 = !{!"0x55c7bf9f64d0.w8.b0", !3285, i64 0}
!3285 = !{!"0x55c7bf9f64d0.w16.b0", !3286, i64 0}
!3286 = !{!"0x55c7bf9f64d0.w32.b0", !3287, i64 0}
!3287 = !{!"0x55c7bf9f64d0.w64.b0", !3288, i64 0}
!3288 = !{!"0x55c7bf9f64d0.w128.b0", !3289, i64 0}
!3289 = !{!"0x55c7bf9f64d0.w256.b0", !3290, i64 0}
!3290 = !{!"0x55c7bf9f64d0.w512.b0", !3291, i64 0}
!3291 = !{!"0x55c7bf9f64d0.w1024.b0", !3292, i64 0}
!3292 = !{!"i64", !3293, i64 0}
!3293 = !{!"0x55c7bf9f64d0", !8, i64 0}
!3294 = !{!3295, !3295, i64 0}
!3295 = !{!"0x55c7bf9f8170.w1.b0", !3296, i64 0}
!3296 = !{!"0x55c7bf9f8170.w2.b0", !3297, i64 0}
!3297 = !{!"0x55c7bf9f8170.w4.b0", !3298, i64 0}
!3298 = !{!"0x55c7bf9f8170.w8.b0", !3299, i64 0}
!3299 = !{!"0x55c7bf9f8170.w16.b0", !3300, i64 0}
!3300 = !{!"0x55c7bf9f8170.w32.b0", !3301, i64 0}
!3301 = !{!"0x55c7bf9f8170.w64.b0", !3302, i64 0}
!3302 = !{!"0x55c7bf9f8170.w128.b0", !3303, i64 0}
!3303 = !{!"0x55c7bf9f8170.w256.b0", !3304, i64 0}
!3304 = !{!"0x55c7bf9f8170.w512.b0", !3305, i64 0}
!3305 = !{!"0x55c7bf9f8170.w1024.b0", !3306, i64 0}
!3306 = !{!"i64", !3307, i64 0}
!3307 = !{!"0x55c7bf9f8170", !8, i64 0}
!3308 = !{!3309, !3309, i64 0}
!3309 = !{!"0x55c7bf9f8170.w1.b1", !3296, i64 0}
!3310 = !{!3311, !3311, i64 0}
!3311 = !{!"0x55c7bf9f8170.w1.b2", !3312, i64 0}
!3312 = !{!"0x55c7bf9f8170.w2.b2", !3297, i64 0}
!3313 = !{!3314, !3314, i64 0}
!3314 = !{!"0x55c7bf9f8170.w1.b3", !3312, i64 0}
!3315 = !{!3316, !3316, i64 0}
!3316 = !{!"0x55c7bf9f8a60.w4.b0", !3317, i64 0}
!3317 = !{!"0x55c7bf9f8a60.w8.b0", !3318, i64 0}
!3318 = !{!"0x55c7bf9f8a60.w16.b0", !3319, i64 0}
!3319 = !{!"0x55c7bf9f8a60.w32.b0", !3320, i64 0}
!3320 = !{!"0x55c7bf9f8a60.w64.b0", !3321, i64 0}
!3321 = !{!"0x55c7bf9f8a60.w128.b0", !3322, i64 0}
!3322 = !{!"0x55c7bf9f8a60.w256.b0", !3323, i64 0}
!3323 = !{!"0x55c7bf9f8a60.w512.b0", !3324, i64 0}
!3324 = !{!"0x55c7bf9f8a60.w1024.b0", !3325, i64 0}
!3325 = !{!"i64", !3326, i64 0}
!3326 = !{!"0x55c7bf9f8a60", !8, i64 0}
!3327 = !{!3328, !3328, i64 0}
!3328 = !{!"i8", !3329, i64 0}
!3329 = !{!"0x55c7bf4d1c70", !8, i64 0}
!3330 = !{!3331, !3331, i64 0}
!3331 = !{!"i8", !3332, i64 0}
!3332 = !{!"0x55c7bf4d6e20", !8, i64 0}
!3333 = !{!3334, !3334, i64 0}
!3334 = !{!"i8", !3335, i64 0}
!3335 = !{!"0x55c7bf4d4360", !8, i64 0}
!3336 = distinct !{!3336, !337}
!3337 = !{!3338, !3338, i64 0}
!3338 = !{!"i8", !3339, i64 0}
!3339 = !{!"0x55c7bf4d4cf0", !8, i64 0}
!3340 = distinct !{!3340, !337}
!3341 = !{!3342, !3342, i64 0}
!3342 = !{!"i8", !3343, i64 0}
!3343 = !{!"0x55c7bf4d04e0", !8, i64 0}
!3344 = distinct !{!3344, !337}
!3345 = !{!3346, !3346, i64 0}
!3346 = !{!"0x55c7bf55bc80.w1.b0", !3347, i64 0}
!3347 = !{!"0x55c7bf55bc80.w2.b0", !3348, i64 0}
!3348 = !{!"0x55c7bf55bc80.w4.b0", !3349, i64 0}
!3349 = !{!"0x55c7bf55bc80.w8.b0", !3350, i64 0}
!3350 = !{!"0x55c7bf55bc80.w16.b0", !3351, i64 0}
!3351 = !{!"0x55c7bf55bc80.w32.b0", !3352, i64 0}
!3352 = !{!"0x55c7bf55bc80.w64.b0", !3353, i64 0}
!3353 = !{!"0x55c7bf55bc80.w128.b0", !3354, i64 0}
!3354 = !{!"0x55c7bf55bc80.w256.b0", !3355, i64 0}
!3355 = !{!"0x55c7bf55bc80.w512.b0", !3356, i64 0}
!3356 = !{!"0x55c7bf55bc80.w1024.b0", !3357, i64 0}
!3357 = !{!"i8", !3358, i64 0}
!3358 = !{!"0x55c7bf55bc80", !8, i64 0}
!3359 = !{!3360, !3360, i64 0}
!3360 = !{!"0x55c7bf55bc80.w1.b1", !3347, i64 0}
!3361 = !{!3362, !3362, i64 0}
!3362 = !{!"0x55c7bf55bc80.w1.b2", !3363, i64 0}
!3363 = !{!"0x55c7bf55bc80.w2.b2", !3348, i64 0}
!3364 = !{!3365, !3365, i64 0}
!3365 = !{!"0x55c7bf55bc80.w1.b3", !3363, i64 0}
!3366 = !{!3367, !3367, i64 0}
!3367 = !{!"0x55c7bf5671d0.w1.b0", !3368, i64 0}
!3368 = !{!"0x55c7bf5671d0.w2.b0", !3369, i64 0}
!3369 = !{!"0x55c7bf5671d0.w4.b0", !3370, i64 0}
!3370 = !{!"0x55c7bf5671d0.w8.b0", !3371, i64 0}
!3371 = !{!"0x55c7bf5671d0.w16.b0", !3372, i64 0}
!3372 = !{!"0x55c7bf5671d0.w32.b0", !3373, i64 0}
!3373 = !{!"0x55c7bf5671d0.w64.b0", !3374, i64 0}
!3374 = !{!"0x55c7bf5671d0.w128.b0", !3375, i64 0}
!3375 = !{!"0x55c7bf5671d0.w256.b0", !3376, i64 0}
!3376 = !{!"0x55c7bf5671d0.w512.b0", !3377, i64 0}
!3377 = !{!"0x55c7bf5671d0.w1024.b0", !3378, i64 0}
!3378 = !{!"i64", !3379, i64 0}
!3379 = !{!"0x55c7bf5671d0", !8, i64 0}
!3380 = !{!3381, !3381, i64 0}
!3381 = !{!"0x55c7bf5671d0.w1.b1", !3368, i64 0}
!3382 = !{!3383, !3383, i64 0}
!3383 = !{!"0x55c7bf5671d0.w1.b2", !3384, i64 0}
!3384 = !{!"0x55c7bf5671d0.w2.b2", !3369, i64 0}
!3385 = !{!3386, !3386, i64 0}
!3386 = !{!"0x55c7bf5671d0.w1.b3", !3384, i64 0}
!3387 = !{!3388, !3388, i64 0}
!3388 = !{!"0x55c7bf5677e0.w4.b0", !3389, i64 0}
!3389 = !{!"0x55c7bf5677e0.w8.b0", !3390, i64 0}
!3390 = !{!"0x55c7bf5677e0.w16.b0", !3391, i64 0}
!3391 = !{!"0x55c7bf5677e0.w32.b0", !3392, i64 0}
!3392 = !{!"0x55c7bf5677e0.w64.b0", !3393, i64 0}
!3393 = !{!"0x55c7bf5677e0.w128.b0", !3394, i64 0}
!3394 = !{!"0x55c7bf5677e0.w256.b0", !3395, i64 0}
!3395 = !{!"0x55c7bf5677e0.w512.b0", !3396, i64 0}
!3396 = !{!"0x55c7bf5677e0.w1024.b0", !3397, i64 0}
!3397 = !{!"i64", !3398, i64 0}
!3398 = !{!"0x55c7bf5677e0", !8, i64 0}
!3399 = !{!3400, !3400, i64 0}
!3400 = !{!"0x55c7bf5699c0.w1.b0", !3401, i64 0}
!3401 = !{!"0x55c7bf5699c0.w2.b0", !3402, i64 0}
!3402 = !{!"0x55c7bf5699c0.w4.b0", !3403, i64 0}
!3403 = !{!"0x55c7bf5699c0.w8.b0", !3404, i64 0}
!3404 = !{!"0x55c7bf5699c0.w16.b0", !3405, i64 0}
!3405 = !{!"0x55c7bf5699c0.w32.b0", !3406, i64 0}
!3406 = !{!"0x55c7bf5699c0.w64.b0", !3407, i64 0}
!3407 = !{!"0x55c7bf5699c0.w128.b0", !3408, i64 0}
!3408 = !{!"0x55c7bf5699c0.w256.b0", !3409, i64 0}
!3409 = !{!"0x55c7bf5699c0.w512.b0", !3410, i64 0}
!3410 = !{!"0x55c7bf5699c0.w1024.b0", !3411, i64 0}
!3411 = !{!"i64", !3412, i64 0}
!3412 = !{!"0x55c7bf5699c0", !8, i64 0}
!3413 = !{!3414, !3414, i64 0}
!3414 = !{!"0x55c7bf5699c0.w1.b1", !3401, i64 0}
!3415 = !{!3416, !3416, i64 0}
!3416 = !{!"0x55c7bf5699c0.w1.b2", !3417, i64 0}
!3417 = !{!"0x55c7bf5699c0.w2.b2", !3402, i64 0}
!3418 = !{!3419, !3419, i64 0}
!3419 = !{!"0x55c7bf5699c0.w1.b3", !3417, i64 0}
!3420 = !{!3421, !3421, i64 0}
!3421 = !{!"0x55c7bf56a350.w4.b0", !3422, i64 0}
!3422 = !{!"0x55c7bf56a350.w8.b0", !3423, i64 0}
!3423 = !{!"0x55c7bf56a350.w16.b0", !3424, i64 0}
!3424 = !{!"0x55c7bf56a350.w32.b0", !3425, i64 0}
!3425 = !{!"0x55c7bf56a350.w64.b0", !3426, i64 0}
!3426 = !{!"0x55c7bf56a350.w128.b0", !3427, i64 0}
!3427 = !{!"0x55c7bf56a350.w256.b0", !3428, i64 0}
!3428 = !{!"0x55c7bf56a350.w512.b0", !3429, i64 0}
!3429 = !{!"0x55c7bf56a350.w1024.b0", !3430, i64 0}
!3430 = !{!"i64", !3431, i64 0}
!3431 = !{!"0x55c7bf56a350", !8, i64 0}
!3432 = !{!3433, !3433, i64 0}
!3433 = !{!"0x55c7bf56c530.w1.b0", !3434, i64 0}
!3434 = !{!"0x55c7bf56c530.w2.b0", !3435, i64 0}
!3435 = !{!"0x55c7bf56c530.w4.b0", !3436, i64 0}
!3436 = !{!"0x55c7bf56c530.w8.b0", !3437, i64 0}
!3437 = !{!"0x55c7bf56c530.w16.b0", !3438, i64 0}
!3438 = !{!"0x55c7bf56c530.w32.b0", !3439, i64 0}
!3439 = !{!"0x55c7bf56c530.w64.b0", !3440, i64 0}
!3440 = !{!"0x55c7bf56c530.w128.b0", !3441, i64 0}
!3441 = !{!"0x55c7bf56c530.w256.b0", !3442, i64 0}
!3442 = !{!"0x55c7bf56c530.w512.b0", !3443, i64 0}
!3443 = !{!"0x55c7bf56c530.w1024.b0", !3444, i64 0}
!3444 = !{!"i64", !3445, i64 0}
!3445 = !{!"0x55c7bf56c530", !8, i64 0}
!3446 = !{!3447, !3447, i64 0}
!3447 = !{!"0x55c7bf56c810.w1.b0", !3448, i64 0}
!3448 = !{!"0x55c7bf56c810.w2.b0", !3449, i64 0}
!3449 = !{!"0x55c7bf56c810.w4.b0", !3450, i64 0}
!3450 = !{!"0x55c7bf56c810.w8.b0", !3451, i64 0}
!3451 = !{!"0x55c7bf56c810.w16.b0", !3452, i64 0}
!3452 = !{!"0x55c7bf56c810.w32.b0", !3453, i64 0}
!3453 = !{!"0x55c7bf56c810.w64.b0", !3454, i64 0}
!3454 = !{!"0x55c7bf56c810.w128.b0", !3455, i64 0}
!3455 = !{!"0x55c7bf56c810.w256.b0", !3456, i64 0}
!3456 = !{!"0x55c7bf56c810.w512.b0", !3457, i64 0}
!3457 = !{!"0x55c7bf56c810.w1024.b0", !3458, i64 0}
!3458 = !{!"i64", !3459, i64 0}
!3459 = !{!"0x55c7bf56c810", !8, i64 0}
!3460 = !{!3461, !3461, i64 0}
!3461 = !{!"0x55c7bf56e4b0.w1.b0", !3462, i64 0}
!3462 = !{!"0x55c7bf56e4b0.w2.b0", !3463, i64 0}
!3463 = !{!"0x55c7bf56e4b0.w4.b0", !3464, i64 0}
!3464 = !{!"0x55c7bf56e4b0.w8.b0", !3465, i64 0}
!3465 = !{!"0x55c7bf56e4b0.w16.b0", !3466, i64 0}
!3466 = !{!"0x55c7bf56e4b0.w32.b0", !3467, i64 0}
!3467 = !{!"0x55c7bf56e4b0.w64.b0", !3468, i64 0}
!3468 = !{!"0x55c7bf56e4b0.w128.b0", !3469, i64 0}
!3469 = !{!"0x55c7bf56e4b0.w256.b0", !3470, i64 0}
!3470 = !{!"0x55c7bf56e4b0.w512.b0", !3471, i64 0}
!3471 = !{!"0x55c7bf56e4b0.w1024.b0", !3472, i64 0}
!3472 = !{!"i64", !3473, i64 0}
!3473 = !{!"0x55c7bf56e4b0", !8, i64 0}
!3474 = !{!3475, !3475, i64 0}
!3475 = !{!"0x55c7bf56e4b0.w1.b1", !3462, i64 0}
!3476 = !{!3477, !3477, i64 0}
!3477 = !{!"0x55c7bf56e4b0.w1.b2", !3478, i64 0}
!3478 = !{!"0x55c7bf56e4b0.w2.b2", !3463, i64 0}
!3479 = !{!3480, !3480, i64 0}
!3480 = !{!"0x55c7bf56e4b0.w1.b3", !3478, i64 0}
!3481 = !{!3482, !3482, i64 0}
!3482 = !{!"0x55c7bf56eda0.w4.b0", !3483, i64 0}
!3483 = !{!"0x55c7bf56eda0.w8.b0", !3484, i64 0}
!3484 = !{!"0x55c7bf56eda0.w16.b0", !3485, i64 0}
!3485 = !{!"0x55c7bf56eda0.w32.b0", !3486, i64 0}
!3486 = !{!"0x55c7bf56eda0.w64.b0", !3487, i64 0}
!3487 = !{!"0x55c7bf56eda0.w128.b0", !3488, i64 0}
!3488 = !{!"0x55c7bf56eda0.w256.b0", !3489, i64 0}
!3489 = !{!"0x55c7bf56eda0.w512.b0", !3490, i64 0}
!3490 = !{!"0x55c7bf56eda0.w1024.b0", !3491, i64 0}
!3491 = !{!"i64", !3492, i64 0}
!3492 = !{!"0x55c7bf56eda0", !8, i64 0}
!3493 = !{!3494, !3494, i64 0}
!3494 = !{!"i8", !3495, i64 0}
!3495 = !{!"0x55c7bf50a430", !8, i64 0}
!3496 = !{!3497, !3497, i64 0}
!3497 = !{!"i8", !3498, i64 0}
!3498 = !{!"0x55c7bf5092d0", !8, i64 0}
!3499 = !{!3500, !3500, i64 0}
!3500 = !{!"i8", !3501, i64 0}
!3501 = !{!"0x55c7bf50a470", !8, i64 0}
!3502 = !{!3503, !3503, i64 0}
!3503 = !{!"i8", !3504, i64 0}
!3504 = !{!"0x55c7bf507e60", !8, i64 0}
!3505 = distinct !{!3505, !337}
!3506 = !{!3507, !3507, i64 0}
!3507 = !{!"i8", !3508, i64 0}
!3508 = !{!"0x55c7bf507b40", !8, i64 0}
!3509 = !{!3510, !3510, i64 0}
!3510 = !{!"0x55c7bfa35610.w1.b0", !3511, i64 0}
!3511 = !{!"0x55c7bfa35610.w2.b0", !3512, i64 0}
!3512 = !{!"0x55c7bfa35610.w4.b0", !3513, i64 0}
!3513 = !{!"0x55c7bfa35610.w8.b0", !3514, i64 0}
!3514 = !{!"0x55c7bfa35610.w16.b0", !3515, i64 0}
!3515 = !{!"0x55c7bfa35610.w32.b0", !3516, i64 0}
!3516 = !{!"0x55c7bfa35610.w64.b0", !3517, i64 0}
!3517 = !{!"0x55c7bfa35610.w128.b0", !3518, i64 0}
!3518 = !{!"0x55c7bfa35610.w256.b0", !3519, i64 0}
!3519 = !{!"0x55c7bfa35610.w512.b0", !3520, i64 0}
!3520 = !{!"0x55c7bfa35610.w1024.b0", !3521, i64 0}
!3521 = !{!"i8", !3522, i64 0}
!3522 = !{!"0x55c7bfa35610", !8, i64 0}
!3523 = !{!3524, !3524, i64 0}
!3524 = !{!"0x55c7bfa35610.w1.b1", !3511, i64 0}
!3525 = !{!3526, !3526, i64 0}
!3526 = !{!"0x55c7bfa35610.w1.b2", !3527, i64 0}
!3527 = !{!"0x55c7bfa35610.w2.b2", !3512, i64 0}
!3528 = !{!3529, !3529, i64 0}
!3529 = !{!"0x55c7bfa35610.w1.b3", !3527, i64 0}
!3530 = !{!3531, !3531, i64 0}
!3531 = !{!"0x55c7bfa3a900.w1.b0", !3532, i64 0}
!3532 = !{!"0x55c7bfa3a900.w2.b0", !3533, i64 0}
!3533 = !{!"0x55c7bfa3a900.w4.b0", !3534, i64 0}
!3534 = !{!"0x55c7bfa3a900.w8.b0", !3535, i64 0}
!3535 = !{!"0x55c7bfa3a900.w16.b0", !3536, i64 0}
!3536 = !{!"0x55c7bfa3a900.w32.b0", !3537, i64 0}
!3537 = !{!"0x55c7bfa3a900.w64.b0", !3538, i64 0}
!3538 = !{!"0x55c7bfa3a900.w128.b0", !3539, i64 0}
!3539 = !{!"0x55c7bfa3a900.w256.b0", !3540, i64 0}
!3540 = !{!"0x55c7bfa3a900.w512.b0", !3541, i64 0}
!3541 = !{!"0x55c7bfa3a900.w1024.b0", !3542, i64 0}
!3542 = !{!"i64", !3543, i64 0}
!3543 = !{!"0x55c7bfa3a900", !8, i64 0}
!3544 = !{!3545, !3545, i64 0}
!3545 = !{!"0x55c7bfa3a900.w1.b1", !3532, i64 0}
!3546 = !{!3547, !3547, i64 0}
!3547 = !{!"0x55c7bfa3a900.w1.b2", !3548, i64 0}
!3548 = !{!"0x55c7bfa3a900.w2.b2", !3533, i64 0}
!3549 = !{!3550, !3550, i64 0}
!3550 = !{!"0x55c7bfa3a900.w1.b3", !3548, i64 0}
!3551 = !{!3552, !3552, i64 0}
!3552 = !{!"0x55c7bfa3b240.w4.b0", !3553, i64 0}
!3553 = !{!"0x55c7bfa3b240.w8.b0", !3554, i64 0}
!3554 = !{!"0x55c7bfa3b240.w16.b0", !3555, i64 0}
!3555 = !{!"0x55c7bfa3b240.w32.b0", !3556, i64 0}
!3556 = !{!"0x55c7bfa3b240.w64.b0", !3557, i64 0}
!3557 = !{!"0x55c7bfa3b240.w128.b0", !3558, i64 0}
!3558 = !{!"0x55c7bfa3b240.w256.b0", !3559, i64 0}
!3559 = !{!"0x55c7bfa3b240.w512.b0", !3560, i64 0}
!3560 = !{!"0x55c7bfa3b240.w1024.b0", !3561, i64 0}
!3561 = !{!"i64", !3562, i64 0}
!3562 = !{!"0x55c7bfa3b240", !8, i64 0}
!3563 = !{!3564, !3564, i64 0}
!3564 = !{!"0x55c7bfa3d420.w1.b0", !3565, i64 0}
!3565 = !{!"0x55c7bfa3d420.w2.b0", !3566, i64 0}
!3566 = !{!"0x55c7bfa3d420.w4.b0", !3567, i64 0}
!3567 = !{!"0x55c7bfa3d420.w8.b0", !3568, i64 0}
!3568 = !{!"0x55c7bfa3d420.w16.b0", !3569, i64 0}
!3569 = !{!"0x55c7bfa3d420.w32.b0", !3570, i64 0}
!3570 = !{!"0x55c7bfa3d420.w64.b0", !3571, i64 0}
!3571 = !{!"0x55c7bfa3d420.w128.b0", !3572, i64 0}
!3572 = !{!"0x55c7bfa3d420.w256.b0", !3573, i64 0}
!3573 = !{!"0x55c7bfa3d420.w512.b0", !3574, i64 0}
!3574 = !{!"0x55c7bfa3d420.w1024.b0", !3575, i64 0}
!3575 = !{!"i64", !3576, i64 0}
!3576 = !{!"0x55c7bfa3d420", !8, i64 0}
!3577 = !{!3578, !3578, i64 0}
!3578 = !{!"0x55c7bfa3d420.w1.b1", !3565, i64 0}
!3579 = !{!3580, !3580, i64 0}
!3580 = !{!"0x55c7bfa3d420.w1.b2", !3581, i64 0}
!3581 = !{!"0x55c7bfa3d420.w2.b2", !3566, i64 0}
!3582 = !{!3583, !3583, i64 0}
!3583 = !{!"0x55c7bfa3d420.w1.b3", !3581, i64 0}
!3584 = !{!3585, !3585, i64 0}
!3585 = !{!"0x55c7bfa3ddb0.w4.b0", !3586, i64 0}
!3586 = !{!"0x55c7bfa3ddb0.w8.b0", !3587, i64 0}
!3587 = !{!"0x55c7bfa3ddb0.w16.b0", !3588, i64 0}
!3588 = !{!"0x55c7bfa3ddb0.w32.b0", !3589, i64 0}
!3589 = !{!"0x55c7bfa3ddb0.w64.b0", !3590, i64 0}
!3590 = !{!"0x55c7bfa3ddb0.w128.b0", !3591, i64 0}
!3591 = !{!"0x55c7bfa3ddb0.w256.b0", !3592, i64 0}
!3592 = !{!"0x55c7bfa3ddb0.w512.b0", !3593, i64 0}
!3593 = !{!"0x55c7bfa3ddb0.w1024.b0", !3594, i64 0}
!3594 = !{!"i64", !3595, i64 0}
!3595 = !{!"0x55c7bfa3ddb0", !8, i64 0}
!3596 = !{!3597, !3597, i64 0}
!3597 = !{!"0x55c7bfa3ff90.w1.b0", !3598, i64 0}
!3598 = !{!"0x55c7bfa3ff90.w2.b0", !3599, i64 0}
!3599 = !{!"0x55c7bfa3ff90.w4.b0", !3600, i64 0}
!3600 = !{!"0x55c7bfa3ff90.w8.b0", !3601, i64 0}
!3601 = !{!"0x55c7bfa3ff90.w16.b0", !3602, i64 0}
!3602 = !{!"0x55c7bfa3ff90.w32.b0", !3603, i64 0}
!3603 = !{!"0x55c7bfa3ff90.w64.b0", !3604, i64 0}
!3604 = !{!"0x55c7bfa3ff90.w128.b0", !3605, i64 0}
!3605 = !{!"0x55c7bfa3ff90.w256.b0", !3606, i64 0}
!3606 = !{!"0x55c7bfa3ff90.w512.b0", !3607, i64 0}
!3607 = !{!"0x55c7bfa3ff90.w1024.b0", !3608, i64 0}
!3608 = !{!"i64", !3609, i64 0}
!3609 = !{!"0x55c7bfa3ff90", !8, i64 0}
!3610 = !{!3611, !3611, i64 0}
!3611 = !{!"0x55c7bfa40270.w1.b0", !3612, i64 0}
!3612 = !{!"0x55c7bfa40270.w2.b0", !3613, i64 0}
!3613 = !{!"0x55c7bfa40270.w4.b0", !3614, i64 0}
!3614 = !{!"0x55c7bfa40270.w8.b0", !3615, i64 0}
!3615 = !{!"0x55c7bfa40270.w16.b0", !3616, i64 0}
!3616 = !{!"0x55c7bfa40270.w32.b0", !3617, i64 0}
!3617 = !{!"0x55c7bfa40270.w64.b0", !3618, i64 0}
!3618 = !{!"0x55c7bfa40270.w128.b0", !3619, i64 0}
!3619 = !{!"0x55c7bfa40270.w256.b0", !3620, i64 0}
!3620 = !{!"0x55c7bfa40270.w512.b0", !3621, i64 0}
!3621 = !{!"0x55c7bfa40270.w1024.b0", !3622, i64 0}
!3622 = !{!"i64", !3623, i64 0}
!3623 = !{!"0x55c7bfa40270", !8, i64 0}
!3624 = !{!3625, !3625, i64 0}
!3625 = !{!"0x55c7bfa41f10.w1.b0", !3626, i64 0}
!3626 = !{!"0x55c7bfa41f10.w2.b0", !3627, i64 0}
!3627 = !{!"0x55c7bfa41f10.w4.b0", !3628, i64 0}
!3628 = !{!"0x55c7bfa41f10.w8.b0", !3629, i64 0}
!3629 = !{!"0x55c7bfa41f10.w16.b0", !3630, i64 0}
!3630 = !{!"0x55c7bfa41f10.w32.b0", !3631, i64 0}
!3631 = !{!"0x55c7bfa41f10.w64.b0", !3632, i64 0}
!3632 = !{!"0x55c7bfa41f10.w128.b0", !3633, i64 0}
!3633 = !{!"0x55c7bfa41f10.w256.b0", !3634, i64 0}
!3634 = !{!"0x55c7bfa41f10.w512.b0", !3635, i64 0}
!3635 = !{!"0x55c7bfa41f10.w1024.b0", !3636, i64 0}
!3636 = !{!"i64", !3637, i64 0}
!3637 = !{!"0x55c7bfa41f10", !8, i64 0}
!3638 = !{!3639, !3639, i64 0}
!3639 = !{!"0x55c7bfa41f10.w1.b1", !3626, i64 0}
!3640 = !{!3641, !3641, i64 0}
!3641 = !{!"0x55c7bfa41f10.w1.b2", !3642, i64 0}
!3642 = !{!"0x55c7bfa41f10.w2.b2", !3627, i64 0}
!3643 = !{!3644, !3644, i64 0}
!3644 = !{!"0x55c7bfa41f10.w1.b3", !3642, i64 0}
!3645 = !{!3646, !3646, i64 0}
!3646 = !{!"0x55c7bfa42800.w4.b0", !3647, i64 0}
!3647 = !{!"0x55c7bfa42800.w8.b0", !3648, i64 0}
!3648 = !{!"0x55c7bfa42800.w16.b0", !3649, i64 0}
!3649 = !{!"0x55c7bfa42800.w32.b0", !3650, i64 0}
!3650 = !{!"0x55c7bfa42800.w64.b0", !3651, i64 0}
!3651 = !{!"0x55c7bfa42800.w128.b0", !3652, i64 0}
!3652 = !{!"0x55c7bfa42800.w256.b0", !3653, i64 0}
!3653 = !{!"0x55c7bfa42800.w512.b0", !3654, i64 0}
!3654 = !{!"0x55c7bfa42800.w1024.b0", !3655, i64 0}
!3655 = !{!"i64", !3656, i64 0}
!3656 = !{!"0x55c7bfa42800", !8, i64 0}
!3657 = !{!3658, !3658, i64 0}
!3658 = !{!"i8", !3659, i64 0}
!3659 = !{!"0x55c7bf3e0dd0", !8, i64 0}
!3660 = !{!3661, !3661, i64 0}
!3661 = !{!"i8", !3662, i64 0}
!3662 = !{!"0x55c7bf3739a0", !8, i64 0}
!3663 = !{!3664, !3664, i64 0}
!3664 = !{!"i8", !3665, i64 0}
!3665 = !{!"0x55c7bf3d50f0", !8, i64 0}
!3666 = !{!3667, !3667, i64 0}
!3667 = !{!"i8", !3668, i64 0}
!3668 = !{!"0x55c7bf37a080", !8, i64 0}
